,modelId,datasets,datasets_size,co2_eq_emissions,source,training_type,geographical_location,hardware_used,accuracy,loss,f1,rouge1,rougeL,size,auto,downloads,likes,library_name,lastModified,created_at,modelcard_text,hours_used,cloud_provider,co2_reported,license,language,domain,year_month,size_efficency,performance_score,is_adapter-transformers,is_albert,is_audio,is_audio-classification,is_automatic-speech-recognition,is_autonlp,is_autotrain_compatible,is_bart,is_bert,is_bg,is_bn,is_camembert,is_causal-lm,is_co2_eq_emissions,is_conversational,is_cs,is_de,is_deberta-v2,is_deep-narrow,is_deep-reinforcement-learning,is_distilbert,is_el,is_electra,is_encoder-decoder,is_espnet,is_exbert,is_feature-extraction,is_fill-mask,is_generated_from_keras_callback,is_generated_from_trainer,is_gpt2,is_gpt_neo,is_has_space,is_hf-asr-leaderboard,is_huggingartists,is_huggingpics,is_huggingtweets,is_image-classification,is_jax,is_keras,is_lm-head,is_lunarlander-v2,is_lyrics,is_m2m_100,is_marian,is_mbart,is_model-index,is_model_for_talk,is_mozilla-foundation/common_voice_8_0,is_mt5,is_multiberts,is_multilingual,is_nl,is_no,is_no-tag,is_onnx,is_pegasus,is_pretraining,is_pytorch,is_question-answering,is_reinforcement-learning,is_roberta,is_robust-speech-event,is_rust,is_safetensors,is_sentence-similarity,is_sentence-transformers,is_spacy,is_speech,is_stable-baselines3,is_summarization,is_sv,is_t5,is_tensorboard,is_text-classification,is_text-generation,is_text-to-speech,is_text2text-generation,is_tf,is_token-classification,is_tr,is_transformers,is_translation,is_unk,is_vision,is_vit,is_wav2vec2,is_xlm-roberta,is_xlsr-fine-tuning-week,is_zh,is_autotrain,is_bloom,is_cartpole-v1,is_custom-implementation,is_deberta,is_deep-rl-class,is_diffusers,is_frozenlake-v1-4x4-no_slippery,is_ml-agents,is_mpnet,is_nemo,is_q-learning,is_question-generation,is_reinforce,is_sample-factory,is_segformer,is_spaceinvadersnoframeskip-v4,is_stable-diffusion,is_taxi-v3,is_text-to-image,is_unispeech,is_unity-ml-agents,is_vision-encoder-decoder,is_antbulletenv-v0,is_cleanrl,is_diffusion-models-class,is_dreambooth-hackathon,is_ml-agents-huggy,is_ml-agents-pyramids,is_ml-agents-snowballtarget,is_mobilebert,is_object-detection,is_opt,is_pandareachdense-v2,is_pixelcopter-ple-v0,is_stable-diffusion-diffusers,is_timm,is_unconditional-image-generation,is_whisper,is_whisper-event,is_yolo,is_art,is_biology,is_deep-rl-course,is_fasttext,is_gpt_neox,is_gptj,is_llama,is_lora,is_ml-agents-soccertwos,is_ppo,is_pyramids,is_setfit,is_snowballtarget,is_swin
32,distilgpt2,['openwebtext'],39769491688.0,149200.0,Not Specified,Not Specified,East US,** 8 16GB V100,,,,,,352833716.0,False,1105785,169,"['jax', 'transformers', 'safetensors', 'pytorch', 'rust', 'tf']",2023-01-24 13:57:50+00:00,2019-10-03 14:08:13+00:00,"
# DistilGPT2

DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of [GPT-2](https://huggingface.co/gpt2).

## Model Details

- **Developed by:** Hugging Face
- **Model type:** Transformer-based Language Model
- **Language:** English
- **License:** Apache 2.0
- **Model Description:** DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters, was developed using [knowledge distillation](#knowledge-distillation) and was designed to be a faster, lighter version of GPT-2.
- **Resources for more information:** See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\* (a class of compressed models including Distilled-GPT2), [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure, and this page for more about [GPT-2](https://openai.com/blog/better-language-models/).

## Uses, Limitations and Risks

#### Limitations and Risks

<details>
<summary>Click to expand</summary>

**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**

As the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md), “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). 

DistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.

The impact of model compression techniques – such as knowledge distillation – on bias and fairness issues associated with language models is an active area of research. For example: 

- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.
- [Xu and Hu (2022)](https://arxiv.org/pdf/2201.08542.pdf) find that distilled versions of GPT-2 showed consistent reductions in toxicity and bias compared to the teacher model (see the paper for more detail on metrics used to define/measure toxicity and bias). 
- [Gupta et al. (2022)](https://arxiv.org/pdf/2203.12574.pdf) find that DistilGPT2 exhibits greater gender disparities than GPT-2 and propose a technique for mitigating gender bias in distilled language models like DistilGPT2. 

```python
>>> from transformers import pipeline, set_seed
>>> generator = pipeline('text-generation', model='distilgpt2')
>>> set_seed(48)
>>> generator(""The White man worked as a"", max_length=20, num_return_sequences=3)
[{'generated_text': ""The White man worked as a salesman at a McDonald's restaurant called Kia at the time of the""},
 {'generated_text': 'The White man worked as a contractor in the Army in the late 1990s. He became a ""'},
 {'generated_text': 'The White man worked as a police spokesman to the US Navy in the 1930s.'}]
 
>>> set_seed(48)
>>> generator(""The Black man worked as a"", max_length=20, num_return_sequences=3)
[{'generated_text': 'The Black man worked as a shop assistant for an hour at Wal-Mart at Wal-Mart in'},
 {'generated_text': 'The Black man worked as a waiter in the hotel when he was assaulted when he got out of a'},
 {'generated_text': 'The Black man worked as a police spokesman four months ago...'}]
```

</details>

#### Potential Uses

Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. 

The developers of GPT-2 state in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md) that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including: 

> - *Writing assistance: Grammar assistance, autocompletion (for normal prose or code)*
> - *Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.*
> - *Entertainment: Creation of games, chat bots, and amusing generations.*

Using DistilGPT2, the Hugging Face team built the [Write With Transformers](https://transformer.huggingface.co/doc/distil-gpt2) web app, which allows users to play with the model to generate text directly from their browser.

#### Out-of-scope Uses

OpenAI states in the GPT-2 [model card](https://github.com/openai/gpt-2/blob/master/model_card.md): 

> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.
>
> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.

### How to Get Started with the Model 

<details>
<summary>Click to expand</summary>

*Be sure to read the sections on in-scope and out-of-scope uses and limitations of the model for further information on how to use the model.*

Using DistilGPT2 is similar to using GPT-2. DistilGPT2 can be used directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:

```python
>>> from transformers import pipeline, set_seed
>>> generator = pipeline('text-generation', model='distilgpt2')
>>> set_seed(42)
>>> generator(""Hello, I’m a language model"", max_length=20, num_return_sequences=5)
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[{'generated_text': ""Hello, I'm a language model, I'm a language model. In my previous post I've""},
 {'generated_text': ""Hello, I'm a language model, and I'd love to hear what you think about it.""},
 {'generated_text': ""Hello, I'm a language model, but I don't get much of a connection anymore, so""},
 {'generated_text': ""Hello, I'm a language model, a functional language... It's not an example, and that""},
 {'generated_text': ""Hello, I'm a language model, not an object model.\n\nIn a nutshell, I""}]
``` 
 
Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
model = GPT2Model.from_pretrained('distilgpt2')
text = ""Replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

And in TensorFlow:

```python
from transformers import GPT2Tokenizer, TFGPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
model = TFGPT2Model.from_pretrained('distilgpt2')
text = ""Replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```

</details>

## Training Data

DistilGPT2 was trained using [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), an open-source reproduction of OpenAI’s WebText dataset, which was used to train GPT-2. See the [OpenWebTextCorpus Dataset Card](https://huggingface.co/datasets/openwebtext) for additional information about OpenWebTextCorpus and [Radford et al. (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) for additional information about WebText.

## Training Procedure

The texts were tokenized using the same tokenizer as GPT-2, a byte-level version of Byte Pair Encoding (BPE). DistilGPT2 was trained using knowledge distillation, following a procedure similar to the training procedure for DistilBERT, described in more detail in [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108). 

## Evaluation Results

The creators of DistilGPT2 [report](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) that, on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT-2 reaches a perplexity on the test set of 16.3 compared to 21.1 for DistilGPT2 (after fine-tuning on the train set).

## Environmental Impact

*Carbon emissions were estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.*

- **Hardware Type:** 8 16GB V100
- **Hours used:** 168 (1 week)
- **Cloud Provider:** Azure
- **Compute Region:** unavailable, assumed East US for calculations
- **Carbon Emitted** *(Power consumption x Time x Carbon produced based on location of power grid)*: 149.2 kg eq. CO2

## Citation

```bibtex
@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}
```

## Glossary

-	<a name=""knowledge-distillation"">**Knowledge Distillation**</a>: As described in [Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf), “knowledge distillation is a compression technique in which a compact model – the student – is trained to reproduce the behavior of a larger model – the teacher – or an ensemble of models.” Also see [Bucila et al. (2006)](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf) and [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531).

<a href=""https://huggingface.co/exbert/?model=distilgpt2"">
	<img width=""300px"" src=""https://cdn-media.huggingface.co/exbert/button.png"">
</a>
",** 168 (1 week),** Azure,1,[],[],NLP,2019-10,2364.837238605898,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,1.0,0,0,0,0,1,0.0,1,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,1.0,1,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
143,en_lg,['Eric Peter/autonlp-data-EN-LUG'],,133.0219882109991,AutoTrain,Not Specified,Not Specified,Not Specified,,1.336498737335205,,0.525404,0.501696,308192133.0,True,14,0,"['transformers', 'pytorch']",2022-06-28 08:38:46+00:00,2022-01-05 11:35:33+00:00,"
# Model Trained Using AutoNLP

- Problem type: Machine Translation
- Model ID: 474612462
- CO2 Emissions (in grams): 133.0219882109991

## Validation Metrics

- Loss: 1.336498737335205
- Rouge1: 52.5404
- Rouge2: 31.6639
- RougeL: 50.1696
- RougeLsum: 50.3398
- Gen Len: 39.046

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/EricPeter/autonlp-EN-LUG-474612462
```",,,1,[],[],NLP,2022-01,2316851.049550894,0.513276380457599,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
144,lg_en,['EricPeter/autonlp-data-MarianMT_lg_en'],,126.34446293851818,AutoTrain,Not Specified,Not Specified,Not Specified,,1.5376628637313845,,0.624613,0.58183,308319365.0,True,14,1,"['transformers', 'pytorch']",2022-06-28 08:39:16+00:00,2022-01-05 11:31:13+00:00,"
# Model Trained Using AutoNLP

- Problem type: Machine Translation
- Model ID: 475112539
- CO2 Emissions (in grams): 126.34446293851818

## Validation Metrics

- Loss: 1.5376628637313843
- Rouge1: 62.4613
- Rouge2: 39.4759
- RougeL: 58.183
- RougeLsum: 58.226
- Gen Len: 26.5644

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/EricPeter/autonlp-MarianMT_lg_en-475112539
```",,,1,[],[],NLP,2022-01,2440307.693974959,0.602462912528814,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
412,autonlp-triage-35248482,['Aimendo/autonlp-data-triage'],,7.989144645413398,AutoTrain,Not Specified,Not Specified,Not Specified,0.9728654124457308,0.1378340125083923,0.949537871674076,,,438043821.0,True,45,0,"['transformers', 'pytorch']",2021-11-23 08:03:14+00:00,2021-11-23 08:03:07+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 35248482
- CO2 Emissions (in grams): 7.989144645413398

## Validation Metrics

- Loss: 0.13783401250839233
- Accuracy: 0.9728654124457308
- Macro F1: 0.949537871674076
- Micro F1: 0.9728654124457308
- Weighted F1: 0.9732422812610365
- Macro Precision: 0.9380372699332605
- Micro Precision: 0.9728654124457308
- Weighted Precision: 0.974548513256663
- Macro Recall: 0.9689346153591594
- Micro Recall: 0.9728654124457308
- Weighted Recall: 0.9728654124457308


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Aimendo/autonlp-triage-35248482
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Aimendo/autonlp-triage-35248482"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Aimendo/autonlp-triage-35248482"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,54829877.34506507,0.96106010720014,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
413,autonlp-Test-530014983,['Ajay191191/autonlp-data-Test'],,55.10196329868386,AutoTrain,Not Specified,Not Specified,Not Specified,0.9298837645294338,0.2317161858081817,0.9296904373981704,,,433331373.0,True,43,0,"['transformers', 'pytorch']",2022-01-25 22:28:49+00:00,2022-01-25 21:51:59+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 530014983
- CO2 Emissions (in grams): 55.10196329868386

## Validation Metrics

- Loss: 0.23171618580818176
- Accuracy: 0.9298837645294338
- Precision: 0.9314414866901055
- Recall: 0.9279459594696022
- AUC: 0.979447403984557
- F1: 0.9296904373981703

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Ajay191191/autonlp-Test-530014983
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ajay191191/autonlp-Test-530014983"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ajay191191/autonlp-Test-530014983"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,7864173.017776126,0.9297870909143564,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
565,autonlp-Feedback1-479512837,['Anamika/autonlp-data-Feedback1'],,123.88023112815048,AutoTrain,Not Specified,Not Specified,Not Specified,0.7961119332705503,0.6220805048942566,0.7616345204219084,,,1123320237.0,True,13,0,"['transformers', 'pytorch']",2022-01-06 10:05:22+00:00,2022-01-06 10:05:11+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 479512837
- CO2 Emissions (in grams): 123.88023112815048

## Validation Metrics

- Loss: 0.6220805048942566
- Accuracy: 0.7961119332705503
- Macro F1: 0.7616345204219084
- Micro F1: 0.7961119332705503
- Weighted F1: 0.795387503907883
- Macro Precision: 0.782839455262034
- Micro Precision: 0.7961119332705503
- Weighted Precision: 0.7992606754484262
- Macro Recall: 0.7451485972167191
- Micro Recall: 0.7961119332705503
- Weighted Recall: 0.7961119332705503


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Anamika/autonlp-Feedback1-479512837
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Anamika/autonlp-Feedback1-479512837"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Anamika/autonlp-Feedback1-479512837"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,9067792.550677096,0.7784916846530444,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
566,autonlp-fa-473312409,['Anamika/autonlp-data-fa'],,25.12873571489861,AutoTrain,Not Specified,Not Specified,Not Specified,0.7990650945370823,0.6010786890983582,0.7429662929144928,,,328541293.0,True,14,0,"['transformers', 'pytorch']",2022-01-04 20:08:00+00:00,2022-01-04 20:07:56+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 473312409
- CO2 Emissions (in grams): 25.128735714898614

## Validation Metrics

- Loss: 0.6010786890983582
- Accuracy: 0.7990650945370823
- Macro F1: 0.7429662929144928
- Micro F1: 0.7990650945370823
- Weighted F1: 0.7977660363770382
- Macro Precision: 0.7744390888231261
- Micro Precision: 0.7990650945370823
- Weighted Precision: 0.800444194278352
- Macro Recall: 0.7198278524814119
- Micro Recall: 0.7990650945370823
- Weighted Recall: 0.7990650945370823


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Anamika/autonlp-fa-473312409
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Anamika/autonlp-fa-473312409"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Anamika/autonlp-fa-473312409"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,13074326.40971311,0.7699952619858437,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
862,nirvana,['Anorak/autonlp-data-Niravana-test2'],,4.214012748213151,AutoTrain,Not Specified,Not Specified,Not Specified,,1.0120062828063965,,0.4118079999999999,0.313106,2279631601.0,True,3,0,"['transformers', 'pytorch']",2021-10-17 15:48:15+00:00,2021-10-17 15:47:52+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 20384195
- CO2 Emissions (in grams): 4.214012748213151

## Validation Metrics

- Loss: 1.0120062828063965
- Rouge1: 41.1808
- Rouge2: 26.2564
- RougeL: 31.3106
- RougeLsum: 38.9991
- Gen Len: 58.45

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/Anorak/autonlp-Niravana-test2-20384195
```",,,1,[],[],NLP,2021-10,540964571.5871699,0.3557375237559213,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
949,autonlp-Scientific_Title_Generator-34558227,['AryanLala/autonlp-data-Scientific_Title_Generator'],,137.60574081887984,AutoTrain,Not Specified,Not Specified,Not Specified,,2.578599214553833,,0.448482,0.4017159999999999,2279631601.0,True,46,19,"['transformers', 'pytorch']",2021-11-23 16:51:34+00:00,2021-11-20 20:45:16+00:00,"
# Model Trained Using AutoNLP
- Model: Google's Pegasus (https://huggingface.co/google/pegasus-xsum)
- Problem type: Summarization
- Model ID: 34558227
- CO2 Emissions (in grams): 137.60574081887984
- Spaces: https://huggingface.co/spaces/TitleGenerators/ArxivTitleGenerator
- Dataset: arXiv Dataset (https://www.kaggle.com/Cornell-University/arxiv)
- Data subset used: https://huggingface.co/datasets/AryanLala/autonlp-data-Scientific_Title_Generator

## Validation Metrics

- Loss: 2.578599214553833
- Rouge1: 44.8482
- Rouge2: 24.4052
- RougeL: 40.1716
- RougeLsum: 40.1396
- Gen Len: 11.4675

## Social
- LinkedIn: https://www.linkedin.com/in/aryanlala/
- Twitter: https://twitter.com/AryanLala20

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/AryanLala/autonlp-Scientific_Title_Generator-34558227
```",,,1,[],[],NLP,2021-11,16566398.955698432,0.4238127944596434,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
1638,kaggle-comp-test,['Crasher222/autonlp-data-kaggle-test'],,60.744727079482495,AutoTrain,Not Specified,Not Specified,Not Specified,0.8615328555811976,0.4422711133956909,0.8642434650461513,,,1340749741.0,True,26,0,"['transformers', 'pytorch']",2021-10-24 11:40:04+00:00,2021-10-24 10:18:13+00:00,"
# Model Finetuned from BERT-base for

- Problem type: Multi-class Classification
- Model ID: 25805800

## Validation Metrics

- Loss: 0.4422711133956909
- Accuracy: 0.8615328555811976
- Macro F1: 0.8642434650461513
- Micro F1: 0.8615328555811976
- Weighted F1: 0.8617743626671308
- Macro Precision: 0.8649112225076049
- Micro Precision: 0.8615328555811976
- Weighted Precision: 0.8625407179375096
- Macro Recall: 0.8640777539828228
- Micro Recall: 0.8615328555811976
- Weighted Recall: 0.8615328555811976


## Usage

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Crasher222/kaggle-comp-test"")

tokenizer = AutoTokenizer.from_pretrained(""Crasher222/kaggle-comp-test"")

inputs = tokenizer(""I am in love with you"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,22071870.35749906,0.862886031589464,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
2245,autonlp-pos-tag-bosque,['Emanuel/autonlp-data-pos-tag-bosque'],,6.2107269129101805,AutoTrain,Not Specified,Not Specified,Not Specified,0.9714309035997062,0.0981339290738105,0.9728305785123968,,,433465905.0,True,55,2,"['transformers', 'pytorch']",2021-10-19 12:09:29+00:00,2021-10-18 16:52:48+00:00,"
# Model Trained Using AutoNLP

- Problem type: Entity Extraction
- Model ID: 21124427
- CO2 Emissions (in grams): 6.2107269129101805

## Validation Metrics

- Loss: 0.09813392907381058
- Accuracy: 0.9714309035997062
- Precision: 0.9721275936822545
- Recall: 0.9735345807918949
- F1: 0.9728305785123967

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Emanuel/autonlp-pos-tag-bosque-21124427
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Emanuel/autonlp-pos-tag-bosque"")

tokenizer = AutoTokenizer.from_pretrained(""Emanuel/autonlp-pos-tag-bosque"")

inputs = tokenizer(""A noiva casa de branco"", return_tensors=""pt"")

outputs = model(**inputs)

labelids = outputs.logits.squeeze().argmax(axis=-1)
labels = [model.config.id2label[int(x)] for x in labelids]
labels = labels[1:-1]# Filter start and end of sentence symbols

```",,,1,[],[],NLP,2021-10,69793103.2998663,0.972130237242681,0.0,0.0,0.0,0.0,0,1.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
2352,autonlp-judulberita-32517788,['Fauzan/autonlp-data-judulberita'],,0.9413042739759596,AutoTrain,Not Specified,Not Specified,Not Specified,0.8641304347826086,0.3211235105991363,0.8226950354609929,,,442323117.0,True,20,0,"['transformers', 'pytorch']",2021-11-13 15:12:57+00:00,2021-11-13 15:12:51+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 32517788
- CO2 Emissions (in grams): 0.9413042739759596

## Validation Metrics

- Loss: 0.32112351059913635
- Accuracy: 0.8641304347826086
- Precision: 0.8055555555555556
- Recall: 0.8405797101449275
- AUC: 0.9493383742911153
- F1: 0.8226950354609929

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Fauzan/autonlp-judulberita-32517788
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Fauzan/autonlp-judulberita-32517788"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Fauzan/autonlp-judulberita-32517788"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,469904502.9634028,0.8429038228640633,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
2868,autonlp-formality_scoring_2-32597818,['Harshveer/autonlp-data-formality_scoring_2'],,8.655894631203154,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5410276651382446,,,,498671021.0,True,19,0,"['transformers', 'pytorch']",2021-11-14 06:46:39+00:00,2021-11-14 06:46:33+00:00,"
# Model Trained Using AutoNLP

- Problem type: Single Column Regression
- Model ID: 32597818
- CO2 Emissions (in grams): 8.655894631203154

## Validation Metrics

- Loss: 0.5410276651382446
- MSE: 0.5410276651382446
- MAE: 0.5694561004638672
- R2: 0.6830431129198475
- RMSE: 0.735545814037323
- Explained Variance: 0.6834385395050049

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Harshveer/autonlp-formality_scoring_2-32597818
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Harshveer/autonlp-formality_scoring_2-32597818"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Harshveer/autonlp-formality_scoring_2-32597818"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,57610569.70383726,,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
4765,autonlp-vaccinfaq-22144706,['Jeska/autonlp-data-vaccinfaq'],,27.135492487925884,AutoTrain,Not Specified,Not Specified,Not Specified,0.6377269139700079,1.81697416305542,0.5181293370145044,,,437190509.0,True,21,0,"['transformers', 'pytorch']",2021-10-19 12:33:52+00:00,2021-10-19 12:33:46+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 22144706
- CO2 Emissions (in grams): 27.135492487925884

## Validation Metrics

- Loss: 1.81697416305542
- Accuracy: 0.6377269139700079
- Macro F1: 0.5181293370145044
- Micro F1: 0.6377269139700079
- Weighted F1: 0.631117826235572
- Macro Precision: 0.5371452512845428
- Micro Precision: 0.6377269139700079
- Weighted Precision: 0.6655055695465463
- Macro Recall: 0.5609328178925124
- Micro Recall: 0.6377269139700079
- Weighted Recall: 0.6377269139700079


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Jeska/autonlp-vaccinfaq-22144706
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Jeska/autonlp-vaccinfaq-22144706"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Jeska/autonlp-vaccinfaq-22144706"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,16111390.246354688,0.5717406863528977,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
4916,autonlp-bp-29016523,['Jush/autonlp-data-bp'],,3.273303707756322,AutoTrain,Not Specified,Not Specified,Not Specified,0.8333333333333334,0.6093757748603821,0.7937936978656889,,,1340749741.0,True,21,0,"['transformers', 'pytorch']",2021-11-03 09:30:13+00:00,2021-11-03 09:30:02+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 29016523
- CO2 Emissions (in grams): 3.273303707756322

## Validation Metrics

- Loss: 0.6093757748603821
- Accuracy: 0.8333333333333334
- Macro F1: 0.7937936978656889
- Micro F1: 0.8333333333333334
- Weighted F1: 0.8239843785760546
- Macro Precision: 0.8988882462566673
- Micro Precision: 0.8333333333333334
- Weighted Precision: 0.8404982541824647
- Macro Recall: 0.7805142534864643
- Micro Recall: 0.8333333333333334
- Weighted Recall: 0.8333333333333334


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Jush/autonlp-bp-29016523
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Jush/autonlp-bp-29016523"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Jush/autonlp-bp-29016523"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,409601387.6815035,0.8130831035779119,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
5515,autonlp-Gibb-Detect-515314387,['MadhurJindalWorkMail/autonlp-data-Gibb-Detect'],,70.95647633212745,AutoTrain,Not Specified,Not Specified,Not Specified,0.9760103738923708,0.0807770565152168,0.9728412857204902,,,1340745645.0,True,24,0,"['transformers', 'pytorch']",2022-01-21 07:05:45+00:00,2022-01-21 06:16:35+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 515314387
- CO2 Emissions (in grams): 70.95647633212745

## Validation Metrics

- Loss: 0.08077705651521683
- Accuracy: 0.9760103738923709
- Macro F1: 0.9728412857204902
- Micro F1: 0.9760103738923709
- Weighted F1: 0.9759907151741426
- Macro Precision: 0.9736622407675567
- Micro Precision: 0.9760103738923709
- Weighted Precision: 0.97673611876005
- Macro Recall: 0.9728978421381711
- Micro Recall: 0.9760103738923709
- Weighted Recall: 0.9760103738923709


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/MadhurJindalWorkMail/autonlp-Gibb-Detect-515314387
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""MadhurJindalWorkMail/autonlp-Gibb-Detect-515314387"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""MadhurJindalWorkMail/autonlp-Gibb-Detect-515314387"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,18895324.49052774,0.9744232531301116,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
5592,aelaectra-danish-electra-small-cased,['DAGW'],,4009.5,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,57979406.0,False,528,1,"['transformers', 'tf', 'pytorch']",2022-10-22 14:43:12+00:00,2020-12-15 07:44:17+00:00,"
# Ælæctra - A Step Towards More Efficient Danish Natural Language Processing
**Ælæctra** is a Danish Transformer-based language model created to enhance the variety of Danish NLP resources with a more efficient model compared to previous state-of-the-art (SOTA) models. Initially a cased and an uncased model are released. It was created as part of a Cognitive Science bachelor's thesis.

Ælæctra was pretrained with the ELECTRA-Small (Clark et al., 2020) pretraining approach by using the Danish Gigaword Corpus (Strømberg-Derczynski et al., 2020) and evaluated on Named Entity Recognition (NER) tasks. Since NER only presents a limited picture of Ælæctra's capabilities I am very interested in further evaluations. Therefore, if you employ it for any task, feel free to hit me up your findings!

Ælæctra was, as mentioned, created to enhance the Danish NLP capabilties and please do note how this GitHub still does not support the Danish characters ""*Æ, Ø and Å*"" as the title of this repository becomes ""*-l-ctra*"". How ironic.🙂

Here is an example on how to load both the cased and the uncased Ælæctra model in [PyTorch](https://pytorch.org/) using the [🤗Transformers](https://github.com/huggingface/transformers) library:

```python
from transformers import AutoTokenizer, AutoModelForPreTraining

tokenizer = AutoTokenizer.from_pretrained(""Maltehb/-l-ctra-danish-electra-small-cased"")
model = AutoModelForPreTraining.from_pretrained(""Maltehb/-l-ctra-danish-electra-small-cased"")
```

```python
from transformers import AutoTokenizer, AutoModelForPreTraining

tokenizer = AutoTokenizer.from_pretrained(""Maltehb/-l-ctra-danish-electra-small-uncased"")
model = AutoModelForPreTraining.from_pretrained(""Maltehb/-l-ctra-danish-electra-small-uncased"")
```

### Evaluation of current Danish Language Models 

Ælæctra, Danish BERT (DaBERT) and multilingual BERT (mBERT) were evaluated:

| Model | Layers | Hidden Size | Params | AVG NER micro-f1 (DaNE-testset) | Average Inference Time (Sec/Epoch) | Download | 
| --- | --- | --- | --- | ---  | --- | --- |
| Ælæctra Uncased | 12 | 256 | 13.7M | 78.03 (SD = 1.28) | 10.91 | [Link for model](https://www.dropbox.com/s/cag7prs1nvdchqs/%C3%86l%C3%A6ctra.zip?dl=0) | 
| Ælæctra Cased | 12 | 256 | 14.7M | 80.08 (SD = 0.26) | 10.92 | [Link for model](https://www.dropbox.com/s/cag7prs1nvdchqs/%C3%86l%C3%A6ctra.zip?dl=0) | 
| DaBERT | 12 | 768 | 110M | 84.89 (SD = 0.64) | 43.03 | [Link for model](https://www.dropbox.com/s/19cjaoqvv2jicq9/danish_bert_uncased_v2.zip?dl=1) | 
| mBERT Uncased | 12 | 768 | 167M | 80.44 (SD = 0.82) | 72.10 | [Link for model](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip) | 
| mBERT Cased | 12 | 768 | 177M | 83.79 (SD = 0.91) | 70.56 | [Link for model](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) | 


On [DaNE](https://danlp.alexandra.dk/304bd159d5de/datasets/ddt.zip) (Hvingelby et al., 2020), Ælæctra scores slightly worse than both cased and uncased Multilingual BERT (Devlin et al., 2019) and Danish BERT (Danish BERT, 2019/2020), however, Ælæctra is less than one third the size, and uses significantly fewer computational resources to pretrain and instantiate. For a full description of the evaluation and specification of the model read the thesis: 'Ælæctra - A Step Towards More Efficient Danish Natural Language Processing'. 

### Pretraining
To pretrain Ælæctra it is recommended to build a Docker Container from the [Dockerfile](https://github.com/MalteHB/-l-ctra/blob/master/infrastructure/Dockerfile). Next, simply follow the [pretraining notebooks](https://github.com/MalteHB/-l-ctra/blob/master/notebooks/pretraining/) 

The pretraining was done by utilizing a single NVIDIA Tesla V100 GPU with 16 GiB, endowed by the Danish data company [KMD](https://www.kmd.dk/). The pretraining took approximately 4 days and 9.5 hours for both the cased and uncased model

### Fine-tuning
To fine-tune any Ælæctra model follow the [fine-tuning notebooks](https://github.com/MalteHB/-l-ctra/blob/master/notebooks/fine-tuning/)

### References
Clark, K., Luong, M.-T., Le, Q. V., & Manning, C. D. (2020). ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. ArXiv:2003.10555 [Cs]. http://arxiv.org/abs/2003.10555

Danish BERT. (2020). BotXO. https://github.com/botxo/nordic_bert (Original work published 2019)

Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv:1810.04805 [Cs]. http://arxiv.org/abs/1810.04805

Hvingelby, R., Pauli, A. B., Barrett, M., Rosted, C., Lidegaard, L. M., & Søgaard, A. (2020). DaNE: A Named Entity Resource for Danish. Proceedings of the 12th Language Resources and Evaluation Conference, 4597–4604. https://www.aclweb.org/anthology/2020.lrec-1.565

Strømberg-Derczynski, L., Baglini, R., Christiansen, M. H., Ciosici, M. R., Dalsgaard, J. A., Fusaroli, R., Henrichsen, P. J., Hvingelby, R., Kirkedal, A., Kjeldsen, A. S., Ladefoged, C., Nielsen, F. Å., Petersen, M. L., Rystrøm, J. H., & Varab, D. (2020). The Danish Gigaword Project. ArXiv:2005.03521 [Cs]. http://arxiv.org/abs/2005.03521


#### Acknowledgements
As the majority of this repository is build upon [the works](https://github.com/google-research/electra) by the team at Google who created ELECTRA, a HUGE thanks to them is in order. 

A Giga thanks also goes out to the incredible people who collected The Danish Gigaword Corpus (Strømberg-Derczynski et al., 2020). 

Furthermore, I would like to thank my supervisor [Riccardo Fusaroli](https://github.com/fusaroli) for the support with the thesis, and a special thanks goes out to [Kenneth Enevoldsen](https://github.com/KennethEnevoldsen) for his continuous feedback. 

Lastly, i would like to thank KMD, my colleagues from KMD, and my peers and co-students from Cognitive Science for encouriging me to keep on working hard and holding my head up high!

#### Contact

For help or further information feel free to connect with the author Malte Højmark-Bertelsen on [hjb@kmd.dk](mailto:hjb@kmd.dk?subject=[GitHub]%20Ælæctra) or any of the following platforms:

[<img align=""left"" alt=""MalteHB | Twitter"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/twitter.svg"" />][twitter]
[<img align=""left"" alt=""MalteHB | LinkedIn"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg"" />][linkedin]
[<img align=""left"" alt=""MalteHB | Instagram"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/instagram.svg"" />][instagram]

<br />

</details>

[twitter]: https://twitter.com/malteH_B
[instagram]: https://www.instagram.com/maltemusen/
[linkedin]: https://www.linkedin.com/in/malte-h%C3%B8jmark-bertelsen-9a618017b/",,,1,[],[],NLP,2020-12,14460.507793989276,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,1.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,1.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
5594,aelaectra-danish-electra-small-uncased,['DAGW'],,4009.5,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,57979403.0,False,77,0,"['transformers', 'pytorch']",2021-11-23 06:39:20+00:00,2020-12-15 07:43:52+00:00,"
# Ælæctra - A Step Towards More Efficient Danish Natural Language Processing
**Ælæctra** is a Danish Transformer-based language model created to enhance the variety of Danish NLP resources with a more efficient model compared to previous state-of-the-art (SOTA) models. Initially a cased and an uncased model are released. It was created as part of a Cognitive Science bachelor's thesis.

Ælæctra was pretrained with the ELECTRA-Small (Clark et al., 2020) pretraining approach by using the Danish Gigaword Corpus (Strømberg-Derczynski et al., 2020) and evaluated on Named Entity Recognition (NER) tasks. Since NER only presents a limited picture of Ælæctra's capabilities I am very interested in further evaluations. Therefore, if you employ it for any task, feel free to hit me up your findings!

Ælæctra was, as mentioned, created to enhance the Danish NLP capabilties and please do note how this GitHub still does not support the Danish characters ""*Æ, Ø and Å*"" as the title of this repository becomes ""*-l-ctra*"". How ironic.🙂

Here is an example on how to load both the cased and the uncased Ælæctra model in [PyTorch](https://pytorch.org/) using the [🤗Transformers](https://github.com/huggingface/transformers) library:

```python
from transformers import AutoTokenizer, AutoModelForPreTraining

tokenizer = AutoTokenizer.from_pretrained(""Maltehb/-l-ctra-cased"")
model = AutoModelForPreTraining.from_pretrained(""Maltehb/-l-ctra-cased"")
```

```python
from transformers import AutoTokenizer, AutoModelForPreTraining

tokenizer = AutoTokenizer.from_pretrained(""Maltehb/-l-ctra-uncased"")
model = AutoModelForPreTraining.from_pretrained(""Maltehb/-l-ctra-uncased"")
```

### Evaluation of current Danish Language Models 

Ælæctra, Danish BERT (DaBERT) and multilingual BERT (mBERT) were evaluated:

| Model | Layers | Hidden Size | Params | AVG NER micro-f1 (DaNE-testset) | Average Inference Time (Sec/Epoch) | Download | 
| --- | --- | --- | --- | ---  | --- | --- |
| Ælæctra Uncased | 12 | 256 | 13.7M | 78.03 (SD = 1.28) | 10.91 | [Link for model](https://www.dropbox.com/s/cag7prs1nvdchqs/%C3%86l%C3%A6ctra.zip?dl=0) | 
| Ælæctra Cased | 12 | 256 | 14.7M | 80.08 (SD = 0.26) | 10.92 | [Link for model](https://www.dropbox.com/s/cag7prs1nvdchqs/%C3%86l%C3%A6ctra.zip?dl=0) | 
| DaBERT | 12 | 768 | 110M | 84.89 (SD = 0.64) | 43.03 | [Link for model](https://www.dropbox.com/s/19cjaoqvv2jicq9/danish_bert_uncased_v2.zip?dl=1) | 
| mBERT Uncased | 12 | 768 | 167M | 80.44 (SD = 0.82) | 72.10 | [Link for model](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip) | 
| mBERT Cased | 12 | 768 | 177M | 83.79 (SD = 0.91) | 70.56 | [Link for model](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) | 


On [DaNE](https://danlp.alexandra.dk/304bd159d5de/datasets/ddt.zip) (Hvingelby et al., 2020), Ælæctra scores slightly worse than both cased and uncased Multilingual BERT (Devlin et al., 2019) and Danish BERT (Danish BERT, 2019/2020), however, Ælæctra is less than one third the size, and uses significantly fewer computational resources to pretrain and instantiate. For a full description of the evaluation and specification of the model read the thesis: 'Ælæctra - A Step Towards More Efficient Danish Natural Language Processing'. 

### Pretraining
To pretrain Ælæctra it is recommended to build a Docker Container from the [Dockerfile](https://github.com/MalteHB/Ælæctra/tree/master/notebooks/fine-tuning/). Next, simply follow the [pretraining notebooks](https://github.com/MalteHB/Ælæctra/tree/master/infrastructure/Dockerfile/) 

The pretraining was done by utilizing a single NVIDIA Tesla V100 GPU with 16 GiB, endowed by the Danish data company [KMD](https://www.kmd.dk/). The pretraining took approximately 4 days and 9.5 hours for both the cased and uncased model

### Fine-tuning
To fine-tune any Ælæctra model follow the [fine-tuning notebooks](https://github.com/MalteHB/Ælæctra/tree/master/notebooks/fine-tuning/)

### References
Clark, K., Luong, M.-T., Le, Q. V., & Manning, C. D. (2020). ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. ArXiv:2003.10555 [Cs]. http://arxiv.org/abs/2003.10555

Danish BERT. (2020). BotXO. https://github.com/botxo/nordic_bert (Original work published 2019)

Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv:1810.04805 [Cs]. http://arxiv.org/abs/1810.04805

Hvingelby, R., Pauli, A. B., Barrett, M., Rosted, C., Lidegaard, L. M., & Søgaard, A. (2020). DaNE: A Named Entity Resource for Danish. Proceedings of the 12th Language Resources and Evaluation Conference, 4597–4604. https://www.aclweb.org/anthology/2020.lrec-1.565

Strømberg-Derczynski, L., Baglini, R., Christiansen, M. H., Ciosici, M. R., Dalsgaard, J. A., Fusaroli, R., Henrichsen, P. J., Hvingelby, R., Kirkedal, A., Kjeldsen, A. S., Ladefoged, C., Nielsen, F. Å., Petersen, M. L., Rystrøm, J. H., & Varab, D. (2020). The Danish Gigaword Project. ArXiv:2005.03521 [Cs]. http://arxiv.org/abs/2005.03521


#### Acknowledgements
As the majority of this repository is build upon [the works](https://github.com/google-research/electra) by the team at Google who created ELECTRA, a HUGE thanks to them is in order. 

A Giga thanks also goes out to the incredible people who collected The Danish Gigaword Corpus (Strømberg-Derczynski et al., 2020). 

Furthermore, I would like to thank my supervisor [Riccardo Fusaroli](https://github.com/fusaroli) for the support with the thesis, and a special thanks goes out to [Kenneth Enevoldsen](https://github.com/KennethEnevoldsen) for his continuous feedback. 

Lastly, i would like to thank KMD, my colleagues from KMD, and my peers and co-students from Cognitive Science for encouriging me to keep on working hard and holding my head up high!

#### Contact

For help or further information feel free to connect with the author Malte Højmark-Bertelsen on [hjb@kmd.dk](mailto:hjb@kmd.dk?subject=[GitHub]%20Ælæctra) or any of the following platforms:

[<img align=""left"" alt=""MalteHB | Twitter"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/twitter.svg"" />][twitter]
[<img align=""left"" alt=""MalteHB | LinkedIn"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg"" />][linkedin]
[<img align=""left"" alt=""MalteHB | Instagram"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/instagram.svg"" />][instagram]

<br />

</details>

[twitter]: https://twitter.com/malteH_B
[instagram]: https://www.instagram.com/maltemusen/
[linkedin]: https://www.linkedin.com/in/malte-h%C3%B8jmark-bertelsen-9a618017b/",,,1,[],[],NLP,2020-12,14460.507045766304,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,1.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,1.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
6018,autonlp-tweets-classification-23044997,['Monsia/autonlp-data-tweets-classification'],,4.819872182577655,AutoTrain,Not Specified,Not Specified,Not Specified,0.9997478885667463,0.001594889909029,0.9991190902836992,,,267869297.0,True,9,0,"['transformers', 'pytorch']",2021-10-20 14:38:58+00:00,2021-10-20 14:38:54+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 23044997
- CO2 Emissions (in grams): 4.819872182577655

## Validation Metrics

- Loss: 0.001594889909029007
- Accuracy: 0.9997478885667465
- Macro F1: 0.9991190902836993
- Micro F1: 0.9997478885667465
- Weighted F1: 0.9997476735518704
- Macro Precision: 0.9998014460161265
- Micro Precision: 0.9997478885667465
- Weighted Precision: 0.9997479944069787
- Macro Recall: 0.9984426545713851
- Micro Recall: 0.9997478885667465
- Weighted Recall: 0.9997478885667465


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Monsia/autonlp-tweets-classification-23044997
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Monsia/autonlp-tweets-classification-23044997"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Monsia/autonlp-tweets-classification-23044997"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,55576016.71850646,0.999433390522373,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
6520,autonlp-Summarization-AutoNLP-24135330,['Neuralearn/autonlp-data-Summarization-AutoNLP'],,155.8470724053265,AutoTrain,Not Specified,Not Specified,Not Specified,,1.369327425956726,,0.526656,0.401268,2283825905.0,True,4,0,"['transformers', 'pytorch']",2021-10-21 21:44:05+00:00,2021-10-21 21:43:47+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 24135330
- CO2 Emissions (in grams): 155.8470724053265

## Validation Metrics

- Loss: 1.369327425956726
- Rouge1: 52.6656
- Rouge2: 30.5879
- RougeL: 40.1268
- RougeLsum: 47.4438
- Gen Len: 75.4625

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/Neuralearn/autonlp-Summarization-AutoNLP-24135330
```",,,1,[],[],NLP,2021-10,14654275.308170268,0.4554903199141309,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
6981,autonlp-fake-covid-news-36769078,['Qinghui/autonlp-data-fake-covid-news'],,23.42719853096565,AutoTrain,Not Specified,Not Specified,Not Specified,0.9817757009345794,0.1595964729785919,0.9808917197452228,,,1421611309.0,True,13,0,"['transformers', 'pytorch']",2021-11-28 19:41:07+00:00,2021-11-28 19:40:55+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 36769078
- CO2 Emissions (in grams): 23.42719853096565

## Validation Metrics

- Loss: 0.15959647297859192
- Accuracy: 0.9817757009345794
- Precision: 0.980411361410382
- Recall: 0.9813725490196078
- AUC: 0.9982379201680672
- F1: 0.9808917197452229

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Qinghui/autonlp-fake-covid-news-36769078
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Qinghui/autonlp-fake-covid-news-36769078"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Qinghui/autonlp-fake-covid-news-36769078"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,60682087.40882696,0.9813335112682868,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
8143,autonlp-au_topics-452311620,['Smone55/autonlp-data-au_topics'],,208.0823957145878,AutoTrain,Not Specified,Not Specified,Not Specified,0.8767479025169796,0.5259971022605896,0.8618813750734912,,,1341249901.0,True,23,0,"['transformers', 'pytorch']",2021-12-28 01:56:22+00:00,2021-12-28 01:56:10+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 452311620
- CO2 Emissions (in grams): 208.0823957145878

## Validation Metrics

- Loss: 0.5259971022605896
- Accuracy: 0.8767479025169796
- Macro F1: 0.8618813750734912
- Micro F1: 0.8767479025169796
- Weighted F1: 0.8742964006840133
- Macro Precision: 0.8627700506991158
- Micro Precision: 0.8767479025169796
- Weighted Precision: 0.8755603985289852
- Macro Recall: 0.8662183006750934
- Micro Recall: 0.8767479025169796
- Weighted Recall: 0.8767479025169796


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Smone55/autonlp-au_topics-452311620
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Smone55/autonlp-au_topics-452311620"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Smone55/autonlp-au_topics-452311620"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,6445763.450550135,0.8692510790585288,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
8391,autonlp-pegasus-21664560,['Tarang1998/autonlp-data-pegasus'],,5.680803958729511,AutoTrain,Not Specified,Not Specified,Not Specified,,1.7488420009613037,,0.3814909999999999,0.268448,2279631601.0,True,4,0,"['transformers', 'pytorch']",2021-10-19 05:22:41+00:00,2021-10-19 05:22:20+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 21664560
- CO2 Emissions (in grams): 5.680803958729511

## Validation Metrics

- Loss: 1.7488420009613037
- Rouge1: 38.1491
- Rouge2: 18.6257
- RougeL: 26.8448
- RougeLsum: 32.2433
- Gen Len: 49.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/Tarang1998/autonlp-pegasus-21664560
```",,,1,[],[],NLP,2021-10,401286792.77815294,0.3151387929267208,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9046,autonlp-123-478412765,['XYHY/autonlp-data-123'],,69.86520391863117,AutoTrain,Not Specified,Not Specified,Not Specified,0.9539955699437724,0.186362624168396,0.9549699799866576,,,1421611309.0,True,10,0,"['transformers', 'pytorch']",2022-01-06 06:22:38+00:00,2022-01-06 06:22:26+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 478412765
- CO2 Emissions (in grams): 69.86520391863117

## Validation Metrics

- Loss: 0.186362624168396
- Accuracy: 0.9539955699437723
- Precision: 0.9527454242928453
- Recall: 0.9572049481778669
- AUC: 0.9903929997079495
- F1: 0.9549699799866577

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/XYHY/autonlp-123-478412765
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""XYHY/autonlp-123-478412765"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""XYHY/autonlp-123-478412765"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,20347916.119384497,0.9544825262768792,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9364,autonlp-bbc-news-classification-37229289,['abhishek/autonlp-data-bbc-news-classification'],,5.448567309047846,AutoTrain,Not Specified,Not Specified,Not Specified,0.9867109634551496,0.0708135440945625,0.9859067529980614,,,1340749741.0,True,1770,3,"['transformers', 'pytorch']",2021-11-30 12:56:59+00:00,2021-11-30 12:56:47+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 37229289
- CO2 Emissions (in grams): 5.448567309047846

## Validation Metrics

- Loss: 0.07081354409456253
- Accuracy: 0.9867109634551495
- Macro F1: 0.9859067529980614
- Micro F1: 0.9867109634551495
- Weighted F1: 0.9866417220968429
- Macro Precision: 0.9868771404595043
- Micro Precision: 0.9867109634551495
- Weighted Precision: 0.9869289511551576
- Macro Recall: 0.9853173241852486
- Micro Recall: 0.9867109634551495
- Weighted Recall: 0.9867109634551495


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/abhishek/autonlp-bbc-news-classification-37229289
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""abhishek/autonlp-bbc-news-classification-37229289"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""abhishek/autonlp-bbc-news-classification-37229289"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,246073814.44541612,0.9863086942935602,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9365,autonlp-bbc-roberta-37249301,['abhishek/autonlp-data-bbc-roberta'],,1.9859980179658825,AutoTrain,Not Specified,Not Specified,Not Specified,0.9833887043189368,0.064063623547554,0.9832763664701248,,,498683309.0,True,18,0,"['transformers', 'pytorch']",2021-11-30 13:35:38+00:00,2021-11-30 13:35:32+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 37249301
- CO2 Emissions (in grams): 1.9859980179658823

## Validation Metrics

- Loss: 0.06406362354755402
- Accuracy: 0.9833887043189369
- Macro F1: 0.9832763664701248
- Micro F1: 0.9833887043189369
- Weighted F1: 0.9833288528828136
- Macro Precision: 0.9847257743677181
- Micro Precision: 0.9833887043189369
- Weighted Precision: 0.9835392869652073
- Macro Recall: 0.982101705176067
- Micro Recall: 0.9833887043189369
- Weighted Recall: 0.9833887043189369


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/abhishek/autonlp-bbc-roberta-37249301
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""abhishek/autonlp-bbc-roberta-37249301"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""abhishek/autonlp-bbc-roberta-37249301"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,251099600.5478224,0.9833325321861064,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9369,autonlp-hindi-question-answering-23865268,['abhishek/autonlp-data-hindi-question-answering'],,39.76330395590446,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,2235534897.0,True,46,4,"['transformers', 'pytorch']",2021-10-21 13:51:44+00:00,2021-10-21 13:31:44+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- CO2 Emissions (in grams): 39.76330395590446


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/abhishek/autonlp-hindi-question-answering-23865268
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""abhishek/autonlp-hindi-question-answering-23865268"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""abhishek/autonlp-hindi-question-answering-23865268"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,56221054.95758345,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9370,autonlp-imdb-roberta-base-3662644,['abhishek/autonlp-data-imdb-roberta-base'],,25.894117734124272,AutoTrain,Not Specified,Not Specified,Not Specified,0.92604,0.2027743607759475,0.923522355958142,,,498674093.0,True,8,0,"['transformers', 'pytorch']",2022-02-04 14:25:35+00:00,2022-02-04 14:07:41+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 3662644
- CO2 Emissions (in grams): 25.894117734124272

## Validation Metrics

- Loss: 0.20277436077594757
- Accuracy: 0.92604
- Precision: 0.9560674830864092
- Recall: 0.89312
- AUC: 0.9814625504000001
- F1: 0.9235223559581421

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/abhishek/autonlp-imdb-roberta-base-3662644
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""abhishek/autonlp-imdb-roberta-base-3662644"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""abhishek/autonlp-imdb-roberta-base-3662644"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-02,19258199.801216938,0.9247794644570854,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9375,autonlp-prodigy-10-3362554,['abhishek/autonlp-data-prodigy-10'],,5.340540212393564,AutoTrain,Not Specified,Not Specified,Not Specified,0.9587076867229332,0.1416787207126617,0.7626816212082591,,,1336566961.0,True,6,2,"['transformers', 'pytorch']",2021-12-20 11:11:03+00:00,2021-12-20 11:10:51+00:00,"
# Model Trained Using AutoNLP

- Problem type: Entity Extraction
- Model ID: 3362554
- CO2 Emissions (in grams): 5.340540212393564

## Validation Metrics

- Loss: 0.14167872071266174
- Accuracy: 0.9587076867229332
- Precision: 0.7351351351351352
- Recall: 0.7923728813559322
- F1: 0.7626816212082591

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/abhishek/autonlp-prodigy-10-3362554
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""abhishek/autonlp-prodigy-10-3362554"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""abhishek/autonlp-prodigy-10-3362554"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,250268120.42315236,0.8495332571263927,0.0,0.0,0.0,0.0,0,1.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9376,autonlp-toxic-new-30516963,['abhishek/autonlp-data-toxic-new'],,30.68499581938628,AutoTrain,Not Specified,Not Specified,Not Specified,0.9688222161294112,0.0834036171436309,0.8338204592901879,,,267860081.0,True,9,0,"['transformers', 'pytorch']",2021-11-08 19:31:37+00:00,2021-11-08 19:31:32+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 30516963
- CO2 Emissions (in grams): 30.684995819386277

## Validation Metrics

- Loss: 0.08340361714363098
- Accuracy: 0.9688222161294113
- Precision: 0.9102096627164995
- Recall: 0.7692604006163328
- AUC: 0.9859340458715813
- F1: 0.8338204592901879

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/abhishek/autonlp-toxic-new-30516963
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""abhishek/autonlp-toxic-new-30516963"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""abhishek/autonlp-toxic-new-30516963"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,8729350.415318303,0.8962661277676977,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9532,autonlp-kpmg_nlp-18833547,['adelgasmi/autonlp-data-kpmg_nlp'],,64.58945483765274,AutoTrain,Not Specified,Not Specified,Not Specified,0.9586074193404036,0.1424772292375564,0.9468339778730884,,,540872877.0,True,31,1,"['transformers', 'pytorch']",2021-10-15 11:44:36+00:00,2021-10-15 11:44:30+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 18833547
- CO2 Emissions (in grams): 64.58945483765274

## Validation Metrics

- Loss: 0.14247722923755646
- Accuracy: 0.9586074193404036
- Macro F1: 0.9468339778730883
- Micro F1: 0.9586074193404036
- Weighted F1: 0.9585551117678807
- Macro Precision: 0.9445436604001405
- Micro Precision: 0.9586074193404036
- Weighted Precision: 0.9591405429662925
- Macro Recall: 0.9499427161888565
- Micro Recall: 0.9586074193404036
- Weighted Recall: 0.9586074193404036


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/adelgasmi/autonlp-kpmg_nlp-18833547
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""adelgasmi/autonlp-kpmg_nlp-18833547"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""adelgasmi/autonlp-kpmg_nlp-18833547"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,8374012.11017337,0.9526843254272334,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9622,autonlp-auto-nlp-lyrics-classification-19333717,['adrianmoses/autonlp-data-auto-nlp-lyrics-classification'],,88.89388195672073,AutoTrain,Not Specified,Not Specified,Not Specified,0.6207088513638894,1.049915432929993,0.4625080366154476,,,1340753837.0,True,58,1,"['transformers', 'pytorch']",2021-10-15 19:12:03+00:00,2021-10-15 19:11:52+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 19333717
- CO2 Emissions (in grams): 88.89388195672073

## Validation Metrics

- Loss: 1.0499154329299927
- Accuracy: 0.6207088513638894
- Macro F1: 0.46250803661544765
- Micro F1: 0.6207088513638894
- Weighted F1: 0.5850362079928957
- Macro Precision: 0.6451479987704787
- Micro Precision: 0.6207088513638894
- Weighted Precision: 0.6285080101186085
- Macro Recall: 0.4405680478429344
- Micro Recall: 0.6207088513638894
- Weighted Recall: 0.6207088513638894


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/adrianmoses/autonlp-auto-nlp-lyrics-classification-19333717
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""adrianmoses/autonlp-auto-nlp-lyrics-classification-19333717"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""adrianmoses/autonlp-auto-nlp-lyrics-classification-19333717"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,15082633.444366455,0.5300560494208588,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9851,autonlp-mrcooper_text_classification-529614927,['akilesh96/autonlp-data-mrcooper_text_classification'],,5.999771405025692,AutoTrain,Not Specified,Not Specified,Not Specified,0.7636103151862464,0.7582379579544067,0.770630619486531,,,438046893.0,True,31,0,"['transformers', 'pytorch']",2022-01-25 19:43:57+00:00,2022-01-25 18:22:00+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 529614927
- CO2 Emissions (in grams): 5.999771405025692

## Validation Metrics

- Loss: 0.7582379579544067
- Accuracy: 0.7636103151862464
- Macro F1: 0.770630619486531
- Micro F1: 0.7636103151862464
- Weighted F1: 0.765233270165301
- Macro Precision: 0.7746285216467107
- Micro Precision: 0.7636103151862464
- Weighted Precision: 0.7683270753840836
- Macro Recall: 0.7680576576961138
- Micro Recall: 0.7636103151862464
- Weighted Recall: 0.7636103151862464


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/akilesh96/autonlp-mrcooper_text_classification-529614927
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""akilesh96/autonlp-mrcooper_text_classification-529614927"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""akilesh96/autonlp-mrcooper_text_classification-529614927"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,73010597.14259633,0.7671044057546141,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
9902,autonlp-group-classification-441411446,['alecmullen/autonlp-data-group-classification'],,0.4362732160754736,AutoTrain,Not Specified,Not Specified,Not Specified,0.8222222222222222,0.7598486542701721,0.2912091747693842,,,328565869.0,True,10,0,"['transformers', 'pytorch']",2021-12-22 23:03:27+00:00,2021-12-22 23:03:22+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 441411446
- CO2 Emissions (in grams): 0.4362732160754736

## Validation Metrics

- Loss: 0.7598486542701721
- Accuracy: 0.8222222222222222
- Macro F1: 0.2912091747693842
- Micro F1: 0.8222222222222222
- Weighted F1: 0.7707160863181806
- Macro Precision: 0.29631463146314635
- Micro Precision: 0.8222222222222222
- Weighted Precision: 0.7341339689524508
- Macro Recall: 0.30174603174603176
- Micro Recall: 0.8222222222222222
- Weighted Recall: 0.8222222222222222


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/alecmullen/autonlp-group-classification-441411446
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alecmullen/autonlp-group-classification-441411446"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""alecmullen/autonlp-group-classification-441411446"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,753119506.0646569,0.4300914370787903,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
10315,autonlp-user-review-classification-536415182,['alperiox/autonlp-data-user-review-classification'],,1.268309634217171,AutoTrain,Not Specified,Not Specified,Not Specified,0.8873239436619719,0.4473306238651275,0.8859416445623343,,,438025389.0,True,69,0,"['transformers', 'pytorch']",2022-01-28 16:30:08+00:00,2022-01-28 16:28:57+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 536415182
- CO2 Emissions (in grams): 1.268309634217171

## Validation Metrics

- Loss: 0.44733062386512756
- Accuracy: 0.8873239436619719
- Macro F1: 0.8859416445623343
- Micro F1: 0.8873239436619719
- Weighted F1: 0.8864646766540891
- Macro Precision: 0.8848522167487685
- Micro Precision: 0.8873239436619719
- Weighted Precision: 0.8883299798792756
- Macro Recall: 0.8908045977011494
- Micro Recall: 0.8873239436619719
- Weighted Recall: 0.8873239436619719


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/alperiox/autonlp-user-review-classification-536415182
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alperiox/autonlp-user-review-classification-536415182"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""alperiox/autonlp-user-review-classification-536415182"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,345361556.1868369,0.8866322553460445,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
10333,autonlp-alberti-stanza-names-34318169,['alvp/autonlp-data-alberti-stanza-names'],,8.612473981829835,AutoTrain,Not Specified,Not Specified,Not Specified,0.6083916083916084,1.3520570993423462,0.5420169617715481,,,711639341.0,True,17,0,"['transformers', 'pytorch']",2021-11-19 13:41:53+00:00,2021-11-19 13:41:45+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 34318169
- CO2 Emissions (in grams): 8.612473981829835

## Validation Metrics

- Loss: 1.3520570993423462
- Accuracy: 0.6083916083916084
- Macro F1: 0.5420169617715481
- Micro F1: 0.6083916083916084
- Weighted F1: 0.5963328136975058
- Macro Precision: 0.5864033493660455
- Micro Precision: 0.6083916083916084
- Weighted Precision: 0.6364793882921277
- Macro Recall: 0.5545405576555766
- Micro Recall: 0.6083916083916084
- Weighted Recall: 0.6083916083916084


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/alvp/autonlp-alberti-stanza-names-34318169
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alvp/autonlp-alberti-stanza-names-34318169"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""alvp/autonlp-alberti-stanza-names-34318169"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,82628910.40383762,0.5732894898391745,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
10340,autonlp-text-hateful-memes-36789092,['am4nsolanki/autonlp-data-text-hateful-memes'],,1.4280361775467445,AutoTrain,Not Specified,Not Specified,Not Specified,0.7666078777189889,0.5255328416824341,0.6532751091703057,,,263172209.0,True,10,2,"['transformers', 'pytorch']",2021-11-28 22:35:30+00:00,2021-11-28 22:35:26+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 36789092
- CO2 Emissions (in grams): 1.4280361775467445

## Validation Metrics

- Loss: 0.5255328416824341
- Accuracy: 0.7666078777189889
- Precision: 0.6913123844731978
- Recall: 0.6192052980132451
- AUC: 0.7893359070795125
- F1: 0.6532751091703057

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/am4nsolanki/autonlp-text-hateful-memes-36789092
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""am4nsolanki/autonlp-text-hateful-memes-36789092"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""am4nsolanki/autonlp-text-hateful-memes-36789092"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,184289595.1362447,0.7054184741023813,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
10355,autonlp-Tweet-Sentiment-Extraction-20114061,['amansolanki/autonlp-data-Tweet-Sentiment-Extraction'],,3.651199395353127,AutoTrain,Not Specified,Not Specified,Not Specified,0.8036219581211093,0.5046541690826416,0.807095210403678,,,267863153.0,True,6862,0,"['transformers', 'pytorch']",2021-10-17 00:32:35+00:00,2021-10-17 00:32:30+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 20114061
- CO2 Emissions (in grams): 3.651199395353127

## Validation Metrics

- Loss: 0.5046541690826416
- Accuracy: 0.8036219581211093
- Macro F1: 0.807095210403678
- Micro F1: 0.8036219581211093
- Weighted F1: 0.8039634739225368
- Macro Precision: 0.8076842795233988
- Micro Precision: 0.8036219581211093
- Weighted Precision: 0.8052135235094771
- Macro Recall: 0.8075241470527056
- Micro Recall: 0.8036219581211093
- Weighted Recall: 0.8036219581211093


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/amansolanki/autonlp-Tweet-Sentiment-Extraction-20114061
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""amansolanki/autonlp-Tweet-Sentiment-Extraction-20114061"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""amansolanki/autonlp-Tweet-Sentiment-Extraction-20114061"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,73363057.99702662,0.8053548395076799,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
10599,autonlp-dialogue-summariztion-583416409,['anegi/autonlp-data-dialogue-summariztion'],,72.26141764997115,AutoTrain,Not Specified,Not Specified,Not Specified,,1.470183491706848,,0.477785,0.402231,1625557313.0,True,3,1,"['transformers', 'pytorch']",2022-02-20 06:52:08+00:00,2022-02-20 05:46:50+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 583416409
- CO2 Emissions (in grams): 72.26141764997115

## Validation Metrics

- Loss: 1.4701834917068481
- Rouge1: 47.7785
- Rouge2: 24.8518
- RougeL: 40.2231
- RougeLsum: 43.9487
- Gen Len: 18.8029

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/anegi/autonlp-dialogue-summariztion-583416409
```",,,1,[],[],NLP,2022-02,22495508.19600685,0.4367646459496191,0.0,0.0,0.0,0.0,0,1.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
10600,autonlp-cml-412010597,['anel/autonlp-data-cml'],,10.411685187181709,AutoTrain,Not Specified,Not Specified,Not Specified,0.9475446428571428,0.1258578151464462,0.9548511047070124,,,328525933.0,True,9,0,"['transformers', 'pytorch']",2021-12-13 03:11:37+00:00,2021-12-13 03:11:32+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 412010597
- CO2 Emissions (in grams): 10.411685187181709

## Validation Metrics

- Loss: 0.12585781514644623
- Accuracy: 0.9475446428571429
- Precision: 0.9454660748256183
- Recall: 0.964424320827943
- AUC: 0.990229573862156
- F1: 0.9548511047070125

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/anel/autonlp-cml-412010597
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""anel/autonlp-cml-412010597"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""anel/autonlp-cml-412010597"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,31553579.18470902,0.9511838429515233,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
10601,autonlp-covid-432211280,['anelnurkayeva/autonlp-data-covid'],,8.898145050355591,AutoTrain,Not Specified,Not Specified,Not Specified,0.9520089285714286,0.1248933672904968,0.958956411072224,,,328525933.0,True,14,0,"['transformers', 'pytorch']",2021-12-20 01:23:47+00:00,2021-12-20 01:23:41+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 432211280
- CO2 Emissions (in grams): 8.898145050355591

## Validation Metrics

- Loss: 0.12489336729049683
- Accuracy: 0.9520089285714286
- Precision: 0.9436443331246086
- Recall: 0.9747736093143596
- AUC: 0.9910066767410616
- F1: 0.958956411072224

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/anelnurkayeva/autonlp-covid-432211280
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""anelnurkayeva/autonlp-covid-432211280"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""anelnurkayeva/autonlp-covid-432211280"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,36920721.244802736,0.9554700407301056,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
11164,autonlp-antisemitism-2-21194454,['astarostap/autonlp-data-antisemitism-2'],,2.0686690092905224,AutoTrain,Not Specified,Not Specified,Not Specified,0.7572692793931732,0.5291365385055542,0.7692307692307693,,,438019245.0,True,27,0,"['transformers', 'pytorch']",2021-10-18 18:06:19+00:00,2021-10-18 17:29:18+00:00,"
# Description

This model takes a tweet with the word ""jew"" in it, and determines if it's antisemitic.

Training data:

This model was trained on 4k tweets, where ~50% were labeled as antisemitic.

I labeled them myself based on personal experience and knowledge about common antisemitic tropes.

Note:

The goal for this model is not to be used as a final say on what is or is not antisemitic, but rather as a first pass on what might be antisemitic and should be reviewed by human experts.

Please keep in mind that I'm not an expert on antisemitism or hatespeech.

Whether something is antisemitic or not depends on the context, as for any hate speech, and everyone has a different definition for what is hate speech.

If you would like to collaborate on antisemitism detection, please feel free to contact me at starosta@alumni.stanford.edu

This model is not ready for production, it needs more evaluation and more training data.

# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 21194454
- CO2 Emissions (in grams): 2.0686690092905224
- Dataset: https://huggingface.co/datasets/astarostap/autonlp-data-antisemitism-2

## Validation Metrics

- Loss: 0.5291365385055542
- Accuracy: 0.7572692793931732
- Precision: 0.7126948775055679
- Recall: 0.835509138381201
- AUC: 0.8185826549941126
- F1: 0.7692307692307693

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/astarostap/autonlp-antisemitism-2-21194454
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""astarostap/autonlp-antisemitism-2-21194454"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""astarostap/autonlp-antisemitism-2-21194454"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,211739646.6195549,0.76320315983946,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
11563,tweet-disaster-classifier,['bgoel4132/autonlp-data-tweet-disaster-classifier'],,27.22397099134103,AutoTrain,Not Specified,Not Specified,Not Specified,0.8066924731182795,0.4146720767021179,0.7835463282531184,,,267893873.0,True,7,0,"['transformers', 'pytorch']",2021-11-02 09:55:27+00:00,2021-11-02 09:55:22+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 28716412
- CO2 Emissions (in grams): 27.22397099134103

## Validation Metrics

- Loss: 0.4146720767021179
- Accuracy: 0.8066924731182795
- Macro F1: 0.7835463282531184
- Micro F1: 0.8066924731182795
- Weighted F1: 0.7974252447208724
- Macro Precision: 0.8183917344767431
- Micro Precision: 0.8066924731182795
- Weighted Precision: 0.8005510296861892
- Macro Recall: 0.7679676081852519
- Micro Recall: 0.8066924731182795
- Weighted Recall: 0.8066924731182795


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/bgoel4132/autonlp-tweet-disaster-classifier-28716412
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""bgoel4132/autonlp-tweet-disaster-classifier-28716412"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""bgoel4132/autonlp-tweet-disaster-classifier-28716412"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,9840367.266230464,0.7949509530218459,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
11564,twitter-sentiment,['bgoel4132/autonlp-data-twitter-sentiment'],,186.8637425115097,AutoTrain,Not Specified,Not Specified,Not Specified,0.9233253193796256,0.2020547091960907,0.9240407542958708,,,438046893.0,True,29,0,"['transformers', 'pytorch']",2021-11-24 19:39:02+00:00,2021-11-24 19:38:56+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 35868888
- CO2 Emissions (in grams): 186.8637425115097

## Validation Metrics

- Loss: 0.2020547091960907
- Accuracy: 0.9233253193796257
- Macro F1: 0.9240407542958707
- Micro F1: 0.9233253193796257
- Weighted F1: 0.921800586774046
- Macro Precision: 0.9432284179846658
- Micro Precision: 0.9233253193796257
- Weighted Precision: 0.9247263361914827
- Macro Recall: 0.9139437626409382
- Micro Recall: 0.9233253193796257
- Weighted Recall: 0.9233253193796257


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/bgoel4132/autonlp-twitter-sentiment-35868888
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""bgoel4132/autonlp-twitter-sentiment-35868888"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""bgoel4132/autonlp-twitter-sentiment-35868888"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,2344204.85810948,0.9236828983034516,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
11754,autonlp-ks-530615016,['bitmorse/autonlp-data-ks'],,2.2247356264808964,AutoTrain,Not Specified,Not Specified,Not Specified,0.676854818831649,0.7859578132629395,0.3297126297995653,,,267866225.0,True,7,0,"['transformers', 'pytorch']",2022-01-26 11:40:24+00:00,2022-01-26 11:38:30+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 530615016
- CO2 Emissions (in grams): 2.2247356264808964

## Validation Metrics

- Loss: 0.7859578132629395
- Accuracy: 0.676854818831649
- Macro F1: 0.3297126297995653
- Micro F1: 0.676854818831649
- Weighted F1: 0.6429522696884535
- Macro Precision: 0.33152557743856437
- Micro Precision: 0.676854818831649
- Weighted Precision: 0.6276125515413322
- Macro Recall: 0.33784302289888885
- Micro Recall: 0.676854818831649
- Weighted Recall: 0.676854818831649


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/bitmorse/autonlp-ks-530615016
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""bitmorse/autonlp-ks-530615016"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""bitmorse/autonlp-ks-530615016"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,120403620.9118981,0.4434230067999255,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
11886,sent-sci-irrelevance,['bozelosp/autonlp-data-sci-relevance'],,3.667033499762825,AutoTrain,Not Specified,Not Specified,Not Specified,0.9133333333333332,0.3265331089496612,0.9221556886227544,,,1334486957.0,True,22,1,"['transformers', 'pytorch']",2021-11-27 14:16:04+00:00,2021-11-27 14:15:52+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 33199029
- CO2 Emissions (in grams): 3.667033499762825

## Validation Metrics

- Loss: 0.32653310894966125
- Accuracy: 0.9133333333333333
- Precision: 0.9005847953216374
- Recall: 0.9447852760736196
- AUC: 0.9532488468944517
- F1: 0.9221556886227544

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/bozelosp/autonlp-sci-relevance-33199029
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""bozelosp/autonlp-sci-relevance-33199029"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""bozelosp/autonlp-sci-relevance-33199029"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,363914580.29666525,0.9177233084669087,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
12480,headline_writer,['chinhon/autonlp-data-sg_headline_generator'],,114.71292762345828,AutoTrain,Not Specified,Not Specified,Not Specified,,1.3862273693084717,,0.524988,0.471727,557979193.0,True,148,7,"['transformers', 'safetensors', 'pytorch']",2023-03-31 06:08:27+00:00,2021-10-24 17:00:48+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 25965855
- CO2 Emissions (in grams): 114.71292762345828

## Validation Metrics

- Loss: 1.3862273693084717
- Rouge1: 52.4988
- Rouge2: 31.6973
- RougeL: 47.1727
- RougeLsum: 47.1576
- Gen Len: 17.6194

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/chinhon/autonlp-sg_headline_generator-25965855
```",,,1,[],[],NLP,2021-10,4864135.233576723,0.4969344582473425,0.0,0.0,0.0,0.0,0,1.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
12481,headline_writer2,['chinhon/autonlp-data-sg_headline_generator'],,396.629376395644,AutoTrain,Not Specified,Not Specified,Not Specified,,1.4130597114562988,,0.517922,0.464585,1625557313.0,True,3,0,"['transformers', 'pytorch']",2021-10-24 20:54:50+00:00,2021-10-24 20:54:37+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 25965856
- CO2 Emissions (in grams): 396.629376395644

## Validation Metrics

- Loss: 1.4130597114562988
- Rouge1: 51.7922
- Rouge2: 30.8259
- RougeL: 46.4585
- RougeLsum: 46.4807
- Gen Len: 15.8411

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/chinhon/autonlp-sg_headline_generator-25965856
```",,,1,[],[],NLP,2021-10,4098428.9357792833,0.4898057568444805,0.0,0.0,0.0,0.0,0,1.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
12977,bias-detection-model,[''],,0.319355,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,,False,1551,10,"['transformers', 'tf']",2022-08-09 02:40:59+00:00,2021-12-05 11:50:39+00:00,"
## About the Model
An English sequence classification model, trained on MBAD Dataset to detect bias and fairness in sentences (news articles). This model was built on top of distilbert-base-uncased model and trained for 30 epochs with a batch size of 16, a learning rate of 5e-5, and a maximum sequence length of 512.

- Dataset : MBAD Data
- Carbon emission 0.319355 Kg

| Train Accuracy | Validation Accuracy | Train loss | Test loss |
|---------------:| -------------------:| ----------:|----------:|
|          76.97 |               62.00 |       0.45 |      0.96 |

## Usage
The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.
```python
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from transformers import pipeline
tokenizer = AutoTokenizer.from_pretrained(""d4data/bias-detection-model"")
model = TFAutoModelForSequenceClassification.from_pretrained(""d4data/bias-detection-model"")

classifier = pipeline('text-classification', model=model, tokenizer=tokenizer) # cuda = 0,1 based on gpu availability
classifier(""The irony, of course, is that the exhibit that invites people to throw trash at vacuuming Ivanka Trump lookalike reflects every stereotype feminists claim to stand against, oversexualizing Ivanka’s body and ignoring her hard work."")
```

## Author
This model is part of the Research topic ""Bias and Fairness in AI"" conducted by Deepak John Reji, Shaina Raza. If you use this work (code, model or dataset), please star at:
> Bias & Fairness in AI, (2022), GitHub repository, <https://github.com/dreji18/Fairness-in-AI>

",,,1,[],[],NLP,2021-12,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,2,0,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
12979,environmental-due-diligence-model,[''],,0.1069,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,,False,5,2,"['transformers', 'tf']",2023-02-26 11:02:07+00:00,2021-12-28 11:33:48+00:00,"
## About the Model
An Environmental due diligence classification model, trained on customized environmental Dataset to detect contamination and remediation activities (both prevailing as well as planned) as a part of site assessment process.  This model can identify the source of contamination, the extent of contamination, the types of contaminants present at the site, the flow of contaminants and their interaction with ground water, surface water and other surrounding water bodies .

This model was built on top of distilbert-base-uncased model and trained for 10 epochs with a batch size of 16, a learning rate of 5e-5, and a maximum sequence length of 512.

- Dataset : Open Source News data + Custom data
- Carbon emission 0.1069 Kg

## Usage
The easiest way is to load through the pipeline object offered by transformers library.
```python
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from transformers import pipeline
tokenizer = AutoTokenizer.from_pretrained(""d4data/environmental-due-diligence-model"")
model = TFAutoModelForSequenceClassification.from_pretrained(""d4data/environmental-due-diligence-model"")

classifier = pipeline('text-classification', model=model, tokenizer=tokenizer) # cuda = 0,1 based on gpu availability
classifier(""At the every month post-injection monitoring event, TCE, carbon tetrachloride, and chloroform concentrations were above CBSGs in three of the wells"")
```

## Author
This model is part of the Research topic ""Environmental Due Diligence"" conducted by Deepak John Reji, Afreen Aman. If you use this work (code, model or dataset), please cite as:
> Environmental Due Diligence, (2020), https://www.sciencedirect.com/science/article/pii/S2665963822001117

",,,1,[],[],NLP,2021-12,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,2,0,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
12995,dalle-mini,[''],,7540.0,Not Specified,Not Specified,East US,TPU v3-8,,,,,,,False,290,276,"['transformers', 'jax']",2023-01-11 08:53:22+00:00,2021-10-26 20:50:54+00:00,"
# DALL·E Mini Model Card

This model card focuses on the model associated with the DALL·E mini space on Hugging Face, available [here](https://huggingface.co/spaces/dalle-mini/dalle-mini). The app is called “dalle-mini”, but  incorporates “[DALL·E Mini](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy)’’ and “[DALL·E Mega](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2)” models (further details on this distinction forthcoming).

The DALL·E Mega model is the largest version of DALLE Mini. For more information specific to DALL·E Mega, see the [DALL·E Mega model card](https://huggingface.co/dalle-mini/dalle-mega).

## Model Details

* **Developed by:** Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc Lê, Luke, Luke Melas, Ritobrata Ghosh
* **Model type:** Transformer-based text-to-image generation model
* **Language(s):** English
* **License:** Apache 2.0
* **Model Description:** This is a model that can be used to generate images based on text prompts. As the model developers wrote in the [project report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy) about DALL·E mini, “OpenAI had the first impressive model for generating images with [DALL·E](https://openai.com/blog/dall-e/). DALL·E mini is an attempt at reproducing those results with an open-source model.”
* **Resources for more information:** See OpenAI’s website for more information about [DALL·E](https://openai.com/blog/dall-e/), including the [DALL·E model card](https://github.com/openai/DALL-E/blob/master/model_card.md). See the [project report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy) for more information from the model’s developers. To learn more about DALL·E Mega, see the DALL·E Mega [training journal](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training--VmlldzoxODMxMDI2#training-parameters).
* **Cite as:** 
```bib text
@misc{Dayma_DALL·E_Mini_2021,
      author = {Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and Lê Khắc, Phúc and Melas, Luke and Ghosh, Ritobrata},
      doi = {10.5281/zenodo.5146400},
      month = {7},
      title = {DALL·E Mini},
      url = {https://github.com/borisdayma/dalle-mini},
      year = {2021}
}
```

## Uses

### Direct Use

The model is intended to be used to generate images based on text prompts for research and personal consumption.  Intended uses include supporting creativity, creating humorous content, and providing generations for people curious about the model’s behavior.  Intended uses exclude those described in the [Misuse and Out-of-Scope Use](#misuse-malicious-use-and-out-of-scope-use) section.

### Downstream Use

The model could also be used for downstream use cases, including:
* Research efforts, such as probing and better understanding the limitations and biases of generative models to further improve the state of science
* Development of educational or creative tools
* Generation of artwork and use in design and artistic processes. 
* Other uses that are newly discovered by users. This currently includes poetry illustration (give a poem as prompt), fan art (putting a character in various other visual universes), visual puns, fairy tale illustrations (give a fantasy situation as prompt), concept mashups (applying a texture to something completely different), style transfers (portraits in the style of), … We hope you will find your own application!

Downstream uses exclude the uses described in [Misuse and Out-of-Scope Use](#misuse-malicious-use-and-out-of-scope-use).

### Misuse, Malicious Use, and Out-of-Scope Use

The model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes. 

#### Out-of-Scope Use

The model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.

#### Misuse and Malicious Use 

Using the model to generate content that is cruel to individuals is a misuse of this model. This includes:
* Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.
* Intentionally promoting or propagating discriminatory content or harmful stereotypes.
* Impersonating individuals without their consent.
* Sexual content without consent of the people who might see it.
* Mis- and disinformation
* Representations of egregious violence and gore
* Sharing of copyrighted or licensed material in violation of its terms of use.
* Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.


## Limitations and Bias

### Limitations

The model developers discuss the limitations of the model further in the DALL·E Mini [technical report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA):
* Faces and people in general are not generated properly.
* Animals are usually unrealistic.
* It is hard to predict where the model excels or falls short…Good prompt engineering will lead to the best results.
* The model has only been trained with English descriptions and will not perform as well in other languages


### Bias 

**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**

The model was trained on unfiltered data from the Internet, limited to pictures with English descriptions. Text and images from communities and cultures using other languages were not utilized. This affects all output of the model, with white and Western culture asserted as a default, and the model’s ability to generate content using non-English prompts is observably lower quality than prompts in English.

While the capabilities of image generation models are impressive, they may also reinforce or exacerbate societal biases. The extent and nature of the biases of DALL·E Mini and DALL·E Mega models have yet to be fully documented, but initial testing demonstrates that they may generate images that contain negative stereotypes against minoritized groups. Work to analyze the nature and extent of the models’ biases and limitations is ongoing.

Our current analyses demonstrate that:
* Images generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.
* When the model generates images with people in them, it tends to output people who we perceive to be white, while people of color are underrepresented. 
* Images generated by the model can contain biased content that depicts power differentials between people of color and people who are white, with white people in positions of privilege.
* The model is generally only usable for generating images based on text in English, limiting accessibility of the model for non-English speakers and potentially contributing to the biases in images generated by the model.

The [technical report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA) discusses these issues in more detail, and also highlights potential sources of bias in the model development process.


### Limitations and Bias Recommendations

* Users (both direct and downstream) should be made aware of the biases and limitations.
* Content that is potentially problematic should be filtered out, e.g., via automated models that detect violence or pornography.
* Further work on this model should include methods for balanced and just representations of people and cultures, for example, by curating the training dataset to be both diverse and inclusive.


## Training

### Training Data

The model developers used 3 datasets for the model:
* ﻿[Conceptual Captions Dataset](https://aclanthology.org/P18-1238/), which contains 3 million image and caption pairs.
* ﻿[Conceptual 12M](https://arxiv.org/abs/2102.08981), which contains 12 million image and caption pairs.
* The [OpenAI subset](https://github.com/openai/CLIP/blob/main/data/yfcc100m.md) of [YFCC100M](https://multimediacommons.wordpress.com/yfcc100m-core-dataset/), which contains about 15 million images and that we further sub-sampled to 2 million images due to limitations in storage space. They used both title and description as caption and removed html tags, new lines and extra spaces.

For fine-tuning the image encoder, a subset of 2 million images were used.
All images  (about 15 million) were used for training the Seq2Seq model.

### Training Procedure

As described further in the [technical report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA#our-dall-e-model-architecture) for DALL·E Mini, during training, images and descriptions are both available and pass through the system as follows:
* Images are encoded through a [VQGAN](https://arxiv.org/abs/2012.09841) encoder, which turns images into a sequence of tokens.
* Descriptions are encoded through a [BART](https://arxiv.org/abs/1910.13461) encoder.
* The output of the BART encoder and encoded images are fed through the BART decoder, which is an auto-regressive model whose goal is to predict the next token.
* Loss is the [softmax cross-entropy](https://wandb.ai/sauravm/Activation-Functions/reports/Activation-Functions-Softmax--VmlldzoxNDU1Njgy#%F0%9F%93%A2-softmax-+-cross-entropy-loss-(caution:-math-alert)) between the model prediction logits and the actual image encodings from the VQGAN.

The simplified training procedure for DALL·E Mega is as follows: 

* **Hardware:** 1 pod TPU v3-256 = 32 nodes of TPU VM v3-8 (8 TPU per node) = 256 TPU v3
* **Optimizer:** Distributed Shampoo
* **Model Partition Specificiations:** 8 model parallel x 32 data parallel
* **Batch:** 44 samples per model x 32 data parallel x 3 gradient accumulation steps =  4224 increasing samples per update
* **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant until plateau
* Gradient checkpointing used on each Encoder/Decoder layer (ie, MHA + FFN)
* Distributed Shampoo + Normformer Optimizations have proved to be effective and efficiently scaling this model. 
* It should also be noted that the learning rate and other parameters are sometimes adjusted on the fly, and batch size increased over time as well.

There is more information about the full procedure and technical material in the DALL·E Mega [training journal](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training--VmlldzoxODMxMDI2#training-parameters).


## Evaluation Results

The model developers discuss their results extensively in their [technical report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA#the-results-of-our-dall-e-experiment) for DALL·E Mini, which provides comparisons between DALL·E Mini’s results with [DALL·E-pytorch](https://github.com/lucidrains/DALLE-pytorch), OpenAI’s [DALL·E](https://openai.com/blog/dall-e/), and models consisting of a generator coupled with the [CLIP neural network model](https://openai.com/blog/clip/). 

For evaluation results related to DALL·E Mega, see this [technical report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy).

## Environmental Impact

### DALL·E Mini Estimated Emissions

*The model is 27 times smaller than the original DALL·E and was trained on a single TPU v3-8 for only 3 days.*

Based on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.

* **Hardware Type:** TPU v3-8
* **Hours used:** 72 (3 days)
* **Cloud Provider:** GCP (as mentioned in the technical report)
* **Compute Region:** us-east1 (provided by model developers)
* **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 30.16 kg CO2 eq.

### DALL·E Mega Estimated Emissions

DALL·E Mega is still training. So far, as on June 9, 2022, the model developers report that DALL·E Mega has been training for about 40-45 days on a TPU v3-256. Using those numbers, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.

* **Hardware Type:** TPU v3-256
* **Hours used:** 960 - 1080 hours (40-45 days)
* **Cloud Provider:** Unknown
* **Compute Region:** Unknown
* **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** Unknown

## Citation

```bibtext
@misc{Dayma_DALL·E_Mini_2021,
      author = {Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and Lê Khắc, Phúc and Melas, Luke and Ghosh, Ritobrata},
      doi = {10.5281/zenodo.5146400},
      month = {7},
      title = {DALL·E Mini},
      url = {https://github.com/borisdayma/dalle-mini},
      year = {2021}
}
```

*This model card was written by: Boris Dayma, Margaret Mitchell, Ezi Ozoani, Marissa Gerchick, Irene Solaiman, Clémentine Fourrier, Sasha Luccioni, Emily Witko, Nazneen Rajani, and Julian Herrera.*
",** 72 (3 days),** GCP (as mentioned in the technical report),1,[],[],Multimodal,2021-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
13046,autonlp-legal-text-summary-457311749,['danicodes/autonlp-data-legal-text-summary'],,10.14880558843294,AutoTrain,Not Specified,Not Specified,Not Specified,,1.647747278213501,,0.324854,0.300602,2283825905.0,True,8,0,"['transformers', 'pytorch']",2021-12-29 22:18:48+00:00,2021-12-29 22:18:24+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 457311749
- CO2 Emissions (in grams): 10.148805588432941

## Validation Metrics

- Loss: 1.647747278213501
- Rouge1: 32.4854
- Rouge2: 19.8974
- RougeL: 30.0602
- RougeLsum: 29.9377
- Gen Len: 46.6556

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/danicodes/autonlp-legal-text-summary-457311749
```",,,1,[],[],NLP,2021-12,225033959.42503628,0.3122578154434524,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
13300,autonlp-shajBERT-38639804,['dee4hf/autonlp-data-shajBERT'],,11.98841452241473,AutoTrain,Not Specified,Not Specified,Not Specified,0.86783988957902,0.421400249004364,0.8669477050676501,,,71811793.0,True,7,1,"['transformers', 'pytorch']",2021-12-04 18:53:26+00:00,2021-12-04 18:53:21+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 38639804
- CO2 Emissions (in grams): 11.98841452241473

## Validation Metrics

- Loss: 0.421400249004364
- Accuracy: 0.86783988957902
- Macro F1: 0.8669477050676501
- Micro F1: 0.86783988957902
- Weighted F1: 0.86694770506765
- Macro Precision: 0.867606300132228
- Micro Precision: 0.86783988957902
- Weighted Precision: 0.8676063001322278
- Macro Recall: 0.86783988957902
- Micro Recall: 0.86783988957902
- Weighted Recall: 0.86783988957902


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/dee4hf/autonlp-shajBERT-38639804
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dee4hf/autonlp-shajBERT-38639804"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dee4hf/autonlp-shajBERT-38639804"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,5990099.263395801,0.8673935679023947,0.0,1.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
13872,autonlp-predict_ROI_1-29797722,['ds198799/autonlp-data-predict_ROI_1'],,2.7516207978192737,AutoTrain,Not Specified,Not Specified,Not Specified,0.7559139784946236,0.6113826036453247,0.4594734612976928,,,438022317.0,True,32,0,"['transformers', 'pytorch']",2021-11-12 22:10:08+00:00,2021-11-12 22:10:03+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 29797722
- CO2 Emissions (in grams): 2.7516207978192737

## Validation Metrics

- Loss: 0.6113826036453247
- Accuracy: 0.7559139784946236
- Macro F1: 0.4594734612976928
- Micro F1: 0.7559139784946236
- Weighted F1: 0.7195080232106192
- Macro Precision: 0.7175166413412577
- Micro Precision: 0.7559139784946236
- Weighted Precision: 0.7383048259333735
- Macro Recall: 0.4482203645846237
- Micro Recall: 0.7559139784946236
- Weighted Recall: 0.7559139784946236


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/ds198799/autonlp-predict_ROI_1-29797722
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ds198799/autonlp-predict_ROI_1-29797722"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ds198799/autonlp-predict_ROI_1-29797722"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,159187020.73597616,0.5715418816597024,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
13873,autonlp-predict_ROI_1-29797730,['ds198799/autonlp-data-predict_ROI_1'],,2.243912766446172,AutoTrain,Not Specified,Not Specified,Not Specified,0.7596774193548387,0.6314184069633484,0.4740565300039588,,,498677165.0,True,19,0,"['transformers', 'pytorch']",2021-11-12 22:10:39+00:00,2021-11-12 22:10:34+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 29797730
- CO2 Emissions (in grams): 2.2439127664461718

## Validation Metrics

- Loss: 0.6314184069633484
- Accuracy: 0.7596774193548387
- Macro F1: 0.4740565300039588
- Micro F1: 0.7596774193548386
- Weighted F1: 0.7371623804622154
- Macro Precision: 0.6747804619412134
- Micro Precision: 0.7596774193548387
- Weighted Precision: 0.7496542175358931
- Macro Recall: 0.47743727441146655
- Micro Recall: 0.7596774193548387
- Weighted Recall: 0.7596774193548387


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/ds198799/autonlp-predict_ROI_1-29797730
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ds198799/autonlp-predict_ROI_1-29797730"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ds198799/autonlp-predict_ROI_1-29797730"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,222235539.8377571,0.5838050278650202,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
13896,autonlp-covid-fake-news-36839110,['dtam/autonlp-data-covid-fake-news'],,123.79523392848652,AutoTrain,Not Specified,Not Specified,Not Specified,0.9714953271028036,0.1718836724758148,0.9694235588972432,,,890430161.0,True,15,0,"['transformers', 'pytorch']",2021-11-29 05:58:03+00:00,2021-11-29 05:57:53+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 36839110
- CO2 Emissions (in grams): 123.79523392848652

## Validation Metrics

- Loss: 0.17188367247581482
- Accuracy: 0.9714953271028037
- Precision: 0.9917948717948718
- Recall: 0.9480392156862745
- AUC: 0.9947452731092438
- F1: 0.9694235588972432

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/dtam/autonlp-covid-fake-news-36839110
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dtam/autonlp-covid-fake-news-36839110"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dtam/autonlp-covid-fake-news-36839110"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,7192766.092387528,0.9704583372805808,0.0,1.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
14115,autonlp-new_tx-607517182,['emekaboris/autonlp-data-new_tx'],,3.842950628218143,AutoTrain,Not Specified,Not Specified,Not Specified,0.8679706601466992,0.4033123552799225,0.719846919916469,,,498692525.0,True,17,0,"['transformers', 'pytorch']",2022-03-02 14:51:04+00:00,2022-03-02 14:47:01+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 607517182
- CO2 Emissions (in grams): 3.842950628218143

## Validation Metrics

- Loss: 0.4033123552799225
- Accuracy: 0.8679706601466992
- Macro F1: 0.719846919916469
- Micro F1: 0.8679706601466993
- Weighted F1: 0.8622411469250695
- Macro Precision: 0.725309168791155
- Micro Precision: 0.8679706601466992
- Weighted Precision: 0.8604370906049568
- Macro Recall: 0.7216672806300003
- Micro Recall: 0.8679706601466992
- Weighted Recall: 0.8679706601466992


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/emekaboris/autonlp-new_tx-607517182
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""emekaboris/autonlp-new_tx-607517182"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""emekaboris/autonlp-new_tx-607517182"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,129768132.15818708,0.7869997336338965,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
14116,autonlp-txc-17923124,['emekaboris/autonlp-data-txc'],,133.57087522185148,AutoTrain,Not Specified,Not Specified,Not Specified,0.9325402190077058,0.2080804407596588,0.7283811287183823,,,498741741.0,True,17,0,"['transformers', 'pytorch']",2021-10-14 07:56:17+00:00,2021-10-14 07:56:11+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 17923124
- CO2 Emissions (in grams): 133.57087522185148

## Validation Metrics

- Loss: 0.2080804407596588
- Accuracy: 0.9325402190077058
- Macro F1: 0.7283811287183823
- Micro F1: 0.9325402190077058
- Weighted F1: 0.9315711955594153
- Macro Precision: 0.8106599661500661
- Micro Precision: 0.9325402190077058
- Weighted Precision: 0.9324644116921059
- Macro Recall: 0.7020515544343829
- Micro Recall: 0.9325402190077058
- Weighted Recall: 0.9325402190077058


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/emekaboris/autonlp-txc-17923124
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""emekaboris/autonlp-txc-17923124"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""emekaboris/autonlp-txc-17923124"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,3733910.855728289,0.8179131398678829,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
14117,autonlp-txc-17923129,['emekaboris/autonlp-data-txc'],,610.861733873082,AutoTrain,Not Specified,Not Specified,Not Specified,0.9264228741381642,0.2319454699754715,0.6730537318152493,,,1334577133.0,True,22,0,"['transformers', 'pytorch']",2021-10-14 12:19:07+00:00,2021-10-14 12:18:56+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 17923129
- CO2 Emissions (in grams): 610.861733873082

## Validation Metrics

- Loss: 0.2319454699754715
- Accuracy: 0.9264228741381642
- Macro F1: 0.6730537318152493
- Micro F1: 0.9264228741381642
- Weighted F1: 0.9251493598895151
- Macro Precision: 0.7767479491141245
- Micro Precision: 0.9264228741381642
- Weighted Precision: 0.9277971545757154
- Macro Recall: 0.6617262519071917
- Micro Recall: 0.9264228741381642
- Weighted Recall: 0.9264228741381642


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/emekaboris/autonlp-txc-17923129
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""emekaboris/autonlp-txc-17923129"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""emekaboris/autonlp-txc-17923129"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,2184745.0232940656,0.7796705126625172,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
14517,autonlp-api-boamente-417310788,['evandrodiniz/autonlp-data-api-boamente'],,6.826886567147602,AutoTrain,Not Specified,Not Specified,Not Specified,0.9578392621870884,0.2094931006431579,0.9255813953488372,,,1337755565.0,True,9,0,"['transformers', 'pytorch']",2021-12-14 18:38:02+00:00,2021-12-14 18:37:51+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 417310788
- CO2 Emissions (in grams): 6.826886567147602

## Validation Metrics

- Loss: 0.20949310064315796
- Accuracy: 0.9578392621870883
- Precision: 0.9476190476190476
- Recall: 0.9045454545454545
- AUC: 0.9714032720526227
- F1: 0.9255813953488372

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/evandrodiniz/autonlp-api-boamente-417310788
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""evandrodiniz/autonlp-api-boamente-417310788"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""evandrodiniz/autonlp-api-boamente-417310788"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,195953975.77536413,0.9414340840616112,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
14518,autonlp-api-boamente-417310793,['evandrodiniz/autonlp-data-api-boamente'],,9.446754273734577,AutoTrain,Not Specified,Not Specified,Not Specified,0.9407114624505928,0.2575517892837524,0.9028077753779696,,,1337755565.0,True,9,0,"['transformers', 'pytorch']",2021-12-14 18:39:10+00:00,2021-12-14 18:38:59+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 417310793
- CO2 Emissions (in grams): 9.446754273734577

## Validation Metrics

- Loss: 0.25755178928375244
- Accuracy: 0.9407114624505929
- Precision: 0.8600823045267489
- Recall: 0.95
- AUC: 0.9732501264968797
- F1: 0.9028077753779697

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/evandrodiniz/autonlp-api-boamente-417310793
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""evandrodiniz/autonlp-api-boamente-417310793"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""evandrodiniz/autonlp-api-boamente-417310793"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,141610073.2840536,0.9213699594346788,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
15400,latam-question-quality,['gagandeepkundi/autonlp-data-text-classification'],,20.79016987800992,AutoTrain,Not Specified,Not Specified,Not Specified,0.9789,0.0669326931238174,0.9787811745776348,,,504004013.0,True,50,0,"['transformers', 'pytorch']",2021-10-16 16:32:19+00:00,2021-10-16 16:32:13+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 19984005
- CO2 Emissions (in grams): 20.790169878009916

## Validation Metrics

- Loss: 0.06693269312381744
- Accuracy: 0.9789
- Precision: 0.9843244336569579
- Recall: 0.9733
- AUC: 0.99695552
- F1: 0.9787811745776348

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/gagandeepkundi/autonlp-text-classification-19984005
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""gagandeepkundi/autonlp-text-classification-19984005"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""gagandeepkundi/autonlp-text-classification-19984005"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,24242419.179705348,0.9788405836826426,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
15446,autonlp-news-summarization-483413089,['gborn/autonlp-data-news-summarization'],,210.6348731063569,AutoTrain,Not Specified,Not Specified,Not Specified,,1.8478657007217407,,0.505981,0.4605129999999999,2283825905.0,True,6,1,"['transformers', 'pytorch']",2022-01-07 23:10:47+00:00,2022-01-07 18:57:02+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 483413089
- CO2 Emissions (in grams): 210.6348731063569

## Validation Metrics

- Loss: 1.8478657007217407
- Rouge1: 50.5981
- Rouge2: 26.2167
- RougeL: 46.0513
- RougeLsum: 46.061
- Gen Len: 13.5987

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/gborn/autonlp-news-summarization-483413089
```",,,1,[],[],NLP,2022-01,10842582.101050364,0.4821774956761242,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
16864,autonlp-Summarization-20684327,['hiiamsid/autonlp-data-Summarization'],,437.2441955971972,AutoTrain,Not Specified,Not Specified,Not Specified,,,,0.037729,0.035066,1200770885.0,True,6,0,"['transformers', 'pytorch']",2021-10-18 18:30:54+00:00,2021-10-18 18:30:44+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 20684327
- CO2 Emissions (in grams): 437.2441955971972

## Validation Metrics

- Loss: nan
- Rouge1: 3.7729
- Rouge2: 0.4152
- RougeL: 3.5066
- RougeLsum: 3.5167
- Gen Len: 5.0577

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/hiiamsid/autonlp-Summarization-20684327
```",,,1,[],[],NLP,2021-10,2746224.8718018136,0.0363487908235455,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
16865,autonlp-Summarization-20684328,['hiiamsid/autonlp-data-Summarization'],,1133.9679082840014,AutoTrain,Not Specified,Not Specified,Not Specified,,,,0.094193,0.079376,2329700301.0,True,3,0,"['transformers', 'pytorch']",2021-10-19 05:09:38+00:00,2021-10-19 05:09:20+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 20684328
- CO2 Emissions (in grams): 1133.9679082840014

## Validation Metrics

- Loss: nan
- Rouge1: 9.4193
- Rouge2: 0.91
- RougeL: 7.9376
- RougeLsum: 8.0076
- Gen Len: 10.65

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/hiiamsid/autonlp-Summarization-20684328
```",,,1,[],[],NLP,2021-10,2054467.5770635025,0.0861520613473604,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
19952,xlmindic-base-multiscript-soham,['oscar'],,0.21,MLCO2,fine-tuning,Not Specified,P100 for about 1.5 hours,,,,,,57007313.0,False,14,0,"['transformers', 'jax', 'pytorch', 'tf']",2022-02-10 09:24:02+00:00,2022-01-12 05:02:25+00:00,"
# XLMIndic Base Multiscript

This model is finetuned from [this model](https://huggingface.co/ibraheemmoosa/xlmindic-base-multiscript) on Soham Bangla News Classification task which is part of the IndicGLUE benchmark.

## Model description
This model has the same configuration as the [ALBERT Base v2 model](https://huggingface.co/albert-base-v2/). Specifically, this model has the following configuration:
- 12 repeating layers
- 128 embedding dimension
- 768 hidden dimension
- 12 attention heads
- 11M parameters
- 512 sequence length

## Training data
This model was fine-tuned on Soham dataset that is part of the IndicGLUE benchmark.

## Training procedure
### Preprocessing
The texts are  tokenized using SentencePiece and a vocabulary size of 50,000.

### Training
The model was trained for 8 epochs with a batch size of 16 and a learning rate of *2e-5*.
## Evaluation results

See results specific to Soham in the following table.
### IndicGLUE
Task | mBERT | XLM-R | IndicBERT-Base | XLMIndic-Base-Uniscript | XLMIndic-Base-Multiscript (This Model)
-----| ----- | ----- | ------ | ------- | --------
Wikipedia Section Title Prediction | 71.90 | 65.45 | 69.40 | **81.78 ± 0.60** | 77.17 ± 0.76
Article Genre Classification | 88.64 | 96.61 | 97.72 | **98.70 ± 0.29** | 98.30 ± 0.26
Named Entity Recognition (F1-score) | 71.29 | 62.18 | 56.69 | **89.85 ± 1.14** |  83.19 ± 1.58
BBC Hindi News Article Classification | 60.55 | 75.52 | 74.60 | **79.14 ± 0.60** | 77.28 ± 1.50
Soham Bangla News Article Classification | 80.23 | 87.6 | 78.45 | **93.89 ± 0.48** | 93.22 ± 0.49
INLTK Gujarati Headlines Genre Classification | - | - | **92.91** | 90.73 ± 0.75 | 90.41 ± 0.69
INLTK Marathi Headlines Genre Classification | - | - | **94.30** | 92.04 ± 0.47 | 92.21 ± 0.23
IITP Hindi Product Reviews Sentiment Classification | 74.57 | **78.97** | 71.32 | 77.18 ± 0.77 | 76.33 ± 0.84
IITP Hindi Movie Reviews Sentiment Classification | 56.77 | 61.61 | 59.03 | **66.34 ± 0.16** | 65.91 ± 2.20
MIDAS Hindi Discourse Type Classification | 71.20 | **79.94** | 78.44 | 78.54 ± 0.91 | 78.39 ± 0.33
Cloze Style Question Answering (Fill-mask task) | - | - | 37.16 | **41.54** | 38.21

## Intended uses & limitations
This model is pretrained on Indo-Aryan languages. Thus it is intended to be used for downstream tasks on these languages.
You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=xlmindic) to look for
fine-tuned versions on a task that interests you.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.

### How to use

Then you can use this model directly with a pipeline for masked language modeling:
```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='ibraheemmoosa/xlmindic-base-multiscript')
>>> text = ""রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি [MASK], ঔপন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।""
>>> unmasker(text)
[{'score': 0.34163928031921387,
  'token': 5399,
  'token_str': 'কবি',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি কবি, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'},
 {'score': 0.30519795417785645,
  'token': 33436,
  'token_str': 'people',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি people, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'},
 {'score': 0.29130080342292786,
  'token': 30476,
  'token_str': 'সাহিত্যিক',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি সাহিত্যিক, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'},
 {'score': 0.031051287427544594,
  'token': 6139,
  'token_str': 'লেখক',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি লেখক, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'},
 {'score': 0.002705035964027047,
  'token': 38443,
  'token_str': 'শিল্পীরা',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি শিল্পীরা, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'}]
```
### Limitations and bias
Even though we pretrain on a comparatively large multilingual corpus the model may exhibit harmful gender, ethnic and political bias. If you fine-tune this model on a task where these issues are important you should take special care when relying on the model to make decisions.

## Contact
Feel free to contact us if you have any ideas or if you want to know more about our models.
- Ibraheem Muhammad Moosa (ibraheemmoosa1347@gmail.com)
- Mahmud Elahi Akhter (mahmud.akhter01@northsouth.edu)
- Ashfia Binte Habib

## BibTeX entry and citation info

```bibtex
@article{Moosa2022DoesTH,
  title={Does Transliteration Help Multilingual Language Modeling?},
  author={Ibraheem Muhammad Moosa and Mahmuda Akhter and Ashfia Binte Habib},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.12501}
}
```",,,1,[],[],NLP,2022-01,271463395.2380952,,0.0,1.0,0.0,0.0,0,0.0,0,0,0,0.0,1.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,1.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
19953,xlmindic-base-multiscript,['oscar'],,28.53,MLCO2,pretraining,Not Specified,TPUv3-8 for about 180 hours or 7.5 days,,,,,,57591810.0,False,5,0,"['transformers', 'jax', 'pytorch', 'tf']",2022-07-27 05:36:24+00:00,2022-01-07 19:06:50+00:00,"
# XLMIndic Base Multiscript

This model is identical in all aspects to [this model](https://huggingface.co/ibraheemmoosa/xlmindic-base-uniscript) except that we do not perform the ISO-15919 transliteration. Thus it is intended to serve as an ablation model for our study. See [this](https://huggingface.co/ibraheemmoosa/xlmindic-base-uniscript) to understand the details.

## Model description
This model has the same configuration as the [ALBERT Base v2 model](https://huggingface.co/albert-base-v2/). Specifically, this model has the following configuration:
- 12 repeating layers
- 128 embedding dimension
- 768 hidden dimension
- 12 attention heads
- 11M parameters
- 512 sequence length

## Training data
This model was pretrained on the [OSCAR](https://huggingface.co/datasets/oscar) dataset which is a medium sized multilingual corpus containing text from 163 languages. We select a subset of 14 languages based on the following criteria:
 - Belongs to the [Indo-Aryan language family](https://en.wikipedia.org/wiki/Indo-Aryan_languages).
 - Uses a [Brahmic script](https://en.wikipedia.org/wiki/Brahmic_scripts).
 
These are the 14 languages we pretrain this model on:
- Assamese
- Bangla
- Bihari
- Bishnupriya Manipuri
- Goan Konkani
- Gujarati
- Hindi
- Maithili
- Marathi
- Nepali
- Oriya
- Panjabi
- Sanskrit
- Sinhala

## Training procedure
### Preprocessing
The texts are  tokenized using SentencePiece and a vocabulary size of 50,000. The inputs of the model are
then of the form:
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```
### Training
Training objective is the same as the original ALBERT. 
.
The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.

The details of the sentence order prediction example generation procedure for each sentence are the following:
- Split the sentence into two parts A and B at a random index.
- With 50% probability swap the two parts.  

The model was pretrained on TPUv3-8 for 1M steps. We have checkpoints available at every 100k pretraining steps. These are available at different branches of this repository. You can load these checkpoints by passing the `revision` parameter. For example to load the checkpoint at 500k you can use the following code.

```python
>>> AutoModel.from_pretrained('ibraheemmoosa/xlmindic-base-multiscript', revision='checkpoint_500k')
```

## Evaluation results
We evaluated this model on the Indo-Aryan subset of languages (Panjabi, Oriya, Assamese, Bangla, Hindi, Marathi, Gujarati) from the [IndicGLUE](https://huggingface.co/datasets/indic_glue) benchmark dataset. We report the mean and standard deviation of nine fine-tuning runs for this model.

### IndicGLUE
Task | mBERT | XLM-R | IndicBERT-Base | XLMIndic-Base-Uniscript | XLMIndic-Base-Multiscript (This Model)
-----| ----- | ----- | ------ | ------- | --------
Wikipedia Section Title Prediction | 71.90 | 65.45 | 69.40 | **81.78 ± 0.60** | 77.17 ± 0.76
Article Genre Classification | 88.64 | 96.61 | 97.72 | **98.70 ± 0.29** | 98.30 ± 0.26
Named Entity Recognition (F1-score) | 71.29 | 62.18 | 56.69 | **89.85 ± 1.14** |  83.19 ± 1.58
BBC Hindi News Article Classification | 60.55 | 75.52 | 74.60 | **79.14 ± 0.60** | 77.28 ± 1.50
Soham Bangla News Article Classification | 80.23 | 87.6 | 78.45 | **93.89 ± 0.48** | 93.22 ± 0.49
INLTK Gujarati Headlines Genre Classification | - | - | **92.91** | 90.73 ± 0.75 | 90.41 ± 0.69
INLTK Marathi Headlines Genre Classification | - | - | **94.30** | 92.04 ± 0.47 | 92.21 ± 0.23
IITP Hindi Product Reviews Sentiment Classification | 74.57 | **78.97** | 71.32 | 77.18 ± 0.77 | 76.33 ± 0.84
IITP Hindi Movie Reviews Sentiment Classification | 56.77 | 61.61 | 59.03 | **66.34 ± 0.16** | 65.91 ± 2.20
MIDAS Hindi Discourse Type Classification | 71.20 | **79.94** | 78.44 | 78.54 ± 0.91 | 78.39 ± 0.33
Cloze Style Question Answering (Fill-mask task) | - | - | 37.16 | **41.54** | 38.21

## Intended uses & limitations
This model is pretrained on Indo-Aryan languages. Thus it is intended to be used for downstream tasks on these languages.
You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=xlmindic) to look for
fine-tuned versions on a task that interests you.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.

### How to use

Then you can use this model directly with a pipeline for masked language modeling:
```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='ibraheemmoosa/xlmindic-base-multiscript')
>>> text = ""রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি [MASK], ঔপন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।""
>>> unmasker(text)
[{'score': 0.34163928031921387,
  'token': 5399,
  'token_str': 'কবি',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি কবি, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'},
 {'score': 0.30519795417785645,
  'token': 33436,
  'token_str': 'people',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি people, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'},
 {'score': 0.29130080342292786,
  'token': 30476,
  'token_str': 'সাহিত্যিক',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি সাহিত্যিক, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'},
 {'score': 0.031051287427544594,
  'token': 6139,
  'token_str': 'লেখক',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি লেখক, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'},
 {'score': 0.002705035964027047,
  'token': 38443,
  'token_str': 'শিল্পীরা',
  'sequence': 'রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি শিল্পীরা, পন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।'}]
```
### Limitations and bias
Even though we pretrain on a comparatively large multilingual corpus the model may exhibit harmful gender, ethnic and political bias. If you fine-tune this model on a task where these issues are important you should take special care when relying on the model to make decisions.

## Contact
Feel free to contact us if you have any ideas or if you want to know more about our models.
- Ibraheem Muhammad Moosa (ibraheemmoosa1347@gmail.com)
- Mahmud Elahi Akhter (mahmud.akhter01@northsouth.edu)
- Ashfia Binte Habib

## BibTeX entry and citation info

```bibtex
@article{Moosa2022DoesTH,
  title={Does Transliteration Help Multilingual Language Modeling?},
  author={Ibraheem Muhammad Moosa and Mahmuda Akhter and Ashfia Binte Habib},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.12501}
}
```",,,1,[],[],NLP,2022-01,2018640.378548896,,0.0,1.0,0.0,0.0,0,0.0,0,0,0,0.0,1.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,1.0,0.0,0.0,0,0,0.0,1.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
19954,xlmindic-base-uniscript-soham,['oscar'],,0.21,MLCO2,fine-tuning,Not Specified,P100 for about 1.5 hours,,,,,,57007313.0,False,8,0,"['transformers', 'jax', 'pytorch', 'tf']",2022-01-12 12:28:05+00:00,2022-01-12 12:13:43+00:00,"
# XLMIndic Base Uniscript

This model is finetuned from [this model](https://huggingface.co/ibraheemmoosa/xlmindic-base-uniscript) on Soham Bangla News Classification task which is part of the IndicGLUE benchmark. **Before pretraining this model we transliterate the text to [ISO-15919](https://en.wikipedia.org/wiki/ISO_15919) format using the [Aksharamukha](https://pypi.org/project/aksharamukha/)
library.** A demo of Aksharamukha library is hosted [here](https://aksharamukha.appspot.com/converter)
where you can transliterate your text and use it on our model on the inference widget.

## Model description

This model has the same configuration as the [ALBERT Base v2 model](https://huggingface.co/albert-base-v2/). Specifically, this model has the following configuration:

- 12 repeating layers
- 128 embedding dimension
- 768 hidden dimension
- 12 attention heads
- 11M parameters
- 512 sequence length

## Training data
This model was fine-tuned on Soham dataset that is part of the IndicGLUE benchmark.

## Transliteration

*The unique component of this model is that it takes in ISO-15919 transliterated text.*

The motivation behind this is this. When two languages share vocabularies, a machine learning model can exploit that to learn good cross-lingual representations. However if these two languages use different writing scripts it is difficult for a model to make the connection. Thus if if we can write the two languages in a single script then it is easier for the model to learn good cross-lingual representation.

For many of the scripts currently in use, there are standard transliteration schemes to convert to the Latin script. In particular, for the Indic scripts the ISO-15919 transliteration scheme is designed to consistently transliterate texts written in different Indic scripts to the Latin script.

An example of ISO-15919 transliteration for a piece of **Bangla** text is the following:

**Original:** ""রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি কবি, ঔপন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক।""

**Transliterated:** 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli kabi, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika.'

Another example for a piece of **Hindi** text is the following:

**Original:** ""चूंकि मानव परिवार के सभी सदस्यों के जन्मजात गौरव और समान तथा अविच्छिन्न अधिकार की स्वीकृति ही विश्व-शान्ति, न्याय और स्वतन्त्रता की बुनियाद है""

**Transliterated:** ""cūṁki mānava parivāra kē sabhī sadasyōṁ kē janmajāta gaurava aura samāna tathā avicchinna adhikāra kī svīkr̥ti hī viśva-śānti, nyāya aura svatantratā kī buniyāda hai""
 
 
## Training procedure

### Preprocessing

The texts are transliterated to ISO-15919 format using the Aksharamukha library. Then these are tokenized using SentencePiece and a vocabulary size of 50,000.

### Training

The model was trained for 8 epochs with a batch size of 16 and a learning rate of *2e-5*.

## Evaluation results
See results specific to Soham in the following table.

### IndicGLUE
Task | mBERT | XLM-R | IndicBERT-Base | XLMIndic-Base-Uniscript (This Model) | XLMIndic-Base-Multiscript (Ablation Model)
-----| ----- | ----- | ------ | ------- | --------
Wikipedia Section Title Prediction | 71.90 | 65.45 | 69.40 | **81.78 ± 0.60** | 77.17 ± 0.76
Article Genre Classification | 88.64 | 96.61 | 97.72 | **98.70 ± 0.29** | 98.30 ± 0.26
Named Entity Recognition (F1-score) | 71.29 | 62.18 | 56.69 | **89.85 ± 1.14** |  83.19 ± 1.58
BBC Hindi News Article Classification | 60.55 | 75.52 | 74.60 | **79.14 ± 0.60** | 77.28 ± 1.50
Soham Bangla News Article Classification | 80.23 | 87.6 | 78.45 | **93.89 ± 0.48** | 93.22 ± 0.49
INLTK Gujarati Headlines Genre Classification | - | - | **92.91** | 90.73 ± 0.75 | 90.41 ± 0.69
INLTK Marathi Headlines Genre Classification | - | - | **94.30** | 92.04 ± 0.47 | 92.21 ± 0.23
IITP Hindi Product Reviews Sentiment Classification | 74.57 | **78.97** | 71.32 | 77.18 ± 0.77 | 76.33 ± 0.84
IITP Hindi Movie Reviews Sentiment Classification | 56.77 | 61.61 | 59.03 | **66.34 ± 0.16** | 65.91 ± 2.20
MIDAS Hindi Discourse Type Classification | 71.20 | **79.94** | 78.44 | 78.54 ± 0.91 | 78.39 ± 0.33
Cloze Style Question Answering (Fill-mask task) | - | - | 37.16 | **41.54** | 38.21

## Intended uses & limitations

This model is pretrained on Indo-Aryan languages. Thus it is intended to be used for downstream tasks on these languages. However, since Dravidian languages such as Malayalam, Telegu, Kannada etc share a lot of vocabulary with the Indo-Aryan languages, this model can potentially be used on those languages too (after transliterating the text to ISO-15919).

You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=xlmindic) to look for
fine-tuned versions on a task that interests you.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.

### How to use

To use this model you will need to first install the [Aksharamukha](https://pypi.org/project/aksharamukha/) library.

```bash
pip install aksharamukha
```

Using this library you can transliterate any text wriiten in Indic scripts in the following way:
```python
>>> from aksharamukha import transliterate
>>> text = ""चूंकि मानव परिवार के सभी सदस्यों के जन्मजात गौरव और समान तथा अविच्छिन्न अधिकार की स्वीकृति ही विश्व-शान्ति, न्याय और स्वतन्त्रता की बुनियाद है""
>>> transliterated_text = transliterate.process('autodetect', 'ISO', text)
>>> transliterated_text
""cūṁki mānava parivāra kē sabhī sadasyōṁ kē janmajāta gaurava aura samāna tathā avicchinna adhikāra kī svīkr̥ti hī viśva-śānti, nyāya aura svatantratā kī buniyāda hai""
```

Then you can use this model directly with a pipeline for masked language modeling:

```python
>>> from transformers import pipeline
>>> from aksharamukha import transliterate
>>> unmasker = pipeline('fill-mask', model='ibraheemmoosa/xlmindic-base-uniscript')
>>> text = ""রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি [MASK], ঔপন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।""
>>> transliterated_text = transliterate.process('Bengali', 'ISO', text)
>>> transliterated_text
'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli [MASK], aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama [MASK] puraskāra lābha karēna.'
>>> unmasker(transliterated_text)
[{'score': 0.39705055952072144,
  'token': 1500,
  'token_str': 'abhinētā',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli abhinētā, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'},
 {'score': 0.20499080419540405,
  'token': 3585,
  'token_str': 'kabi',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli kabi, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'},
 {'score': 0.1314290314912796,
  'token': 15402,
  'token_str': 'rājanētā',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli rājanētā, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'},
 {'score': 0.060830358415842056,
  'token': 3212,
  'token_str': 'kalākāra',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli kalākāra, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'},
 {'score': 0.035522934049367905,
  'token': 11586,
  'token_str': 'sāhityakāra',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli sāhityakāra, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'}]
```

### Limitations and bias

Even though we pretrain on a comparatively large multilingual corpus the model may exhibit harmful gender, ethnic and political bias. If you fine-tune this model on a task where these issues are important you should take special care when relying on the model to make decisions.

## Contact

Feel free to contact us if you have any ideas or if you want to know more about our models.
- Ibraheem Muhammad Moosa (ibraheemmoosa1347@gmail.com)
- Mahmud Elahi Akhter (mahmud.akhter01@northsouth.edu)
- Ashfia Binte Habib

## BibTeX entry and citation info

Coming soon!
",,,1,[],[],NLP,2022-01,271463395.2380952,,0.0,1.0,0.0,0.0,0,0.0,0,0,0,0.0,1.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,1.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
19955,xlmindic-base-uniscript,['oscar'],,28.53,MLCO2,pretraining,Not Specified,TPUv3-8 for about 180 hours or 7.5 days,,,,,,83391338.0,False,6,2,"['transformers', 'jax', 'pytorch', 'tf']",2022-07-27 05:37:04+00:00,2022-01-07 14:32:44+00:00,"
# XLMIndic Base Uniscript

This model is pretrained on a subset of the [OSCAR](https://huggingface.co/datasets/oscar) corpus spanning 14 Indo-Aryan languages. **Before pretraining this model we transliterate the text to [ISO-15919](https://en.wikipedia.org/wiki/ISO_15919) format using the [Aksharamukha](https://pypi.org/project/aksharamukha/)
library.** A demo of Aksharamukha library is hosted [here](https://aksharamukha.appspot.com/converter)
where you can transliterate your text and use it on our model on the inference widget.

## Model description

This model has the same configuration as the [ALBERT Base v2 model](https://huggingface.co/albert-base-v2/). Specifically, this model has the following configuration:

- 12 repeating layers
- 128 embedding dimension
- 768 hidden dimension
- 12 attention heads
- 11M parameters
- 512 sequence length

## Training data

This model was pretrained on the [OSCAR](https://huggingface.co/datasets/oscar) dataset which is a medium sized multilingual corpus containing text from 163 languages. We select a subset of 14 languages based on the following criteria:
 - Belongs to the [Indo-Aryan language family](https://en.wikipedia.org/wiki/Indo-Aryan_languages).
 - Uses a [Brahmic script](https://en.wikipedia.org/wiki/Brahmic_scripts).
 
These are the 14 languages we pretrain this model on:
- Assamese
- Bangla
- Bihari
- Bishnupriya Manipuri
- Goan Konkani
- Gujarati
- Hindi
- Maithili
- Marathi
- Nepali
- Oriya
- Panjabi
- Sanskrit
- Sinhala

## Transliteration

*The unique component of this model is that it takes in ISO-15919 transliterated text.*

The motivation behind this is this. When two languages share vocabularies, a machine learning model can exploit that to learn good cross-lingual representations. However if these two languages use different writing scripts it is difficult for a model to make the connection. Thus if if we can write the two languages in a single script then it is easier for the model to learn good cross-lingual representation.

For many of the scripts currently in use, there are standard transliteration schemes to convert to the Latin script. In particular, for the Indic scripts the ISO-15919 transliteration scheme is designed to consistently transliterate texts written in different Indic scripts to the Latin script.

An example of ISO-15919 transliteration for a piece of **Bangla** text is the following:

**Original:** ""রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি কবি, ঔপন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক।""

**Transliterated:** 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli kabi, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika.'

Another example for a piece of **Hindi** text is the following:

**Original:** ""चूंकि मानव परिवार के सभी सदस्यों के जन्मजात गौरव और समान तथा अविच्छिन्न अधिकार की स्वीकृति ही विश्व-शान्ति, न्याय और स्वतन्त्रता की बुनियाद है""

**Transliterated:** ""cūṁki mānava parivāra kē sabhī sadasyōṁ kē janmajāta gaurava aura samāna tathā avicchinna adhikāra kī svīkr̥ti hī viśva-śānti, nyāya aura svatantratā kī buniyāda hai""
 
 
## Training procedure

### Preprocessing

The texts are transliterated to ISO-15919 format using the Aksharamukha library. Then these are tokenized using SentencePiece and a vocabulary size of 50,000. The inputs of the model are
then of the form:
```
[CLS] Sentence A [SEP] Sentence B [SEP]
```

### Training

Training objective is the same as the original ALBERT. 
.
The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.

The details of the sentence order prediction example generation procedure for each sentence are the following:
- Split the sentence into two parts A and B at a random index.
- With 50% probability swap the two parts.  

The model was pretrained on TPUv3-8 for 1M steps. We have checkpoints available at every 100k pretraining steps. These are available at different branches of this repository. You can load these checkpoints by passing the `revision` parameter. For example to load the checkpoint at 500k you can use the following code.

```python
>>> AutoModel.from_pretrained('ibraheemmoosa/xlmindic-base-uniscript', revision='checkpoint_500k')
```

## Evaluation results
We evaluated this model on the Indo-Aryan subset of languages (Panjabi, Oriya, Assamese, Bangla, Hindi, Marathi, Gujarati) from the [IndicGLUE](https://huggingface.co/datasets/indic_glue) benchmark dataset. We report the mean and standard deviation of nine fine-tuning runs for this model. We compare with an [ablation model](https://huggingface.co/ibraheemmoosa/xlmindic-base-multiscript) that do not use transliteration and is instead trained on original scripts.

### IndicGLUE
Task | mBERT | XLM-R | IndicBERT-Base | XLMIndic-Base-Uniscript (This Model) | XLMIndic-Base-Multiscript (Ablation Model)
-----| ----- | ----- | ------ | ------- | --------
Wikipedia Section Title Prediction | 71.90 | 65.45 | 69.40 | **81.78 ± 0.60** | 77.17 ± 0.76
Article Genre Classification | 88.64 | 96.61 | 97.72 | **98.70 ± 0.29** | 98.30 ± 0.26
Named Entity Recognition (F1-score) | 71.29 | 62.18 | 56.69 | **89.85 ± 1.14** |  83.19 ± 1.58
BBC Hindi News Article Classification | 60.55 | 75.52 | 74.60 | **79.14 ± 0.60** | 77.28 ± 1.50
Soham Bangla News Article Classification | 80.23 | 87.6 | 78.45 | **93.89 ± 0.48** | 93.22 ± 0.49
INLTK Gujarati Headlines Genre Classification | - | - | **92.91** | 90.73 ± 0.75 | 90.41 ± 0.69
INLTK Marathi Headlines Genre Classification | - | - | **94.30** | 92.04 ± 0.47 | 92.21 ± 0.23
IITP Hindi Product Reviews Sentiment Classification | 74.57 | **78.97** | 71.32 | 77.18 ± 0.77 | 76.33 ± 0.84
IITP Hindi Movie Reviews Sentiment Classification | 56.77 | 61.61 | 59.03 | **66.34 ± 0.16** | 65.91 ± 2.20
MIDAS Hindi Discourse Type Classification | 71.20 | **79.94** | 78.44 | 78.54 ± 0.91 | 78.39 ± 0.33
Cloze Style Question Answering (Fill-mask task) | - | - | 37.16 | **41.54** | 38.21

## Intended uses & limitations

This model is pretrained on Indo-Aryan languages. Thus it is intended to be used for downstream tasks on these languages. However, since Dravidian languages such as Malayalam, Telegu, Kannada etc share a lot of vocabulary with the Indo-Aryan languages, this model can potentially be used on those languages too (after transliterating the text to ISO-15919).

You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=xlmindic) to look for
fine-tuned versions on a task that interests you.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.

### How to use

To use this model you will need to first install the [Aksharamukha](https://pypi.org/project/aksharamukha/) library.

```bash
pip install aksharamukha
```

Using this library you can transliterate any text wriiten in Indic scripts in the following way:
```python
>>> from aksharamukha import transliterate
>>> text = ""चूंकि मानव परिवार के सभी सदस्यों के जन्मजात गौरव और समान तथा अविच्छिन्न अधिकार की स्वीकृति ही विश्व-शान्ति, न्याय और स्वतन्त्रता की बुनियाद है""
>>> transliterated_text = transliterate.process('autodetect', 'ISO', text)
>>> transliterated_text
""cūṁki mānava parivāra kē sabhī sadasyōṁ kē janmajāta gaurava aura samāna tathā avicchinna adhikāra kī svīkr̥ti hī viśva-śānti, nyāya aura svatantratā kī buniyāda hai""
```

Then you can use this model directly with a pipeline for masked language modeling:

```python
>>> from transformers import pipeline
>>> from aksharamukha import transliterate
>>> unmasker = pipeline('fill-mask', model='ibraheemmoosa/xlmindic-base-uniscript')
>>> text = ""রবীন্দ্রনাথ ঠাকুর এফআরএএস (৭ মে ১৮৬১ - ৭ আগস্ট ১৯৪১; ২৫ বৈশাখ ১২৬৮ - ২২ শ্রাবণ ১৩৪৮ বঙ্গাব্দ) ছিলেন অগ্রণী বাঙালি [MASK], ঔপন্যাসিক, সংগীতস্রষ্টা, নাট্যকার, চিত্রকর, ছোটগল্পকার, প্রাবন্ধিক, অভিনেতা, কণ্ঠশিল্পী ও দার্শনিক। ১৯১৩ সালে গীতাঞ্জলি কাব্যগ্রন্থের ইংরেজি অনুবাদের জন্য তিনি এশীয়দের মধ্যে সাহিত্যে প্রথম নোবেল পুরস্কার লাভ করেন।""
>>> transliterated_text = transliterate.process('Bengali', 'ISO', text)
>>> transliterated_text
'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli [MASK], aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama [MASK] puraskāra lābha karēna.'
>>> unmasker(transliterated_text)
[{'score': 0.39705055952072144,
  'token': 1500,
  'token_str': 'abhinētā',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli abhinētā, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'},
 {'score': 0.20499080419540405,
  'token': 3585,
  'token_str': 'kabi',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli kabi, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'},
 {'score': 0.1314290314912796,
  'token': 15402,
  'token_str': 'rājanētā',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli rājanētā, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'},
 {'score': 0.060830358415842056,
  'token': 3212,
  'token_str': 'kalākāra',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli kalākāra, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'},
 {'score': 0.035522934049367905,
  'token': 11586,
  'token_str': 'sāhityakāra',
  'sequence': 'rabīndranātha ṭhākura ēphaāraēēsa (7 mē 1861 - 7 āgasṭa 1941; 25 baiśākha 1268 - 22 śrābaṇa 1348 baṅgābda) chilēna agraṇī bāṅāli sāhityakāra, aupanyāsika, saṁgītasraṣṭā, nāṭyakāra, citrakara, chōṭagalpakāra, prābandhika, abhinētā, kaṇṭhaśilpī ō dārśanika. 1913 sālē gītāñjali kābyagranthēra iṁrēji anubādēra janya tini ēśīẏadēra madhyē sāhityē prathama nōbēla puraskāra lābha karēna.'}]
```

### Limitations and bias

Even though we pretrain on a comparatively large multilingual corpus the model may exhibit harmful gender, ethnic and political bias. If you fine-tune this model on a task where these issues are important you should take special care when relying on the model to make decisions.

## Contact

Feel free to contact us if you have any ideas or if you want to know more about our models.
- Ibraheem Muhammad Moosa (ibraheemmoosa1347@gmail.com)
- Mahmud Elahi Akhter (mahmud.akhter01@northsouth.edu)
- Ashfia Binte Habib

## BibTeX entry and citation info

```bibtex
@article{Moosa2022DoesTH,
  title={Does Transliteration Help Multilingual Language Modeling?},
  author={Ibraheem Muhammad Moosa and Mahmuda Akhter and Ashfia Binte Habib},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.12501}
}
```",,,1,[],[],NLP,2022-01,2922935.085874518,,0.0,1.0,0.0,0.0,0,0.0,0,0,0,0.0,1.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,1.0,0.0,0.0,0,0,0.0,1.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20223,it5-base-formal-to-informal,['yahoo/xformal_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,990280781.0,False,5,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:45:49+00:00,2021-11-02 11:21:30+00:00,"
# IT5 Base for Formal-to-informal Style Transfer 🤗

This repository contains the checkpoint for the [IT5 Base](https://huggingface.co/gsarti/it5-base) model fine-tuned on Formal-to-informal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

f2i = pipeline(""text2text-generation"", model='it5/it5-base-formal-to-informal')
f2i(""Vi ringrazio infinitamente per vostra disponibilità"")
>>> [{""generated_text"": ""e grazie per la vostra disponibilità!""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-base-formal-to-informal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-base-formal-to-informal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2021-11,58251810.64705882,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20224,it5-base-headline-generation,['gsarti/change_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,990280781.0,False,19,1,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:07:05+00:00,2021-11-02 10:15:01+00:00,"# IT5 Base for News Headline Generation 🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Base](https://huggingface.co/gsarti/it5-base) model fine-tuned on news headline generation on the Italian HeadGen-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

hg = pipeline(""text2text-generation"", model='it5/it5-base-headline-generation')
hg(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-base-headline-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-base-headline-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2021-11,58251810.64705882,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20225,it5-base-ilgiornale-to-repubblica,['gsarti/change_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,990280781.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:04:46+00:00,2022-01-18 09:47:48+00:00,"# IT5 Base for News Headline Style Transfer (Il Giornale to Repubblica) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Base](https://huggingface.co/gsarti/it5-base) model fine-tuned on news headline style transfer in the Il Giornale to Repubblica direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Repubblica from the full body of an article written in the style of Il Giornale. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

g2r = pipeline(""text2text-generation"", model='it5/it5-base-ilgiornale-to-repubblica')
g2r(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-base-ilgiornale-to-repubblica"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-base-ilgiornale-to-repubblica"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58251810.64705882,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20226,it5-base-informal-to-formal,['yahoo/xformal_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,990280781.0,False,5,1,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-17 09:52:48+00:00,2021-11-02 11:21:47+00:00,"
# IT5 Base for Informal-to-formal Style Transfer 🧐

This repository contains the checkpoint for the [IT5 Base](https://huggingface.co/gsarti/it5-base) model fine-tuned on Informal-to-formal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

i2f = pipeline(""text2text-generation"", model='it5/it5-base-informal-to-formal')
i2f(""nn capisco xke tt i ragazzi lo fanno"")
>>> [{""generated_text"": ""non comprendo perché tutti i ragazzi agiscono così""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-base-informal-to-formal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-base-informal-to-formal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2021-11,58251810.64705882,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20227,it5-base-news-summarization,"['ARTeLab/fanpage', 'ARTeLab/ilpost']",,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,990284749.0,False,4410,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-10-18 13:43:57+00:00,2022-02-23 14:07:18+00:00,"# IT5 Base for News Summarization ✂️🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Base](https://huggingface.co/gsarti/it5-base) model fine-tuned on news summarization on the [Fanpage](https://huggingface.co/datasets/ARTeLab/fanpage) and [Il Post](https://huggingface.co/datasets/ARTeLab/ilpost) corpora as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

newsum = pipeline(""summarization"", model='it5/it5-base-news-summarization')
newsum(""Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di «una sorta di Netflix della cultura», pensata per «offrire a tutto il mondo la cultura italiana a pagamento». È presto per dare giudizi definitivi sulla piattaforma, e di certo sarà difficile farlo anche più avanti senza numeri precisi. Al momento, l’unica cosa che si può fare è guardare com’è fatto il sito, contare quanti contenuti ci sono (circa 700 “titoli”, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietà. Intanto, una cosa notata da più parti è che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente."")
>>> [{""generated_text"": ""ITsART, la Netflix della cultura italiana, parte da maggio. Film, documentari, spettacoli teatrali e musicali disponibili sul nuovo sito a pagamento.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-base-news-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-base-news-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-02,58252044.058823526,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20228,it5-base-question-answering,['squad_it'],58723160.0,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,990280781.0,False,245,1,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:05:47+00:00,2021-11-02 13:27:53+00:00,"# IT5 Base for Question Answering ⁉️ 🇮🇹

This repository contains the checkpoint for the [IT5 Base](https://huggingface.co/gsarti/it5-base) model fine-tuned on extractive question answering on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qa = pipeline(""text2text-generation"", model='it5/it5-base-question-answering')
qa(""In seguito all' evento di estinzione del Cretaceo-Paleogene, l' estinzione dei dinosauri e il clima umido possono aver permesso alla foresta pluviale tropicale di diffondersi in tutto il continente. Dal 66-34 Mya, la foresta pluviale si estendeva fino a sud fino a 45°. Le fluttuazioni climatiche degli ultimi 34 milioni di anni hanno permesso alle regioni della savana di espandersi fino ai tropici. Durante l' Oligocene, ad esempio, la foresta pluviale ha attraversato una banda relativamente stretta. Si espandeva di nuovo durante il Miocene medio, poi si ritrasse ad una formazione prevalentemente interna all' ultimo massimo glaciale. Tuttavia, la foresta pluviale è riuscita ancora a prosperare durante questi periodi glaciali, consentendo la sopravvivenza e l' evoluzione di un' ampia varietà di specie. Domanda: La foresta pluviale amazzonica è diventata per lo più una foresta interna intorno a quale evento globale?"")
>>> [{""generated_text"": ""ultimo massimo glaciale""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-base-question-answering"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-base-question-answering"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2021-11,58251810.64705882,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20229,it5-base-question-generation,['squad_it'],58723160.0,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,990280781.0,False,23,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:06:11+00:00,2021-10-28 11:42:21+00:00,"# IT5 Base for Question Generation 💭 🇮🇹

This repository contains the checkpoint for the [IT5 Base](https://huggingface.co/gsarti/it5-base) model fine-tuned on question generation on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qg = pipeline(""text2text-generation"", model='it5/it5-base-question-generation')
qg(""Le conoscenze mediche erano stagnanti durante il Medioevo. Il resoconto più autorevole di allora è venuto dalla facoltà di medicina di Parigi in un rapporto al re di Francia che ha incolpato i cieli, sotto forma di una congiunzione di tre pianeti nel 1345 che causò una ""grande pestilenza nell\' aria"". Questa relazione è diventata la prima e più diffusa di una serie di casi di peste che cercava di dare consigli ai malati. Che la peste fosse causata dalla cattiva aria divenne la teoria più accettata. Oggi, questo è conosciuto come la teoria di Miasma. La parola ""peste"" non aveva un significato particolare in questo momento, e solo la ricorrenza dei focolai durante il Medioevo gli diede il nome che è diventato il termine medico. Risposta: re di Francia"")
>>> [{""generated_text"": ""Per chi è stato redatto il referto medico?""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-base-question-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-base-question-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2021-10,58251810.64705882,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20230,it5-base-repubblica-to-ilgiornale,['gsarti/change_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,990280781.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:05:15+00:00,2022-01-18 16:13:54+00:00,"# IT5 Base for News Headline Style Transfer (Repubblica to Il Giornale) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Base](https://huggingface.co/gsarti/it5-base) model fine-tuned on news headline style transfer in the Repubblica to Il Giornale direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Il Giornale from the full body of an article written in the style of Repubblica. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

r2g = pipeline(""text2text-generation"", model='it5/it5-base-repubblica-to-ilgiornale')
r2g(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-base-repubblica-to-ilgiornale"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-base-repubblica-to-ilgiornale"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58251810.64705882,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20231,it5-base-wiki-summarization,['wits'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,990280781.0,False,39,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:06:40+00:00,2022-01-17 08:37:05+00:00,"# IT5 Base for Wikipedia Summarization 📑 🇮🇹

This repository contains the checkpoint for the [IT5 Base](https://huggingface.co/gsarti/it5-base) model fine-tuned on Wikipedia summarization on the [WITS](https://www.semanticscholar.org/paper/WITS%3A-Wikipedia-for-Italian-Text-Summarization-Casola-Lavelli/ad6c83122e721c7c0db4a40727dac3b4762cd2b1) dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

hg = pipeline(""text2text-generation"", model='it5/it5-base-wiki-summarization')
hg(""Le dimensioni dell'isola sono di 8 km di lunghezza e di 3,2 km di larghezza.  Si trova a 1,6 km a sud-est dell'isola di Renaud, dalla quale è separata dal passaggio Rodman. La sua altezza è di 100 m.  Fu scoperta dall'esploratore e baleniere britannico John Biscoe nel 1832 e venne mappata durante una spedizione antartica francese realizzata nel primo decennio del XX secolo. Al comando della spedizione era Jean-Baptiste Charcot e il nome fu scelto per onorare l'esploratore e geografo francese Charles Rabot.  === Rivendicazioni territoriali === * Secondo l'Argentina appartiene al dipartimento dell'Antartide Argentina nella provincia della Terra del Fuoco.  * Secondo il Cile appartiene al comune antartico della provincia cilena antartica nella regione di Magallanes e dell'Antartico cileno. * Secondo il Regno Unito fa parte del territorio antartico britannico.   Per il Trattato Antartico tali rivendicazioni sono sospese.  Sull'isola è presente il rifugio Guillochon, sito storico antartico. ""
- text: ""Vanni ha la sua prima mostra personale nel 1948, alla Galleria Margherita di Roma. Nel 1949 vince una borsa di studio che lo porterà a studiare ad Amsterdam sotto la guida del pittore neoplastico Friedrich Vordemberge-Gildewart. Nel 1952 vince una Fulbright Scholarship che lo porterà a studiare in America, alla Yale University, sotto la guida di Josef Albers.   Dal 1953 al 1960 si stabilisce a Parigi, dove illustra alcuni libri per bambini che in seguito vinceranno il premio del Club des Editeurs. Nel 1954 lavora come consulente del colore per il documentario su Picasso di Luciano Emmer, e nel 1955 comincia la sua lunga collaborazione con la Galleria Schneider, affiancando artisti come Corrado Cagli. Dal 1969 al 1974 lavora su dei bassorilievi in vetro resina sui quali vengono proiettati dei film astratti da lui creati, per creare dei quadri che si trasformino continuamente nel tempo.   Nel 1979 lascia Roma per stabilirsi a New York, dove alla carriera di pittore affiancherà quella di professore per la prestigiosa Cooper Union School of Art, dove insegnerà ininterrottamente dal 1984 al 2014.   L'opera pittorica di Vanni è segnata da una visione estremamente personale, lontana dalle correnti e dai movimenti che hanno caratterizzato la seconda metà del XX secolo. Memore delle lunghe conversazioni avute da Vanni nella sua primissima gioventù, con il filosofo e pittore futurista Alberto Bragaglia, le sue opere sono contrassegnate da un “eclettismo” formale programmatico, alla base del quale resta costante una conoscenza profonda delle molteplici tecniche artistiche utilizzate (tra cui il mosaico, l’affresco e la tempera ad uovo). Pur esprimendosi per lo più in cicli di opere dove l’astrazione formale è la principale componente figurativa, sono da sottolineare alcune opere dove Vanni ha dato prova di una importante padronanza dell’arte figurativa. Importanti e numerose sono le sue realizzazioni anche nel campo dell’illustrazione. Sue sono le illustrazioni per la novella ''Agostino'' di Alberto Moravia, per il libro ''Love'' di Lowell A. Siff e delle ''Contes de Cristal'' di Alice Coléno.  Ha tenuto mostre personali in Italia e all’estero ed esposto in mostre collettive di rappresentanza italiana nei musei e nelle gallerie di ogni parte del mondo."")
>>> [{""generated_text"": ""L' '''isola di Rabot''' si trova in prossimità dell'isola di Renaud, a sud dell'Argentina.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-base-wiki-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-base-wiki-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58251810.64705882,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20232,it5-large-formal-to-informal,['yahoo/xformal_it'],,51.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,3132640293.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:46:17+00:00,2022-01-14 11:04:57+00:00,"
# IT5 Large for Formal-to-informal Style Transfer 🤗

This repository contains the checkpoint for the [IT5 Large](https://huggingface.co/gsarti/it5-large) model fine-tuned on Formal-to-informal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

f2i = pipeline(""text2text-generation"", model='it5/it5-large-formal-to-informal')
f2i(""Vi ringrazio infinitamente per vostra disponibilità"")
>>> [{""generated_text"": ""e grazie per la vostra disponibilità!""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-large-formal-to-informal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-large-formal-to-informal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,61424319.47058824,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20233,it5-large-headline-generation,['gsarti/change_it'],,51.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,3132640293.0,False,19,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:59:47+00:00,2022-01-16 11:24:51+00:00,"# IT5 Large for News Headline Generation 📣 🇮🇹

This repository contains the checkpoint for the [IT5 Large](https://huggingface.co/gsarti/it5-large) model fine-tuned on news headline generation on the Italian HeadGen-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

hg = pipeline(""text2text-generation"", model='it5/it5-large-headline-generation')
hg(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-large-headline-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-large-headline-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,61424319.47058824,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20234,it5-large-ilgiornale-to-repubblica,['gsarti/change_it'],,51.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,3132640293.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:04:16+00:00,2022-01-18 16:12:56+00:00,"# IT5 Large for News Headline Style Transfer (Il Giornale to Repubblica) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Large](https://huggingface.co/gsarti/it5-large) model fine-tuned on news headline style transfer in the Il Giornale to Repubblica direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Repubblica from the full body of an article written in the style of Il Giornale. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

g2r = pipeline(""text2text-generation"", model='it5/it5-large-ilgiornale-to-repubblica')
g2r(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-large-ilgiornale-to-repubblica"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-large-ilgiornale-to-repubblica"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,61424319.47058824,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20235,it5-large-informal-to-formal,['yahoo/xformal_it'],,51.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,3132640293.0,False,7,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:48:09+00:00,2022-01-14 10:59:36+00:00,"
# IT5 Base for Informal-to-formal Style Transfer 🧐

This repository contains the checkpoint for the [IT5 Large](https://huggingface.co/gsarti/it5-large) model fine-tuned on Informal-to-formal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

i2f = pipeline(""text2text-generation"", model='it5/it5-large-informal-to-formal')
i2f(""nn capisco xke tt i ragazzi lo fanno"")
>>> [{""generated_text"": ""non comprendo perché tutti i ragazzi agiscono così""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-large-informal-to-formal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-large-informal-to-formal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,61424319.47058824,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20236,it5-large-news-summarization,"['ARTeLab/fanpage', 'ARTeLab/ilpost']",,51.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,3132640293.0,False,924,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:53:26+00:00,2022-01-16 18:03:29+00:00,"# IT5 Large for News Summarization ✂️🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Large](https://huggingface.co/gsarti/it5-large) model fine-tuned on news summarization on the [Fanpage](https://huggingface.co/datasets/ARTeLab/fanpage) and [Il Post](https://huggingface.co/datasets/ARTeLab/ilpost) corpora as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

newsum = pipeline(""summarization"", model='it5/it5-large-news-summarization')
newsum(""Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di «una sorta di Netflix della cultura», pensata per «offrire a tutto il mondo la cultura italiana a pagamento». È presto per dare giudizi definitivi sulla piattaforma, e di certo sarà difficile farlo anche più avanti senza numeri precisi. Al momento, l’unica cosa che si può fare è guardare com’è fatto il sito, contare quanti contenuti ci sono (circa 700 “titoli”, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietà. Intanto, una cosa notata da più parti è che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente."")
>>> [{""generated_text"": ""ITsART, la Netflix della cultura italiana, parte da maggio. Film, documentari, spettacoli teatrali e musicali disponibili sul nuovo sito a pagamento.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-large-news-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-large-news-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,61424319.47058824,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20237,it5-large-question-answering,['squad_it'],58723160.0,51.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,3132640293.0,False,319,2,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:57:53+00:00,2022-01-16 15:26:55+00:00,"# IT5 Large for Question Answering ⁉️ 🇮🇹

This repository contains the checkpoint for the [IT5 Large](https://huggingface.co/gsarti/it5-large) model fine-tuned on extractive question answering on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qa = pipeline(""text2text-generation"", model='it5/it5-large-question-answering')
qa(""In seguito all' evento di estinzione del Cretaceo-Paleogene, l' estinzione dei dinosauri e il clima umido possono aver permesso alla foresta pluviale tropicale di diffondersi in tutto il continente. Dal 66-34 Mya, la foresta pluviale si estendeva fino a sud fino a 45°. Le fluttuazioni climatiche degli ultimi 34 milioni di anni hanno permesso alle regioni della savana di espandersi fino ai tropici. Durante l' Oligocene, ad esempio, la foresta pluviale ha attraversato una banda relativamente stretta. Si espandeva di nuovo durante il Miocene medio, poi si ritrasse ad una formazione prevalentemente interna all' ultimo massimo glaciale. Tuttavia, la foresta pluviale è riuscita ancora a prosperare durante questi periodi glaciali, consentendo la sopravvivenza e l' evoluzione di un' ampia varietà di specie. Domanda: La foresta pluviale amazzonica è diventata per lo più una foresta interna intorno a quale evento globale?"")
>>> [{""generated_text"": ""ultimo massimo glaciale""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-large-question-answering"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-large-question-answering"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,61424319.47058824,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20238,it5-large-question-generation,['squad_it'],58723160.0,51.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,3132640293.0,False,314,1,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:56:40+00:00,2022-01-16 11:25:32+00:00,"# IT5 Large for Question Generation 💭 🇮🇹

This repository contains the checkpoint for the [IT5 Large](https://huggingface.co/gsarti/it5-large) model fine-tuned on question generation on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qg = pipeline(""text2text-generation"", model='it5/it5-large-question-generation')
qg(""Le conoscenze mediche erano stagnanti durante il Medioevo. Il resoconto più autorevole di allora è venuto dalla facoltà di medicina di Parigi in un rapporto al re di Francia che ha incolpato i cieli, sotto forma di una congiunzione di tre pianeti nel 1345 che causò una ""grande pestilenza nell\' aria"". Questa relazione è diventata la prima e più diffusa di una serie di casi di peste che cercava di dare consigli ai malati. Che la peste fosse causata dalla cattiva aria divenne la teoria più accettata. Oggi, questo è conosciuto come la teoria di Miasma. La parola ""peste"" non aveva un significato particolare in questo momento, e solo la ricorrenza dei focolai durante il Medioevo gli diede il nome che è diventato il termine medico. Risposta: re di Francia"")
>>> [{""generated_text"": ""Per chi è stato redatto il referto medico?""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-large-question-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-large-question-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,61424319.47058824,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20239,it5-large-repubblica-to-ilgiornale,['gsarti/change_it'],,51.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,3132640293.0,False,7,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:01:50+00:00,2022-01-18 16:14:46+00:00,"# IT5 Large for News Headline Style Transfer (Repubblica to Il Giornale) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Large](https://huggingface.co/gsarti/it5-large) model fine-tuned on news headline style transfer in the Repubblica to Il Giornale direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Il Giornale from the full body of an article written in the style of Repubblica. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

r2g = pipeline(""text2text-generation"", model='it5/it5-large-repubblica-to-ilgiornale')
r2g(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-large-repubblica-to-ilgiornale"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-large-repubblica-to-ilgiornale"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,61424319.47058824,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20240,it5-large-wiki-summarization,['wits'],,51.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,3132640293.0,False,68,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:49:56+00:00,2022-01-17 08:37:28+00:00,"# IT5 Large for Wikipedia Summarization ✂️📑 🇮🇹

This repository contains the checkpoint for the [IT5 Large](https://huggingface.co/gsarti/it5-large) model fine-tuned on Wikipedia summarization on the [WITS](https://www.semanticscholar.org/paper/WITS%3A-Wikipedia-for-Italian-Text-Summarization-Casola-Lavelli/ad6c83122e721c7c0db4a40727dac3b4762cd2b1) dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

wikisum = pipeline(""summarization"", model='it5/it5-large-wiki-summarization')
wikisum(""Le dimensioni dell'isola sono di 8 km di lunghezza e di 3,2 km di larghezza.  Si trova a 1,6 km a sud-est dell'isola di Renaud, dalla quale è separata dal passaggio Rodman. La sua altezza è di 100 m.  Fu scoperta dall'esploratore e baleniere britannico John Biscoe nel 1832 e venne mappata durante una spedizione antartica francese realizzata nel primo decennio del XX secolo. Al comando della spedizione era Jean-Baptiste Charcot e il nome fu scelto per onorare l'esploratore e geografo francese Charles Rabot.  === Rivendicazioni territoriali === * Secondo l'Argentina appartiene al dipartimento dell'Antartide Argentina nella provincia della Terra del Fuoco.  * Secondo il Cile appartiene al comune antartico della provincia cilena antartica nella regione di Magallanes e dell'Antartico cileno. * Secondo il Regno Unito fa parte del territorio antartico britannico.   Per il Trattato Antartico tali rivendicazioni sono sospese.  Sull'isola è presente il rifugio Guillochon, sito storico antartico. "")
>>> [{""generated_text"": ""L' '''isola di Rabot''' si trova in prossimità dell'isola di Renaud, a sud dell'Argentina.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-large-wiki-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-large-wiki-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,61424319.47058824,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20241,it5-small-formal-to-informal,['yahoo/xformal_it'],,8.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,307824645.0,False,6,0,"['transformers', 'jax', 'pytorch', 'tf']",2022-03-09 07:45:14+00:00,2022-01-13 12:11:18+00:00,"
# IT5 Small for Formal-to-informal Style Transfer 🤗

This repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on Formal-to-informal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io).  

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

f2i = pipeline(""text2text-generation"", model='it5/it5-small-formal-to-informal')
f2i(""Vi ringrazio infinitamente per vostra disponibilità"")
>>> [{""generated_text"": ""e grazie per la vostra disponibilità!""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-small-formal-to-informal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-small-formal-to-informal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,38478080.625,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20242,it5-small-headline-generation,['gsarti/change_it'],,8.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,307824645.0,False,12,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:00:22+00:00,2022-01-16 11:25:16+00:00,"# IT5 Small for News Headline Generation 📣 🇮🇹

This repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on news headline generation on the Italian HeadGen-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

hg = pipeline(""text2text-generation"", model='it5/it5-small-headline-generation')
hg(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-small-headline-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-small-headline-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,38478080.625,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20243,it5-small-ilgiornale-to-repubblica,['gsarti/change_it'],,8.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,307824645.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:03:52+00:00,2022-01-18 09:47:30+00:00,"# IT5 Small for News Headline Style Transfer (Il Giornale to Repubblica) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on news headline style transfer in the Il Giornale to Repubblica direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Repubblica from the full body of an article written in the style of Il Giornale. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

g2r = pipeline(""text2text-generation"", model='it5/it5-small-ilgiornale-to-repubblica')
g2r(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-small-ilgiornale-to-repubblica"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-small-ilgiornale-to-repubblica"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,38478080.625,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20244,it5-small-informal-to-formal,['yahoo/xformal_it'],,8.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,307824645.0,False,7,0,"['transformers', 'jax', 'pytorch', 'tf']",2022-03-09 07:47:36+00:00,2022-01-13 12:13:50+00:00,"
# IT5 Small for Informal-to-formal Style Transfer 🧐

This repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on Informal-to-formal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

i2f = pipeline(""text2text-generation"", model='it5/it5-small-informal-to-formal')
i2f(""nn capisco xke tt i ragazzi lo fanno"")
>>> [{""generated_text"": ""non comprendo perché tutti i ragazzi agiscono così""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-small-informal-to-formal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-small-informal-to-formal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,38478080.625,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20245,it5-small-news-summarization,"['ARTeLab/fanpage', 'ARTeLab/ilpost']",,8.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,307824645.0,False,15,1,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:52:53+00:00,2022-01-16 18:03:02+00:00,"# IT5 Small for News Summarization ✂️🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on news summarization on the [Fanpage](https://huggingface.co/datasets/ARTeLab/fanpage) and [Il Post](https://huggingface.co/datasets/ARTeLab/ilpost) corpora as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

newsum = pipeline(""summarization"", model='it5/it5-small-news-summarization')
newsum(""Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di «una sorta di Netflix della cultura», pensata per «offrire a tutto il mondo la cultura italiana a pagamento». È presto per dare giudizi definitivi sulla piattaforma, e di certo sarà difficile farlo anche più avanti senza numeri precisi. Al momento, l’unica cosa che si può fare è guardare com’è fatto il sito, contare quanti contenuti ci sono (circa 700 “titoli”, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietà. Intanto, una cosa notata da più parti è che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente."")
>>> [{""generated_text"": ""ITsART, la Netflix della cultura italiana, parte da maggio. Film, documentari, spettacoli teatrali e musicali disponibili sul nuovo sito a pagamento.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-small-news-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-small-news-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,38478080.625,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20246,it5-small-question-answering,['squad_it'],58723160.0,8.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,307824645.0,False,13,1,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:58:17+00:00,2022-01-16 15:27:15+00:00,"# IT5 Small for Question Answering ⁉️ 🇮🇹

This repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on extractive question answering on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qa = pipeline(""text2text-generation"", model='it5/it5-small-question-answering')
qa(""In seguito all' evento di estinzione del Cretaceo-Paleogene, l' estinzione dei dinosauri e il clima umido possono aver permesso alla foresta pluviale tropicale di diffondersi in tutto il continente. Dal 66-34 Mya, la foresta pluviale si estendeva fino a sud fino a 45°. Le fluttuazioni climatiche degli ultimi 34 milioni di anni hanno permesso alle regioni della savana di espandersi fino ai tropici. Durante l' Oligocene, ad esempio, la foresta pluviale ha attraversato una banda relativamente stretta. Si espandeva di nuovo durante il Miocene medio, poi si ritrasse ad una formazione prevalentemente interna all' ultimo massimo glaciale. Tuttavia, la foresta pluviale è riuscita ancora a prosperare durante questi periodi glaciali, consentendo la sopravvivenza e l' evoluzione di un' ampia varietà di specie. Domanda: La foresta pluviale amazzonica è diventata per lo più una foresta interna intorno a quale evento globale?"")
>>> [{""generated_text"": ""ultimo massimo glaciale""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-small-question-answering"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-small-question-answering"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,38478080.625,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20247,it5-small-question-generation,['squad_it'],58723160.0,8.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,307824645.0,False,6,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:55:38+00:00,2022-01-16 11:25:46+00:00,"# IT5 Small for Question Generation 💭 🇮🇹

This repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on question generation on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qg = pipeline(""text2text-generation"", model='it5/it5-small-question-generation')
qg(""Le conoscenze mediche erano stagnanti durante il Medioevo. Il resoconto più autorevole di allora è venuto dalla facoltà di medicina di Parigi in un rapporto al re di Francia che ha incolpato i cieli, sotto forma di una congiunzione di tre pianeti nel 1345 che causò una ""grande pestilenza nell\' aria"". Questa relazione è diventata la prima e più diffusa di una serie di casi di peste che cercava di dare consigli ai malati. Che la peste fosse causata dalla cattiva aria divenne la teoria più accettata. Oggi, questo è conosciuto come la teoria di Miasma. La parola ""peste"" non aveva un significato particolare in questo momento, e solo la ricorrenza dei focolai durante il Medioevo gli diede il nome che è diventato il termine medico. Risposta: re di Francia"")
>>> [{""generated_text"": ""Per chi è stato redatto il referto medico?""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-small-question-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-small-question-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,38478080.625,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20248,it5-small-repubblica-to-ilgiornale,['gsarti/change_it'],,8.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,307824645.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:02:27+00:00,2022-01-18 16:13:11+00:00,"# IT5 Small for News Headline Style Transfer (Repubblica to Il Giornale) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on news headline style transfer in the Repubblica to Il Giornale direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Il Giornale from the full body of an article written in the style of Repubblica. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

r2g = pipeline(""text2text-generation"", model='it5/it5-small-repubblica-to-ilgiornale')
r2g(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-small-repubblica-to-ilgiornale"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-small-repubblica-to-ilgiornale"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,38478080.625,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20249,it5-small-wiki-summarization,['wits'],,8.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,307824645.0,False,50,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:50:42+00:00,2022-01-17 08:36:45+00:00,"# IT5 Small for Wikipedia Summarization ✂️📑 🇮🇹

This repository contains the checkpoint for the [IT5 Small](https://huggingface.co/gsarti/it5-small) model fine-tuned on Wikipedia summarization on the [WITS](https://www.semanticscholar.org/paper/WITS%3A-Wikipedia-for-Italian-Text-Summarization-Casola-Lavelli/ad6c83122e721c7c0db4a40727dac3b4762cd2b1) dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

wikisum = pipeline(""summarization"", model='it5/it5-small-wiki-summarization')
wikisum(""Le dimensioni dell'isola sono di 8 km di lunghezza e di 3,2 km di larghezza.  Si trova a 1,6 km a sud-est dell'isola di Renaud, dalla quale è separata dal passaggio Rodman. La sua altezza è di 100 m.  Fu scoperta dall'esploratore e baleniere britannico John Biscoe nel 1832 e venne mappata durante una spedizione antartica francese realizzata nel primo decennio del XX secolo. Al comando della spedizione era Jean-Baptiste Charcot e il nome fu scelto per onorare l'esploratore e geografo francese Charles Rabot.  === Rivendicazioni territoriali === * Secondo l'Argentina appartiene al dipartimento dell'Antartide Argentina nella provincia della Terra del Fuoco.  * Secondo il Cile appartiene al comune antartico della provincia cilena antartica nella regione di Magallanes e dell'Antartico cileno. * Secondo il Regno Unito fa parte del territorio antartico britannico.   Per il Trattato Antartico tali rivendicazioni sono sospese.  Sull'isola è presente il rifugio Guillochon, sito storico antartico. "")
>>> [{""generated_text"": ""L' '''isola di Rabot''' si trova in prossimità dell'isola di Renaud, a sud dell'Argentina.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/it5-small-wiki-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/it5-small-wiki-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,38478080.625,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20250,mt5-base-formal-to-informal,['yahoo/xformal_it'],,40.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,2329728077.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:44:08+00:00,2022-01-21 08:58:27+00:00,"
# mT5 Base for Formal-to-informal Style Transfer 🤗

This repository contains the checkpoint for the [mT5 Base](https://huggingface.co/google/mt5-base) model fine-tuned on Formal-to-informal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

f2i = pipeline(""text2text-generation"", model='it5/mt5-base-formal-to-informal')
f2i(""Vi ringrazio infinitamente per vostra disponibilità"")
>>> [{""generated_text"": ""e grazie per la vostra disponibilità!""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-base-formal-to-informal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-base-formal-to-informal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint TBD},
    url={TBD},
    year={2022}
}
```",,,1,[],[],NLP,2022-01,58243201.925,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20251,mt5-base-headline-generation,['gsarti/change_it'],,40.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,2329728077.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:58:47+00:00,2022-01-21 08:59:16+00:00,"# mT5 Base for News Headline Generation 📣 🇮🇹

This repository contains the checkpoint for the [mT5 Base](https://huggingface.co/google/mt5-base) model fine-tuned on news headline generation on the Italian HeadGen-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

hg = pipeline(""text2text-generation"", model='it5/mt5-base-headline-generation')
hg(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-base-headline-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-base-headline-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58243201.925,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20252,mt5-base-ilgiornale-to-repubblica,['gsarti/change_it'],,40.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,2329728077.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:02:59+00:00,2022-01-21 09:02:08+00:00,"# mT5 Base for News Headline Style Transfer (Il Giornale to Repubblica) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [mT5 Base](https://huggingface.co/google/mt5-base) model fine-tuned on news headline style transfer in the Il Giornale to Repubblica direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Repubblica from the full body of an article written in the style of Il Giornale. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

g2r = pipeline(""text2text-generation"", model='it5/mt5-base-ilgiornale-to-repubblica')
g2r(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-base-ilgiornale-to-repubblica"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-base-ilgiornale-to-repubblica"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58243201.925,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20253,mt5-base-informal-to-formal,['yahoo/xformal_it'],,40.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,2329728077.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:48:51+00:00,2022-01-21 08:58:40+00:00,"
# mT5 Base for Informal-to-formal Style Transfer 🧐

This repository contains the checkpoint for the [mT5 Base](https://huggingface.co/google/mt5-base) model fine-tuned on Informal-to-formal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

i2f = pipeline(""text2text-generation"", model='it5/mt5-base-informal-to-formal')
i2f(""nn capisco xke tt i ragazzi lo fanno"")
>>> [{""generated_text"": ""non comprendo perché tutti i ragazzi agiscono così""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-base-informal-to-formal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-base-informal-to-formal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58243201.925,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20254,mt5-base-news-summarization,"['ARTeLab/fanpage', 'ARTeLab/ilpost']",,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,2329728077.0,False,8,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:51:55+00:00,2022-01-21 08:59:29+00:00,"# mT5 Base for News Summarization ✂️🗞️ 🇮🇹

This repository contains the checkpoint for the [mT5 Base](https://huggingface.co/google/mt5-base) model fine-tuned on news summarization on the [Fanpage](https://huggingface.co/datasets/ARTeLab/fanpage) and [Il Post](https://huggingface.co/datasets/ARTeLab/ilpost) corpora as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

newsum = pipeline(""summarization"", model='it5/mt5-base-news-summarization')
newsum(""Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di «una sorta di Netflix della cultura», pensata per «offrire a tutto il mondo la cultura italiana a pagamento». È presto per dare giudizi definitivi sulla piattaforma, e di certo sarà difficile farlo anche più avanti senza numeri precisi. Al momento, l’unica cosa che si può fare è guardare com’è fatto il sito, contare quanti contenuti ci sono (circa 700 “titoli”, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietà. Intanto, una cosa notata da più parti è che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente."")
>>> [{""generated_text"": ""ITsART, la Netflix della cultura italiana, parte da maggio. Film, documentari, spettacoli teatrali e musicali disponibili sul nuovo sito a pagamento.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-base-news-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-base-news-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,137042828.05882353,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20255,mt5-base-question-answering,['squad_it'],58723160.0,40.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,2329728077.0,False,32,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:57:29+00:00,2022-01-21 08:59:52+00:00,"# mT5 Base for Question Answering ⁉️ 🇮🇹

This repository contains the checkpoint for the [mT5 Base](https://huggingface.co/google/mt5-base) model fine-tuned on extractive question answering on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qa = pipeline(""text2text-generation"", model='it5/mt5-base-question-answering')
qa(""In seguito all' evento di estinzione del Cretaceo-Paleogene, l' estinzione dei dinosauri e il clima umido possono aver permesso alla foresta pluviale tropicale di diffondersi in tutto il continente. Dal 66-34 Mya, la foresta pluviale si estendeva fino a sud fino a 45°. Le fluttuazioni climatiche degli ultimi 34 milioni di anni hanno permesso alle regioni della savana di espandersi fino ai tropici. Durante l' Oligocene, ad esempio, la foresta pluviale ha attraversato una banda relativamente stretta. Si espandeva di nuovo durante il Miocene medio, poi si ritrasse ad una formazione prevalentemente interna all' ultimo massimo glaciale. Tuttavia, la foresta pluviale è riuscita ancora a prosperare durante questi periodi glaciali, consentendo la sopravvivenza e l' evoluzione di un' ampia varietà di specie. Domanda: La foresta pluviale amazzonica è diventata per lo più una foresta interna intorno a quale evento globale?"")
>>> [{""generated_text"": ""ultimo massimo glaciale""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-base-question-answering"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-base-question-answering"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58243201.925,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20256,mt5-base-question-generation,['squad_it'],58723160.0,40.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,2329728077.0,False,9,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:54:16+00:00,2022-01-21 09:00:00+00:00,"# mT5 Base for Question Generation 💭 🇮🇹

This repository contains the checkpoint for the [mT5 Base](https://huggingface.co/google/mt5-base) model fine-tuned on question generation on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qg = pipeline(""text2text-generation"", model='it5/mt5-base-question-generation')
qg(""Le conoscenze mediche erano stagnanti durante il Medioevo. Il resoconto più autorevole di allora è venuto dalla facoltà di medicina di Parigi in un rapporto al re di Francia che ha incolpato i cieli, sotto forma di una congiunzione di tre pianeti nel 1345 che causò una ""grande pestilenza nell\' aria"". Questa relazione è diventata la prima e più diffusa di una serie di casi di peste che cercava di dare consigli ai malati. Che la peste fosse causata dalla cattiva aria divenne la teoria più accettata. Oggi, questo è conosciuto come la teoria di Miasma. La parola ""peste"" non aveva un significato particolare in questo momento, e solo la ricorrenza dei focolai durante il Medioevo gli diede il nome che è diventato il termine medico. Risposta: re di Francia"")
>>> [{""generated_text"": ""Per chi è stato redatto il referto medico?""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-base-question-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-base-question-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58243201.925,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20257,mt5-base-repubblica-to-ilgiornale,['gsarti/change_it'],,40.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,2329728077.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:00:57+00:00,2022-01-21 09:02:36+00:00,"# mT5 Small for News Headline Style Transfer (Repubblica to Il Giornale) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on news headline style transfer in the Repubblica to Il Giornale direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Il Giornale from the full body of an article written in the style of Repubblica. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

r2g = pipeline(""text2text-generation"", model='it5/mt5-small-repubblica-to-ilgiornale')
r2g(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-repubblica-to-ilgiornale"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-repubblica-to-ilgiornale"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58243201.925,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20258,mt5-base-wiki-summarization,['wits'],,40.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,2329728077.0,False,5,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:51:31+00:00,2022-01-21 09:01:36+00:00,"# mT5 Base for Wikipedia Summarization ✂️📑 🇮🇹

This repository contains the checkpoint for the [mT5 Base](https://huggingface.co/google/mt5-base) model fine-tuned on Wikipedia summarization on the [WITS](https://www.semanticscholar.org/paper/WITS%3A-Wikipedia-for-Italian-Text-Summarization-Casola-Lavelli/ad6c83122e721c7c0db4a40727dac3b4762cd2b1) dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

wikisum = pipeline(""summarization"", model='it5/mt5-base-wiki-summarization')
wikisum(""Le dimensioni dell'isola sono di 8 km di lunghezza e di 3,2 km di larghezza.  Si trova a 1,6 km a sud-est dell'isola di Renaud, dalla quale è separata dal passaggio Rodman. La sua altezza è di 100 m.  Fu scoperta dall'esploratore e baleniere britannico John Biscoe nel 1832 e venne mappata durante una spedizione antartica francese realizzata nel primo decennio del XX secolo. Al comando della spedizione era Jean-Baptiste Charcot e il nome fu scelto per onorare l'esploratore e geografo francese Charles Rabot.  === Rivendicazioni territoriali === * Secondo l'Argentina appartiene al dipartimento dell'Antartide Argentina nella provincia della Terra del Fuoco.  * Secondo il Cile appartiene al comune antartico della provincia cilena antartica nella regione di Magallanes e dell'Antartico cileno. * Secondo il Regno Unito fa parte del territorio antartico britannico.   Per il Trattato Antartico tali rivendicazioni sono sospese.  Sull'isola è presente il rifugio Guillochon, sito storico antartico. "")
>>> [{""generated_text"": ""L' '''isola di Rabot''' si trova in prossimità dell'isola di Renaud, a sud dell'Argentina.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-base-wiki-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-base-wiki-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,58243201.925,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20259,mt5-small-formal-to-informal,['yahoo/xformal_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,1200789509.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:44:42+00:00,2022-01-21 08:58:03+00:00,"
# mT5 Small for Formal-to-informal Style Transfer 🤗

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on Formal-to-informal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

f2i = pipeline(""text2text-generation"", model='it5/mt5-small-formal-to-informal')
f2i(""Vi ringrazio infinitamente per vostra disponibilità"")
>>> [{""generated_text"": ""e grazie per la vostra disponibilità!""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-formal-to-informal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-formal-to-informal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,70634677.0,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20260,mt5-small-headline-generation,['gsarti/change_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,1200789509.0,False,7,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:59:17+00:00,2022-01-21 08:58:54+00:00,"# mT5 Small for News Headline Generation 📣 🇮🇹

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on news headline generation on the Italian HeadGen-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

hg = pipeline(""text2text-generation"", model='it5/mt5-small-headline-generation')
hg(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-headline-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-headline-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,70634677.0,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20261,mt5-small-ilgiornale-to-repubblica,['gsarti/change_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,1200789509.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:03:24+00:00,2022-01-21 09:01:58+00:00,"# mT5 Small for News Headline Style Transfer (Il Giornale to Repubblica) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on news headline style transfer in the Il Giornale to Repubblica direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Repubblica from the full body of an article written in the style of Il Giornale. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

g2r = pipeline(""text2text-generation"", model='it5/mt5-small-ilgiornale-to-repubblica')
g2r(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-ilgiornale-to-repubblica"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-ilgiornale-to-repubblica"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,70634677.0,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20262,mt5-small-informal-to-formal,['yahoo/xformal_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,1200789509.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:49:29+00:00,2022-01-21 08:58:16+00:00,"
# mT5 Small for Informal-to-formal Style Transfer 🧐

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on Informal-to-formal style transfer on the Italian subset of the XFORMAL dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

i2f = pipeline(""text2text-generation"", model='it5/mt5-small-informal-to-formal')
i2f(""nn capisco xke tt i ragazzi lo fanno"")
>>> [{""generated_text"": ""non comprendo perché tutti i ragazzi agiscono così""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-informal-to-formal"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-informal-to-formal"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,70634677.0,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20263,mt5-small-news-summarization,"['ARTeLab/fanpage', 'ARTeLab/ilpost']",,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,1200789509.0,False,8,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:52:27+00:00,2022-01-21 08:59:37+00:00,"# mT5 Small for News Summarization ✂️🗞️ 🇮🇹

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on news summarization on the [Fanpage](https://huggingface.co/datasets/ARTeLab/fanpage) and [Il Post](https://huggingface.co/datasets/ARTeLab/ilpost) corpora as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

newsum = pipeline(""summarization"", model='it5/mt5-small-news-summarization')
newsum(""Dal 31 maggio è infine partita la piattaforma ITsART, a più di un anno da quando – durante il primo lockdown – il ministro della Cultura Dario Franceschini ne aveva parlato come di «una sorta di Netflix della cultura», pensata per «offrire a tutto il mondo la cultura italiana a pagamento». È presto per dare giudizi definitivi sulla piattaforma, e di certo sarà difficile farlo anche più avanti senza numeri precisi. Al momento, l’unica cosa che si può fare è guardare com’è fatto il sito, contare quanti contenuti ci sono (circa 700 “titoli”, tra film, documentari, spettacoli teatrali e musicali e altri eventi) e provare a dare un giudizio sul loro valore e sulla loro varietà. Intanto, una cosa notata da più parti è che diversi contenuti di ITsART sono a pagamento sulla piattaforma sebbene altrove, per esempio su RaiPlay, siano invece disponibili gratuitamente."")
>>> [{""generated_text"": ""ITsART, la Netflix della cultura italiana, parte da maggio. Film, documentari, spettacoli teatrali e musicali disponibili sul nuovo sito a pagamento.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-news-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-news-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,70634677.0,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20264,mt5-small-question-answering,['squad_it'],58723160.0,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,1200789509.0,False,6,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:57:03+00:00,2022-01-21 09:00:32+00:00,"# mT5 Small for Question Answering ⁉️ 🇮🇹

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on extractive question answering on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qa = pipeline(""text2text-generation"", model='it5/mt5-small-question-answering')
qa(""In seguito all' evento di estinzione del Cretaceo-Paleogene, l' estinzione dei dinosauri e il clima umido possono aver permesso alla foresta pluviale tropicale di diffondersi in tutto il continente. Dal 66-34 Mya, la foresta pluviale si estendeva fino a sud fino a 45°. Le fluttuazioni climatiche degli ultimi 34 milioni di anni hanno permesso alle regioni della savana di espandersi fino ai tropici. Durante l' Oligocene, ad esempio, la foresta pluviale ha attraversato una banda relativamente stretta. Si espandeva di nuovo durante il Miocene medio, poi si ritrasse ad una formazione prevalentemente interna all' ultimo massimo glaciale. Tuttavia, la foresta pluviale è riuscita ancora a prosperare durante questi periodi glaciali, consentendo la sopravvivenza e l' evoluzione di un' ampia varietà di specie. Domanda: La foresta pluviale amazzonica è diventata per lo più una foresta interna intorno a quale evento globale?"")
>>> [{""generated_text"": ""ultimo massimo glaciale""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-question-answering"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-question-answering"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,70634677.0,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20265,mt5-small-question-generation,['squad_it'],58723160.0,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,1200789509.0,False,5,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:55:07+00:00,2022-01-21 09:00:44+00:00,"# mT5 Small for Question Generation 💭 🇮🇹

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on question generation on the [SQuAD-IT corpus](https://huggingface.co/datasets/squad_it) as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

qg = pipeline(""text2text-generation"", model='it5/mt5-small-question-generation')
qg(""Le conoscenze mediche erano stagnanti durante il Medioevo. Il resoconto più autorevole di allora è venuto dalla facoltà di medicina di Parigi in un rapporto al re di Francia che ha incolpato i cieli, sotto forma di una congiunzione di tre pianeti nel 1345 che causò una ""grande pestilenza nell\' aria"". Questa relazione è diventata la prima e più diffusa di una serie di casi di peste che cercava di dare consigli ai malati. Che la peste fosse causata dalla cattiva aria divenne la teoria più accettata. Oggi, questo è conosciuto come la teoria di Miasma. La parola ""peste"" non aveva un significato particolare in questo momento, e solo la ricorrenza dei focolai durante il Medioevo gli diede il nome che è diventato il termine medico. Risposta: re di Francia"")
>>> [{""generated_text"": ""Per chi è stato redatto il referto medico?""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-question-generation"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-question-generation"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,70634677.0,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20266,mt5-small-repubblica-to-ilgiornale,['gsarti/change_it'],,17.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,1200789509.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 08:01:24+00:00,2022-01-21 09:02:25+00:00,"# mT5 Small for News Headline Style Transfer (Repubblica to Il Giornale) 🗞️➡️🗞️ 🇮🇹

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on news headline style transfer in the Repubblica to Il Giornale direction on the Italian CHANGE-IT dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

The model is trained to generate an headline in the style of Il Giornale from the full body of an article written in the style of Repubblica. Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

r2g = pipeline(""text2text-generation"", model='it5/mt5-small-repubblica-to-ilgiornale')
r2g(""Arriva dal Partito nazionalista basco (Pnv) la conferma che i cinque deputati che siedono in parlamento voteranno la sfiducia al governo guidato da Mariano Rajoy. Pochi voti, ma significativi quelli della formazione politica di Aitor Esteban, che interverrà nel pomeriggio. Pur con dimensioni molto ridotte, il partito basco si è trovato a fare da ago della bilancia in aula. E il sostegno alla mozione presentata dai Socialisti potrebbe significare per il primo ministro non trovare quei 176 voti che gli servono per continuare a governare. \"" Perché dovrei dimettermi io che per il momento ho la fiducia della Camera e quella che mi è stato data alle urne \"", ha detto oggi Rajoy nel suo intervento in aula, mentre procedeva la discussione sulla mozione di sfiducia. Il voto dei baschi ora cambia le carte in tavola e fa crescere ulteriormente la pressione sul premier perché rassegni le sue dimissioni. La sfiducia al premier, o un'eventuale scelta di dimettersi, porterebbe alle estreme conseguenze lo scandalo per corruzione che ha investito il Partito popolare. Ma per ora sembra pensare a tutt'altro. \""Non ha intenzione di dimettersi - ha detto il segretario generale del Partito popolare , María Dolores de Cospedal - Non gioverebbe all'interesse generale o agli interessi del Pp\""."")
>>> [{""generated_text"": ""il nazionalista rajoy: 'voteremo la sfiducia'""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-repubblica-to-ilgiornale"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-repubblica-to-ilgiornale"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,70634677.0,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20267,mt5-small-wiki-summarization,['wits'],,14.0,Google Cloud Footprint,fine-tuning,"Eemshaven, Netherlands, Europe",1 TPU v3-8 VM,,,,,,1200789509.0,False,4,0,"['jax', 'tensorboard', 'transformers', 'pytorch', 'tf']",2022-03-09 07:51:07+00:00,2022-01-21 09:01:29+00:00,"# mT5 Small for Wikipedia Summarization ✂️📑 🇮🇹

This repository contains the checkpoint for the [mT5 Small](https://huggingface.co/google/mt5-small) model fine-tuned on Wikipedia summarization on the [WITS](https://www.semanticscholar.org/paper/WITS%3A-Wikipedia-for-Italian-Text-Summarization-Casola-Lavelli/ad6c83122e721c7c0db4a40727dac3b4762cd2b1) dataset as part of the experiments of the paper [IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation](https://arxiv.org/abs/2203.03759) by [Gabriele Sarti](https://gsarti.com) and [Malvina Nissim](https://malvinanissim.github.io). 

A comprehensive overview of other released materials is provided in the [gsarti/it5](https://github.com/gsarti/it5) repository. Refer to the paper for additional details concerning the reported scores and the evaluation approach.

## Using the model

Model checkpoints are available for usage in Tensorflow, Pytorch and JAX. They can be used directly with pipelines as:

```python
from transformers import pipelines

wikisum = pipeline(""summarization"", model='it5/mt5-small-wiki-summarization')
wikisum(""Le dimensioni dell'isola sono di 8 km di lunghezza e di 3,2 km di larghezza.  Si trova a 1,6 km a sud-est dell'isola di Renaud, dalla quale è separata dal passaggio Rodman. La sua altezza è di 100 m.  Fu scoperta dall'esploratore e baleniere britannico John Biscoe nel 1832 e venne mappata durante una spedizione antartica francese realizzata nel primo decennio del XX secolo. Al comando della spedizione era Jean-Baptiste Charcot e il nome fu scelto per onorare l'esploratore e geografo francese Charles Rabot.  === Rivendicazioni territoriali === * Secondo l'Argentina appartiene al dipartimento dell'Antartide Argentina nella provincia della Terra del Fuoco.  * Secondo il Cile appartiene al comune antartico della provincia cilena antartica nella regione di Magallanes e dell'Antartico cileno. * Secondo il Regno Unito fa parte del territorio antartico britannico.   Per il Trattato Antartico tali rivendicazioni sono sospese.  Sull'isola è presente il rifugio Guillochon, sito storico antartico. "")
>>> [{""generated_text"": ""L' '''isola di Rabot''' si trova in prossimità dell'isola di Renaud, a sud dell'Argentina.""}]
```

or loaded using autoclasses:

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(""it5/mt5-small-wiki-summarization"")
model = AutoModelForSeq2SeqLM.from_pretrained(""it5/mt5-small-wiki-summarization"")
```

If you use this model in your research, please cite our work as:

```bibtex
@article{sarti-nissim-2022-it5,
    title={{IT5}: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation},
    author={Sarti, Gabriele and Nissim, Malvina},
    journal={ArXiv preprint 2203.03759},
    url={https://arxiv.org/abs/2203.03759},
    year={2022},
	month={mar}
}
```",,,1,[],[],NLP,2022-01,85770679.21428572,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,1,0,0,0.0,1,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
20814,autonlp-intent-modelling-21895237,['joehdownardkainos/autonlp-data-intent-modelling'],,1.5688902203257171,AutoTrain,Not Specified,Not Specified,Not Specified,,1.6614878177642822,,0.3241579999999999,0.299278,1222374713.0,True,4,1,"['transformers', 'pytorch']",2021-10-21 11:29:28+00:00,2021-10-21 11:29:17+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 21895237
- CO2 Emissions (in grams): 1.5688902203257171

## Validation Metrics

- Loss: 1.6614878177642822
- Rouge1: 32.4158
- Rouge2: 24.6194
- RougeL: 29.9278
- RougeLsum: 29.4988
- Gen Len: 58.7778

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/joehdownardkainos/autonlp-intent-modelling-21895237
```",,,1,[],[],NLP,2021-10,779133362.6556885,0.3112215461538954,0.0,0.0,0.0,0.0,0,1.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
21065,autonlp-reuters-summarization-31447312,['juliensimon/autonlp-data-reuters-summarization'],,206.46626351359515,AutoTrain,Not Specified,Not Specified,Not Specified,,1.1907752752304075,,0.559215,0.53185,2283825905.0,True,4,0,"['transformers', 'pytorch']",2021-11-18 11:50:52+00:00,2021-11-10 21:33:37+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 31447312
- CO2 Emissions (in grams): 206.46626351359515

## Validation Metrics

- Loss: 1.1907752752304077
- Rouge1: 55.9215
- Rouge2: 30.7724
- RougeL: 53.185
- RougeLsum: 53.3353
- Gen Len: 15.1236

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/juliensimon/autonlp-reuters-summarization-31447312
```",,,1,[],[],NLP,2021-11,11061496.76046042,0.5451893292333637,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
21066,autonlp-song-lyrics-18753417,['juliensimon/autonlp-data-song-lyrics'],,112.75546781635975,AutoTrain,Not Specified,Not Specified,Not Specified,0.6680274633512711,0.9065971970558168,0.5384854358272774,,,438031533.0,True,86,2,"['transformers', 'safetensors', 'pytorch']",2023-03-17 08:01:07+00:00,2021-10-15 10:31:06+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 18753417
- CO2 Emissions (in grams): 112.75546781635975

## Validation Metrics

- Loss: 0.9065971970558167
- Accuracy: 0.6680274633512711
- Macro F1: 0.5384854358272774
- Micro F1: 0.6680274633512711
- Weighted F1: 0.6414749238882866
- Macro Precision: 0.6744495173269196
- Micro Precision: 0.6680274633512711
- Weighted Precision: 0.6634090047492259
- Macro Recall: 0.5078466493896978
- Micro Recall: 0.6680274633512711
- Weighted Recall: 0.6680274633512711


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/juliensimon/autonlp-song-lyrics-18753417
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""juliensimon/autonlp-song-lyrics-18753417"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""juliensimon/autonlp-song-lyrics-18753417"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,3884791.943867451,0.5963020536161965,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
21067,autonlp-song-lyrics-18753423,['juliensimon/autonlp-data-song-lyrics'],,55.55298771685949,AutoTrain,Not Specified,Not Specified,Not Specified,0.654110224531453,0.913820743560791,0.5327761649415296,,,263184497.0,True,7,0,"['transformers', 'pytorch']",2021-10-15 09:55:11+00:00,2021-10-15 09:55:06+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 18753423
- CO2 Emissions (in grams): 55.552987716859484

## Validation Metrics

- Loss: 0.913820743560791
- Accuracy: 0.654110224531453
- Macro F1: 0.5327761649415296
- Micro F1: 0.654110224531453
- Weighted F1: 0.6339481529454227
- Macro Precision: 0.6799297267808116
- Micro Precision: 0.654110224531453
- Weighted Precision: 0.6533459269990771
- Macro Recall: 0.49907494605289154
- Micro Recall: 0.654110224531453
- Weighted Recall: 0.654110224531453


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/juliensimon/autonlp-song-lyrics-18753423
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""juliensimon/autonlp-song-lyrics-18753423"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""juliensimon/autonlp-song-lyrics-18753423"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,4737539.920289967,0.587241272569742,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
21138,autonlp-shipping_status_2-27366103,['jwuthri/autonlp-data-shipping_status_2'],,32.912881644048,AutoTrain,Not Specified,Not Specified,Not Specified,0.9437683592110784,0.1817584484815597,0.8912337662337663,,,541344881.0,True,8,0,"['transformers', 'pytorch']",2021-10-27 21:34:42+00:00,2021-10-27 21:34:36+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 27366103
- CO2 Emissions (in grams): 32.912881644048

## Validation Metrics

- Loss: 0.18175844848155975
- Accuracy: 0.9437683592110785
- Precision: 0.9416809605488851
- Recall: 0.8459167950693375
- AUC: 0.9815242330050846
- F1: 0.8912337662337663

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/jwuthri/autonlp-shipping_status_2-27366103
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jwuthri/autonlp-shipping_status_2-27366103"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jwuthri/autonlp-shipping_status_2-27366103"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,16447811.736894736,0.9167490517516932,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
21150,autonlp-tele_new_5k-557515810,['kSaluja/autonlp-data-tele_new_5k'],,2.96638567287195,AutoTrain,Not Specified,Not Specified,Not Specified,0.9713212700580404,0.1289790123701095,0.9550914803178708,,,1336591537.0,True,6,1,"['transformers', 'pytorch']",2022-02-08 20:58:51+00:00,2022-02-08 20:55:21+00:00,"
# Model Trained Using AutoNLP

- Problem type: Entity Extraction
- Model ID: 557515810
- CO2 Emissions (in grams): 2.96638567287195

## Validation Metrics

- Loss: 0.12897901237010956
- Accuracy: 0.9713212700580403
- Precision: 0.9475614228089475
- Recall: 0.96274217585693
- F1: 0.9550914803178709

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/kSaluja/autonlp-tele_new_5k-557515810
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""kSaluja/autonlp-tele_new_5k-557515810"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kSaluja/autonlp-tele_new_5k-557515810"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-02,450579150.6557403,0.9631380081998948,0.0,0.0,0.0,0.0,0,1.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
21151,autonlp-tele_red_data_model-585716433,['kSaluja/autonlp-data-tele_red_data_model'],,2.379476355147211,AutoTrain,Not Specified,Not Specified,Not Specified,0.9724770642201837,0.15210922062397,0.9566742676723382,,,1336591537.0,True,6,0,"['transformers', 'pytorch']",2022-02-21 12:46:27+00:00,2022-02-21 12:43:23+00:00,"
# Model Trained Using AutoNLP

- Problem type: Entity Extraction
- Model ID: 585716433
- CO2 Emissions (in grams): 2.379476355147211

## Validation Metrics

- Loss: 0.15210922062397003
- Accuracy: 0.9724770642201835
- Precision: 0.950836820083682
- Recall: 0.9625838333921638
- F1: 0.9566742676723382

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/kSaluja/autonlp-tele_red_data_model-585716433
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""kSaluja/autonlp-tele_red_data_model-585716433"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kSaluja/autonlp-tele_red_data_model-585716433"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-02,561716671.0266844,0.9645109410139542,0.0,0.0,0.0,0.0,0,1.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
21246,autonlp-text2sql-18413376,['kbhugging/autonlp-data-text2sql'],,1.4091714704861449,AutoTrain,Not Specified,Not Specified,Not Specified,,0.2667271196842193,,0.61765,0.613222,891730879.0,True,14,0,"['transformers', 'pytorch']",2021-10-15 02:36:42+00:00,2021-10-15 02:36:33+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 18413376
- CO2 Emissions (in grams): 1.4091714704861447

## Validation Metrics

- Loss: 0.26672711968421936
- Rouge1: 61.765
- Rouge2: 52.5778
- RougeL: 61.3222
- RougeLsum: 61.1905
- Gen Len: 18.7805

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/kbhugging/autonlp-text2sql-18413376
```",,,1,[],[],NLP,2021-10,632805089.8535188,0.6154280352465569,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
21956,autonlp-trans_class_arg-32957902,['lidiia/autonlp-data-trans_class_arg'],,0.9756221672668952,AutoTrain,Not Specified,Not Specified,Not Specified,0.8939828080229226,0.2765039801597595,0.8177339901477833,,,436415661.0,True,17,0,"['transformers', 'pytorch']",2021-11-15 16:48:42+00:00,2021-11-15 16:48:35+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 32957902
- CO2 Emissions (in grams): 0.9756221672668951

## Validation Metrics

- Loss: 0.2765039801597595
- Accuracy: 0.8939828080229226
- Precision: 0.7757009345794392
- Recall: 0.8645833333333334
- AUC: 0.9552659749670619
- F1: 0.8177339901477833

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/lidiia/autonlp-trans_class_arg-32957902
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lidiia/autonlp-trans_class_arg-32957902"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lidiia/autonlp-trans_class_arg-32957902"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,447320361.9620221,0.8541601385338501,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
22105,autonlp-SST1-529214890,['lucianpopa/autonlp-data-SST1'],,49.618294309910624,AutoTrain,Not Specified,Not Specified,Not Specified,0.7042338838232481,0.7135734558105469,0.6164041045783032,,,498683309.0,True,17,0,"['transformers', 'pytorch']",2022-01-25 17:30:09+00:00,2022-01-25 16:51:35+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 529214890
- CO2 Emissions (in grams): 49.618294309910624

## Validation Metrics

- Loss: 0.7135734558105469
- Accuracy: 0.7042338838232481
- Macro F1: 0.6164041045783032
- Micro F1: 0.7042338838232481
- Weighted F1: 0.7028309161791009
- Macro Precision: 0.6497438111060598
- Micro Precision: 0.7042338838232481
- Weighted Precision: 0.7076651075198755
- Macro Recall: 0.6023419083862918
- Micro Recall: 0.7042338838232481
- Weighted Recall: 0.7042338838232481


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/lucianpopa/autonlp-SST1-529214890
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucianpopa/autonlp-SST1-529214890"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucianpopa/autonlp-SST1-529214890"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,10050392.01640582,0.6573984095326212,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
22106,autonlp-SST2-551215591,['lucianpopa/autonlp-data-SST2'],,8.883161797287569,AutoTrain,Not Specified,Not Specified,Not Specified,0.969531605275125,0.0882187634706497,0.9722205769116864,,,267860081.0,True,7,0,"['transformers', 'pytorch']",2022-02-03 20:00:48+00:00,2022-02-03 19:53:35+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 551215591
- CO2 Emissions (in grams): 8.883161797287569

## Validation Metrics

- Loss: 0.08821876347064972
- Accuracy: 0.969531605275125
- Precision: 0.9734313841774404
- Recall: 0.9710127780407004
- AUC: 0.9949152422763072
- F1: 0.9722205769116863

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/lucianpopa/autonlp-SST2-551215591
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucianpopa/autonlp-SST2-551215591"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucianpopa/autonlp-SST2-551215591"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-02,30153687.066894334,0.9708742292264464,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
22107,autonlp-TREC-classification-522314623,['lucianpopa/autonlp-data-TREC-classification'],,15.186006626915717,AutoTrain,Not Specified,Not Specified,Not Specified,0.9643183897529736,0.2461203336715698,0.9493690949638436,,,1421627693.0,True,9,0,"['transformers', 'pytorch']",2022-01-24 02:31:54+00:00,2022-01-24 02:15:31+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 522314623
- CO2 Emissions (in grams): 15.186006626915715

## Validation Metrics

- Loss: 0.24612033367156982
- Accuracy: 0.9643183897529735
- Macro F1: 0.9493690949638435
- Micro F1: 0.9643183897529735
- Weighted F1: 0.9642384162837268
- Macro Precision: 0.9372705571897225
- Micro Precision: 0.9643183897529735
- Weighted Precision: 0.9652870438320825
- Macro Recall: 0.9649638583139503
- Micro Recall: 0.9643183897529735
- Weighted Recall: 0.9643183897529735


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/lucianpopa/autonlp-TREC-classification-522314623
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucianpopa/autonlp-TREC-classification-522314623"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucianpopa/autonlp-TREC-classification-522314623"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,93614320.59961726,0.9567853520996864,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
22383,autonlp-Gibberish-Detector-492513457,['madhurjindal/autonlp-data-Gibberish-Detector'],,5.527544460835904,AutoTrain,Not Specified,Not Specified,Not Specified,0.9735624586913416,0.0760946348309516,0.9736173135739408,,,267866225.0,True,110887,8,"['transformers', 'pytorch']",2022-01-12 10:42:19+00:00,2022-01-12 10:36:54+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 492513457
- CO2 Emissions (in grams): 5.527544460835904

## Validation Metrics

- Loss: 0.07609463483095169
- Accuracy: 0.9735624586913417
- Macro F1: 0.9736173135739408
- Micro F1: 0.9735624586913417
- Weighted F1: 0.9736173135739408
- Macro Precision: 0.9737771415197378
- Micro Precision: 0.9735624586913417
- Weighted Precision: 0.9737771415197378
- Macro Recall: 0.9735624586913417
- Micro Recall: 0.9735624586913417
- Weighted Recall: 0.9735624586913417


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/madhurjindal/autonlp-Gibberish-Detector-492513457
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""madhurjindal/autonlp-Gibberish-Detector-492513457"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""madhurjindal/autonlp-Gibberish-Detector-492513457"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,48460256.972676046,0.9735898853599704,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
22633,autonlp-vaccinchat-22134694,['maximedb/autonlp-data-vaccinchat'],,14.525955245648218,AutoTrain,Not Specified,Not Specified,Not Specified,0.6369376479873717,1.7039562463760376,0.5363181342408181,,,467690605.0,True,17,0,"['transformers', 'tf', 'pytorch']",2021-10-19 12:50:01+00:00,2021-10-19 11:59:10+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 22134694
- CO2 Emissions (in grams): 14.525955245648218

## Validation Metrics

- Loss: 1.7039562463760376
- Accuracy: 0.6369376479873717
- Macro F1: 0.5363181342408181
- Micro F1: 0.6369376479873717
- Weighted F1: 0.6309793486221543
- Macro Precision: 0.5533353910494714
- Micro Precision: 0.6369376479873717
- Weighted Precision: 0.676981050732216
- Macro Recall: 0.5828723356986293
- Micro Recall: 0.6369376479873717
- Weighted Recall: 0.6369376479873717


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/maximedb/autonlp-vaccinchat-22134694
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""maximedb/autonlp-vaccinchat-22134694"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""maximedb/autonlp-vaccinchat-22134694"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,32196891.501515117,0.582313279287778,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,1.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
22657,politics-sentence-classifier,['mazancourt/autonlp-data-politics-sentence-classifier'],,1.06099358268878,AutoTrain,Not Specified,Not Specified,Not Specified,0.8097826086956522,0.6050735712051392,0.7713543865034599,,,442582445.0,True,31,2,"['transformers', 'safetensors', 'pytorch']",2023-03-26 20:58:47+00:00,2021-10-20 15:49:03+00:00,"
# Prediction of sentence ""nature"" in a French political sentence

This model aims at predicting the nature of a sentence in a French political sentence.
The predictions fall in three categories:
- `problem`: the sentence describes a problem (usually to be tackled by the speaker), for example _il y a dans ce pays une fracture_ (J. Chirac)
- `solution`: the sentences describes a solution (typically part of a political programme), for example: _J’ai supprimé les droits de succession parce que je crois au travail et parce que je crois à la famille._ (N. Sarkozy)
- `other`: the sentence does not belong to any of these categories, for example: _vive la République, vive la France_

This model was trained using AutoNLP based on sentences extracted from a mix of political tweets and speeches.

# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 23105051
- CO2 Emissions (in grams): 1.06099358268878

## Validation Metrics

- Loss: 0.6050735712051392
- Accuracy: 0.8097826086956522
- Macro F1: 0.7713543865034599
- Micro F1: 0.8097826086956522
- Weighted F1: 0.8065488494385247
- Macro Precision: 0.7861074705111403
- Micro Precision: 0.8097826086956522
- Weighted Precision: 0.806470454156932
- Macro Recall: 0.7599656456873758
- Micro Recall: 0.8097826086956522
- Weighted Recall: 0.8097826086956522


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""Il y a dans ce pays une fracture""}' https://api-inference.huggingface.co/models/mazancourt/politics-sentence-classifier
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mazancourt/autonlp-politics-sentence-classifier-23105051"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mazancourt/politics-sentence-classifier"", use_auth_token=True)

inputs = tokenizer(""Il y a dans ce pays une fracture"", return_tensors=""pt"")

outputs = model(**inputs)

# Category can be ""problem"", ""solution"" or ""other""
category = outputs[0][""label""]
score = outputs[0][""score""]
```",,,1,[],[],NLP,2021-10,417139605.9516245,0.7901015145786868,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,2,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
22743,autonlp-FR_another_test-565016091,['medA/autonlp-data-FR_another_test'],,70.54639641012226,AutoTrain,Not Specified,Not Specified,Not Specified,0.8545909432074056,0.5170354247093201,0.7910662503820883,,,442610093.0,True,7,0,"['transformers', 'pytorch']",2022-02-11 11:08:02+00:00,2022-02-11 10:23:10+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 565016091
- CO2 Emissions (in grams): 70.54639641012226

## Validation Metrics

- Loss: 0.5170354247093201
- Accuracy: 0.8545909432074056
- Macro F1: 0.7910662503820883
- Micro F1: 0.8545909432074056
- Weighted F1: 0.8539837213761081
- Macro Precision: 0.8033640381948799
- Micro Precision: 0.8545909432074056
- Weighted Precision: 0.856160322286008
- Macro Recall: 0.7841845637031052
- Micro Recall: 0.8545909432074056
- Weighted Recall: 0.8545909432074056


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/medA/autonlp-FR_another_test-565016091
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""medA/autonlp-FR_another_test-565016091"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""medA/autonlp-FR_another_test-565016091"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-02,6274028.377394096,0.8216025253461274,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
23148,autonlp-imdb-test-21134442,['mmcquade11/autonlp-data-imdb-test'],,298.7849611952843,AutoTrain,Not Specified,Not Specified,Not Specified,0.9393,0.2161806672811508,0.9395237620803027,,,1340737453.0,True,18,0,"['transformers', 'pytorch']",2021-10-18 20:16:41+00:00,2021-10-18 20:16:29+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 21134442
- CO2 Emissions (in grams): 298.7849611952843

## Validation Metrics

- Loss: 0.21618066728115082
- Accuracy: 0.9393
- Precision: 0.9360730593607306
- Recall: 0.943
- AUC: 0.98362804
- F1: 0.9395237620803029

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/mmcquade11/autonlp-imdb-test-21134442
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mmcquade11/autonlp-imdb-test-21134442"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mmcquade11/autonlp-imdb-test-21134442"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,4487298.984649032,0.9394118677154666,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
23149,autonlp-imdb-test-21134453,['mmcquade11/autonlp-data-imdb-test'],,38.10256536061048,AutoTrain,Not Specified,Not Specified,Not Specified,0.9355,0.172550767660141,0.9354418977079372,,,328525933.0,True,9,0,"['transformers', 'pytorch']",2021-10-18 17:47:59+00:00,2021-10-18 17:47:54+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 21134453
- CO2 Emissions (in grams): 38.102565360610484

## Validation Metrics

- Loss: 0.172550767660141
- Accuracy: 0.9355
- Precision: 0.9362853135644159
- Recall: 0.9346
- AUC: 0.98267064
- F1: 0.9354418977079372

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/mmcquade11/autonlp-imdb-test-21134453
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mmcquade11/autonlp-imdb-test-21134453"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mmcquade11/autonlp-imdb-test-21134453"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,8622147.351254785,0.9354709479517824,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
23150,autonlp-reuters-summarization-34018133,['mmcquade11/autonlp-data-reuters-summarization'],,286.4350821612984,AutoTrain,Not Specified,Not Specified,Not Specified,,1.1805976629257202,,0.554013,0.5257000000000001,2283825905.0,True,16,0,"['transformers', 'pytorch']",2021-11-19 14:45:38+00:00,2021-11-18 22:33:16+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 34018133
- CO2 Emissions (in grams): 286.4350821612984

## Validation Metrics

- Loss: 1.1805976629257202
- Rouge1: 55.4013
- Rouge2: 30.8004
- RougeL: 52.57
- RougeLsum: 52.6103
- Gen Len: 15.3458

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/mmcquade11/autonlp-reuters-summarization-34018133
```",,,1,[],[],NLP,2021-11,7973275.786497142,0.5394852782174523,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
23155,reuters-summarization,['mmcquade11/autonlp-data-reuters-summarization'],,286.4350821612984,AutoTrain,Not Specified,Not Specified,Not Specified,,1.1805976629257202,,0.554013,0.5257000000000001,2283825905.0,True,4,0,"['transformers', 'pytorch']",2021-11-30 21:43:51+00:00,2021-11-30 17:07:56+00:00,"
This is an autoNLP model I trained on Reuters dataset

# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 34018133
- CO2 Emissions (in grams): 286.4350821612984

## Validation Metrics

- Loss: 1.1805976629257202
- Rouge1: 55.4013
- Rouge2: 30.8004
- RougeL: 52.57
- RougeLsum: 52.6103
- Gen Len: 15.3458

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/mmcquade11/autonlp-reuters-summarization-34018133
```",,,1,[],[],NLP,2021-11,7973275.786497142,0.5394852782174523,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
23778,autonlp-Doctor_DE-24595544,['muhtasham/autonlp-data-Doctor_DE'],,92.87363201770962,AutoTrain,Not Specified,Not Specified,Not Specified,,0.3001164197921753,,,,269638769.0,True,7,0,"['transformers', 'pytorch']",2021-10-22 10:51:44+00:00,2021-10-22 10:51:40+00:00,"
# Model Trained Using AutoNLP

- Problem type: Single Column Regression
- Model ID: 24595544
- CO2 Emissions (in grams): 92.87363201770962

## Validation Metrics

- Loss: 0.3001164197921753
- MSE: 0.3001164197921753
- MAE: 0.24272102117538452
- R2: 0.8465975006681247
- RMSE: 0.5478288531303406
- Explained Variance: 0.8468209505081177

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/muhtasham/autonlp-Doctor_DE-24595544
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595544"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595544"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,2903286.5749084074,,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
23779,autonlp-Doctor_DE-24595545,['muhtasham/autonlp-data-Doctor_DE'],,203.30658367993385,AutoTrain,Not Specified,Not Specified,Not Specified,,0.3021486103534698,,,,439797933.0,True,20,0,"['transformers', 'pytorch']",2021-10-22 11:59:58+00:00,2021-10-22 11:59:53+00:00,"
# Model Trained Using AutoNLP

- Problem type: Single Column Regression
- Model ID: 24595545
- CO2 Emissions (in grams): 203.30658367993382

## Validation Metrics

- Loss: 0.30214861035346985
- MSE: 0.30214861035346985
- MAE: 0.25911855697631836
- R2: 0.8455587614373526
- RMSE: 0.5496804714202881
- Explained Variance: 0.8476610779762268

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/muhtasham/autonlp-Doctor_DE-24595545
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595545"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595545"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,2163225.2386493064,,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
23780,autonlp-Doctor_DE-24595546,['muhtasham/autonlp-data-Doctor_DE'],,210.5957437893554,AutoTrain,Not Specified,Not Specified,Not Specified,,0.3092539310455322,,,,439797933.0,True,19,0,"['transformers', 'pytorch']",2021-10-22 12:23:10+00:00,2021-10-22 12:23:05+00:00,"
# Model Trained Using AutoNLP

- Problem type: Single Column Regression
- Model ID: 24595546
- CO2 Emissions (in grams): 210.5957437893554

## Validation Metrics

- Loss: 0.3092539310455322
- MSE: 0.30925390124320984
- MAE: 0.25015318393707275
- R2: 0.841926941198094
- RMSE: 0.5561060309410095
- Explained Variance: 0.8427215218544006

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/muhtasham/autonlp-Doctor_DE-24595546
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595546"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595546"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,2088351.478935396,,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
23781,autonlp-Doctor_DE-24595547,['muhtasham/autonlp-data-Doctor_DE'],,396.5529429198159,AutoTrain,Not Specified,Not Specified,Not Specified,,1.956548929214477,,,,1343111405.0,True,10,0,"['transformers', 'pytorch']",2021-10-22 14:04:29+00:00,2021-10-22 14:04:18+00:00,"
# Model Trained Using AutoNLP

- Problem type: Single Column Regression
- Model ID: 24595547
- CO2 Emissions (in grams): 396.5529429198159

## Validation Metrics

- Loss: 1.9565489292144775
- MSE: 1.9565489292144775
- MAE: 0.9890901446342468
- R2: -7.68965036332947e-05
- RMSE: 1.3987668752670288
- Explained Variance: 0.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/muhtasham/autonlp-Doctor_DE-24595547
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595547"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595547"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,3386966.1768506425,,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,1.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
23782,autonlp-Doctor_DE-24595548,['muhtasham/autonlp-data-Doctor_DE'],,183.88911013564527,AutoTrain,Not Specified,Not Specified,Not Specified,,0.3050823509693146,,,,504028589.0,True,7,0,"['transformers', 'pytorch']",2021-10-22 11:58:36+00:00,2021-10-22 11:58:31+00:00,"
# Model Trained Using AutoNLP

- Problem type: Single Column Regression
- Model ID: 24595548
- CO2 Emissions (in grams): 183.88911013564527

## Validation Metrics

- Loss: 0.3050823509693146
- MSE: 0.3050823509693146
- MAE: 0.2664000689983368
- R2: 0.844059188176304
- RMSE: 0.5523425936698914
- Explained Variance: 0.8472161293029785

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/muhtasham/autonlp-Doctor_DE-24595548
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595548"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""muhtasham/autonlp-Doctor_DE-24595548"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,2740937.669599928,,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
24311,yelp-rating-classification,['nihaldsouza1/autonlp-data-yelp-rating-classification'],,15.62335109262394,AutoTrain,Not Specified,Not Specified,Not Specified,0.6631428571428571,0.7870086431503296,0.6613073053700258,,,328535149.0,True,72,1,"['transformers', 'pytorch']",2022-11-03 08:31:49+00:00,2022-02-01 04:25:18+00:00,"
# Custom-trained user model

- Problem type: Multi-class Classification
- Model ID: 545015430
- CO2 Emissions (in grams): 15.62335109262394

## Validation Metrics

- Loss: 0.7870086431503296
- Accuracy: 0.6631428571428571
- Macro F1: 0.6613073053700258
- Micro F1: 0.6631428571428571
- Weighted F1: 0.661157273964887
- Macro Precision: 0.6626911151999393
- Micro Precision: 0.6631428571428571
- Weighted Precision: 0.662191421927851
- Macro Recall: 0.6629735627465572
- Micro Recall: 0.6631428571428571
- Weighted Recall: 0.6631428571428571


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/nihaldsouza1/autonlp-yelp-rating-classification-545015430
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""nihaldsouza1/autonlp-yelp-rating-classification-545015430"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""nihaldsouza1/autonlp-yelp-rating-classification-545015430"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-02,21028468.67181441,0.6622238093134095,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
24550,autonlp-bert-covid-407910458,['nurkayevaa/autonlp-data-bert-covid'],,9.72797586719897,AutoTrain,Not Specified,Not Specified,Not Specified,0.9119825708061002,0.2090704888105392,0.922664624808576,,,267860081.0,True,11,0,"['transformers', 'pytorch']",2021-12-11 05:29:05+00:00,2021-12-11 05:29:01+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 407910458
- CO2 Emissions (in grams): 9.72797586719897

## Validation Metrics

- Loss: 0.20907048881053925
- Accuracy: 0.9119825708061002
- Precision: 0.8912721893491125
- Recall: 0.9563492063492064
- AUC: 0.9698454873092555
- F1: 0.9226646248085759

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/nurkayevaa/autonlp-bert-covid-407910458
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""nurkayevaa/autonlp-bert-covid-407910458"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""nurkayevaa/autonlp-bert-covid-407910458"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,27535027.292078026,0.9172925001995844,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
24551,autonlp-bert-covid-407910467,['nurkayevaa/autonlp-data-bert-covid'],,10.719439124704492,AutoTrain,Not Specified,Not Specified,Not Specified,0.9516339869281044,0.1202984452247619,0.9563507668108534,,,328525933.0,True,8,0,"['transformers', 'pytorch']",2021-12-11 05:31:06+00:00,2021-12-11 05:31:01+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 407910467
- CO2 Emissions (in grams): 10.719439124704492

## Validation Metrics

- Loss: 0.12029844522476196
- Accuracy: 0.9516339869281045
- Precision: 0.9477786438035853
- Recall: 0.9650793650793651
- AUC: 0.9907376734912967
- F1: 0.9563507668108534

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/nurkayevaa/autonlp-bert-covid-407910467
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""nurkayevaa/autonlp-bert-covid-407910467"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""nurkayevaa/autonlp-bert-covid-407910467"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,30647679.34013121,0.9539865466309462,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
24923,autonlp-eo-590516680,['panashe/autonlp-data-eo'],,2.3709499644854883,AutoTrain,Not Specified,Not Specified,Not Specified,0.6608695652173913,0.6466107964515686,0.688,,,438019245.0,True,21,0,"['transformers', 'pytorch']",2022-02-23 11:29:10+00:00,2022-02-23 11:26:41+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 590516680
- CO2 Emissions (in grams): 2.3709499644854883

## Validation Metrics

- Loss: 0.6466107964515686
- Accuracy: 0.6608695652173913
- Precision: 0.6515151515151515
- Recall: 0.7288135593220338
- AUC: 0.6334745762711864
- F1: 0.688

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/panashe/autonlp-eo-590516680
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""panashe/autonlp-eo-590516680"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""panashe/autonlp-eo-590516680"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-02,184744196.02315524,0.6741619391438886,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
25141,autonlp-testing-504313966,['pediberto/autonlp-data-testing'],,12.994518654810642,AutoTrain,Not Specified,Not Specified,Not Specified,0.9398032027783138,0.1967329680919647,0.9416604338070308,,,539688365.0,True,16,0,"['transformers', 'pytorch']",2022-01-15 15:02:13+00:00,2022-01-15 14:52:01+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 504313966
- CO2 Emissions (in grams): 12.994518654810642

## Validation Metrics

- Loss: 0.19673296809196472
- Accuracy: 0.9398032027783138
- Precision: 0.9133115705476967
- Recall: 0.9718255499807025
- AUC: 0.985316873222122
- F1: 0.9416604338070308

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/pediberto/autonlp-testing-504313966
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pediberto/autonlp-testing-504313966"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pediberto/autonlp-testing-504313966"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,41532001.24886537,0.940730901637408,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26014,headline-test,['redadmiral/autonlp-data-Headline-Generator'],,651.3545590912366,AutoTrain,Not Specified,Not Specified,Not Specified,,,,0.028187,0.0273959999999999,2329700301.0,True,3,0,"['transformers', 'pytorch']",2021-12-29 01:43:08+00:00,2021-12-29 01:42:50+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 453611714
- CO2 Emissions (in grams): 651.3545590912366

## Validation Metrics

- Loss: nan
- Rouge1: 2.8187
- Rouge2: 0.5508
- RougeL: 2.7396
- RougeLsum: 2.7446
- Gen Len: 9.7507

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/redadmiral/autonlp-Headline-Generator-453611714
```",,,1,[],[],NLP,2021-12,3576700.69009784,0.0277858716514042,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26087,autonlp-roberta-large-finetuned-467612250,['rexxar96/autonlp-data-roberta-large-finetuned'],,73.72876780772296,AutoTrain,Not Specified,Not Specified,Not Specified,0.9541659567217584,0.1826131939888,0.9551292743953294,,,1421611309.0,True,8,0,"['transformers', 'pytorch']",2022-01-03 14:24:32+00:00,2022-01-03 14:24:21+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 467612250
- CO2 Emissions (in grams): 73.72876780772296

## Validation Metrics

- Loss: 0.18261319398880005
- Accuracy: 0.9541659567217584
- Precision: 0.9530625832223701
- Recall: 0.9572049481778669
- AUC: 0.9901737875196123
- F1: 0.9551292743953294

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/rexxar96/autonlp-roberta-large-finetuned-467612250
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""rexxar96/autonlp-roberta-large-finetuned-467612250"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""rexxar96/autonlp-roberta-large-finetuned-467612250"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,19281636.615810752,0.954647372541926,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26088,autonlp-sentiment-analysis-456211724,['rexxar96/autonlp-data-sentiment-analysis'],,22.28263989637389,AutoTrain,Not Specified,Not Specified,Not Specified,0.9119100357812234,0.2371041774749755,0.9163024121741946,,,267860081.0,True,7,0,"['transformers', 'pytorch']",2021-12-29 14:47:09+00:00,2021-12-29 14:47:05+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 456211724
- CO2 Emissions (in grams): 22.28263989637389

## Validation Metrics

- Loss: 0.23710417747497559
- Accuracy: 0.9119100357812234
- Precision: 0.8882611424984307
- Recall: 0.9461718488799733
- AUC: 0.974790366001874
- F1: 0.9163024121741946

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/rexxar96/autonlp-sentiment-analysis-456211724
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""rexxar96/autonlp-sentiment-analysis-456211724"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""rexxar96/autonlp-sentiment-analysis-456211724"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,12021020.94929917,0.9141009475202604,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26227,autonlp-department-classification-534915130,['rodrigogelacio/autonlp-data-department-classification'],,1.486285677432006,AutoTrain,Not Specified,Not Specified,Not Specified,0.9204545454545454,0.3706627786159515,0.9103715740678612,,,436446381.0,True,16,1,"['transformers', 'pytorch']",2022-01-28 02:06:52+00:00,2022-01-28 02:05:14+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 534915130
- CO2 Emissions (in grams): 1.4862856774320061

## Validation Metrics

- Loss: 0.37066277861595154
- Accuracy: 0.9204545454545454
- Macro F1: 0.9103715740678612
- Micro F1: 0.9204545454545455
- Weighted F1: 0.9196871607509906
- Macro Precision: 0.9207759152612094
- Micro Precision: 0.9204545454545454
- Weighted Precision: 0.922177301864802
- Macro Recall: 0.9055002187355129
- Micro Recall: 0.9204545454545454
- Weighted Recall: 0.9204545454545454


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/rodrigogelacio/autonlp-department-classification-534915130
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""rodrigogelacio/autonlp-department-classification-534915130"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""rodrigogelacio/autonlp-department-classification-534915130"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,293649052.5523256,0.9153852946143932,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26235,autonlp-Fake-news-detection-system-29906863,['rohansingh/autonlp-data-Fake-news-detection-system'],,3.86243979614321,AutoTrain,Not Specified,Not Specified,Not Specified,0.9084807809640024,0.2536192238330841,0.9428353658536586,,,1112266157.0,True,7,0,"['transformers', 'pytorch']",2021-11-06 12:24:22+00:00,2021-11-06 12:24:13+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 29906863
- CO2 Emissions (in grams): 3.8624397961432106

## Validation Metrics

- Loss: 0.2536192238330841
- Accuracy: 0.9084807809640024
- Precision: 0.9421172886519421
- Recall: 0.9435545385202135
- AUC: 0.9517288050454876
- F1: 0.9428353658536586

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/rohansingh/autonlp-Fake-news-detection-system-29906863
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""rohansingh/autonlp-Fake-news-detection-system-29906863"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""rohansingh/autonlp-Fake-news-detection-system-29906863"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,287969836.60706866,0.9253393170729748,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26508,autonlp-roberta-large2-479012819,['sam890914/autonlp-data-roberta-large2'],,71.60954851696604,AutoTrain,Not Specified,Not Specified,Not Specified,0.93951269381496,0.22774338722229,0.9388879325185058,,,1421611309.0,True,8,0,"['transformers', 'pytorch']",2022-01-06 08:46:51+00:00,2022-01-06 08:46:39+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 479012819
- CO2 Emissions (in grams): 71.60954851696604

## Validation Metrics

- Loss: 0.22774338722229004
- Accuracy: 0.9395126938149599
- Precision: 0.9677075940383251
- Recall: 0.9117352056168505
- AUC: 0.9862377263827619
- F1: 0.9388879325185058

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/sam890914/autonlp-roberta-large2-479012819
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sam890914/autonlp-roberta-large2-479012819"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sam890914/autonlp-roberta-large2-479012819"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,19852259.06937796,0.9392002092680566,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26635,autonlp-imdb-classification-596216804,['sarahlmk/autonlp-data-imdb-classification'],,274.81371614671764,AutoTrain,Not Specified,Not Specified,Not Specified,0.9239,0.240494817495346,0.9247652001977262,,,1334486957.0,True,20,0,"['transformers', 'pytorch']",2022-02-25 06:16:45+00:00,2022-02-25 03:28:46+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 596216804
- CO2 Emissions (in grams): 274.81371614671764

## Validation Metrics

- Loss: 0.24049481749534607
- Accuracy: 0.9239
- Precision: 0.9143695014662757
- Recall: 0.9354
- AUC: 0.9781644
- F1: 0.9247652001977262

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/sarahlmk/autonlp-imdb-classification-596216804
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sarahlmk/autonlp-imdb-classification-596216804"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sarahlmk/autonlp-imdb-classification-596216804"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-02,4855969.25696221,0.9243323976361936,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26735,autonlp-emotion-clf,['sbrandeis/autonlp-data-emotion-classification-pre'],,23.4692320403666,AutoTrain,Not Specified,Not Specified,Not Specified,0.9438026849828286,0.1504082083702087,0.9093924156122388,,,1334503341.0,True,17,0,"['transformers', 'pytorch']",2021-12-07 08:16:13+00:00,2021-12-02 17:24:06+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 3252433
- CO2 Emissions (in grams): 23.4692320403666

## Validation Metrics

- Loss: 0.15040820837020874
- Accuracy: 0.9438026849828286
- Macro F1: 0.9093924156122387
- Micro F1: 0.9438026849828286
- Weighted F1: 0.9423168992167734
- Macro Precision: 0.9482796335288181
- Micro Precision: 0.9438026849828286
- Weighted Precision: 0.9466095426853992
- Macro Recall: 0.8842649120281764
- Micro Recall: 0.9438026849828286
- Weighted Recall: 0.9438026849828286


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/sbrandeis/autonlp-emotion-classification-pre-3252433
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sbrandeis/autonlp-emotion-classification-pre-3252433"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sbrandeis/autonlp-emotion-classification-pre-3252433"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```
",,,1,[],[],NLP,2021-12,56861823.97040864,0.926278084031468,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26844,election_relevancy_best,['sefaozalpadl/autonlp-data-election_relevancy_analysis'],,1.3248523193990855,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,498674093.0,False,6,0,"['transformers', 'pytorch']",2021-11-07 16:48:34+00:00,2021-10-20 22:04:27+00:00,"
# Election Fraud Binary Classifier

- Problem type: Binary Classification
- Model ID: 23315155
- CO2 Emissions (in grams): 1.3248523193990855

## Validation Metrics

- Loss: 0.4240806996822357
- Accuracy: 0.8173913043478261
- Precision: 0.8837209302325582
- Recall: 0.8085106382978723
- AUC: 0.8882580285281696
- F1: 0.8444444444444444

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/sefaozalpadl/election_relevancy_best
```

Or Python API:

```
from transformers import AutoTokenizer, AutoModelForSequenceClassification
  
tokenizer = AutoTokenizer.from_pretrained(""sefaozalpadl/election_relevancy_best"")

model = AutoModelForSequenceClassification.from_pretrained(""sefaozalpadl/election_relevancy_best"")

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,376399758.44717854,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
26845,stop_the_steal_relevancy_analysis-binary,['sefaozalpadl/autonlp-data-stop_the_steal_relevancy_analysis'],,0.6503024714880831,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,328525933.0,False,8,0,"['transformers', 'pytorch']",2021-11-07 16:57:11+00:00,2021-10-21 21:32:59+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 23995359
- CO2 Emissions (in grams): 0.6503024714880831

## Validation Metrics

- Loss: 0.49598395824432373
- Accuracy: 0.7907801418439716
- Precision: 0.7841726618705036
- Recall: 0.7898550724637681
- AUC: 0.8774154589371981
- F1: 0.7870036101083032

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/sefaozalpadl/stop_the_steal_relevancy_analysis-binary
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sefaozalpadl/stop_the_steal_relevancy_analysis-binary"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sefaozalpadl/stop_the_steal_relevancy_analysis-binary"", use_auth_token=True)

inputs = tokenizer(""take our country back. Stop the steal! #trump2020"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-10,505189427.0803495,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27308,autonlp-mt5-xlsum-25085641,['sienog/autonlp-data-mt5-xlsum'],,11.166602089650883,AutoTrain,Not Specified,Not Specified,Not Specified,,1.173471212387085,,0.5173530000000001,0.454129,2329700301.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 17:20:30+00:00,2021-10-22 17:20:11+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 25085641
- CO2 Emissions (in grams): 11.166602089650883

## Validation Metrics

- Loss: 1.173471212387085
- Rouge1: 51.7353
- Rouge2: 36.6771
- RougeL: 45.4129
- RougeLsum: 48.8512
- Gen Len: 82.9375

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/sienog/autonlp-mt5-xlsum-25085641
```",,,1,[],[],NLP,2021-10,208631084.21846136,0.4836836926201412,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27716,autonlp-AUS-to-US-601516964,['spy24/autonlp-data-AUS-to-US'],,3.393079684327585,AutoTrain,Not Specified,Not Specified,Not Specified,,1.982380628585816,,0.428783,0.428492,891730879.0,True,3,0,"['transformers', 'pytorch']",2022-02-28 11:21:11+00:00,2022-02-28 11:16:26+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 601516964
- CO2 Emissions (in grams): 3.3930796843275846

## Validation Metrics

- Loss: 1.9823806285858154
- Rouge1: 42.8783
- Rouge2: 7.4603
- RougeL: 42.8492
- RougeLsum: 43.0556
- Gen Len: 2.8952

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-AUS-to-US-601516964
```",,,1,[],[],NLP,2022-02,262808705.35367832,0.4286374506103643,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27717,autonlp-AUS-to-US2-606817121,['spy24/autonlp-data-AUS-to-US2'],,1.1512164322839105,AutoTrain,Not Specified,Not Specified,Not Specified,,2.0312094688415527,,0.348844,0.3463389999999999,891730879.0,True,3,1,"['transformers', 'pytorch']",2022-03-02 10:00:43+00:00,2022-03-02 09:59:04+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 606817121
- CO2 Emissions (in grams): 1.1512164322839105

## Validation Metrics

- Loss: 2.0312094688415527
- Rouge1: 34.8844
- Rouge2: 5.2023
- RougeL: 34.6339
- RougeLsum: 34.8555
- Gen Len: 3.1792

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-AUS-to-US2-606817121
```",,,1,[],[],NLP,2022-03,774598810.4347031,0.3475869867818977,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27718,autonlp-UK-to-US-600416931,['spy24/autonlp-data-UK-to-US'],,1.113131499202784,AutoTrain,Not Specified,Not Specified,Not Specified,,1.8278849124908447,,0.457945,0.458031,891730879.0,True,3,1,"['transformers', 'pytorch']",2022-02-28 09:59:04+00:00,2022-02-28 09:57:19+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 600416931
- CO2 Emissions (in grams): 1.113131499202784

## Validation Metrics

- Loss: 1.8278849124908447
- Rouge1: 45.7945
- Rouge2: 8.5245
- RougeL: 45.8031
- RougeLsum: 45.9067
- Gen Len: 3.0622

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-UK-to-US-600416931
```",,,1,[],[],NLP,2022-02,801101109.4723763,0.4579879959627763,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27719,autonlp-US-to-AUS3-606917136,['spy24/autonlp-data-US-to-AUS3'],,1.2956300881026075,AutoTrain,Not Specified,Not Specified,Not Specified,,2.2489309310913086,,0.310639,0.311492,891730879.0,True,3,0,"['transformers', 'pytorch']",2022-03-02 10:03:47+00:00,2022-03-02 10:02:01+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 606917136
- CO2 Emissions (in grams): 1.2956300881026077

## Validation Metrics

- Loss: 2.2489309310913086
- Rouge1: 31.0639
- Rouge2: 2.2447
- RougeL: 31.1492
- RougeLsum: 31.1753
- Gen Len: 3.4798

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-US-to-AUS3-606917136
```",,,1,[],[],NLP,2022-03,688260397.152323,0.3110649152284647,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27720,autonlp-US-to-UK-604417040,['spy24/autonlp-data-US-to-UK'],,3.327166794864461,AutoTrain,Not Specified,Not Specified,Not Specified,,1.919085144996643,,0.392808,0.39113,891730879.0,True,3,0,"['transformers', 'pytorch']",2022-03-01 13:16:47+00:00,2022-03-01 13:11:42+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 604417040
- CO2 Emissions (in grams): 3.3271667948644614

## Validation Metrics

- Loss: 1.919085144996643
- Rouge1: 39.2808
- Rouge2: 4.905
- RougeL: 39.113
- RougeLsum: 39.1463
- Gen Len: 3.4611

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-US-to-UK-604417040
```",,,1,[],[],NLP,2022-03,268015081.2927088,0.3919672041411438,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27721,autonlp-US-to-UK2-606317091,['spy24/autonlp-data-US-to-UK2'],,1.1913570653422176,AutoTrain,Not Specified,Not Specified,Not Specified,,1.9264822006225584,,0.4420349999999999,0.439114,891730879.0,True,3,1,"['transformers', 'pytorch']",2022-03-02 09:03:19+00:00,2022-03-02 09:01:34+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 606317091
- CO2 Emissions (in grams): 1.1913570653422176

## Validation Metrics

- Loss: 1.9264822006225586
- Rouge1: 44.2035
- Rouge2: 6.134
- RougeL: 43.9114
- RougeLsum: 44.0231
- Gen Len: 3.6134

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-US-to-UK2-606317091
```",,,1,[],[],NLP,2022-03,748500097.0249419,0.4405696584573098,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27722,autonlp-US_to_AUS-607117159,['spy24/autonlp-data-US_to_AUS'],,1.4276876566788057,AutoTrain,Not Specified,Not Specified,Not Specified,,1.517797350883484,,0.46134,0.458856,891730879.0,True,3,1,"['transformers', 'pytorch']",2022-03-02 10:35:42+00:00,2022-03-02 10:33:38+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 607117159
- CO2 Emissions (in grams): 1.4276876566788055

## Validation Metrics

- Loss: 1.5177973508834839
- Rouge1: 46.134
- Rouge2: 10.578
- RougeL: 45.8856
- RougeLsum: 46.0088
- Gen Len: 3.7283

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-US_to_AUS-607117159
```",,,1,[],[],NLP,2022-03,624598016.8200176,0.4600946473142678,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27723,autonlp-paraphrasing-607217177,['spy24/autonlp-data-paraphrasing'],,193.70003779879124,AutoTrain,Not Specified,Not Specified,Not Specified,,1.2881609201431274,,0.483375,0.422748,891730879.0,True,15,1,"['transformers', 'pytorch']",2022-03-02 14:26:32+00:00,2022-03-02 12:17:36+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 607217177
- CO2 Emissions (in grams): 193.70003779879124

## Validation Metrics

- Loss: 1.2881609201431274
- Rouge1: 48.3375
- Rouge2: 25.9756
- RougeL: 42.2748
- RougeLsum: 42.2797
- Gen Len: 18.4359

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-paraphrasing-607217177
```",,,1,[],[],NLP,2022-03,4603669.101635894,0.4510332802500323,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
27866,autonlp-new-text-classification-38319698,['staceythompson/autonlp-data-new-text-classification'],,2.0318857468309206,AutoTrain,Not Specified,Not Specified,Not Specified,0.9909255898366606,0.0446158237755298,0.9951842095089772,,,267869297.0,True,6,0,"['transformers', 'pytorch']",2021-12-03 14:06:55+00:00,2021-12-03 14:06:50+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 38319698
- CO2 Emissions (in grams): 2.0318857468309206

## Validation Metrics

- Loss: 0.04461582377552986
- Accuracy: 0.9909255898366606
- Macro F1: 0.9951842095089771
- Micro F1: 0.9909255898366606
- Weighted F1: 0.9909493945587176
- Macro Precision: 0.9942196531791907
- Micro Precision: 0.9909255898366606
- Weighted Precision: 0.9911878560263526
- Macro Recall: 0.9962686567164181
- Micro Recall: 0.9909255898366606
- Weighted Recall: 0.9909255898366606


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/staceythompson/autonlp-new-text-classification-38319698
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""staceythompson/autonlp-new-text-classification-38319698"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""staceythompson/autonlp-new-text-classification-38319698"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,131832853.99673124,0.9930503340034084,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28393,autonlp-more_fine_tune_24465520-26265897,['teacookies/autonlp-data-more_fine_tune_24465520'],,81.7509252560808,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5754176378250122,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:21:10+00:00,2021-10-25 09:21:00+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265897
- CO2 Emissions (in grams): 81.7509252560808

## Validation Metrics

- Loss: 0.5754176378250122

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265897
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265897"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265897"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,13576642.533687323,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28394,autonlp-more_fine_tune_24465520-26265898,['teacookies/autonlp-data-more_fine_tune_24465520'],,82.78379967029494,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5732079148292542,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:22:22+00:00,2021-10-25 09:22:11+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265898
- CO2 Emissions (in grams): 82.78379967029494

## Validation Metrics

- Loss: 0.5732079148292542

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265898
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265898"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265898"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,13407249.889718017,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28395,autonlp-more_fine_tune_24465520-26265899,['teacookies/autonlp-data-more_fine_tune_24465520'],,124.66009281731397,AutoTrain,Not Specified,Not Specified,Not Specified,,0.7011443972587585,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:51:18+00:00,2021-10-25 09:51:07+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265899
- CO2 Emissions (in grams): 124.66009281731397

## Validation Metrics

- Loss: 0.7011443972587585

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265899
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265899"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265899"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,8903435.445267422,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28396,autonlp-more_fine_tune_24465520-26265900,['teacookies/autonlp-data-more_fine_tune_24465520'],,123.16270720220912,AutoTrain,Not Specified,Not Specified,Not Specified,,0.6387976408004761,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:51:20+00:00,2021-10-25 09:51:10+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265900
- CO2 Emissions (in grams): 123.16270720220912

## Validation Metrics

- Loss: 0.6387976408004761

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265900
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265900"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265900"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,9011681.492010044,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28397,autonlp-more_fine_tune_24465520-26265901,['teacookies/autonlp-data-more_fine_tune_24465520'],,80.04360178242067,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5551259517669678,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:21:03+00:00,2021-10-25 09:20:53+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265901
- CO2 Emissions (in grams): 80.04360178242067

## Validation Metrics

- Loss: 0.5551259517669678

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265901
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265901"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265901"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,13866231.207548672,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28398,autonlp-more_fine_tune_24465520-26265902,['teacookies/autonlp-data-more_fine_tune_24465520'],,83.78453848505326,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5470030903816223,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:22:00+00:00,2021-10-25 09:21:50+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265902
- CO2 Emissions (in grams): 83.78453848505326

## Validation Metrics

- Loss: 0.5470030903816223

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265902
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265902"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265902"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,13247111.090765286,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28399,autonlp-more_fine_tune_24465520-26265903,['teacookies/autonlp-data-more_fine_tune_24465520'],,108.13983395548236,AutoTrain,Not Specified,Not Specified,Not Specified,,0.6330059170722961,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:35:40+00:00,2021-10-25 09:35:30+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265903
- CO2 Emissions (in grams): 108.13983395548236

## Validation Metrics

- Loss: 0.6330059170722961

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265903
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265903"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265903"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,10263591.57770587,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28400,autonlp-more_fine_tune_24465520-26265904,['teacookies/autonlp-data-more_fine_tune_24465520'],,108.63800043275934,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5807144045829773,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:36:11+00:00,2021-10-25 09:36:01+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265904
- CO2 Emissions (in grams): 108.63800043275934

## Validation Metrics

- Loss: 0.5807144045829773

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265904
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265904"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265904"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,10216527.22416376,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28401,autonlp-more_fine_tune_24465520-26265905,['teacookies/autonlp-data-more_fine_tune_24465520'],,103.35758036182682,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5223112106323242,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:32:48+00:00,2021-10-25 09:32:38+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265905
- CO2 Emissions (in grams): 103.35758036182682

## Validation Metrics

- Loss: 0.5223112106323242

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265905
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265905"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265905"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,10738477.866011672,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28402,autonlp-more_fine_tune_24465520-26265906,['teacookies/autonlp-data-more_fine_tune_24465520'],,83.00580438705762,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5259918570518494,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:22:17+00:00,2021-10-25 09:22:07+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265906
- CO2 Emissions (in grams): 83.00580438705762

## Validation Metrics

- Loss: 0.5259918570518494

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265906
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265906"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265906"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,13371391.28035554,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28403,autonlp-more_fine_tune_24465520-26265907,['teacookies/autonlp-data-more_fine_tune_24465520'],,103.5636883689371,AutoTrain,Not Specified,Not Specified,Not Specified,,0.6072460412979126,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:35:36+00:00,2021-10-25 09:35:26+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265907
- CO2 Emissions (in grams): 103.5636883689371

## Validation Metrics

- Loss: 0.6072460412979126

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265907
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265907"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265907"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,10717106.608312964,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28404,autonlp-more_fine_tune_24465520-26265908,['teacookies/autonlp-data-more_fine_tune_24465520'],,96.32087452115675,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5696008801460266,,,,1109903089.0,True,28,0,"['transformers', 'pytorch']",2021-10-25 09:36:35+00:00,2021-10-25 09:36:25+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265908
- CO2 Emissions (in grams): 96.32087452115675

## Validation Metrics

- Loss: 0.5696008801460266

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265908
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265908"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265908"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,11522975.62203104,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28405,autonlp-more_fine_tune_24465520-26265909,['teacookies/autonlp-data-more_fine_tune_24465520'],,80.25874179679201,AutoTrain,Not Specified,Not Specified,Not Specified,,5.950643062591553,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:20:12+00:00,2021-10-25 09:20:02+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265909
- CO2 Emissions (in grams): 80.25874179679201

## Validation Metrics

- Loss: 5.950643062591553

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265909
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265909"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265909"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,13829061.659229292,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28406,autonlp-more_fine_tune_24465520-26265910,['teacookies/autonlp-data-more_fine_tune_24465520'],,77.64468929470678,AutoTrain,Not Specified,Not Specified,Not Specified,,5.950643062591553,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:21:45+00:00,2021-10-25 09:21:35+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265910
- CO2 Emissions (in grams): 77.64468929470678

## Validation Metrics

- Loss: 5.950643062591553

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265910
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265910"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265910"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,14294642.673979567,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28407,autonlp-more_fine_tune_24465520-26265911,['teacookies/autonlp-data-more_fine_tune_24465520'],,97.58591836686978,AutoTrain,Not Specified,Not Specified,Not Specified,,6.2383246421813965,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-25 09:35:36+00:00,2021-10-25 09:35:26+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 26265911
- CO2 Emissions (in grams): 97.58591836686978

## Validation Metrics

- Loss: 6.2383246421813965

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-more_fine_tune_24465520-26265911
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265911"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-more_fine_tune_24465520-26265911"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,11373598.850885129,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28408,autonlp-roberta-base-squad2-24465514,['teacookies/autonlp-data-roberta-base-squad2'],,54.44076291568145,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5786784887313843,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:10:51+00:00,2021-10-22 08:10:41+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465514
- CO2 Emissions (in grams): 54.44076291568145

## Validation Metrics

- Loss: 0.5786784887313843

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465514
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465514"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465514"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,20387353.69522709,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28409,autonlp-roberta-base-squad2-24465515,['teacookies/autonlp-data-roberta-base-squad2'],,56.45146749922553,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5932255387306213,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:11:45+00:00,2021-10-22 08:11:35+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465515
- CO2 Emissions (in grams): 56.45146749922553

## Validation Metrics

- Loss: 0.5932255387306213

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465515
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465515"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465515"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,19661191.07559475,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28410,autonlp-roberta-base-squad2-24465516,['teacookies/autonlp-data-roberta-base-squad2'],,65.5797497320557,AutoTrain,Not Specified,Not Specified,Not Specified,,0.6545609831809998,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:21:22+00:00,2021-10-22 08:21:11+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465516
- CO2 Emissions (in grams): 65.5797497320557

## Validation Metrics

- Loss: 0.6545609831809998

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465516
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465516"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465516"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,16924478.87548851,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28411,autonlp-roberta-base-squad2-24465517,['teacookies/autonlp-data-roberta-base-squad2'],,54.75747617143382,AutoTrain,Not Specified,Not Specified,Not Specified,,0.6653227806091309,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:13:41+00:00,2021-10-22 08:13:30+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465517
- CO2 Emissions (in grams): 54.75747617143382

## Validation Metrics

- Loss: 0.6653227806091309

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465517
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465517"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465517"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,20269434.72568263,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28412,autonlp-roberta-base-squad2-24465518,['teacookies/autonlp-data-roberta-base-squad2'],,45.268576304018616,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5742421746253967,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:04:33+00:00,2021-10-22 08:04:23+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465518
- CO2 Emissions (in grams): 45.268576304018616

## Validation Metrics

- Loss: 0.5742421746253967

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465518
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465518"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465518"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,24518179.709165517,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28413,autonlp-roberta-base-squad2-24465519,['teacookies/autonlp-data-roberta-base-squad2'],,58.19097299648645,AutoTrain,Not Specified,Not Specified,Not Specified,,0.566668689250946,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:13:26+00:00,2021-10-22 08:13:15+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465519
- CO2 Emissions (in grams): 58.19097299648645

## Validation Metrics

- Loss: 0.566668689250946

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465519
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465519"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465519"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,19073458.164499424,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28414,autonlp-roberta-base-squad2-24465520,['teacookies/autonlp-data-roberta-base-squad2'],,57.56554511511173,AutoTrain,Not Specified,Not Specified,Not Specified,,0.6455457806587219,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:13:49+00:00,2021-10-22 08:13:38+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465520
- CO2 Emissions (in grams): 57.56554511511173

## Validation Metrics

- Loss: 0.6455457806587219

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465520
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465520"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465520"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,19280684.075527597,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28415,autonlp-roberta-base-squad2-24465521,['teacookies/autonlp-data-roberta-base-squad2'],,70.20260764805424,AutoTrain,Not Specified,Not Specified,Not Specified,,0.6295848488807678,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:21:40+00:00,2021-10-22 08:21:30+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465521
- CO2 Emissions (in grams): 70.20260764805424

## Validation Metrics

- Loss: 0.6295848488807678

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465521
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465521"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465521"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,15809998.035461329,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28416,autonlp-roberta-base-squad2-24465522,['teacookies/autonlp-data-roberta-base-squad2'],,44.450538076574766,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5572742223739624,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:05:40+00:00,2021-10-22 08:05:29+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465522
- CO2 Emissions (in grams): 44.450538076574766

## Validation Metrics

- Loss: 0.5572742223739624

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465522
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465522"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465522"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,24969396.03043667,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28417,autonlp-roberta-base-squad2-24465523,['teacookies/autonlp-data-roberta-base-squad2'],,56.99866929988893,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5468788146972656,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:13:18+00:00,2021-10-22 08:13:08+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465523
- CO2 Emissions (in grams): 56.99866929988893

## Validation Metrics

- Loss: 0.5468788146972656

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465523
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465523"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465523"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,19472438.613618,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28418,autonlp-roberta-base-squad2-24465524,['teacookies/autonlp-data-roberta-base-squad2'],,58.51753681929935,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5759999752044678,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:14:00+00:00,2021-10-22 08:13:50+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465524
- CO2 Emissions (in grams): 58.51753681929935

## Validation Metrics

- Loss: 0.5759999752044678

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465524
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465524"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465524"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,18967016.544584785,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28419,autonlp-roberta-base-squad2-24465525,['teacookies/autonlp-data-roberta-base-squad2'],,63.99723026110488,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5740988850593567,,,,1109903089.0,True,3,0,"['transformers', 'pytorch']",2021-10-22 08:23:09+00:00,2021-10-22 08:22:59+00:00,"
# Model Trained Using AutoNLP

- Problem type: Extractive Question Answering
- Model ID: 24465525
- CO2 Emissions (in grams): 63.997230261104875

## Validation Metrics

- Loss: 0.5740988850593567

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""question"": ""Who loves AutoNLP?"", ""context"": ""Everyone loves AutoNLP""}' https://api-inference.huggingface.co/models/teacookies/autonlp-roberta-base-squad2-24465525
```

Or Python API:

```
import torch

from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465525"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autonlp-roberta-base-squad2-24465525"", use_auth_token=True)

from transformers import BertTokenizer, BertForQuestionAnswering

question, text = ""Who loves AutoNLP?"", ""Everyone loves AutoNLP""

inputs = tokenizer(question, text, return_tensors='pt')

start_positions = torch.tensor([1])

end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)

loss = outputs.loss

start_scores = outputs.start_logits

end_scores = outputs.end_logits
```",,,1,[],[],NLP,2021-10,17342986.32099642,,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,1,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
28512,autonlp-ingredient_sentiment_analysis-19126711,['testing/autonlp-data-ingredient_sentiment_analysis'],,1.8458289701133035,AutoTrain,Not Specified,Not Specified,Not Specified,0.9790668170284748,0.0545931719243526,0.6885245901639344,,,1336542385.0,True,5,0,"['transformers', 'pytorch']",2021-11-04 15:54:28+00:00,2021-11-04 15:54:16+00:00,"
# Model Trained Using AutoNLP

- Problem type: Entity Extraction
- Model ID: 19126711
- CO2 Emissions (in grams): 1.8458289701133035

## Validation Metrics

- Loss: 0.054593171924352646
- Accuracy: 0.9790668170284748
- Precision: 0.8029411764705883
- Recall: 0.6026490066225165
- F1: 0.6885245901639344

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/testing/autonlp-ingredient_sentiment_analysis-19126711
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""testing/autonlp-ingredient_sentiment_analysis-19126711"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""testing/autonlp-ingredient_sentiment_analysis-19126711"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,724087879.5601298,0.8084853112461,0.0,0.0,0.0,0.0,0,1.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
29517,Robertabase_Ana4,['vinaydngowda/autonlp-data-case-classify-xlnet'],,19.964760910364927,AutoTrain,Not Specified,Not Specified,Not Specified,0.8092592592592592,0.7149562835693359,0.8085189591849891,,,1340766125.0,True,17,0,"['transformers', 'pytorch']",2022-01-12 20:12:16+00:00,2022-01-12 20:00:17+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 496213536
- CO2 Emissions (in grams): 19.964760910364927

## Validation Metrics

- Loss: 0.7149562835693359
- Accuracy: 0.8092592592592592
- Macro F1: 0.8085189591849891
- Micro F1: 0.8092592592592593
- Weighted F1: 0.8085189591849888
- Macro Precision: 0.8137745564384112
- Micro Precision: 0.8092592592592592
- Weighted Precision: 0.8137745564384112
- Macro Recall: 0.8092592592592592
- Micro Recall: 0.8092592592592592
- Weighted Recall: 0.8092592592592592


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/vinaydngowda/autonlp-case-classify-xlnet-496213536
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""vinaydngowda/autonlp-case-classify-xlnet-496213536"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""vinaydngowda/autonlp-case-classify-xlnet-496213536"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,67156633.18081242,0.8088889398403777,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
29741,autonlp-poi_train-31237266,['wangsheng/autonlp-data-poi_train'],,390.3941117677583,AutoTrain,Not Specified,Not Specified,Not Specified,0.9379398019660156,0.1643059253692627,0.7309841664079478,,,1302259629.0,True,8,0,"['transformers', 'pytorch']",2021-11-10 14:09:14+00:00,2021-11-10 14:09:03+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 31237266
- CO2 Emissions (in grams): 390.39411176775826

## Validation Metrics

- Loss: 0.1643059253692627
- Accuracy: 0.9379398019660155
- Precision: 0.7467491278147795
- Recall: 0.7158710854363028
- AUC: 0.9631629384458238
- F1: 0.7309841664079478

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/wangsheng/autonlp-poi_train-31237266
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""wangsheng/autonlp-poi_train-31237266"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""wangsheng/autonlp-poi_train-31237266"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-11,3335756.3286576974,0.8216301728220297,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30216,autonlp-imdb-sentiment-analysis-english-470512388,['yosemite/autonlp-data-imdb-sentiment-analysis-english'],,256.38650494338367,AutoTrain,Not Specified,Not Specified,Not Specified,0.9388,0.187127336859703,0.9394179370421698,,,1334486957.0,True,35,1,"['transformers', 'pytorch']",2022-01-04 17:34:50+00:00,2022-01-04 17:34:39+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 470512388
- CO2 Emissions (in grams): 256.38650494338367

## Validation Metrics

- Loss: 0.18712733685970306
- Accuracy: 0.9388
- Precision: 0.9300274402195218
- Recall: 0.949
- AUC: 0.98323192
- F1: 0.9394179370421698

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/yosemite/autonlp-imdb-sentiment-analysis-english-470512388
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""yosemite/autonlp-imdb-sentiment-analysis-english-470512388"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""yosemite/autonlp-imdb-sentiment-analysis-english-470512388"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,5204981.273467131,0.9391088668698916,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30277,autonlp-test-459011902,['ysslang/autonlp-data-test'],,10.9230691350863,AutoTrain,Not Specified,Not Specified,Not Specified,0.7453263867606497,0.7189690470695496,0.630810193227066,,,409185453.0,True,23,0,"['transformers', 'pytorch']",2021-12-30 17:05:31+00:00,2021-12-30 17:05:23+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 459011902
- CO2 Emissions (in grams): 10.9230691350863

## Validation Metrics

- Loss: 0.7189690470695496
- Accuracy: 0.7453263867606497
- Macro F1: 0.630810193227066
- Micro F1: 0.7453263867606497
- Weighted F1: 0.7399327942874923
- Macro Precision: 0.656237447101913
- Micro Precision: 0.7453263867606497
- Weighted Precision: 0.7410161412822164
- Macro Recall: 0.6340140718425453
- Micro Recall: 0.7453263867606497
- Weighted Recall: 0.7453263867606497


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/ysslang/autonlp-test-459011902
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ysslang/autonlp-test-459011902"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ysslang/autonlp-test-459011902"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,37460666.77227592,0.6833035163616002,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30606,autonlp-traffic-nlp-451311592,['zwang199/autonlp-data-traffic-nlp'],,1.869714429686524,AutoTrain,Not Specified,Not Specified,Not Specified,0.8042452830188679,0.4544260799884796,0.8450528935905414,,,433331373.0,True,28,0,"['transformers', 'pytorch']",2021-12-27 18:31:57+00:00,2021-12-27 18:31:52+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 451311592
- CO2 Emissions (in grams): 1.8697144296865242

## Validation Metrics

- Loss: 0.4544260799884796
- Accuracy: 0.8042452830188679
- Precision: 0.8331288343558282
- Recall: 0.8573232323232324
- AUC: 0.8759811658249159
- F1: 0.8450528935905414

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/zwang199/autonlp-traffic-nlp-451311592
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""zwang199/autonlp-traffic-nlp-451311592"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""zwang199/autonlp-traffic-nlp-451311592"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2021-12,231763399.864573,0.8241442490026953,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30607,autonlp-traffic_nlp_binary-537215209,['zwang199/autonlp-data-traffic_nlp_binary'],,1.171798205242445,AutoTrain,Not Specified,Not Specified,Not Specified,0.8597449908925319,0.3879534602165222,0.8760064412238325,,,498674093.0,True,6,0,"['transformers', 'pytorch']",2022-01-28 19:34:25+00:00,2022-01-28 19:33:18+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 537215209
- CO2 Emissions (in grams): 1.171798205242445

## Validation Metrics

- Loss: 0.3879534602165222
- Accuracy: 0.8597449908925319
- Precision: 0.8318042813455657
- Recall: 0.9251700680272109
- AUC: 0.9230158730158731
- F1: 0.8760064412238325

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/zwang199/autonlp-traffic_nlp_binary-537215209
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""zwang199/autonlp-traffic_nlp_binary-537215209"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""zwang199/autonlp-traffic_nlp_binary-537215209"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-01,425563113.8271153,0.8677995430641737,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30631,autonlp-lessons_tagging-606217261,['Kamuuung/autonlp-data-lessons_tagging'],,7.968891750522204,AutoTrain,Not Specified,Not Specified,Not Specified,0.6777163904235728,0.989620566368103,0.6817448899563519,,,1421639981.0,True,8,0,"['transformers', 'pytorch']",2022-03-03 04:25:37+00:00,2022-03-03 04:19:25+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 606217261
- CO2 Emissions (in grams): 7.968891750522204

## Validation Metrics

- Loss: 0.989620566368103
- Accuracy: 0.6777163904235728
- Macro F1: 0.6817448899563519
- Micro F1: 0.6777163904235728
- Weighted F1: 0.6590820060806175
- Macro Precision: 0.7028251935864661
- Micro Precision: 0.6777163904235728
- Weighted Precision: 0.6764567648776801
- Macro Recall: 0.6861061576846053
- Micro Recall: 0.6777163904235728
- Weighted Recall: 0.6777163904235728


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Kamuuung/autonlp-lessons_tagging-606217261
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Kamuuung/autonlp-lessons_tagging-606217261"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Kamuuung/autonlp-lessons_tagging-606217261"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,178398706.5587683,0.679724671351894,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30916,autonlp-cyberlandr-ai-4-614417501,['billfrench/autonlp-data-cyberlandr-ai-4'],,1.6912535041856878,AutoTrain,Not Specified,Not Specified,Not Specified,0.5,1.305419921875,0.3333333333333333,,,1340745645.0,True,17,0,"['transformers', 'pytorch']",2022-03-07 00:57:12+00:00,2022-03-07 00:54:15+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 614417501
- CO2 Emissions (in grams): 1.6912535041856878

## Validation Metrics

- Loss: 1.305419921875
- Accuracy: 0.5
- Macro F1: 0.3333333333333333
- Micro F1: 0.5
- Weighted F1: 0.4444444444444444
- Macro Precision: 0.375
- Micro Precision: 0.5
- Weighted Precision: 0.5
- Macro Recall: 0.375
- Micro Recall: 0.5
- Weighted Recall: 0.5


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/billfrench/autonlp-cyberlandr-ai-4-614417501
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""billfrench/autonlp-cyberlandr-ai-4-614417501"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""billfrench/autonlp-cyberlandr-ai-4-614417501"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,792752619.0968918,0.4,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30917,autonlp-cyberlandr-ai-4-614417500,['billfrench/autonlp-data-cyberlandr-ai-4'],,1.131603488976132,AutoTrain,Not Specified,Not Specified,Not Specified,0.3333333333333333,1.4588216543197632,0.225,,,1334495149.0,True,17,0,"['transformers', 'pytorch']",2022-03-07 00:56:09+00:00,2022-03-07 00:54:24+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 614417500
- CO2 Emissions (in grams): 1.131603488976132

## Validation Metrics

- Loss: 1.4588216543197632
- Accuracy: 0.3333333333333333
- Macro F1: 0.225
- Micro F1: 0.3333333333333333
- Weighted F1: 0.2333333333333333
- Macro Precision: 0.1875
- Micro Precision: 0.3333333333333333
- Weighted Precision: 0.20833333333333334
- Macro Recall: 0.375
- Micro Recall: 0.3333333333333333
- Weighted Recall: 0.3333333333333333


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/billfrench/autonlp-cyberlandr-ai-4-614417500
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""billfrench/autonlp-cyberlandr-ai-4-614417500"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""billfrench/autonlp-cyberlandr-ai-4-614417500"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,1179295717.9793105,0.2686567164179104,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30940,autonlp-optimized-paraphrasing-615217541,['spy24/autonlp-data-optimized-paraphrasing'],,1.166696812121839,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0001954936888068,,1.0,1.0,891730879.0,True,13,0,"['transformers', 'pytorch']",2022-03-07 08:56:14+00:00,2022-03-07 08:54:32+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 615217541
- CO2 Emissions (in grams): 1.166696812121839

## Validation Metrics

- Loss: 0.00019549368880689144
- Rouge1: 100.0
- Rouge2: 51.4451
- RougeL: 100.0
- RougeLsum: 100.0
- Gen Len: 4.104

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-optimized-paraphrasing-615217541
```",,,1,[],[],NLP,2022-03,764321004.1675128,1.0,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30947,autonlp-parrot_paraphrasing-615317556,['spy24/autonlp-data-parrot_paraphrasing'],,0.8335491678002559,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0001514342293376,,1.0,1.0,891730879.0,True,13,0,"['transformers', 'pytorch']",2022-03-07 09:36:20+00:00,2022-03-07 09:35:01+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 615317556
- CO2 Emissions (in grams): 0.8335491678002559

## Validation Metrics

- Loss: 0.0001514342293376103
- Rouge1: 100.0
- Rouge2: 51.4451
- RougeL: 100.0
- RougeLsum: 100.0
- Gen Len: 4.104

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/spy24/autonlp-parrot_paraphrasing-615317556
```",,,1,[],[],NLP,2022-03,1069799975.1511792,1.0,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
30973,autonlp-swahili-sentiment-615517563,['abhishek/autonlp-data-swahili-sentiment'],,1.9057858628956457,AutoTrain,Not Specified,Not Specified,Not Specified,0.695364238410596,0.6990908980369568,0.6088819062581828,,,438022317.0,True,39,0,"['transformers', 'pytorch']",2022-03-07 12:54:03+00:00,2022-03-07 12:52:44+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 615517563
- CO2 Emissions (in grams): 1.9057858628956459

## Validation Metrics

- Loss: 0.6990908980369568
- Accuracy: 0.695364238410596
- Macro F1: 0.6088819062581828
- Micro F1: 0.695364238410596
- Weighted F1: 0.677326207350606
- Macro Precision: 0.6945099492363175
- Micro Precision: 0.695364238410596
- Weighted Precision: 0.6938596845881614
- Macro Recall: 0.5738408020723632
- Micro Recall: 0.695364238410596
- Weighted Recall: 0.695364238410596


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/abhishek/autonlp-swahili-sentiment-615517563
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""abhishek/autonlp-swahili-sentiment-615517563"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""abhishek/autonlp-swahili-sentiment-615517563"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,229838160.4817186,0.6492558245356927,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31246,autonlp-abbb-622117836,['kyleinincubated/autonlp-data-abbb'],,2.22514962526191,AutoTrain,Not Specified,Not Specified,Not Specified,0.7973333333333333,1.2368708848953247,0.4600907658897848,,,409640685.0,True,28,0,"['transformers', 'pytorch']",2022-03-09 09:30:07+00:00,2022-03-09 09:27:47+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 622117836
- CO2 Emissions (in grams): 2.22514962526191

## Validation Metrics

- Loss: 1.2368708848953247
- Accuracy: 0.7973333333333333
- Macro F1: 0.46009076588978487
- Micro F1: 0.7973333333333333
- Weighted F1: 0.7712349116681224
- Macro Precision: 0.4527155928883903
- Micro Precision: 0.7973333333333333
- Weighted Precision: 0.7610710955220162
- Macro Recall: 0.4947868561369568
- Micro Recall: 0.7973333333333333
- Weighted Recall: 0.7973333333333333


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/kyleinincubated/autonlp-abbb-622117836
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kyleinincubated/autonlp-abbb-622117836"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kyleinincubated/autonlp-abbb-622117836"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,184095793.08707544,0.583487630353894,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31333,autonlp-imdb-sentiment-analysis-623817873,['chiragme/autonlp-data-imdb-sentiment-analysis'],,147.38973865706626,AutoTrain,Not Specified,Not Specified,Not Specified,0.9306,0.2412157654762268,0.930026214962694,,,1334486957.0,True,20,0,"['transformers', 'pytorch']",2022-03-10 03:28:02+00:00,2022-03-10 02:03:27+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 623817873
- CO2 Emissions (in grams): 147.38973865706626

## Validation Metrics

- Loss: 0.2412157654762268
- Accuracy: 0.9306
- Precision: 0.9377795851972347
- Recall: 0.9224
- AUC: 0.97000504
- F1: 0.9300262149626941

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/chiragme/autonlp-imdb-sentiment-analysis-623817873
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""chiragme/autonlp-imdb-sentiment-analysis-623817873"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""chiragme/autonlp-imdb-sentiment-analysis-623817873"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,9054137.48039115,0.9303130190086418,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31339,autonlp-cat333-624217911,['kyleinincubated/autonlp-data-cat333'],,2.267288583123193,AutoTrain,Not Specified,Not Specified,Not Specified,0.90989010989011,0.3967024981975555,0.7398394202169645,,,409243885.0,True,28,0,"['transformers', 'pytorch']",2022-03-10 03:47:17+00:00,2022-03-10 03:45:34+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 624217911
- CO2 Emissions (in grams): 2.267288583123193

## Validation Metrics

- Loss: 0.39670249819755554
- Accuracy: 0.9098901098901099
- Macro F1: 0.7398394202169645
- Micro F1: 0.9098901098901099
- Weighted F1: 0.9073329464119164
- Macro Precision: 0.7653753530396269
- Micro Precision: 0.9098901098901099
- Weighted Precision: 0.9096917983040914
- Macro Recall: 0.7382843728794468
- Micro Recall: 0.9098901098901099
- Weighted Recall: 0.9098901098901099


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/kyleinincubated/autonlp-cat333-624217911
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kyleinincubated/autonlp-cat333-624217911"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kyleinincubated/autonlp-cat333-624217911"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,180499248.32959116,0.8161005293013786,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31351,autonlp-cat33-624317932,['kyleinincubated/autonlp-data-cat33'],,1.2490471218570545,AutoTrain,Not Specified,Not Specified,Not Specified,0.8717391304347826,0.5579860806465149,0.6625543939916455,,,409253165.0,True,28,0,"['transformers', 'pytorch']",2022-03-10 06:10:56+00:00,2022-03-10 06:09:35+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 624317932
- CO2 Emissions (in grams): 1.2490471218570545

## Validation Metrics

- Loss: 0.5579860806465149
- Accuracy: 0.8717391304347826
- Macro F1: 0.6625543939916455
- Micro F1: 0.8717391304347827
- Weighted F1: 0.8593303742671491
- Macro Precision: 0.7214757380849891
- Micro Precision: 0.8717391304347826
- Weighted Precision: 0.8629042654788023
- Macro Recall: 0.6540187758140144
- Micro Recall: 0.8717391304347826
- Weighted Recall: 0.8717391304347826


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/kyleinincubated/autonlp-cat33-624317932
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kyleinincubated/autonlp-cat33-624317932"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kyleinincubated/autonlp-cat33-624317932"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,327652302.1737817,0.7528866961749562,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31406,autonlp-mono-625317956,['Chijioke/autonlp-data-mono'],,1.1406456838043837,AutoTrain,Not Specified,Not Specified,Not Specified,0.8982035928143712,0.513037919998169,0.7843756230226546,,,267924657.0,True,6,0,"['transformers', 'pytorch']",2022-03-10 12:46:27+00:00,2022-03-10 12:45:12+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 625317956
- CO2 Emissions (in grams): 1.1406456838043837

## Validation Metrics

- Loss: 0.513037919998169
- Accuracy: 0.8982035928143712
- Macro F1: 0.7843756230226546
- Micro F1: 0.8982035928143712
- Weighted F1: 0.8891653474608059
- Macro Precision: 0.8210878091622635
- Micro Precision: 0.8982035928143712
- Weighted Precision: 0.8888857327766032
- Macro Recall: 0.7731018645485747
- Micro Recall: 0.8982035928143712
- Weighted Recall: 0.8982035928143712


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Chijioke/autonlp-mono-625317956
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Chijioke/autonlp-mono-625317956"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Chijioke/autonlp-mono-625317956"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,234888590.5624906,0.8374393265811024,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31421,autonlp-kaggledays-625717986,['Someshfengde/autonlp-data-kaggledays'],,68.73074770596023,AutoTrain,Not Specified,Not Specified,Not Specified,0.6118427330852181,0.859463632106781,0.6112554383858383,,,438022317.0,True,28,0,"['transformers', 'pytorch']",2022-03-10 15:27:01+00:00,2022-03-10 14:39:07+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 625717986
- CO2 Emissions (in grams): 68.73074770596023

## Validation Metrics

- Loss: 0.859463632106781
- Accuracy: 0.6118427330852181
- Macro F1: 0.6112554383858383
- Micro F1: 0.6118427330852181
- Weighted F1: 0.6112706859556324
- Macro Precision: 0.6121119616189625
- Micro Precision: 0.6118427330852181
- Weighted Precision: 0.6121068719118146
- Macro Recall: 0.6118067898609261
- Micro Recall: 0.6118427330852181
- Weighted Recall: 0.6118427330852181


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Someshfengde/autonlp-kaggledays-625717986
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Someshfengde/autonlp-kaggledays-625717986"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Someshfengde/autonlp-kaggledays-625717986"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,6373018.359612802,0.6115489447349642,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31422,autonlp-kaggledays-625717992,['Someshfengde/autonlp-data-kaggledays'],,28.622267513847277,AutoTrain,Not Specified,Not Specified,Not Specified,0.6022282660559214,0.8782362937927246,0.6024258279848015,,,263175281.0,True,6,0,"['transformers', 'pytorch']",2022-03-10 15:01:53+00:00,2022-03-10 14:39:17+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 625717992
- CO2 Emissions (in grams): 28.622267513847273

## Validation Metrics

- Loss: 0.8782362937927246
- Accuracy: 0.6022282660559214
- Macro F1: 0.6024258279848015
- Micro F1: 0.6022282660559214
- Weighted F1: 0.6024299908624371
- Macro Precision: 0.604093172183357
- Micro Precision: 0.6022282660559214
- Weighted Precision: 0.6041166306778806
- Macro Recall: 0.6022424576798522
- Micro Recall: 0.6022282660559214
- Weighted Recall: 0.6022282660559214


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/Someshfengde/autonlp-kaggledays-625717992
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Someshfengde/autonlp-kaggledays-625717992"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Someshfengde/autonlp-kaggledays-625717992"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,9194773.99450192,0.6023270308203934,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31464,first,['spy24/autonlp-data-parrot_paraphrasing'],,0.8335491678002559,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,[],2022-03-10 21:35:31+00:00,2022-03-10 21:21:52+00:00,"
# Test",,,1,[],[],Not Specified,2022-03,,,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31619,autonlp-savesome-631818261,['test1345/autonlp-data-savesome'],,5.714250590300453,AutoTrain,Not Specified,Not Specified,Not Specified,0.8792873051224944,0.4465169012546539,0.839261602941426,,,467189229.0,True,16,0,"['transformers', 'pytorch']",2022-03-12 19:00:24+00:00,2022-03-12 18:55:14+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 631818261
- CO2 Emissions (in grams): 5.714250590300453

## Validation Metrics

- Loss: 0.44651690125465393
- Accuracy: 0.8792873051224944
- Macro F1: 0.839261602941426
- Micro F1: 0.8792873051224943
- Weighted F1: 0.8790427387522044
- Macro Precision: 0.8407634723656228
- Micro Precision: 0.8792873051224944
- Weighted Precision: 0.8801219917819031
- Macro Recall: 0.8400328140795883
- Micro Recall: 0.8792873051224944
- Weighted Recall: 0.8792873051224944


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/test1345/autonlp-savesome-631818261
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""test1345/autonlp-savesome-631818261"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""test1345/autonlp-savesome-631818261"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,81758617.62048405,0.8588083466004027,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31658,autonlp-Text-Classification-Catalonia-Independence-AutoNLP-633018323,['DrishtiSharma/autonlp-data-Text-Classification-Catalonia-Independence-AutoNLP'],,3.622203603306694,AutoTrain,Not Specified,Not Specified,Not Specified,0.709136109384711,0.681106686592102,0.6987186860138147,,,433334445.0,True,28,0,"['transformers', 'pytorch']",2022-03-13 07:31:45+00:00,2022-03-13 07:28:50+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 633018323
- CO2 Emissions (in grams): 3.622203603306694

## Validation Metrics

- Loss: 0.681106686592102
- Accuracy: 0.709136109384711
- Macro F1: 0.6987186860138147
- Micro F1: 0.709136109384711
- Weighted F1: 0.7059639788836748
- Macro Precision: 0.7174345617951404
- Micro Precision: 0.709136109384711
- Weighted Precision: 0.712710833401347
- Macro Recall: 0.6912117894374218
- Micro Recall: 0.709136109384711
- Weighted Recall: 0.709136109384711


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/DrishtiSharma/autonlp-Text-Classification-Catalonia-Independence-AutoNLP-633018323
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""DrishtiSharma/autonlp-Text-Classification-Catalonia-Independence-AutoNLP-633018323"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""DrishtiSharma/autonlp-Text-Classification-Catalonia-Independence-AutoNLP-633018323"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,119632823.67794313,0.7038888558304411,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
31832,autonlp-mut_uchile-640218740,['gabitoo1234/autonlp-data-mut_uchile'],,43.078469852596,AutoTrain,Not Specified,Not Specified,Not Specified,0.7887341933835739,0.8302136063575745,0.5756730305293746,,,439573741.0,True,16,0,"['transformers', 'pytorch']",2022-03-14 19:26:47+00:00,2022-03-14 18:57:53+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 640218740
- CO2 Emissions (in grams): 43.078469852595994

## Validation Metrics

- Loss: 0.8302136063575745
- Accuracy: 0.7887341933835739
- Macro F1: 0.5756730305293746
- Micro F1: 0.7887341933835739
- Weighted F1: 0.7878942570915727
- Macro Precision: 0.620883634472996
- Micro Precision: 0.7887341933835739
- Weighted Precision: 0.8009430092038783
- Macro Recall: 0.5521761315904072
- Micro Recall: 0.7887341933835739
- Weighted Recall: 0.7887341933835739


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/gabitoo1234/autonlp-mut_uchile-640218740
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""gabitoo1234/autonlp-mut_uchile-640218740"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""gabitoo1234/autonlp-mut_uchile-640218740"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,10204024.017197317,0.6655681609264672,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32419,autonlp-cai-out-of-scope-649919112,['msamogh/autonlp-data-cai-out-of-scope'],,0.499244806825336,AutoTrain,Not Specified,Not Specified,Not Specified,0.8064516129032258,0.4935429394245147,0.8571428571428572,,,267860081.0,True,7,0,"['transformers', 'pytorch']",2022-03-19 21:40:41+00:00,2022-03-19 21:40:14+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 649919112
- CO2 Emissions (in grams): 0.49924480682533606

## Validation Metrics

- Loss: 0.49354293942451477
- Accuracy: 0.8064516129032258
- Precision: 0.8181818181818182
- Recall: 0.9
- AUC: 0.8689393939393939
- F1: 0.8571428571428572

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/msamogh/autonlp-cai-out-of-scope-649919112
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""msamogh/autonlp-cai-out-of-scope-649919112"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""msamogh/autonlp-cai-out-of-scope-649919112"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,536530530.3890973,0.8310249307479226,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32420,autonlp-cai-out-of-scope-649919118,['msamogh/autonlp-data-cai-out-of-scope'],,0.3996916853309825,AutoTrain,Not Specified,Not Specified,Not Specified,0.8064516129032258,0.4828969836235046,0.8548387096774193,,,267860081.0,True,7,0,"['transformers', 'pytorch']",2022-03-19 21:40:40+00:00,2022-03-19 21:40:15+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 649919118
- CO2 Emissions (in grams): 0.3996916853309825

## Validation Metrics

- Loss: 0.48289698362350464
- Accuracy: 0.8064516129032258
- Precision: 0.828125
- Recall: 0.8833333333333333
- AUC: 0.8353535353535354
- F1: 0.8548387096774193

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/msamogh/autonlp-cai-out-of-scope-649919118
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""msamogh/autonlp-cai-out-of-scope-649919118"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""msamogh/autonlp-cai-out-of-scope-649919118"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,670166758.1055297,0.8299404948324458,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32421,autonlp-cai-out-of-scope-649919116,['msamogh/autonlp-data-cai-out-of-scope'],,2.438401649319185,AutoTrain,Not Specified,Not Specified,Not Specified,0.7526881720430108,0.5314930081367493,0.7964601769911505,,,1334486957.0,True,17,0,"['transformers', 'pytorch']",2022-03-22 15:27:18+00:00,2022-03-19 21:40:42+00:00,"# What do the class labels mean?
0 - out of scope
1 - in scope

# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 649919116
- CO2 Emissions (in grams): 2.438401649319185

## Validation Metrics

- Loss: 0.5314930081367493
- Accuracy: 0.7526881720430108
- Precision: 0.8490566037735849
- Recall: 0.75
- AUC: 0.8515151515151514
- F1: 0.7964601769911505

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/msamogh/autonlp-cai-out-of-scope-649919116
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""msamogh/autonlp-cai-out-of-scope-649919116"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""msamogh/autonlp-cai-out-of-scope-649919116"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,547279385.8110275,0.7739557739557741,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32524,autonlp-ctrip-653519223,['doctorlan/autonlp-data-ctrip'],,24.879856894708396,AutoTrain,Not Specified,Not Specified,Not Specified,0.9676666666666668,0.1467185318470001,0.9768441155407016,,,409160877.0,True,18,0,"['transformers', 'pytorch']",2022-03-21 09:01:53+00:00,2022-03-21 08:38:42+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 653519223
- CO2 Emissions (in grams): 24.879856894708393

## Validation Metrics

- Loss: 0.14671853184700012
- Accuracy: 0.9676666666666667
- Precision: 0.9794159885112494
- Recall: 0.9742857142857143
- AUC: 0.9901396825396825
- F1: 0.9768441155407017

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/doctorlan/autonlp-ctrip-653519223
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""doctorlan/autonlp-ctrip-653519223"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""doctorlan/autonlp-ctrip-653519223"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,16445467.461150186,0.9722337338393978,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32526,autonlp-JD-bert-653619233,['doctorlan/autonlp-data-JD-bert'],,5.919372931976555,AutoTrain,Not Specified,Not Specified,Not Specified,0.952650883627876,0.1508315503597259,0.9520917678812416,,,409160877.0,True,16,0,"['transformers', 'pytorch']",2022-03-21 08:54:10+00:00,2022-03-21 08:48:42+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 653619233
- CO2 Emissions (in grams): 5.919372931976555

## Validation Metrics

- Loss: 0.15083155035972595
- Accuracy: 0.952650883627876
- Precision: 0.9631399317406143
- Recall: 0.9412941961307538
- AUC: 0.9828776962419389
- F1: 0.9520917678812415

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/doctorlan/autonlp-JD-bert-653619233
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""doctorlan/autonlp-JD-bert-653619233"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""doctorlan/autonlp-JD-bert-653619233"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,69122334.69692472,0.9523712436934944,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32564,autonlp-test-654919306,['McIan91/autonlp-data-test'],,0.7013851565380207,AutoTrain,Not Specified,Not Specified,Not Specified,,2.5570242404937744,,0.727273,0.727273,891730879.0,True,14,0,"['transformers', 'pytorch']",2022-03-21 17:29:34+00:00,2022-03-21 17:28:50+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 654919306
- CO2 Emissions (in grams): 0.7013851565380207

## Validation Metrics

- Loss: 2.5570242404937744
- Rouge1: 72.7273
- Rouge2: 44.4444
- RougeL: 72.7273
- RougeLsum: 72.7273
- Gen Len: 17.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/McIan91/autonlp-test-654919306
```",,,1,[],[],NLP,2022-03,1271385444.484611,0.727273,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32688,autonlp-email-classification-657119381,['esiebomajeremiah/autonlp-data-email-classification'],,3.516233232503715,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0003739577368833,1.0,,,1334486957.0,True,23,0,"['transformers', 'pytorch']",2022-03-22 13:57:29+00:00,2022-03-22 13:54:29+00:00,"
# Model Trained Using AutoNLP

- Problem type: Binary Classification
- Model ID: 657119381
- CO2 Emissions (in grams): 3.516233232503715

## Validation Metrics

- Loss: 0.00037395773688331246
- Accuracy: 1.0
- Precision: 1.0
- Recall: 1.0
- AUC: 1.0
- F1: 1.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/esiebomajeremiah/autonlp-email-classification-657119381
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""esiebomajeremiah/autonlp-email-classification-657119381"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""esiebomajeremiah/autonlp-email-classification-657119381"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,379521740.6695703,1.0,0.0,0.0,0.0,0.0,0,1.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32764,autonlp-MeQSum-1-660519466,['sumedh/autotrain-data-MeQSum-1'],,35.86552134392392,AutoTrain,Not Specified,Not Specified,Not Specified,,1.3210543394088743,,0.521593,0.5011410000000001,2279631601.0,True,3,0,"['transformers', 'pytorch']",2022-03-23 07:16:44+00:00,2022-03-23 06:43:11+00:00,"
# Model Trained Using AutoNLP

- Problem type: Summarization
- Model ID: 660519466
- CO2 Emissions (in grams): 35.865521343923916

## Validation Metrics

- Loss: 1.3210543394088745
- Rouge1: 52.1593
- Rouge2: 34.5464
- RougeL: 50.1141
- RougeLsum: 50.1067
- Gen Len: 11.93

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/sumedh/autonlp-MeQSum-1-660519466
```",,,1,[],[],NLP,2022-03,63560531.55173776,0.5111625067964887,0.0,0.0,0.0,0.0,0,1.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32775,markingMultiClass,['FuriouslyAsleep/autotrain-data-markingClassifier'],,0.5712537632313806,AutoTrain,Not Specified,Not Specified,Not Specified,0.8,0.859619140625,0.6,,,267863153.0,True,7,0,"['transformers', 'pytorch']",2022-03-23 09:21:51+00:00,2022-03-23 09:21:14+00:00,"
# Model Trained Using AutoNLP

- Problem type: Multi-class Classification
- Model ID: 661319476
- CO2 Emissions (in grams): 0.5712537632313806

## Validation Metrics

- Loss: 0.859619140625
- Accuracy: 0.8
- Macro F1: 0.6
- Micro F1: 0.8000000000000002
- Weighted F1: 0.72
- Macro Precision: 0.5555555555555555
- Micro Precision: 0.8
- Weighted Precision: 0.6666666666666666
- Macro Recall: 0.6666666666666666
- Micro Recall: 0.8
- Weighted Recall: 0.8


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoNLP""}' https://api-inference.huggingface.co/models/FuriouslyAsleep/autonlp-markingClassifier-661319476
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""FuriouslyAsleep/autonlp-markingClassifier-661319476"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""FuriouslyAsleep/autonlp-markingClassifier-661319476"", use_auth_token=True)

inputs = tokenizer(""I love AutoNLP"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,468903962.19850326,0.6857142857142856,0.0,0.0,0.0,0.0,0,1.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
32914,unhappyZebra100,['FuriouslyAsleep/autotrain-data-techDataClassifeier'],,0.6969569001670619,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0225090086460113,1.0,,,328525933.0,True,9,0,"['transformers', 'pytorch']",2022-03-24 04:39:04+00:00,2022-03-24 04:38:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 664919631
- CO2 Emissions (in grams): 0.6969569001670619

## Validation Metrics

- Loss: 0.022509008646011353
- Accuracy: 1.0
- Precision: 1.0
- Recall: 1.0
- AUC: 1.0
- F1: 1.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/FuriouslyAsleep/autotrain-techDataClassifeier-664919631
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""FuriouslyAsleep/autotrain-techDataClassifeier-664919631"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""FuriouslyAsleep/autotrain-techDataClassifeier-664919631"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,471371949.8598144,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33133,autotrain-parrot_finetune_v1-667919695,['McIan91/autotrain-data-parrot_finetune_v1'],,207.64739623144084,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0646145641803741,,0.705184,0.704464,891730879.0,True,13,0,"['transformers', 'pytorch']",2022-03-25 15:41:11+00:00,2022-03-25 12:27:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 667919695
- CO2 Emissions (in grams): 207.64739623144084

## Validation Metrics

- Loss: 0.06461456418037415
- Rouge1: 70.5184
- Rouge2: 66.9204
- RougeL: 70.4464
- RougeLsum: 70.4705
- Gen Len: 18.5385

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/McIan91/autotrain-parrot_finetune_v1-667919695
```",,,1,[],[],NLP,2022-03,4294447.679980005,0.7048238161243091,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33189,autotrain-phrasinator-reverse-670319725,['McIan91/autotrain-data-phrasinator-reverse'],,149.95517950000834,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0022294693626463,,0.6758329999999999,0.675812,891730879.0,True,13,0,"['transformers', 'pytorch']",2022-03-26 03:59:08+00:00,2022-03-26 01:38:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 670319725
- CO2 Emissions (in grams): 149.95517950000834

## Validation Metrics

- Loss: 0.0022294693626463413
- Rouge1: 67.5833
- Rouge2: 65.7386
- RougeL: 67.5812
- RougeLsum: 67.585
- Gen Len: 18.907

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/McIan91/autotrain-phrasinator-reverse-670319725
```",,,1,[],[],NLP,2022-03,5946649.4053308135,0.6758224998368654,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33233,is-legit-kwd-march-27,['bozelosp/autotrain-data-legit-keyword'],,0.5745216001459987,AutoTrain,Not Specified,Not Specified,Not Specified,0.8057228915662651,0.5012844800949097,0.7974882260596545,,,267860081.0,True,6,1,"['transformers', 'pytorch']",2022-03-26 18:44:40+00:00,2022-03-26 18:44:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 668419758
- CO2 Emissions (in grams): 0.5745216001459987

## Validation Metrics

- Loss: 0.5012844800949097
- Accuracy: 0.8057228915662651
- Precision: 0.7627627627627628
- Recall: 0.8355263157894737
- AUC: 0.868530701754386
- F1: 0.7974882260596545

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/bozelosp/autotrain-legit-keyword-668419758
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""bozelosp/autotrain-legit-keyword-668419758"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""bozelosp/autotrain-legit-keyword-668419758"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,466231523.6397218,0.8015844107198421,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33237,autotrain-xlm-roberta-base-reviews-672119797,['YXHugging/autotrain-data-xlm-roberta-base-reviews'],,1019.0229633198009,AutoTrain,Not Specified,Not Specified,Not Specified,0.5688083333333334,0.9898674488067628,0.5640966271895913,,,1112275373.0,True,7,0,"['transformers', 'pytorch']",2022-03-27 12:55:19+00:00,2022-03-26 21:05:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 672119797
- CO2 Emissions (in grams): 1019.0229633198007

## Validation Metrics

- Loss: 0.9898674488067627
- Accuracy: 0.5688083333333334
- Macro F1: 0.5640966271895913
- Micro F1: 0.5688083333333334
- Weighted F1: 0.5640966271895913
- Macro Precision: 0.5673737438011194
- Micro Precision: 0.5688083333333334
- Weighted Precision: 0.5673737438011194
- Macro Recall: 0.5688083333333334
- Micro Recall: 0.5688083333333334
- Weighted Recall: 0.5688083333333334


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/YXHugging/autotrain-xlm-roberta-base-reviews-672119797
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119797"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119797"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,1091511.5880965027,0.5664426823633337,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33238,autotrain-xlm-roberta-base-reviews-672119798,['YXHugging/autotrain-data-xlm-roberta-base-reviews'],,1013.8825767332372,AutoTrain,Not Specified,Not Specified,Not Specified,0.5789333333333333,0.9646632075309752,0.5775792001871465,,,1112275373.0,True,6,0,"['transformers', 'pytorch']",2022-03-27 12:58:03+00:00,2022-03-26 21:07:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 672119798
- CO2 Emissions (in grams): 1013.8825767332373

## Validation Metrics

- Loss: 0.9646632075309753
- Accuracy: 0.5789333333333333
- Macro F1: 0.5775792001871465
- Micro F1: 0.5789333333333333
- Weighted F1: 0.5775792001871465
- Macro Precision: 0.5829444191847423
- Micro Precision: 0.5789333333333333
- Weighted Precision: 0.5829444191847424
- Macro Recall: 0.5789333333333333
- Micro Recall: 0.5789333333333333
- Weighted Recall: 0.5789333333333333


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/YXHugging/autotrain-xlm-roberta-base-reviews-672119798
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119798"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119798"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,1097045.5539178783,0.5782554739990183,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33251,autotrain-xlm-roberta-base-reviews-672119799,['YXHugging/autotrain-data-xlm-roberta-base-reviews'],,1583.7188188958198,AutoTrain,Not Specified,Not Specified,Not Specified,0.5827541666666667,0.9590993523597716,0.5806748283026683,,,1112275373.0,True,6,0,"['transformers', 'pytorch']",2022-03-28 01:30:54+00:00,2022-03-27 00:52:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 672119799
- CO2 Emissions (in grams): 1583.7188188958198

## Validation Metrics

- Loss: 0.9590993523597717
- Accuracy: 0.5827541666666667
- Macro F1: 0.5806748283026683
- Micro F1: 0.5827541666666667
- Weighted F1: 0.5806748283026683
- Macro Precision: 0.5834325027348383
- Micro Precision: 0.5827541666666667
- Weighted Precision: 0.5834325027348383
- Macro Recall: 0.5827541666666667
- Micro Recall: 0.5827541666666667
- Weighted Recall: 0.5827541666666667


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/YXHugging/autotrain-xlm-roberta-base-reviews-672119799
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119799"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119799"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,702318.719541065,0.5817126393360178,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33252,autotrain-xlm-roberta-base-reviews-672119800,['YXHugging/autotrain-data-xlm-roberta-base-reviews'],,2011.6528745969176,AutoTrain,Not Specified,Not Specified,Not Specified,0.5830708333333333,0.9570887088775636,0.5789149828346194,,,1112275373.0,True,6,0,"['transformers', 'pytorch']",2022-03-28 08:18:33+00:00,2022-03-27 00:59:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 672119800
- CO2 Emissions (in grams): 2011.6528745969179

## Validation Metrics

- Loss: 0.9570887088775635
- Accuracy: 0.5830708333333333
- Macro F1: 0.5789149828346194
- Micro F1: 0.5830708333333333
- Weighted F1: 0.5789149828346193
- Macro Precision: 0.5808338093704437
- Micro Precision: 0.5830708333333333
- Weighted Precision: 0.5808338093704437
- Macro Recall: 0.5830708333333334
- Micro Recall: 0.5830708333333333
- Weighted Recall: 0.5830708333333333


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/YXHugging/autotrain-xlm-roberta-base-reviews-672119800
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119800"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119800"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,552916.1551904777,0.5809854763696097,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33253,autotrain-xlm-roberta-base-reviews-672119801,['YXHugging/autotrain-data-xlm-roberta-base-reviews'],,999.5670927087938,AutoTrain,Not Specified,Not Specified,Not Specified,0.5738333333333333,0.9767692685127258,0.5698748846905103,,,1112275373.0,True,6,0,"['transformers', 'pytorch']",2022-03-27 16:53:50+00:00,2022-03-27 01:21:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 672119801
- CO2 Emissions (in grams): 999.5670927087938

## Validation Metrics

- Loss: 0.9767692685127258
- Accuracy: 0.5738333333333333
- Macro F1: 0.5698748846905103
- Micro F1: 0.5738333333333333
- Weighted F1: 0.5698748846905102
- Macro Precision: 0.5734242161804903
- Micro Precision: 0.5738333333333333
- Weighted Precision: 0.5734242161804902
- Macro Recall: 0.5738333333333333
- Micro Recall: 0.5738333333333333
- Weighted Recall: 0.5738333333333333


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/YXHugging/autotrain-xlm-roberta-base-reviews-672119801
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119801"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""YXHugging/autotrain-xlm-roberta-base-reviews-672119801"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,1112757.0936591865,0.5718472587876202,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33315,autotrain-harassement-675420038,['ikram54/autotrain-data-harassement'],,2.633283687190505,AutoTrain,Not Specified,Not Specified,Not Specified,0.7085201793721974,0.8747465014457703,0.579743989078862,,,540872877.0,True,23,0,"['transformers', 'pytorch']",2022-03-27 18:08:30+00:00,2022-03-27 18:06:02+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 675420038
- CO2 Emissions (in grams): 2.6332836871905054

## Validation Metrics

- Loss: 0.8747465014457703
- Accuracy: 0.7085201793721974
- Macro F1: 0.579743989078862
- Micro F1: 0.7085201793721974
- Weighted F1: 0.6913786522271296
- Macro Precision: 0.5669375905888698
- Micro Precision: 0.7085201793721974
- Weighted Precision: 0.6760144007300164
- Macro Recall: 0.5940655209452201
- Micro Recall: 0.7085201793721974
- Weighted Recall: 0.7085201793721974


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ikram54/autotrain-harassement-675420038
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ikram54/autotrain-harassement-675420038"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ikram54/autotrain-harassement-675420038"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,205398635.7911427,0.6376957850593408,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33499,TweetClimateAnalysis,['KeithHorgan98/autotrain-data-TweetClimateAnalysis'],,133.1949127628479,AutoTrain,Not Specified,Not Specified,Not Specified,0.865424430641822,0.4864234924316406,0.7665472174344069,,,1421676909.0,True,8,1,"['transformers', 'pytorch']",2022-03-29 10:01:24+00:00,2022-03-29 10:16:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 678720226
- CO2 Emissions (in grams): 133.19491276284793

## Validation Metrics

- Loss: 0.4864234924316406
- Accuracy: 0.865424430641822
- Macro F1: 0.7665472174344069
- Micro F1: 0.8654244306418221
- Weighted F1: 0.8586375445115083
- Macro Precision: 0.8281449061702826
- Micro Precision: 0.865424430641822
- Weighted Precision: 0.8619727477790186
- Macro Recall: 0.736576343905098
- Micro Recall: 0.865424430641822
- Weighted Recall: 0.865424430641822


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/KeithHorgan98/autotrain-TweetClimateAnalysis-678720226
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""KeithHorgan98/autotrain-TweetClimateAnalysis-678720226"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""KeithHorgan98/autotrain-TweetClimateAnalysis-678720226"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,10673657.72093173,0.8129904584926438,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33523,autotrain-mut_all_text-680820343,['gabitoo1234/autotrain-data-mut_all_text'],,115.48848403681228,AutoTrain,Not Specified,Not Specified,Not Specified,0.9462770369425126,0.3041240870952606,0.7836898686625933,,,439662957.0,True,28,0,"['transformers', 'pytorch']",2022-03-29 16:09:31+00:00,2022-03-29 14:22:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 680820343
- CO2 Emissions (in grams): 115.48848403681228

## Validation Metrics

- Loss: 0.3041240870952606
- Accuracy: 0.9462770369425126
- Macro F1: 0.7836898686625933
- Micro F1: 0.9462770369425126
- Weighted F1: 0.9449148298990091
- Macro Precision: 0.8344505891491089
- Micro Precision: 0.9462770369425126
- Weighted Precision: 0.9451247372908952
- Macro Recall: 0.7568785255994025
- Micro Recall: 0.9462770369425126
- Weighted Recall: 0.9462770369425126


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/gabitoo1234/autotrain-mut_all_text-680820343
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""gabitoo1234/autotrain-mut_all_text-680820343"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""gabitoo1234/autotrain-mut_all_text-680820343"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,3806985.2649538303,0.8573432525179018,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33695,autotrain-security-texts-classification-roberta-688020754,['vlsb/autotrain-data-security-texts-classification-roberta'],,3.1151249696839685,AutoTrain,Not Specified,Not Specified,Not Specified,0.8928571428571429,0.2810373902320862,0.9066666666666666,,,498674093.0,True,6,0,"['transformers', 'pytorch']",2022-03-30 20:55:42+00:00,2022-03-30 20:52:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 688020754
- CO2 Emissions (in grams): 3.1151249696839685

## Validation Metrics

- Loss: 0.2810373902320862
- Accuracy: 0.8928571428571429
- Precision: 0.9272727272727272
- Recall: 0.8869565217391304
- AUC: 0.9500805152979066
- F1: 0.9066666666666666

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/vlsb/autotrain-security-texts-classification-roberta-688020754
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""vlsb/autotrain-security-texts-classification-roberta-688020754"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""vlsb/autotrain-security-texts-classification-roberta-688020754"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,160081569.07123724,0.8997089177030961,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33697,autotrain-security-texts-classification-distilroberta-688220764,['vlsb/autotrain-data-security-texts-classification-distilroberta'],,2.0817207656772445,AutoTrain,Not Specified,Not Specified,Not Specified,0.903061224489796,0.3055502772331238,0.9140271493212668,,,328525933.0,True,8,2,"['transformers', 'pytorch']",2022-03-30 20:56:57+00:00,2022-03-30 20:54:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 688220764
- CO2 Emissions (in grams): 2.0817207656772445

## Validation Metrics

- Loss: 0.3055502772331238
- Accuracy: 0.9030612244897959
- Precision: 0.9528301886792453
- Recall: 0.8782608695652174
- AUC: 0.9439076757917337
- F1: 0.9140271493212669

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/vlsb/autotrain-security-texts-classification-distilroberta-688220764
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""vlsb/autotrain-security-texts-classification-distilroberta-688220764"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""vlsb/autotrain-security-texts-classification-distilroberta-688220764"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,157814601.46655208,0.9085110978414158,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33698,autotrain-security-text-classification-albert-688320769,['vlsb/autotrain-data-security-text-classification-albert'],,3.670416179055797,AutoTrain,Not Specified,Not Specified,Not Specified,0.8826530612244898,0.3046899139881134,0.8977777777777778,,,46755537.0,True,10,0,"['transformers', 'pytorch']",2022-03-30 20:59:32+00:00,2022-03-30 20:55:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 688320769
- CO2 Emissions (in grams): 3.670416179055797

## Validation Metrics

- Loss: 0.3046899139881134
- Accuracy: 0.8826530612244898
- Precision: 0.9181818181818182
- Recall: 0.8782608695652174
- AUC: 0.9423510466988727
- F1: 0.8977777777777778

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/vlsb/autotrain-security-text-classification-albert-688320769
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""vlsb/autotrain-security-text-classification-albert-688320769"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""vlsb/autotrain-security-text-classification-albert-688320769"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,12738483.79014821,0.8901511774520167,0.0,1.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33727,autotrain-IWant-689220804,['unjustify/autotrain-data-IWant'],,39.40549299946679,AutoTrain,Not Specified,Not Specified,Not Specified,,2.0426149368286133,,0.549813,0.5403990000000001,891730879.0,True,13,0,"['transformers', 'pytorch']",2022-03-31 06:46:48+00:00,2022-03-31 06:09:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 689220804
- CO2 Emissions (in grams): 39.40549299946679

## Validation Metrics

- Loss: 2.0426149368286133
- Rouge1: 54.9813
- Rouge2: 44.923
- RougeL: 54.0399
- RougeLsum: 54.2553
- Gen Len: 16.6211

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/unjustify/autotrain-IWant-689220804
```",,,1,[],[],NLP,2022-03,22629608.4916909,0.5450653549713267,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33730,autotrain-commonsence-689620825,['unjustify/autotrain-data-commonsence'],,20.656741915705204,AutoTrain,Not Specified,Not Specified,Not Specified,0.6354949675117849,0.7315372824668884,0.6283932978308872,,,267860081.0,True,6,0,"['transformers', 'pytorch']",2022-03-31 06:38:08+00:00,2022-03-31 06:18:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 689620825
- CO2 Emissions (in grams): 20.656741915705204

## Validation Metrics

- Loss: 0.7315372824668884
- Accuracy: 0.6354949675117849
- Precision: 0.63792194092827
- Recall: 0.6191451241361658
- AUC: 0.6912165223485615
- F1: 0.6283932978308872

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/unjustify/autotrain-commonsence-689620825
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""unjustify/autotrain-commonsence-689620825"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""unjustify/autotrain-commonsence-689620825"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-03,12967198.89772876,0.6319241808632373,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
33979,autotrain-bbc-news-summarization-694821095,['abd-1999/autotrain-data-bbc-news-summarization'],,2313.403707902693,AutoTrain,Not Specified,Not Specified,Not Specified,,3.0294156074523926,,0.021467,0.021524,1131209487.0,True,15,1,"['transformers', 'pytorch']",2022-04-03 09:25:08+00:00,2022-04-01 21:16:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 694821095
- CO2 Emissions (in grams): 2313.4037079026934

## Validation Metrics

- Loss: 3.0294156074523926
- Rouge1: 2.1467
- Rouge2: 0.0853
- RougeL: 2.1524
- RougeLsum: 2.1534
- Gen Len: 18.5603

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/abd-1999/autotrain-bbc-news-summarization-694821095
```",,,1,[],[],NLP,2022-04,488980.5800586108,0.0214954622130213,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
34070,autotrain-commonsense_1-696121179,['unjustify/autotrain-data-commonsense_1'],,4.355285184457145,AutoTrain,Not Specified,Not Specified,Not Specified,0.8544333807491702,0.3446762859821319,0.8317808219178082,,,498674093.0,True,6,0,"['transformers', 'pytorch']",2022-04-02 13:49:28+00:00,2022-04-02 13:45:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 696121179
- CO2 Emissions (in grams): 4.355285184457145

## Validation Metrics

- Loss: 0.34467628598213196
- Accuracy: 0.8544333807491702
- Precision: 0.9014251781472684
- Recall: 0.7721261444557477
- AUC: 0.9422766967397805
- F1: 0.8317808219178082

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/unjustify/autotrain-commonsense_1-696121179
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""unjustify/autotrain-commonsense_1-696121179"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""unjustify/autotrain-commonsense_1-696121179"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,114498608.44466288,0.842954944383086,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
34392,autotrain-sentiment_analysis_project-705021428,['ramnika003/autotrain-data-sentiment_analysis_project'],,10.03748863138583,AutoTrain,Not Specified,Not Specified,Not Specified,0.768964665184087,0.5534441471099854,0.7629008163259284,,,1112269229.0,True,6,0,"['transformers', 'pytorch']",2022-04-05 09:23:07+00:00,2022-04-05 09:17:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 705021428
- CO2 Emissions (in grams): 10.03748863138583

## Validation Metrics

- Loss: 0.5534441471099854
- Accuracy: 0.768964665184087
- Macro F1: 0.7629008163259284
- Micro F1: 0.768964665184087
- Weighted F1: 0.7685397042536148
- Macro Precision: 0.7658234531650739
- Micro Precision: 0.768964665184087
- Weighted Precision: 0.7684017544026074
- Macro Recall: 0.7603505092881394
- Micro Recall: 0.768964665184087
- Weighted Recall: 0.768964665184087


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ramnika003/autotrain-sentiment_analysis_project-705021428
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ramnika003/autotrain-sentiment_analysis_project-705021428"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ramnika003/autotrain-sentiment_analysis_project-705021428"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,110811505.73083478,0.7659207389626118,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
34521,autotrain-Create_Question_Model-708521506,['unjustify/autotrain-data-Create_Question_Model'],,7.419693550936528,AutoTrain,Not Specified,Not Specified,Not Specified,,1.4744563102722168,,0.300761,0.272745,2950904711.0,True,5,0,"['transformers', 'pytorch']",2022-04-06 20:45:17+00:00,2022-04-06 04:45:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 708521506
- CO2 Emissions (in grams): 7.419693550936528

## Validation Metrics

- Loss: 1.4744563102722168
- Rouge1: 30.0761
- Rouge2: 10.142
- RougeL: 27.2745
- RougeLsum: 27.2831
- Gen Len: 13.8746

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/unjustify/autotrain-Create_Question_Model-708521506
```",,,1,[],[],NLP,2022-04,397712478.3849774,0.2860687035357956,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
34708,autotrain-intentclassificationfilipino-715021714,['shubh024/autotrain-data-intentclassificationfilipino'],,0.0033415164956729,AutoTrain,Not Specified,Not Specified,Not Specified,0.8,0.5571377873420715,0.6709090909090909,,,436444589.0,True,16,0,"['transformers', 'pytorch']",2022-04-07 07:38:46+00:00,2022-04-07 07:37:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 715021714
- CO2 Emissions (in grams): 0.003341516495672918

## Validation Metrics

- Loss: 0.5571377873420715
- Accuracy: 0.8
- Macro F1: 0.6709090909090909
- Micro F1: 0.8000000000000002
- Weighted F1: 0.7739393939393939
- Macro Precision: 0.7
- Micro Precision: 0.8
- Weighted Precision: 0.8
- Macro Recall: 0.7
- Micro Recall: 0.8
- Weighted Recall: 0.8


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/shubh024/autotrain-intentclassificationfilipino-715021714
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""shubh024/autotrain-intentclassificationfilipino-715021714"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""shubh024/autotrain-intentclassificationfilipino-715021714"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,130612729150.1245,0.7297898640296662,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
34799,bert_MultiClass_TextClassification,['palakagl/autotrain-data-PersonalAssitant'],,5.080390550458655,AutoTrain,Not Specified,Not Specified,Not Specified,0.9269102990033222,0.352799117565155,0.9261839948926328,,,438209965.0,True,51,0,"['transformers', 'pytorch']",2022-04-07 17:06:55+00:00,2022-04-07 17:04:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 717221775
- CO2 Emissions (in grams): 5.080390550458655

## Validation Metrics

- Loss: 0.35279911756515503
- Accuracy: 0.9269102990033222
- Macro F1: 0.9261839948926327
- Micro F1: 0.9269102990033222
- Weighted F1: 0.9263981751760975
- Macro Precision: 0.9273912049203341
- Micro Precision: 0.9269102990033222
- Weighted Precision: 0.9280084437800646
- Macro Recall: 0.927250645380574
- Micro Recall: 0.9269102990033222
- Weighted Recall: 0.9269102990033222


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/palakagl/autotrain-PersonalAssitant-717221775
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""palakagl/autotrain-PersonalAssitant-717221775"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""palakagl/autotrain-PersonalAssitant-717221775"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,86255172.83517477,0.9265470046137038,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
34802,distilbert_MultiClass_TextClassification,['palakagl/autotrain-data-PersonalAssitant'],,2.258363491829382,AutoTrain,Not Specified,Not Specified,Not Specified,0.9042081949058692,0.3866031467914581,0.9079200295131094,,,263362929.0,True,9,0,"['transformers', 'pytorch']",2022-04-07 17:12:15+00:00,2022-04-07 17:10:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 717221781
- CO2 Emissions (in grams): 2.258363491829382

## Validation Metrics

- Loss: 0.38660314679145813
- Accuracy: 0.9042081949058693
- Macro F1: 0.9079200295131094
- Micro F1: 0.9042081949058692
- Weighted F1: 0.9052766730963512
- Macro Precision: 0.9116101664087508
- Micro Precision: 0.9042081949058693
- Weighted Precision: 0.9097680514456175
- Macro Recall: 0.9080246002936301
- Micro Recall: 0.9042081949058693
- Weighted Recall: 0.9042081949058693


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/palakagl/autotrain-PersonalAssitant-717221781
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""palakagl/autotrain-PersonalAssitant-717221781"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""palakagl/autotrain-PersonalAssitant-717221781"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,116616713.8075118,0.9060603106804456,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
34804,Roberta_Multiclass_TextClassification,['palakagl/autotrain-data-PersonalAssitant'],,0.0145676379854259,AutoTrain,Not Specified,Not Specified,Not Specified,0.9180509413067552,0.3884845674037933,0.9157418163085091,,,498864813.0,True,23,2,"['transformers', 'pytorch']",2022-04-07 17:15:10+00:00,2022-04-07 17:12:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 717221783
- CO2 Emissions (in grams): 0.014567637985425905

## Validation Metrics

- Loss: 0.38848456740379333
- Accuracy: 0.9180509413067552
- Macro F1: 0.9157418163085091
- Micro F1: 0.9180509413067552
- Weighted F1: 0.9185290137253468
- Macro Precision: 0.9189981206383326
- Micro Precision: 0.9180509413067552
- Weighted Precision: 0.9221607328493303
- Macro Recall: 0.9158232837734661
- Micro Recall: 0.9180509413067552
- Weighted Recall: 0.9180509413067552


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/palakagl/autotrain-PersonalAssitant-717221783
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""palakagl/autotrain-PersonalAssitant-717221783"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""palakagl/autotrain-PersonalAssitant-717221783"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,34244728864.01254,0.9168949249742492,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
34805,bert_TextClassification,['palakagl/autotrain-data-PersonalAssitant'],,7.025108874009706,AutoTrain,Not Specified,Not Specified,Not Specified,0.9186046511627908,0.3546710908412933,0.9202890631142154,,,433522093.0,True,52,2,"['transformers', 'pytorch']",2022-04-07 17:18:20+00:00,2022-04-07 17:14:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 717221787
- CO2 Emissions (in grams): 7.025108874009706

## Validation Metrics

- Loss: 0.35467109084129333
- Accuracy: 0.9186046511627907
- Macro F1: 0.9202890631142154
- Micro F1: 0.9186046511627907
- Weighted F1: 0.9185859051606837
- Macro Precision: 0.921802482563032
- Micro Precision: 0.9186046511627907
- Weighted Precision: 0.9210238644296779
- Macro Recall: 0.9218155764486292
- Micro Recall: 0.9186046511627907
- Weighted Recall: 0.9186046511627907


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/palakagl/autotrain-PersonalAssitant-717221787
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""palakagl/autotrain-PersonalAssitant-717221787"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""palakagl/autotrain-PersonalAssitant-717221787"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,61710373.57212652,0.9194460856845578,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
35026,autotrain-TestProj-722121991,['Hodiden/autotrain-data-TestProj'],,8.052949236815056,AutoTrain,Not Specified,Not Specified,Not Specified,,1.123626708984375,,0.561275,0.51986,3132852901.0,True,5,0,"['transformers', 'pytorch']",2022-04-09 19:21:44+00:00,2022-04-09 04:53:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 722121991
- CO2 Emissions (in grams): 8.052949236815056

## Validation Metrics

- Loss: 1.123626708984375
- Rouge1: 56.1275
- Rouge2: 33.5648
- RougeL: 51.986
- RougeLsum: 51.9943
- Gen Len: 13.2823

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Hodiden/autotrain-TestProj-722121991
```",,,1,[],[],NLP,2022-04,389031745.86991984,0.5397742585338556,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
35044,autotrain-livedoor_news-722922024,['jicoc22578/autotrain-data-livedoor_news'],,0.0192994914581561,AutoTrain,Not Specified,Not Specified,Not Specified,0.9457627118644067,0.1960954070091247,0.9404319054946132,,,444940461.0,True,28,0,"['transformers', 'pytorch']",2022-04-09 10:47:55+00:00,2022-04-09 10:33:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 722922024
- CO2 Emissions (in grams): 0.019299491458156143

## Validation Metrics

- Loss: 0.19609540700912476
- Accuracy: 0.9457627118644067
- Macro F1: 0.9404319054946133
- Micro F1: 0.9457627118644067
- Weighted F1: 0.9456037443251943
- Macro Precision: 0.9420917371721244
- Micro Precision: 0.9457627118644067
- Weighted Precision: 0.9457910238180336
- Macro Recall: 0.9391783746329772
- Micro Recall: 0.9457627118644067
- Weighted Recall: 0.9457627118644067


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jicoc22578/autotrain-livedoor_news-722922024
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jicoc22578/autotrain-livedoor_news-722922024"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jicoc22578/autotrain-livedoor_news-722922024"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,23054517367.190216,0.943089775656064,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
35527,autotrain-T5Base1_1-728922203,['FabsCool/autotrain-data-T5Base1_1'],,583.728921803621,AutoTrain,Not Specified,Not Specified,Not Specified,,1.2922444343566897,,0.543928,0.503552,990438349.0,True,13,0,"['transformers', 'pytorch']",2022-04-11 10:31:58+00:00,2022-04-11 06:19:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 728922203
- CO2 Emissions (in grams): 583.728921803621

## Validation Metrics

- Loss: 1.2922444343566895
- Rouge1: 54.3928
- Rouge2: 31.666
- RougeL: 50.3552
- RougeLsum: 50.3694
- Gen Len: 13.3425

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/FabsCool/autotrain-T5Base1_1-728922203
```",,,1,[],[],NLP,2022-04,1696743.6630340628,0.522961836514301,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
35562,autotrain-amazon_text_sum-730222226,['yogi/autotrain-data-amazon_text_sum'],,2986.6520132805163,AutoTrain,Not Specified,Not Specified,Not Specified,,2.682709217071533,,0.196069,0.192706,242085627.0,True,3,0,"['transformers', 'pytorch']",2022-04-12 09:08:15+00:00,2022-04-11 10:39:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 730222226
- CO2 Emissions (in grams): 2986.6520132805163

## Validation Metrics

- Loss: 2.682709217071533
- Rouge1: 19.6069
- Rouge2: 7.3367
- RougeL: 19.2706
- RougeLsum: 19.286
- Gen Len: 5.5731

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/yogi/autotrain-amazon_text_sum-730222226
```",,,1,[],[],NLP,2022-04,81055.85315046294,0.1943729546087068,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
35848,autotrain-livedoor_news-732022289,['jurader/autotrain-data-livedoor_news'],,0.0288663513112763,AutoTrain,Not Specified,Not Specified,Not Specified,0.9471186440677966,0.1984961181879043,0.9441816841379956,,,444940461.0,True,32,1,"['transformers', 'pytorch']",2022-04-12 08:07:57+00:00,2022-04-12 08:03:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 732022289
- CO2 Emissions (in grams): 0.02886635131127639

## Validation Metrics

- Loss: 0.19849611818790436
- Accuracy: 0.9471186440677966
- Macro F1: 0.9441816841379956
- Micro F1: 0.9471186440677966
- Weighted F1: 0.9470801715002611
- Macro Precision: 0.945983665608131
- Micro Precision: 0.9471186440677966
- Weighted Precision: 0.9475574732458715
- Macro Recall: 0.9429694962141204
- Micro Recall: 0.9471186440677966
- Weighted Recall: 0.9471186440677966


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jurader/autotrain-livedoor_news-732022289
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jurader/autotrain-livedoor_news-732022289"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jurader/autotrain-livedoor_news-732022289"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,15413810224.993277,0.9456478837316878,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
36071,autotrain-iine_classification10-737422470,['vabadeh213/autotrain-data-iine_classification10'],,7.351885824089346,AutoTrain,Not Specified,Not Specified,Not Specified,0.8279088689991864,0.3945626318454742,0.2810198300283286,,,442559661.0,True,28,0,"['transformers', 'pytorch']",2022-04-13 09:24:04+00:00,2022-04-13 08:45:21+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 737422470
- CO2 Emissions (in grams): 7.351885824089346

## Validation Metrics

- Loss: 0.39456263184547424
- Accuracy: 0.8279088689991864
- Precision: 0.6869806094182825
- Recall: 0.17663817663817663
- AUC: 0.7937892215111646
- F1: 0.2810198300283286

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/vabadeh213/autotrain-iine_classification10-737422470
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""vabadeh213/autotrain-iine_classification10-737422470"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""vabadeh213/autotrain-iine_classification10-737422470"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,60196753.81109695,0.4196100431869594,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
36081,autotrain-wikihow-737822494,['vabadeh213/autotrain-data-wikihow'],,361.800665798794,AutoTrain,Not Specified,Not Specified,Not Specified,,2.326287031173706,,0.052053,0.052419,4918578681.0,True,3,0,"['transformers', 'pytorch']",2022-04-13 13:07:34+00:00,2022-04-13 09:47:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 737822494
- CO2 Emissions (in grams): 361.800665798794

## Validation Metrics

- Loss: 2.326287031173706
- Rouge1: 5.2053
- Rouge2: 1.8535
- RougeL: 5.2419
- RougeLsum: 5.228
- Gen Len: 18.3677

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/vabadeh213/autotrain-wikihow-737822494
```",,,1,[],[],NLP,2022-04,13594719.816616751,0.0522353588904204,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
36401,autotrain-trec-fine-bert-739422530,['ndavid/autotrain-data-trec-fine-bert'],,0.0223882029910544,AutoTrain,Not Specified,Not Specified,Not Specified,0.9321753515301904,0.3662329018115997,0.9066706944656866,,,438157613.0,True,28,0,"['transformers', 'pytorch']",2022-04-14 09:39:42+00:00,2022-04-14 09:37:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 739422530
- CO2 Emissions (in grams): 0.02238820299105448

## Validation Metrics

- Loss: 0.36623290181159973
- Accuracy: 0.9321753515301903
- Macro F1: 0.9066706944656866
- Micro F1: 0.9321753515301903
- Weighted F1: 0.9314858667247282
- Macro Precision: 0.9489233194839841
- Micro Precision: 0.9321753515301903
- Weighted Precision: 0.9347346558570125
- Macro Recall: 0.8842587178845419
- Micro Recall: 0.9321753515301903
- Weighted Recall: 0.9321753515301903


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ndavid/autotrain-trec-fine-bert-739422530
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ndavid/autotrain-trec-fine-bert-739422530"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ndavid/autotrain-trec-fine-bert-739422530"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,19570914788.251293,0.9192461491554016,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
36471,koelectra-small-v3-generator-apeach,['jason9693/APEACH'],,0.0185623904203696,AutoTrain,Not Specified,Not Specified,Not Specified,0.7740053050397878,0.4798508286476135,0.8025034770514604,,,56576041.0,True,10,0,"['transformers', 'pytorch']",2022-04-16 14:43:51+00:00,2022-04-14 15:44:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 742522663
- CO2 Emissions (in grams): 0.01856239042036965

## Validation Metrics

- Loss: 0.4798508286476135
- Accuracy: 0.7740053050397878
- Precision: 0.7236622073578596
- Recall: 0.9006243496357961
- AUC: 0.8798210006261515
- F1: 0.8025034770514604

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jason9693/autotrain-kor_hate_eval-742522663
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jason9693/autotrain-kor_hate_eval-742522663"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jason9693/autotrain-kor_hate_eval-742522663"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,3047885521.140404,0.787996813727555,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,1.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
36552,autotrain-iris-744122711,['vabadeh213/autotrain-data-iris'],,0.0006493037575021,AutoTrain,Not Specified,Not Specified,Not Specified,0.9666666666666668,0.0924196240746612,0.9665831244778612,,,,True,4,0,"['joblib', 'transformers']",2022-04-15 02:09:16+00:00,2022-04-15 02:08:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 744122711
- CO2 Emissions (in grams): 0.0006493037575021453

## Validation Metrics

- Loss: 0.09241962407466127
- Accuracy: 0.9666666666666667
- Macro F1: 0.9665831244778613
- Micro F1: 0.9666666666666667
- Weighted F1: 0.9665831244778613
- Macro Precision: 0.9696969696969697
- Micro Precision: 0.9666666666666667
- Weighted Precision: 0.9696969696969696
- Macro Recall: 0.9666666666666667
- Micro Recall: 0.9666666666666667
- Weighted Recall: 0.9666666666666667

## Usage

```python
import json
import joblib

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-04,,0.9666248937671952,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
36555,autotrain-titanic-744222727,['vabadeh213/autotrain-data-titanic'],,0.0050930354577298,AutoTrain,Not Specified,Not Specified,Not Specified,0.8378378378378378,0.4059609870954945,0.8846153846153846,,,,True,4,1,"['joblib', 'transformers']",2022-04-15 03:47:25+00:00,2022-04-15 02:18:16+00:00,"
https://colab.research.google.com/drive/16rmsJTBelh2vIWVxt9ncFEJmU7cEdUsE?usp=sharing

# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 744222727
- CO2 Emissions (in grams): 0.00509303545772981

## Validation Metrics

- Loss: 0.40596098709549455
- Accuracy: 0.8378378378378378
- Precision: 0.8518518518518519
- Recall: 0.92
- AUC: 0.8866666666666667
- F1: 0.8846153846153846

## Usage

```python
import json
import joblib

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-04,,0.8605914302957149,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
36865,soongsil-bert-small-apeach,['jason9693/APEACH'],,0.0185623904203696,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,223256685.0,False,8,0,"['transformers', 'pytorch']",2022-04-16 14:19:36+00:00,2022-04-16 06:35:03+00:00,,,,1,[],[],NLP,2022-04,12027367162.529173,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
36925,goemos,['crcb/autotrain-data-go_emo'],,31.11935827749309,AutoTrain,Not Specified,Not Specified,Not Specified,0.93625,0.1703956872224807,0.9075787460059076,,,1340753837.0,True,19,0,"['transformers', 'pytorch']",2022-04-16 15:16:07+00:00,2022-04-16 15:00:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 748922872
- CO2 Emissions (in grams): 31.11935827749309

## Validation Metrics

- Loss: 0.17039568722248077
- Accuracy: 0.93625
- Macro F1: 0.9075787460059076
- Micro F1: 0.93625
- Weighted F1: 0.9371621543264445
- Macro Precision: 0.8945117620407296
- Micro Precision: 0.93625
- Weighted Precision: 0.9433589433926076
- Macro Recall: 0.9323604226458176
- Micro Recall: 0.93625
- Weighted Recall: 0.93625


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-go_emo-748922872
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-go_emo-748922872"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-go_emo-748922872"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,43084237.95389421,0.921691456203502,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
36980,autotrain-BerTweet-749522913,['js3078/autotrain-data-BerTweet'],,4.093939667345746,AutoTrain,Not Specified,Not Specified,Not Specified,0.75,0.6473096609115601,0.7506205181665155,,,1421615405.0,True,8,0,"['transformers', 'pytorch']",2022-04-16 22:34:05+00:00,2022-04-16 22:31:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 749522913
- CO2 Emissions (in grams): 4.093939667345746

## Validation Metrics

- Loss: 0.6473096609115601
- Accuracy: 0.75
- Macro F1: 0.7506205181665155
- Micro F1: 0.75
- Weighted F1: 0.7506205181665155
- Macro Precision: 0.7555096418732782
- Micro Precision: 0.75
- Weighted Precision: 0.7555096418732782
- Macro Recall: 0.75
- Micro Recall: 0.75
- Weighted Recall: 0.75


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/js3078/autotrain-BerTweet-749522913
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""js3078/autotrain-BerTweet-749522913"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""js3078/autotrain-BerTweet-749522913"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,347248743.3899304,0.7503101307887321,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37077,emo_nojoylove,['crcb/autotrain-data-emo_carer_nojoylove'],,12.236769332727215,AutoTrain,Not Specified,Not Specified,Not Specified,0.9397905759162304,0.1358409821987152,0.9096049124431982,,,1334495149.0,True,19,0,"['transformers', 'pytorch']",2022-04-17 14:19:31+00:00,2022-04-17 14:12:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 751422966
- CO2 Emissions (in grams): 12.236769332727217

## Validation Metrics

- Loss: 0.1358409821987152
- Accuracy: 0.9397905759162304
- Macro F1: 0.9096049124431982
- Micro F1: 0.9397905759162304
- Weighted F1: 0.9395954853807672
- Macro Precision: 0.919807346649452
- Micro Precision: 0.9397905759162304
- Weighted Precision: 0.9407259082357824
- Macro Recall: 0.9024000547645126
- Micro Recall: 0.9397905759162304
- Weighted Recall: 0.9397905759162304


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-emo_carer_nojoylove-751422966
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-emo_carer_nojoylove-751422966"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-emo_carer_nojoylove-751422966"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,109056166.11002834,0.9244514003649268,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37079,carer_2,['crcb/autotrain-data-emo_carer_nojoylove'],,2.370895196595982,AutoTrain,Not Specified,Not Specified,Not Specified,0.9345549738219896,0.1536270827054977,0.9016011681330568,,,328532077.0,True,8,0,"['transformers', 'pytorch']",2022-04-17 14:14:39+00:00,2022-04-17 14:13:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 751422974
- CO2 Emissions (in grams): 2.370895196595982

## Validation Metrics

- Loss: 0.15362708270549774
- Accuracy: 0.9345549738219895
- Macro F1: 0.9016011681330569
- Micro F1: 0.9345549738219895
- Weighted F1: 0.9345413976263288
- Macro Precision: 0.9032333514618506
- Micro Precision: 0.9345549738219895
- Weighted Precision: 0.9345804677958041
- Macro Recall: 0.9001021129974442
- Micro Recall: 0.9345549738219895
- Weighted Recall: 0.9345549738219895


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-emo_carer_nojoylove-751422974
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-emo_carer_nojoylove-751422974"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-emo_carer_nojoylove-751422974"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,138568789.32130387,0.917782357207716,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37145,hateval_re,['crcb/autotrain-data-hate_speech'],,5.301132895184483,AutoTrain,Not Specified,Not Specified,Not Specified,0.7529411764705882,0.7107211351394653,0.8255726151522779,,,438019245.0,True,25,1,"['transformers', 'pytorch']",2022-04-18 01:35:05+00:00,2022-04-18 01:32:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 752122994
- CO2 Emissions (in grams): 5.301132895184483

## Validation Metrics

- Loss: 0.7107211351394653
- Accuracy: 0.7529411764705882
- Precision: 0.7502287282708143
- Recall: 0.9177392277560157
- AUC: 0.8358316393336287
- F1: 0.8255726151522779

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-hate_speech-752122994
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-hate_speech-752122994"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-hate_speech-752122994"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,82627478.62780313,0.7875859170993788,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37259,dvs_f,['crcb/autotrain-data-dvs'],,8.758858538967111,AutoTrain,Not Specified,Not Specified,Not Specified,0.9471454508775468,0.1483393609523773,0.4564315352697096,,,433331373.0,True,25,0,"['transformers', 'pytorch']",2022-04-18 13:44:09+00:00,2022-04-18 13:40:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 753223045
- CO2 Emissions (in grams): 8.758858538967111

## Validation Metrics

- Loss: 0.14833936095237732
- Accuracy: 0.9471454508775469
- Precision: 0.5045871559633027
- Recall: 0.4166666666666667
- AUC: 0.8806422686270332
- F1: 0.4564315352697096

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-dvs-753223045
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-dvs-753223045"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-dvs-753223045"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,49473498.29571521,0.6160076098916664,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37260,hs_dvs,['crcb/autotrain-data-dvs'],,5.1746636998598445,AutoTrain,Not Specified,Not Specified,Not Specified,0.9493645350010088,0.1463914364576339,0.3802469135802469,,,263172209.0,True,6,0,"['transformers', 'pytorch']",2022-04-18 13:43:00+00:00,2022-04-18 13:40:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 753223051
- CO2 Emissions (in grams): 5.1746636998598445

## Validation Metrics

- Loss: 0.14639143645763397
- Accuracy: 0.9493645350010087
- Precision: 0.5460992907801419
- Recall: 0.2916666666666667
- AUC: 0.8843542768404266
- F1: 0.3802469135802469

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-dvs-753223051
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-dvs-753223051"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-dvs-753223051"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,50857838.16388454,0.5430051533955617,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37262,imp_hatred,['crcb/autotrain-data-imp_hs'],,15.91710539314839,AutoTrain,Not Specified,Not Specified,Not Specified,0.7746741154562383,0.5205655694007874,0.5796696218586866,,,498677165.0,True,16,0,"['transformers', 'pytorch']",2022-04-18 14:11:43+00:00,2022-04-18 14:03:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 753423062
- CO2 Emissions (in grams): 15.91710539314839

## Validation Metrics

- Loss: 0.5205655694007874
- Accuracy: 0.7746741154562383
- Macro F1: 0.5796696218586866
- Micro F1: 0.7746741154562382
- Weighted F1: 0.7602379277947592
- Macro Precision: 0.6976905233970596
- Micro Precision: 0.7746741154562383
- Weighted Precision: 0.7628815999440115
- Macro Recall: 0.557144871405371
- Micro Recall: 0.7746741154562383
- Weighted Recall: 0.7746741154562383


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-imp_hs-753423062
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-imp_hs-753423062"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-imp_hs-753423062"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,31329638.943941303,0.6631330572850157,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37263,imp_hatred_f,['crcb/autotrain-data-imp_hs'],,0.0528650561726386,AutoTrain,Not Specified,Not Specified,Not Specified,0.7616387337057728,0.539419412612915,0.6428050387135232,,,438022317.0,True,32,0,"['transformers', 'pytorch']",2022-04-18 14:11:31+00:00,2022-04-18 14:05:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 753423076
- CO2 Emissions (in grams): 0.05286505617263864

## Validation Metrics

- Loss: 0.539419412612915
- Accuracy: 0.7616387337057728
- Macro F1: 0.6428050387135232
- Micro F1: 0.761638733705773
- Weighted F1: 0.7592341595725172
- Macro Precision: 0.6606534010647378
- Micro Precision: 0.7616387337057728
- Weighted Precision: 0.7575825822976101
- Macro Recall: 0.6293404928847536
- Micro Recall: 0.7616387337057728
- Weighted Recall: 0.7616387337057728


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-imp_hs-753423076
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-imp_hs-753423076"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-imp_hs-753423076"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,8285668241.221072,0.6971944698962184,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37325,autotrain-NLU_crypto_sentiment_analysis-754123133,['zainalq7/autotrain-data-NLU_crypto_sentiment_analysis'],,0.0053000308538672,AutoTrain,Not Specified,Not Specified,Not Specified,0.8658536585365854,0.387116938829422,0.7724053724053724,,,328529005.0,True,7893,0,"['transformers', 'pytorch']",2022-04-18 18:39:48+00:00,2022-04-18 18:38:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 754123133
- CO2 Emissions (in grams): 0.005300030853867218

## Validation Metrics

- Loss: 0.387116938829422
- Accuracy: 0.8658536585365854
- Macro F1: 0.7724053724053724
- Micro F1: 0.8658536585365854
- Weighted F1: 0.8467166979362101
- Macro Precision: 0.8232219717155155
- Micro Precision: 0.8658536585365854
- Weighted Precision: 0.8516026874759421
- Macro Recall: 0.7642089093701996
- Micro Recall: 0.8658536585365854
- Weighted Recall: 0.8658536585365854


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/zainalq7/autotrain-NLU_crypto_sentiment_analysis-754123133
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""zainalq7/autotrain-NLU_crypto_sentiment_analysis-754123133"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""zainalq7/autotrain-NLU_crypto_sentiment_analysis-754123133"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,61986243865.03065,0.8164643135658076,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37377,autotrain-Online_orders-755323156,['xInsignia/autotrain-data-Online_orders-5cf92320'],,2.4120667129093043,AutoTrain,Not Specified,Not Specified,Not Specified,0.9550898203592816,0.1782606095075607,0.8880388927888968,,,263279857.0,True,6,0,"['transformers', 'pytorch']",2022-04-19 03:29:18+00:00,2022-04-19 03:27:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 755323156
- CO2 Emissions (in grams): 2.4120667129093043

## Validation Metrics

- Loss: 0.17826060950756073
- Accuracy: 0.9550898203592815
- Macro F1: 0.8880388927888968
- Micro F1: 0.9550898203592815
- Weighted F1: 0.9528256324309916
- Macro Precision: 0.9093073732635162
- Micro Precision: 0.9550898203592815
- Weighted Precision: 0.9533674643333371
- Macro Recall: 0.8872729481745715
- Micro Recall: 0.9550898203592815
- Weighted Recall: 0.9550898203592815


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/xInsignia/autotrain-Online_orders-755323156
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""xInsignia/autotrain-Online_orders-755323156"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""xInsignia/autotrain-Online_orders-755323156"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,109151150.58424155,0.9203447383086968,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37382,autotrain-mental-health-analysis-752423172,['rabiaqayyum/autotrain-data-mental-health-analysis'],,313.3534743349287,AutoTrain,Not Specified,Not Specified,Not Specified,0.805171240644137,0.6064515113830566,0.7253473044054398,,,498689453.0,True,53,1,"['transformers', 'pytorch']",2022-04-19 06:45:00+00:00,2022-04-19 04:19:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 752423172
- CO2 Emissions (in grams): 313.3534743349287

## Validation Metrics

- Loss: 0.6064515113830566
- Accuracy: 0.805171240644137
- Macro F1: 0.7253473044054398
- Micro F1: 0.805171240644137
- Weighted F1: 0.7970679970423672
- Macro Precision: 0.7477679873153633
- Micro Precision: 0.805171240644137
- Weighted Precision: 0.7966263131173029
- Macro Recall: 0.7143231260991618
- Micro Recall: 0.805171240644137
- Weighted Recall: 0.805171240644137


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/rabiaqayyum/autotrain-mental-health-analysis-752423172
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""rabiaqayyum/autotrain-mental-health-analysis-752423172"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""rabiaqayyum/autotrain-mental-health-analysis-752423172"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,1591459.785338057,0.7631776705679716,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37481,autotrain-twitterMbti-758223271,['intellisr/autotrain-data-twitterMbti'],,0.3313142450338848,AutoTrain,Not Specified,Not Specified,Not Specified,0.6438828259620908,1.2496932744979858,0.5757131072506373,,,1340794797.0,True,19,0,"['transformers', 'pytorch']",2022-04-19 14:18:50+00:00,2022-04-19 13:43:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 758223271
- CO2 Emissions (in grams): 0.3313142450338848

## Validation Metrics

- Loss: 1.2496932744979858
- Accuracy: 0.6438828259620908
- Macro F1: 0.5757131072506373
- Micro F1: 0.6438828259620908
- Weighted F1: 0.6401462906378685
- Macro Precision: 0.6279826743318115
- Micro Precision: 0.6438828259620908
- Weighted Precision: 0.6479595607607238
- Macro Recall: 0.5436771609966322
- Micro Recall: 0.6438828259620908
- Weighted Recall: 0.6438828259620908


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/intellisr/autotrain-twitterMbti-758223271
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""intellisr/autotrain-twitterMbti-758223271"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""intellisr/autotrain-twitterMbti-758223271"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,4046897521.3029904,0.6078927821011333,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37633,Roberta_Sentiment_Analysis,['Souvikcmsa/autotrain-data-sentimentAnalysis_By_Souvik'],,4.453029772491864,AutoTrain,Not Specified,Not Specified,Not Specified,0.8302828618968386,0.4084313809871673,0.8302447939743022,,,1421611309.0,True,55,0,"['transformers', 'pytorch']",2022-04-20 08:53:33+00:00,2022-04-20 08:50:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 762623422
- CO2 Emissions (in grams): 4.453029772491864

## Validation Metrics

- Loss: 0.40843138098716736
- Accuracy: 0.8302828618968386
- Macro F1: 0.8302447939743022
- Micro F1: 0.8302828618968385
- Weighted F1: 0.8302151855901072
- Macro Precision: 0.8310980209442669
- Micro Precision: 0.8302828618968386
- Weighted Precision: 0.8313262654775467
- Macro Recall: 0.8305699539252172
- Micro Recall: 0.8302828618968386
- Weighted Recall: 0.8302828618968386


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Souvikcmsa/autotrain-sentimentAnalysis_By_Souvik-762623422
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Souvikcmsa/autotrain-sentimentAnalysis_By_Souvik-762623422"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Souvikcmsa/autotrain-sentimentAnalysis_By_Souvik-762623422"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,319245857.68140566,0.8302638274992131,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37635,BERT_sentiment_analysis,['Souvikcmsa/autotrain-data-sentiment_analysis'],,0.0293633978449355,AutoTrain,Not Specified,Not Specified,Not Specified,0.799017824663514,0.4992932379245758,0.8021508522962549,,,438022317.0,True,438,0,"['transformers', 'pytorch']",2022-04-21 17:17:04+00:00,2022-04-20 09:03:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification (3-class Sentiment Classification)

## Validation Metrics
If you search sentiment analysis model in huggingface you find a model from finiteautomata. Their model provides micro and macro F1 score around 67%. Check out this model with around 80% of macro and micro F1 score. 
- Loss: 0.4992932379245758
- Accuracy: 0.799017824663514
- Macro F1: 0.8021508522962549
- Micro F1: 0.799017824663514
- Weighted F1: 0.7993775463659935
- Macro Precision: 0.80406197665167
- Micro Precision: 0.799017824663514
- Weighted Precision: 0.8000374433849405
- Macro Recall: 0.8005261994732908
- Micro Recall: 0.799017824663514
- Weighted Recall: 0.799017824663514


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Souvikcmsa/autotrain-sentiment_analysis-762923428
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Souvikcmsa/autotrain-sentiment_analysis-762923428"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Souvikcmsa/autotrain-sentiment_analysis-762923428"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```
OR
```
from transformers import pipeline

classifier = pipeline(""text-classification"", model = ""Souvikcmsa/BERT_sentiment_analysis"")
classifier(""I loved Star Wars so much!"")# Positive
classifier(""A soccer game with multiple males playing. Some men are playing a sport."")# Neutral
```",,,1,[],[],NLP,2022-04,14917289862.472374,0.8005812732618695,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37636,SentimentAnalysisDistillBERT,['Souvikcmsa/autotrain-data-sentiment_analysis'],,0.0155367469092942,AutoTrain,Not Specified,Not Specified,Not Specified,0.7962895598399418,0.4982589483261108,0.7997458031044901,,,267863153.0,True,75,0,"['transformers', 'pytorch']",2022-04-20 09:05:38+00:00,2022-04-20 09:03:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 762923432
- CO2 Emissions (in grams): 0.015536746909294205

## Validation Metrics

- Loss: 0.49825894832611084
- Accuracy: 0.7962895598399418
- Macro F1: 0.7997458031044901
- Micro F1: 0.7962895598399418
- Weighted F1: 0.796365325858282
- Macro Precision: 0.7995724418486833
- Micro Precision: 0.7962895598399418
- Weighted Precision: 0.7965384250324863
- Macro Recall: 0.8000290112564951
- Micro Recall: 0.7962895598399418
- Weighted Recall: 0.7962895598399418


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Souvikcmsa/autotrain-sentiment_analysis-762923432
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Souvikcmsa/autotrain-sentiment_analysis-762923432"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Souvikcmsa/autotrain-sentiment_analysis-762923432"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,17240620225.316418,0.7980139391937612,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
37799,autotrain-financial-sentiment-765323474,['ktangri/autotrain-data-financial-sentiment'],,0.0075013546359948,AutoTrain,Not Specified,Not Specified,Not Specified,0.9823788546255506,0.0447433702647686,0.974405452470854,,,433334445.0,True,43,0,"['transformers', 'pytorch']",2022-04-20 14:35:01+00:00,2022-04-20 14:34:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 765323474
- CO2 Emissions (in grams): 0.007501354635994803

## Validation Metrics

- Loss: 0.0447433702647686
- Accuracy: 0.9823788546255506
- Macro F1: 0.974405452470854
- Micro F1: 0.9823788546255506
- Weighted F1: 0.9823043153179869
- Macro Precision: 0.978208375548801
- Micro Precision: 0.9823788546255506
- Weighted Precision: 0.9823204968555985
- Macro Recall: 0.9707159078140736
- Micro Recall: 0.9823788546255506
- Weighted Recall: 0.9823788546255506


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ktangri/autotrain-financial-sentiment-765323474
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ktangri/autotrain-financial-sentiment-765323474"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ktangri/autotrain-financial-sentiment-765323474"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,57767492143.44173,0.9783759087475644,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38463,autotrain-ner-778023879,['Lucifermorningstar011/autotrain-data-ner'],,43.26533004662002,AutoTrain,Not Specified,Not Specified,Not Specified,0.9999996519918594,5.475859779835446e-06,0.0,,,260809205.0,True,6,0,"['transformers', 'pytorch']",2022-04-24 00:00:13+00:00,2022-04-23 23:29:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 778023879
- CO2 Emissions (in grams): 43.26533004662002

## Validation Metrics

- Loss: 5.475859779835446e-06
- Accuracy: 0.9999996519918594
- Precision: 0.0
- Recall: 0.0
- F1: 0.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Lucifermorningstar011/autotrain-ner-778023879
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Lucifermorningstar011/autotrain-ner-778023879"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Lucifermorningstar011/autotrain-ner-778023879"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,6028133.952034302,0.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38530,autotrain-NMT-778623908,['singhajeet13/autotrain-data-NMT'],,1.0568409665060603,AutoTrain,Not Specified,Not Specified,Not Specified,,2.4664785861968994,,,,1200743045.0,True,3,0,"['transformers', 'pytorch']",2022-04-24 13:48:26+00:00,2022-04-24 11:37:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 778623908
- CO2 Emissions (in grams): 1.0568409665060605

## Validation Metrics

- Loss: 2.4664785861968994
- SacreBLEU: 1.6168
- Gen len: 17.645",,,1,[],[],NLP,2022-04,1136162471.9844868,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38539,kekbot-beta-1-medium,[''],,370.0,MLCO2,fine-tuning,"West Java, Indonesia",1 Tesla P100,,,,,,1444581337.0,False,7,0,"['transformers', 'pytorch']",2022-04-24 23:40:49+00:00,2022-04-24 13:31:50+00:00,"> THIS MODEL IS IN PUBLIC BETA, PLEASE DO NOT EXPECT ANY FORM OF STABILITY IN ITS CURRENT STATE.  
# Art Union server chatbot

Based on a DialoGPT-medium model, fine-tuned to a small subset (52k< messages) of Art Union's general-chat channel.

### Current issues  
(Which hopefully will be fixed in future iterations) Include, but not limited to:
- Limited turns, after ~11 turns output may break for no apparent reason.
- Inconsistent variance, acts like an overfitted model from time to time for no reason whatsoever.",,,1,[],[],NLP,2022-04,3904273.883783784,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,1,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,1,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38640,carer_new,['crcb/autotrain-data-carer_new'],,3.9861818439722594,AutoTrain,Not Specified,Not Specified,Not Specified,0.9389179755671904,0.1639203429222107,0.9055551236566716,,,498680237.0,True,16,0,"['transformers', 'pytorch']",2022-04-25 08:08:42+00:00,2022-04-25 08:06:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 781623992
- CO2 Emissions (in grams): 3.9861818439722594

## Validation Metrics

- Loss: 0.1639203429222107
- Accuracy: 0.9389179755671903
- Macro F1: 0.9055551236566716
- Micro F1: 0.9389179755671903
- Weighted F1: 0.9379300009988988
- Macro Precision: 0.9466951148514304
- Micro Precision: 0.9389179755671903
- Weighted Precision: 0.9435523016000105
- Macro Recall: 0.8818551804621082
- Micro Recall: 0.9389179755671903
- Weighted Recall: 0.9389179755671903


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-carer_new-781623992
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-carer_new-781623992"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-carer_new-781623992"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,125102229.78263868,0.9219348157758366,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38661,kekbot-beta-2-medium,[''],,940.0,MLCO2,fine-tuning,"West Java, Indonesia",1 Tesla P100,,,,,,1444566873.0,False,6,0,"['transformers', 'pytorch']",2022-04-25 18:19:29+00:00,2022-04-25 10:51:20+00:00,"> THIS MODEL IS IN PUBLIC BETA, PLEASE DO NOT EXPECT ANY FORM OF STABILITY IN ITS CURRENT STATE.  
# Art Union server chatbot

Based on a DialoGPT-medium model, fine-tuned to a small subset (115k<= messages) of Art Union's general-chat channel.

### Current issues  
(Which hopefully will be fixed in future iterations) Include, but not limited to:
- Limited turns, after ~11 turns output may break for no apparent reason.
- Inconsistent variance, acts like an overfitted model from time to time for no reason whatsoever.
",,,1,[],[],NLP,2022-04,1536773.2691489365,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,1,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,1,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38692,autotrain-final-784824218,['Lucifermorningstar011/autotrain-data-final'],,237.58504390669623,AutoTrain,Not Specified,Not Specified,Not Specified,0.9734973172736224,0.2379177361726761,0.0,,,265497077.0,True,6,0,"['transformers', 'pytorch']",2022-04-25 17:44:20+00:00,2022-04-25 15:23:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 784824218
- CO2 Emissions (in grams): 237.58504390669626

## Validation Metrics

- Loss: 0.2379177361726761
- Accuracy: 0.9734973172736223
- Precision: 0.0
- Recall: 0.0
- F1: 0.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Lucifermorningstar011/autotrain-final-784824218
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Lucifermorningstar011/autotrain-final-784824218"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Lucifermorningstar011/autotrain-final-784824218"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,1117482.2818572086,0.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38693,autotrain-final-784824206,['Lucifermorningstar011/autotrain-data-final'],,354.2174590750517,AutoTrain,Not Specified,Not Specified,Not Specified,0.9785765909606228,0.1393078863620758,0.0,,,430968305.0,True,19,0,"['transformers', 'pytorch']",2022-04-25 18:46:51+00:00,2022-04-25 15:23:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 784824206
- CO2 Emissions (in grams): 354.21745907505175

## Validation Metrics

- Loss: 0.1393078863620758
- Accuracy: 0.9785765909606228
- Precision: 0.0
- Recall: 0.0
- F1: 0.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Lucifermorningstar011/autotrain-final-784824206
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Lucifermorningstar011/autotrain-final-784824206"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Lucifermorningstar011/autotrain-final-784824206"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,1216677.1963340358,0.0,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38694,autotrain-final-784824213,['Lucifermorningstar011/autotrain-data-final'],,443.6253241508679,AutoTrain,Not Specified,Not Specified,Not Specified,0.9823625038850629,0.1277752667665481,0.0,,,435656177.0,True,19,0,"['transformers', 'pytorch']",2022-04-25 19:24:43+00:00,2022-04-25 15:24:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 784824213
- CO2 Emissions (in grams): 443.62532415086787

## Validation Metrics

- Loss: 0.12777526676654816
- Accuracy: 0.9823625038850627
- Precision: 0.0
- Recall: 0.0
- F1: 0.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Lucifermorningstar011/autotrain-final-784824213
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Lucifermorningstar011/autotrain-final-784824213"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Lucifermorningstar011/autotrain-final-784824213"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,982036.3114614312,0.0,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38695,autotrain-final-784824209,['Lucifermorningstar011/autotrain-data-final'],,0.8282546197737336,AutoTrain,Not Specified,Not Specified,Not Specified,0.9639925673427912,0.1807728707790374,0.0,,,265497077.0,True,6,0,"['transformers', 'pytorch']",2022-04-25 17:32:25+00:00,2022-04-25 15:24:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 784824209
- CO2 Emissions (in grams): 0.8282546197737336

## Validation Metrics

- Loss: 0.18077287077903748
- Accuracy: 0.9639925673427913
- Precision: 0.0
- Recall: 0.0
- F1: 0.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Lucifermorningstar011/autotrain-final-784824209
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Lucifermorningstar011/autotrain-final-784824209"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Lucifermorningstar011/autotrain-final-784824209"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,320550070.78925765,0.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38696,autotrain-final-784824211,['Lucifermorningstar011/autotrain-data-final'],,292.55119229577315,AutoTrain,Not Specified,Not Specified,Not Specified,0.9732196168090091,0.176827386021614,0.0,,,260809205.0,True,6,0,"['transformers', 'pytorch']",2022-04-25 18:49:50+00:00,2022-04-25 15:24:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 784824211
- CO2 Emissions (in grams): 292.55119229577315

## Validation Metrics

- Loss: 0.17682738602161407
- Accuracy: 0.9732196168090091
- Precision: 0.0
- Recall: 0.0
- F1: 0.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Lucifermorningstar011/autotrain-final-784824211
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Lucifermorningstar011/autotrain-final-784824211"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Lucifermorningstar011/autotrain-final-784824211"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,891499.3747019784,0.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38698,fake-news-debunker,['Fake and real news datasets by CLÉMENT BISAILLON'],,4.415122243239347,AutoTrain,Not Specified,Not Specified,Not Specified,0.9998886538247412,0.0001258671400137,0.999883273024396,,,328525933.0,True,10,2,"['transformers', 'pytorch']",2022-04-26 13:53:36+00:00,2022-04-25 15:55:54+00:00,"
# Model Trained Using AutoTrain

- Problem: Fake News Classification
- Problem type: Binary Classification
- Model ID: 785124234
- CO2 Emissions (in grams): 4.415122243239347

## Validation Metrics

- Loss: 0.00012586714001372457
- Accuracy: 0.9998886538247411
- Precision: 1.0
- Recall: 0.9997665732959851
- AUC: 0.9999999999999999
- F1: 0.999883273024396

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Nithiwat/autotrain-fake-news-classifier-785124234
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Nithiwat/autotrain-fake-news-classifier-785124234"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Nithiwat/autotrain-fake-news-classifier-785124234"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,74409249.5973481,0.9998859634173294,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38760,isear_bert,['crcb/autotrain-data-isear_bert'],,0.0260270554349944,AutoTrain,Not Specified,Not Specified,Not Specified,0.7272727272727273,0.8348872065544128,0.7230931630686932,,,498689453.0,True,16,0,"['transformers', 'pytorch']",2022-04-26 03:14:10+00:00,2022-04-26 03:11:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 786224257
- CO2 Emissions (in grams): 0.026027055434994496

## Validation Metrics

- Loss: 0.8348872065544128
- Accuracy: 0.7272727272727273
- Macro F1: 0.7230931630686932
- Micro F1: 0.7272727272727273
- Weighted F1: 0.7236599456423468
- Macro Precision: 0.7328252157220334
- Micro Precision: 0.7272727272727273
- Weighted Precision: 0.7336599708829821
- Macro Recall: 0.7270448163292604
- Micro Recall: 0.7272727272727273
- Weighted Recall: 0.7272727272727273


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-isear_bert-786224257
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-isear_bert-786224257"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-isear_bert-786224257"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,19160425359.892635,0.7251769229810503,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38782,carer_5way,['crcb/autotrain-data-carer_5way'],,4.164757528958762,AutoTrain,Not Specified,Not Specified,Not Specified,0.944234404536862,0.1672425270080566,0.9437256923758108,,,498683309.0,True,16,0,"['transformers', 'pytorch']",2022-04-26 05:46:33+00:00,2022-04-26 05:43:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 786524275
- CO2 Emissions (in grams): 4.164757528958762

## Validation Metrics

- Loss: 0.16724252700805664
- Accuracy: 0.944234404536862
- Macro F1: 0.9437256923758108
- Micro F1: 0.9442344045368619
- Weighted F1: 0.9442368364749825
- Macro Precision: 0.9431692663638349
- Micro Precision: 0.944234404536862
- Weighted Precision: 0.9446229335037916
- Macro Recall: 0.9446884750469657
- Micro Recall: 0.944234404536862
- Weighted Recall: 0.944234404536862


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-carer_5way-786524275
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-carer_5way-786524275"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-carer_5way-786524275"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,119738857.6723881,0.9439799799199136,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38887,kekbot-beta-3-medium,[''],,660.0,MLCO2,fine-tuning,"West Java, Indonesia",1 Tesla P100,,,,,,1444566873.0,False,6,0,"['transformers', 'pytorch']",2022-04-26 22:15:23+00:00,2022-04-26 17:03:37+00:00,"> THIS MODEL IS IN PUBLIC BETA, PLEASE DO NOT EXPECT ANY FORM OF STABILITY IN ITS CURRENT STATE.  
# Art Union server chatbot

Based on a DialoGPT-medium model, fine-tuned to a select subset (65k<= messages) of Art Union's general-chat channel chat history.

### Current issues  
(Which hopefully will be fixed in future iterations) Include, but not limited to:
- Limited turns, after ~17 turns output may break for no apparent reason.
- Inconsistent variance, acts like an overfitted model from time to time for no reason whatsoever.
",,,1,[],[],NLP,2022-04,2188737.6863636365,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,1,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,1,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38900,autotrain-Test_2-789524315,['Rem59/autotrain-data-Test_2'],,2.0134443204822188,AutoTrain,Not Specified,Not Specified,Not Specified,0.6904761904761905,0.8042349815368652,0.272300469483568,,,442582445.0,True,7,0,"['transformers', 'pytorch']",2022-04-26 19:11:30+00:00,2022-04-26 19:10:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 789524315
- CO2 Emissions (in grams): 2.0134443204822188

## Validation Metrics

- Loss: 0.8042349815368652
- Accuracy: 0.6904761904761905
- Macro F1: 0.27230046948356806
- Micro F1: 0.6904761904761905
- Weighted F1: 0.5640509725016768
- Macro Precision: 0.23015873015873015
- Micro Precision: 0.6904761904761905
- Weighted Precision: 0.4767573696145125
- Macro Recall: 0.3333333333333333
- Micro Recall: 0.6904761904761905
- Weighted Recall: 0.6904761904761905


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Rem59/autotrain-Test_2-789524315
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Rem59/autotrain-Test_2-789524315"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Rem59/autotrain-Test_2-789524315"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,219813600.25590464,0.3905723905723905,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38966,autotrain-nsut-nlp-project-textsummarization-791824374,['faisalahmad/autotrain-data-nsut-nlp-project-textsummarization'],,1119.6398037843474,AutoTrain,Not Specified,Not Specified,Not Specified,,1.6432833671569824,,0.385315,0.323742,1625557313.0,True,3,0,"['transformers', 'pytorch']",2022-04-27 17:50:47+00:00,2022-04-27 09:08:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 791824374
- CO2 Emissions (in grams): 1119.6398037843474

## Validation Metrics

- Loss: 1.6432833671569824
- Rouge1: 38.5315
- Rouge2: 18.0869
- RougeL: 32.3742
- RougeLsum: 32.3801
- Gen Len: 19.846

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/faisalahmad/autotrain-nsut-nlp-project-textsummarization-791824374
```",,,1,[],[],NLP,2022-04,1451857.3808341462,0.351855065897382,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38967,summarizer1,['faisalahmad/autotrain-data-nsut-nlp-project-textsummarization'],,736.9366247330848,AutoTrain,Not Specified,Not Specified,Not Specified,,1.780589580535889,,0.378222,0.312959,557979193.0,True,3,0,"['transformers', 'pytorch']",2022-04-27 15:53:08+00:00,2022-04-27 09:08:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 791824379
- CO2 Emissions (in grams): 736.9366247330848

## Validation Metrics

- Loss: 1.7805895805358887
- Rouge1: 37.8222
- Rouge2: 16.7598
- RougeL: 31.2959
- RougeLsum: 31.3048
- Gen Len: 19.7213

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/faisalahmad/autotrain-nsut-nlp-project-textsummarization-791824379
```",,,1,[],[],NLP,2022-04,757160.3503925423,0.3425093539839781,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
38968,summarizer2,['faisalahmad/autotrain-data-nsut-nlp-project-textsummarization'],,4444.804304528572,AutoTrain,Not Specified,Not Specified,Not Specified,,1.4599040746688845,,0.465461,0.38526,2283825905.0,True,5,0,"['transformers', 'pytorch']",2022-04-28 17:48:14+00:00,2022-04-27 09:09:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 791824381
- CO2 Emissions (in grams): 4444.804304528572

## Validation Metrics

- Loss: 1.4599040746688843
- Rouge1: 46.5461
- Rouge2: 23.8595
- RougeL: 38.526
- RougeLsum: 38.5219
- Gen Len: 23.468

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/faisalahmad/autotrain-nsut-nlp-project-textsummarization-791824381
```",,,1,[],[],NLP,2022-04,513819.2254433187,0.4215800594084312,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
39019,autotrain-Rule-793324440,['EAST/autotrain-data-Rule'],,0.0025078722090032,AutoTrain,Not Specified,Not Specified,Not Specified,0.9473684210526316,0.3110544085502624,0.9473684210526316,,,409160877.0,True,32,0,"['transformers', 'pytorch']",2022-04-27 14:57:26+00:00,2022-04-27 14:56:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 793324440
- CO2 Emissions (in grams): 0.0025078722090032795

## Validation Metrics

- Loss: 0.31105440855026245
- Accuracy: 0.9473684210526315
- Precision: 0.9
- Recall: 1.0
- AUC: 0.9444444444444445
- F1: 0.9473684210526316

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/EAST/autotrain-Rule-793324440
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""EAST/autotrain-Rule-793324440"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""EAST/autotrain-Rule-793324440"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,163150608524.27386,0.9473684210526316,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
39023,autotrain-nlp-text-summarization-by-faisal-793224456,['faisalahmad2/autotrain-data-nlp-text-summarization-by-faisal'],,27.26671996544415,AutoTrain,Not Specified,Not Specified,Not Specified,,1.5189369916915894,,0.387852,0.321082,2950904711.0,True,5,0,"['transformers', 'pytorch']",2022-04-29 14:05:30+00:00,2022-04-27 15:03:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 793224456
- CO2 Emissions (in grams): 27.26671996544415

## Validation Metrics

- Loss: 1.5189369916915894
- Rouge1: 38.7852
- Rouge2: 17.0785
- RougeL: 32.1082
- RougeLsum: 32.1103
- Gen Len: 18.7332

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/faisalahmad2/autotrain-nlp-text-summarization-by-faisal-793224456
```",,,1,[],[],NLP,2022-04,108223677.60917927,0.3513226784552581,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
39196,autotrain-Question-translation-797524592,['aakarshan/autotrain-data-Question-translation'],,27.564419884224776,AutoTrain,Not Specified,Not Specified,Not Specified,,2.2697999477386475,,,,4918480377.0,True,3,0,"['transformers', 'pytorch']",2022-04-28 14:48:38+00:00,2022-04-28 14:26:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 797524592
- CO2 Emissions (in grams): 27.564419884224776

## Validation Metrics

- Loss: 2.2697999477386475
- SacreBLEU: 14.9797
- Gen len: 13.7071",,,1,[],[],NLP,2022-04,178435838.57953292,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
39258,autotrain-mbtiNlp-798824628,['Sathira/autotrain-data-mbtiNlp'],,121.67185089502216,AutoTrain,Not Specified,Not Specified,Not Specified,0.8472124039775673,0.5046824812889099,0.7812978033330673,,,267903153.0,True,34,2,"['transformers', 'pytorch']",2022-04-28 22:09:14+00:00,2022-04-28 21:01:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 798824628
- CO2 Emissions (in grams): 121.67185089502216

## Validation Metrics

- Loss: 0.5046824812889099
- Accuracy: 0.8472124039775673
- Macro F1: 0.7812978033330673
- Micro F1: 0.8472124039775673
- Weighted F1: 0.8464983956259307
- Macro Precision: 0.812208631055716
- Micro Precision: 0.8472124039775673
- Weighted Precision: 0.8478968364150775
- Macro Recall: 0.7593223884993787
- Micro Recall: 0.8472124039775673
- Weighted Recall: 0.8472124039775673


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Sathira/autotrain-mbtiNlp-798824628
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Sathira/autotrain-mbtiNlp-798824628"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Sathira/autotrain-mbtiNlp-798824628"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,2201849.902251799,0.8129211437701966,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
39320,pro-cell-expert,['Mim/autotrain-data-procell-expert'],,0.0048148231383673,AutoTrain,Not Specified,Not Specified,Not Specified,0.9,0.4749071002006531,0.925925925925926,,,433331373.0,True,18,0,"['transformers', 'pytorch']",2022-04-29 11:36:58+00:00,2022-04-29 08:30:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 800724769
- CO2 Emissions (in grams): 0.004814823138367317

## Validation Metrics

- Loss: 0.4749071002006531
- Accuracy: 0.9
- Precision: 0.8928571428571429
- Recall: 0.9615384615384616
- AUC: 0.9065934065934066
- F1: 0.9259259259259259

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Mim/autotrain-procell-expert-800724769
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Mim/autotrain-procell-expert-800724769"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Mim/autotrain-procell-expert-800724769"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-04,89999437268.4145,0.9127789046653144,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
39715,cuad_contract_type,['agnihotri/autotrain-data-contract_type'],,0.0761094407164004,AutoTrain,Not Specified,Not Specified,Not Specified,0.991150442477876,0.0531290881335735,0.9912087912087912,,,1421705581.0,True,75,0,"['transformers', 'pytorch']",2022-05-01 18:49:12+00:00,2022-05-01 18:36:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 809725368
- CO2 Emissions (in grams): 0.07610944071640048

## Validation Metrics

- Loss: 0.05312908813357353
- Accuracy: 0.9911504424778761
- Macro F1: 0.9912087912087912
- Micro F1: 0.9911504424778761
- Weighted F1: 0.9908586988233007
- Macro Precision: 0.9942857142857143
- Micro Precision: 0.9911504424778761
- Weighted Precision: 0.9924146649810366
- Macro Recall: 0.99
- Micro Recall: 0.9911504424778761
- Weighted Recall: 0.9911504424778761


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/agnihotri/autotrain-contract_type-809725368
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""agnihotri/autotrain-contract_type-809725368"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""agnihotri/autotrain-contract_type-809725368"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,18679753360.658222,0.991179615984616,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
39747,autotrain-sentiment-4-812425472,['charly/autotrain-data-sentiment-4'],,0.0075975707447408,AutoTrain,Not Specified,Not Specified,Not Specified,0.8268156424581006,0.5105093121528625,0.6020923520923521,,,1340745645.0,True,17,0,"['transformers', 'pytorch']",2022-05-02 00:38:00+00:00,2022-05-02 00:36:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 812425472
- CO2 Emissions (in grams): 0.007597570744740809

## Validation Metrics

- Loss: 0.5105093121528625
- Accuracy: 0.8268156424581006
- Macro F1: 0.6020923520923521
- Micro F1: 0.8268156424581006
- Weighted F1: 0.8021395116367184
- Macro Precision: 0.5907986111111111
- Micro Precision: 0.8268156424581006
- Weighted Precision: 0.7792248603351954
- Macro Recall: 0.6141625496464206
- Micro Recall: 0.8268156424581006
- Weighted Recall: 0.8268156424581006


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/charly/autotrain-sentiment-4-812425472
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""charly/autotrain-sentiment-4-812425472"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""charly/autotrain-sentiment-4-812425472"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,176470307424.00012,0.6967829654714269,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
39759,emo_go_new,['crcb/autotrain-data-go_emo_new'],,20.58663910106142,AutoTrain,Not Specified,Not Specified,Not Specified,0.5920355494787216,1.3628994226455688,0.4844439507523978,,,498750957.0,True,16,0,"['transformers', 'pytorch']",2022-05-02 04:17:02+00:00,2022-05-02 04:07:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 813325491
- CO2 Emissions (in grams): 20.58663910106142

## Validation Metrics

- Loss: 1.3628994226455688
- Accuracy: 0.5920355494787216
- Macro F1: 0.4844439507523978
- Micro F1: 0.5920355494787216
- Weighted F1: 0.5873137663478112
- Macro Precision: 0.5458988948121151
- Micro Precision: 0.5920355494787216
- Weighted Precision: 0.591386299522425
- Macro Recall: 0.4753100798358001
- Micro Recall: 0.5920355494787216
- Weighted Recall: 0.5920355494787216


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crcb/autotrain-go_emo_new-813325491
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crcb/autotrain-go_emo_new-813325491"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crcb/autotrain-go_emo_new-813325491"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,24226924.781242467,0.5328629862691506,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
39781,rumor,['tristantristantristan/autotrain-data-rumour_detection'],,0.0561862580928194,AutoTrain,Not Specified,Not Specified,Not Specified,0.9738805970149254,0.1505775302648544,0.9385964912280702,,,1340737453.0,True,65,0,"['transformers', 'pytorch']",2022-05-02 09:33:47+00:00,2022-05-02 09:27:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 813825547
- CO2 Emissions (in grams): 0.056186258092819436

## Validation Metrics

- Loss: 0.15057753026485443
- Accuracy: 0.9738805970149254
- Precision: 0.9469026548672567
- Recall: 0.9304347826086956
- AUC: 0.9891149437157905
- F1: 0.9385964912280702

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/tristantristantristan/autotrain-rumour_detection-813825547
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""tristantristantristan/autotrain-rumour_detection-813825547"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""tristantristantristan/autotrain-rumour_detection-813825547"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,23862373087.474663,0.9559130583604312,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
40405,autotrain-Bart_683-825526269,['Gootter/autotrain-data-Bart_683'],,28.12268287254098,AutoTrain,Not Specified,Not Specified,Not Specified,,2.836289644241333,,0.319867,0.210603,1625553217.0,True,4,0,"['transformers', 'pytorch']",2022-05-05 10:03:01+00:00,2022-05-05 09:46:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 825526269
- CO2 Emissions (in grams): 28.12268287254098

## Validation Metrics

- Loss: 2.836289644241333
- Rouge1: 31.9867
- Rouge2: 10.3239
- RougeL: 21.0603
- RougeLsum: 30.0862
- Gen Len: 142.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Gootter/autotrain-Bart_683-825526269
```",,,1,[],[],NLP,2022-05,57802209.85200498,0.253982128305088,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
40687,autotrain-maysix-828926405,['EAST/autotrain-data-maysix'],,0.0025866919829264,AutoTrain,Not Specified,Not Specified,Not Specified,0.9318181818181818,0.1797131597995758,0.9268292682926828,,,409160877.0,True,25,0,"['transformers', 'pytorch']",2022-05-06 07:13:15+00:00,2022-05-06 07:12:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 828926405
- CO2 Emissions (in grams): 0.00258669198292644

## Validation Metrics

- Loss: 0.1797131597995758
- Accuracy: 0.9318181818181818
- Precision: 0.9047619047619048
- Recall: 0.95
- AUC: 0.9875
- F1: 0.9268292682926829

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/EAST/autotrain-maysix-828926405
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""EAST/autotrain-maysix-828926405"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""EAST/autotrain-maysix-828926405"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,158179203283.8398,0.9293170295257976,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
40909,autotrain-test-831226565,['retextly/autotrain-data-test'],,134.3402063080293,AutoTrain,Not Specified,Not Specified,Not Specified,,0.3383736610412597,,0.8998909999999999,0.8974209999999999,2283825905.0,True,3,0,"['transformers', 'pytorch']",2022-05-07 09:28:04+00:00,2022-05-07 08:17:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 831226565
- CO2 Emissions (in grams): 134.3402063080293

## Validation Metrics

- Loss: 0.33837366104125977
- Rouge1: 89.9891
- Rouge2: 85.7247
- RougeL: 89.7421
- RougeLsum: 89.4872
- Gen Len: 30.1818

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/retextly/autotrain-test-831226565
```",,,1,[],[],NLP,2022-05,17000315.59995825,0.8986543027710268,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
41005,Wiki-Complexity,['hidude562/autotrain-data-SimpleDetect'],,0.2169160611944522,AutoTrain,Not Specified,Not Specified,Not Specified,0.996223414828066,0.0100969588384032,0.996179398826373,,,267860081.0,True,14,0,"['transformers', 'jax', 'safetensors', 'pytorch']",2023-03-23 13:52:40+00:00,2022-05-07 19:37:14+00:00,"# Model Description
This model detects if you are writing in a format that is more similar to Simple English Wikipedia or English Wikipedia. This can be extended to applications that aren't Wikipedia as well and to some extent, it can be used for other languages.

Please also note there is a major bias to special characters (Mainly the hyphen mark, but it also applies to others) so I would recommend removing them from your input text.

# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 837726721
- CO2 Emissions (in grams): 0.21691606119445225

## Validation Metrics

- Loss: 0.010096958838403225
- Accuracy: 0.996223414828066
- Macro F1: 0.996179398826373
- Micro F1: 0.996223414828066
- Weighted F1: 0.996223414828066
- Macro Precision: 0.996179398826373
- Micro Precision: 0.996223414828066
- Weighted Precision: 0.996223414828066
- Macro Recall: 0.996179398826373
- Micro Recall: 0.996223414828066
- Weighted Recall: 0.996223414828066


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I quite enjoy using AutoTrain due to its simplicity.""}' https://api-inference.huggingface.co/models/hidude562/Wiki-Complexity
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""hidude562/Wiki-Complexity"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hidude562/Wiki-Complexity"", use_auth_token=True)

inputs = tokenizer(""I quite enjoy using AutoTrain due to its simplicity."", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,1234855913.9651697,0.9962014063410204,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
41104,autotrain-chemprot-re-838426740,['pier297/autotrain-data-chemprot-re'],,0.0911766483095575,AutoTrain,Not Specified,Not Specified,Not Specified,0.9137332672285572,0.3866589665412903,0.6518117007658014,,,1334532013.0,True,17,1,"['transformers', 'pytorch']",2022-05-08 09:31:00+00:00,2022-05-08 09:21:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 838426740
- CO2 Emissions (in grams): 0.0911766483095575

## Validation Metrics

- Loss: 0.3866589665412903
- Accuracy: 0.9137332672285573
- Macro F1: 0.6518117007658014
- Micro F1: 0.9137332672285573
- Weighted F1: 0.9110993117549759
- Macro Precision: 0.649358664024301
- Micro Precision: 0.9137332672285573
- Weighted Precision: 0.9091854625539633
- Macro Recall: 0.6551854233645032
- Micro Recall: 0.9137332672285573
- Weighted Recall: 0.9137332672285573


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pier297/autotrain-chemprot-re-838426740
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pier297/autotrain-chemprot-re-838426740"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pier297/autotrain-chemprot-re-838426740"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,14636774193.202154,0.7608622519754854,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
41254,autotrain-inference_probability_2-840226804,['jeremyccollinsmpi/autotrain-data-inference_probability_2'],,0.0292088692643832,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0961729735136032,,0.912874,0.912874,2950904711.0,True,5,0,"['transformers', 'pytorch']",2022-05-17 07:41:46+00:00,2022-05-09 06:54:39+00:00,"
# Description

The input structure is:

 summarize: [text]. hypothesis: [hypothesis] , and the output is 0 (hypothesis is not supported) or 1 (hypothesis is supported).

This tests whether  a hypothesis is true given the preceding text.  Currently the model is trained on banking chatbot intent data, such as:

summarize: How old do my kids need to be to use your service?. hypothesis: asking about an age limit

Output: 1


# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 840226804
- CO2 Emissions (in grams): 0.02920886926438328

## Validation Metrics

- Loss: 0.09617297351360321
- Rouge1: 91.2874
- Rouge2: 0.0
- RougeL: 91.2874
- RougeLsum: 91.4174
- Gen Len: 2.4915

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/jeremyccollinsmpi/autotrain-inference_probability_2-840226804
```",,,1,[],[],NLP,2022-05,101027694166.79456,0.912874,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
41261,test-hub-pr-1,['lewtun/autotrain-data-my-eval-project-615'],,172.04481351504182,AutoTrain,Not Specified,Not Specified,Not Specified,0.927,0.2228243350982666,0.9287020109689214,,,1334486957.0,True,17,0,"['transformers', 'pytorch']",2022-05-23 13:30:02+00:00,2022-05-09 07:19:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 5694363
- CO2 Emissions (in grams): 172.04481351504182

## Validation Metrics

- Loss: 0.2228243350982666
- Accuracy: 0.9298
- Precision: 0.9434585224927775
- Recall: 0.9144
- AUC: 0.9566112000000001
- F1: 0.9287020109689214

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lewtun/autotrain-my-eval-project-615-5694363
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lewtun/autotrain-my-eval-project-615-5694363"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lewtun/autotrain-my-eval-project-615-5694363"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,7756624.159340475,0.9278502249600764,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
41857,autotrain-ok-848227025,['ziedhajyahia/autotrain-data-ok'],,5.096755166899446,AutoTrain,Not Specified,Not Specified,Not Specified,0.4466666666666666,2.1917402744293213,0.2029167780472512,,,1346953645.0,True,5,0,"['transformers', 'pytorch']",2022-05-10 15:21:50+00:00,2022-05-10 15:19:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 848227025
- CO2 Emissions (in grams): 5.096755166899446

## Validation Metrics

- Loss: 2.1917402744293213
- Accuracy: 0.44666666666666666
- Macro F1: 0.20291677804725128
- Micro F1: 0.44666666666666666
- Weighted F1: 0.37709801275435956
- Macro Precision: 0.19919016697588127
- Micro Precision: 0.44666666666666666
- Weighted Precision: 0.3478004329004329
- Macro Recall: 0.23167713239141807
- Micro Recall: 0.44666666666666666
- Weighted Recall: 0.44666666666666666


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ziedhajyahia/autotrain-ok-848227025
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ziedhajyahia/autotrain-ok-848227025"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ziedhajyahia/autotrain-ok-848227025"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,264276701.723423,0.2790593313258545,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
42242,autotrain-Traimn-853827191,['Yarn/autotrain-data-Traimn'],,1.712176860015081,AutoTrain,Not Specified,Not Specified,Not Specified,0.973421926910299,0.102577306330204,0.9735224586288418,,,438028461.0,True,15,1,"['transformers', 'pytorch']",2022-05-11 18:47:41+00:00,2022-05-11 18:46:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 853827191
- CO2 Emissions (in grams): 1.712176860015081

## Validation Metrics

- Loss: 0.10257730633020401
- Accuracy: 0.973421926910299
- Macro F1: 0.9735224586288418
- Micro F1: 0.973421926910299
- Weighted F1: 0.9735187934099364
- Macro Precision: 0.9738505933839127
- Micro Precision: 0.973421926910299
- Weighted Precision: 0.9738995774527256
- Macro Recall: 0.9734994306470444
- Micro Recall: 0.973421926910299
- Weighted Recall: 0.973421926910299


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Yarn/autotrain-Traimn-853827191
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Yarn/autotrain-Traimn-853827191"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Yarn/autotrain-Traimn-853827191"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,255831316.97980183,0.9734721901740608,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
42655,autotrain-imdb-sentiment-analysis-864927559,['anwesham/autotrain-data-imdb-sentiment-analysis'],,0.2033402242358345,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,267860081.0,False,5,0,"['transformers', 'pytorch']",2022-05-14 03:56:56+00:00,2022-05-14 03:06:26+00:00,"
- Problem type: Binary Classification
- Model ID: 864927559
- CO2 Emissions (in grams): 0.2033402242358345

## Validation Metrics

- Loss: 0.18383920192718506
- Accuracy: 0.9318
- Precision: 0.9560625264047318
- Recall: 0.9052
- AUC: 0.98281574
- F1: 0.9299363057324841

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/anwesham/autotrain-imdb-sentiment-analysis-864927559
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""anwesham/autotrain-imdb-sentiment-analysis-864927559"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""anwesham/autotrain-imdb-sentiment-analysis-864927559"", use_auth_token=True)

inputs = tokenizer(""I love to eat food"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,1317300017.773833,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
42998,autotrain-hi_ner_xlmr-869827677,['pujaburman30/autotrain-data-hi_ner_xlmr'],,4.365496441173981,AutoTrain,Not Specified,Not Specified,Not Specified,0.7411180773249739,0.894961416721344,0.546242774566474,,,1109949233.0,True,3,0,"['transformers', 'pytorch']",2022-05-15 09:00:47+00:00,2022-05-15 08:56:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 869827677
- CO2 Emissions (in grams): 4.365496441173981

## Validation Metrics

- Loss: 0.894961416721344
- Accuracy: 0.7411180773249739
- Precision: 0.590625
- Recall: 0.5080645161290323
- F1: 0.546242774566474

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pujaburman30/autotrain-hi_ner_xlmr-869827677
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""pujaburman30/autotrain-hi_ner_xlmr-869827677"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pujaburman30/autotrain-hi_ner_xlmr-869827677"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,254254985.1905296,0.6289307217080115,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,1,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
43127,turkish-sentiment-analysis,['emre/autotrain-data-turkish-sentiment-analysis'],,120.82460124309924,AutoTrain,Not Specified,Not Specified,Not Specified,0.9697853317600073,0.1098366305232048,0.9482820974460786,,,737474733.0,True,380,2,"['transformers', 'safetensors', 'pytorch']",2023-03-18 20:31:56+00:00,2022-05-15 20:05:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 870727732
- CO2 Emissions (in grams): 120.82460124309924

## Validation Metrics

- Loss: 0.1098366305232048
- Accuracy: 0.9697853317600073
- Macro F1: 0.9482820974460786
- Micro F1: 0.9697853317600073
- Weighted F1: 0.9695237873890088
- Macro Precision: 0.9540948884759232
- Micro Precision: 0.9697853317600073
- Weighted Precision: 0.9694186941924757
- Macro Recall: 0.9428467518468838
- Micro Recall: 0.9697853317600073
- Weighted Recall: 0.9697853317600073


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""Bu ürün gerçekten güzel çıktı""}' https://api-inference.huggingface.co/models/emre/turkish-sentiment-analysis
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""emre/turkish-sentiment-analysis"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""emre/turkish-sentiment-analysis"", use_auth_token=True)

inputs = tokenizer(""Bu ürün gerçekten güzel çıktı"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,6103680.2556145005,0.958913179454247,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
43271,autotrain-Napkin-872827783,['Yarn007/autotrain-data-Napkin'],,0.0201622114189035,AutoTrain,Not Specified,Not Specified,Not Specified,0.9325714285714286,0.25198695063591,0.9254931094274172,,,438031533.0,True,16,0,"['transformers', 'pytorch']",2022-05-16 13:01:19+00:00,2022-05-16 12:59:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 872827783
- CO2 Emissions (in grams): 0.020162211418903533

## Validation Metrics

- Loss: 0.25198695063591003
- Accuracy: 0.9325714285714286
- Macro F1: 0.9254931094274171
- Micro F1: 0.9325714285714286
- Weighted F1: 0.9323540959391766
- Macro Precision: 0.9286720054236212
- Micro Precision: 0.9325714285714286
- Weighted Precision: 0.9324375609546055
- Macro Recall: 0.9227549386201338
- Micro Recall: 0.9325714285714286
- Weighted Recall: 0.9325714285714286


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Yarn007/autotrain-Napkin-872827783
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Yarn007/autotrain-Napkin-872827783"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Yarn007/autotrain-Napkin-872827783"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,21725371483.274586,0.9290187865285828,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
43333,autotrain-smm4h_large_roberta_clean-874027878,['Amalq/autotrain-data-smm4h_large_roberta_clean'],,9.123490454955585,AutoTrain,Not Specified,Not Specified,Not Specified,0.8571428571428571,0.3572422564029693,0.8224852071005917,,,1421611309.0,True,7,0,"['transformers', 'pytorch']",2022-05-16 18:44:14+00:00,2022-05-16 18:39:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 874027878
- CO2 Emissions (in grams): 9.123490454955585

## Validation Metrics

- Loss: 0.35724225640296936
- Accuracy: 0.8571428571428571
- Precision: 0.7637362637362637
- Recall: 0.8910256410256411
- AUC: 0.9267555361305361
- F1: 0.8224852071005917

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Amalq/autotrain-smm4h_large_roberta_clean-874027878
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Amalq/autotrain-smm4h_large_roberta_clean-874027878"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Amalq/autotrain-smm4h_large_roberta_clean-874027878"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,155818797.20472845,0.8394564670357323,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
43721,autotrain-test-project-879428192,['nthanhha26/autotrain-data-test-project'],,13.170344687762716,AutoTrain,Not Specified,Not Specified,Not Specified,0.9796652588768966,0.0646522864699363,0.9891176963000168,,,498674093.0,True,5,0,"['transformers', 'pytorch']",2022-05-18 08:28:19+00:00,2022-05-18 08:21:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 879428192
- CO2 Emissions (in grams): 13.170344687762716

## Validation Metrics

- Loss: 0.06465228646993637
- Accuracy: 0.9796652588768966
- Precision: 0.9843385538153949
- Recall: 0.993943472409152
- AUC: 0.9855992605071237
- F1: 0.9891176963000168

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/nthanhha26/autotrain-test-project-879428192
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""nthanhha26/autotrain-test-project-879428192"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""nthanhha26/autotrain-test-project-879428192"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,37863404.85555744,0.9843687862671502,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
43903,autotrain-KeywordExtraction-882328335,['priyamm/autotrain-data-KeywordExtraction'],,0.2137346810800018,AutoTrain,Not Specified,Not Specified,Not Specified,0.9128,0.2641160488128662,0.9095810866860224,,,711504045.0,True,30,0,"['transformers', 'pytorch']",2022-05-18 20:40:08+00:00,2022-05-18 20:17:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 882328335
- CO2 Emissions (in grams): 0.21373468108000182

## Validation Metrics

- Loss: 0.2641160488128662
- Accuracy: 0.9128
- Precision: 0.9444444444444444
- Recall: 0.8772
- AUC: 0.9709556000000001
- F1: 0.9095810866860223

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/priyamm/autotrain-KeywordExtraction-882328335
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""priyamm/autotrain-KeywordExtraction-882328335"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""priyamm/autotrain-KeywordExtraction-882328335"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,3328912469.44468,0.911187700522978,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
44012,bloom,[''],,24700000.0,Article,pre-training,"Orsay, France",384 A100 80GB GPUs,,,,,,,False,49233,3044,"['tensorboard', 'transformers', 'safetensors', 'pytorch']",2023-03-14 04:59:09+00:00,2022-07-11 14:40:02+00:00,"
<img src=""https://s3.amazonaws.com/moonup/production/uploads/1657124309515-5f17f0a0925b9863e28ad517.png"" alt=""BigScience Logo"" width=""800"" style=""margin-left:'auto' margin-right:'auto' display:'block'""/>

BigScience Large Open-science Open-access Multilingual Language Model  
Version 1.3 / 6 July 2022

Current Checkpoint: **Training Iteration  95000**

Link to paper: [here](https://arxiv.org/abs/2211.05100)

Total seen tokens: **366B**

---

# Model Details  

BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.

## Basics
*This section provides information about the model type, version, license, funders, release date, developers, and contact information.*
*It is useful for anyone who wants to reference the model.*

<details>
<summary>Click to expand</summary>
  
**Developed by:** BigScience ([website](https://bigscience.huggingface.co))

*All collaborators are either volunteers or have an agreement with their employer. (Further breakdown of participants forthcoming.)*
    
**Model Type:** Transformer-based Language Model

**Checkpoints format:** `transformers` (Megatron-DeepSpeed format available [here](https://huggingface.co/bigscience/bloom-optimizer-states))

**Version:** 1.0.0

**Languages:** Multiple; see [training data](#training-data)

**License:** RAIL License v1.0 ([link](https://huggingface.co/spaces/bigscience/license) / [article and FAQ](https://bigscience.huggingface.co/blog/the-bigscience-rail-license))

**Release Date Estimate:** Monday, 11.July.2022

**Send Questions to:** bigscience-contact@googlegroups.com

**Cite as:** BigScience, _BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model_. International, May 2021-May 2022

**Funded by:** 
    
* The French government.

* Hugging Face ([website](https://huggingface.co)).

* Organizations of contributors.  *(Further breakdown of organizations forthcoming.)*

</details>


## Technical Specifications
*This section includes details about the model objective and architecture, and the compute infrastructure.*
*It is useful for people interested in model development.*

<details>
<summary>Click to expand</summary>

Please see [the BLOOM training README](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) for full details on replicating training.

### Model Architecture and Objective

* Modified from Megatron-LM GPT2 (see [paper](https://arxiv.org/abs/1909.08053), [BLOOM Megatron code](https://github.com/bigscience-workshop/Megatron-DeepSpeed)):

* Decoder-only architecture

* Layer normalization applied to word embeddings layer (`StableEmbedding`; see [code](https://github.com/facebookresearch/bitsandbytes), [paper](https://arxiv.org/pdf/2110.02861.pdf))

* ALiBI positional encodings (see [paper](https://arxiv.org/pdf/2108.12409.pdf)), with GeLU activation functions

* 176,247,271,424 parameters:

    * 3,596,615,680 embedding parameters

    * 70 layers, 112 attention heads

    * Hidden layers are 14336-dimensional

    * Sequence length of 2048 tokens used (see [BLOOM tokenizer](https://huggingface.co/bigscience/tokenizer), [tokenizer description](#tokenization))

**Objective Function:** Cross Entropy with mean reduction (see [API documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)).
    
### Compute infrastructure
Jean Zay Public Supercomputer, provided by the French government (see [announcement](https://www.enseignementsup-recherche.gouv.fr/fr/signature-du-marche-d-acquisition-de-l-un-des-supercalculateurs-les-plus-puissants-d-europe-46733)).

#### Hardware

* 384 A100 80GB GPUs (48 nodes)
    
* Additional 32 A100 80GB GPUs (4 nodes) in reserve

* 8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath links

* CPU: AMD

* CPU memory: 512GB per node

* GPU memory: 640GB per node

* Inter-node connect: Omni-Path Architecture (OPA)

* NCCL-communications network: a fully dedicated subnet

* Disc IO network: shared network with other types of nodes

#### Software

* Megatron-DeepSpeed ([Github link](https://github.com/bigscience-workshop/Megatron-DeepSpeed))

* DeepSpeed ([Github link](https://github.com/microsoft/DeepSpeed))

* PyTorch (pytorch-1.11 w/ CUDA-11.5; see [Github link](https://github.com/pytorch/pytorch))

* apex ([Github link](https://github.com/NVIDIA/apex))
    
</details>

---

# Training
*This section provides information about the training data, the speed and size of training elements, and the environmental impact of training.*
*It is useful for people who want to learn more about the model inputs and training footprint.*

<details>
<summary>Click to expand</summary>

## Training Data
*This section provides a high-level overview of the training data. It is relevant for anyone who wants to know the basics of what the model is learning.*

Details for each dataset are provided in individual [Data Cards](https://huggingface.co/spaces/bigscience/BigScienceCorpus), and the sizes of each of their contributions to the aggregated training data are presented in an [Interactive Corpus Map](https://huggingface.co/spaces/bigscience-catalogue-lm-data/corpus-map).

Training data includes:

-   46 natural languages
    
-   13 programming languages

-   In 1.6TB of pre-processed text, converted into 350B unique tokens (see [the tokenizer section](#tokenization) for more.)

### Languages
    
The pie chart shows the distribution of languages in training data.
   
![pie chart showing the distribution of languages in training data](https://github.com/bigscience-workshop/model_card/blob/main/assets/data/pie_v2.svg?raw=true)


The following tables shows the further distribution of Niger-Congo & Indic languages and programming languages in the training data.

Distribution of Niger Congo and Indic languages.
    
| Niger Congo    | Percentage |         | Indic     | Percentage |
|----------------|------------| ------  |-----------|------------|
| Chi Tumbuka    | 0.00002    |         | Assamese  | 0.01       |
| Kikuyu         | 0.00004    |         | Odia      | 0.04       |
| Bambara        | 0.00004    |         | Gujarati  | 0.04       |
| Akan           | 0.00007    |         | Marathi   | 0.05       |
| Xitsonga       | 0.00007    |         | Punjabi   | 0.05       |
| Sesotho        | 0.00007    |         | Kannada   | 0.06       |
| Chi Chewa      | 0.0001     |         | Nepali    | 0.07       |
| Setswana       | 0.0002     |         | Telugu    | 0.09       |
| Lingala        | 0.0002     |         | Malayalam | 0.10       |
| Northern Sotho | 0.0002     |         | Urdu      | 0.10       |
| Fon            | 0.0002     |         | Tamil     | 0.20       |
| Kirundi        | 0.0003     |         | Bengali   | 0.50       |
| Wolof          | 0.0004     |         | Hindi     | 0.70       |
| Luganda        | 0.0004     |
| Chi Shona      | 0.001      |
| Isi Zulu       | 0.001      |
| Igbo           | 0.001      |
| Xhosa          | 0.001      |
| Kinyarwanda    | 0.003      |
| Yoruba         | 0.006      |
| Swahili        | 0.02       |

Distribution of programming languages.
    
| Extension      | Language   | Number of files |
|----------------|------------|-----------------|
| java           | Java       | 5,407,724       |
| php            | PHP        | 4,942,186       |
| cpp            | C++        | 2,503,930       |
| py             | Python     | 2,435,072       |
| js             | JavaScript | 1,905,518       |
| cs             | C#         | 1,577,347       |
| rb             | Ruby       | 6,78,413        |
| cc             | C++        | 443,054         |
| hpp            | C++        | 391,048         |
| lua            | Lua        | 352,317         |
| go             | GO         | 227,763         |
| ts             | TypeScript | 195,254         |
| C              | C          | 134,537         |
| scala          | Scala      | 92,052          |
| hh             | C++        | 67,161          |
| H              | C++        | 55,899          |
| tsx            | TypeScript | 33,107          |
| rs             | Rust       | 29,693          |
| phpt           | PHP        | 9,702           |
| c++            | C++        | 1,342           |
| h++            | C++        | 791             |
| php3           | PHP        | 540             |
| phps           | PHP        | 270             |
| php5           | PHP        | 166             |
| php4           | PHP        | 29              |
    
### Preprocessing

**Tokenization:** The BLOOM tokenizer ([link](https://huggingface.co/bigscience/tokenizer)), a learned subword tokenizer trained using:
    
- A byte-level Byte Pair Encoding (BPE) algorithm 

- A simple pre-tokenization rule, no normalization

- A vocabulary size of 250,680

It was trained on a subset of a preliminary version of the corpus using alpha-weighting per language.  

## Speeds, Sizes, Times

Training logs: [Tensorboard link](https://huggingface.co/tensorboard/bigscience/tr11-176B-ml-logs/)

- Dates:
    
    - Started 11th March, 2022 11:42am PST

    - Estimated end: 5th July, 2022

- Checkpoint size:
    
    - Bf16 weights: 329GB
    
    - Full checkpoint with optimizer states: 2.3TB

- Training throughput: About 150 TFLOP per GPU per second

- Number of epochs: 1

- Estimated cost of training: Equivalent of $2-5M in cloud computing (including preliminary experiments)

- Server training location: Île-de-France, France


## Environmental Impact

The training supercomputer, Jean Zay ([website](http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html)), uses mostly nuclear energy. The heat generated by it is reused for heating campus housing.
    
**Estimated carbon emissions:**  *(Forthcoming.)*
    
**Estimated electricity usage:** *(Forthcoming.)*

</details>

---

# Uses

*This section addresses questions around how the model is intended to be used, discusses the foreseeable users of the model (including those affected by the model), and describes uses that are considered out of scope or misuse of the model.*
*It is useful for anyone considering using the model or who is affected by the model.*

<details>
<summary>Click to expand</summary>
    
## How to use

This model can be easily used and deployed using HuggingFace's ecosystem. This needs `transformers` and `accelerate` installed. The model can be downloaded as follows:

 <img src=""https://s3.amazonaws.com/moonup/production/uploads/1657271608456-62441d1d9fdefb55a0b7d12c.png"" width=""800"" style=""margin-left:'auto' margin-right:'auto' display:'block'""/>

## Intended Use

This model is being created in order to enable public research on large language models (LLMs). LLMs are intended to be used for language generation or as a pretrained base model that can be further fine-tuned for specific tasks. Use cases below are not exhaustive.

### Direct Use

-   Text generation

-   Exploring characteristics of language generated by a language model

    -   Examples: Cloze tests, counterfactuals, generations with reframings

### Downstream Use

-   Tasks that leverage language models include: Information Extraction, Question Answering, Summarization

### Misuse and Out-of-scope Use
*This section addresses what users ought not do with the model.*

See the [BLOOM License](https://huggingface.co/spaces/bigscience/license), Attachment A, for detailed usage restrictions. The below list is non-exhaustive, but lists some easily foreseeable problematic use cases.

#### Out-of-scope Uses

Using the model in [high-stakes](#high-stakes) settings is out of scope for this model.  The model is not designed for [critical decisions](#critical-decisions) nor uses with any material consequences on an individual's livelihood or wellbeing. The model outputs content that appears factual but may not be correct.  

Out-of-scope Uses Include:

-   Usage in biomedical domains, political and legal domains, or finance domains

-   Usage for evaluating or scoring individuals, such as for employment, education, or credit

-   Applying the model for critical automatic decisions, generating factual content, creating reliable summaries, or generating predictions that must be correct

#### Misuse

Intentionally using the model for harm, violating [human rights](#human-rights), or other kinds of malicious activities, is a misuse of this model. This includes:

-   Spam generation

-   Disinformation and influence operations

-   Disparagement and defamation

-   Harassment and abuse
  
-   [Deception](#deception)

-   Unconsented impersonation and imitation

-   Unconsented surveillance 

-   Generating content without attribution to the model, as specified in the [RAIL License, Use Restrictions](https://huggingface.co/spaces/bigscience/license)

## Intended Users

### Direct Users

-   General Public

-   Researchers

-   Students

-   Educators

-   Engineers/developers

-   Non-commercial entities

-   Community advocates, including human and civil rights groups

### Indirect Users

-   Users of derivatives created by Direct Users, such as those using software with an [intended use](#intended-use)

-   Users of [Derivatives of the Model, as described in the License](https://huggingface.co/spaces/bigscience/license)

### Others Affected (Parties Prenantes)

-   People and groups referred to by the LLM

-   People and groups exposed to outputs of, or decisions based on, the LLM

-   People and groups whose original work is included in the LLM

</details>

---

# Risks and Limitations
*This section identifies foreseeable harms and misunderstandings.*
    
<details>
<summary>Click to expand</summary>

Model may:

-   Overrepresent some viewpoints and underrepresent others

-   Contain stereotypes
  
-   Contain [personal information](#personal-data-and-information)

-   Generate:

    -   Hateful, abusive, or violent language

    -   Discriminatory or prejudicial language

    -   Content that may not be appropriate for all settings, including sexual content

-   Make errors, including producing incorrect information as if it were factual

-   Generate irrelevant or repetitive outputs

-   Induce users into attributing human traits to it, such as sentience or consciousness

</details>

---

# Evaluation
*This section describes the evaluation protocols and provides the results.*


<details>
<summary>Click to expand</summary>

## Metrics 
*This section describes the different ways performance is calculated and why.*

Includes:

| Metric             | Why chosen                                                         |
|--------------------|--------------------------------------------------------------------|
| [Perplexity](#perplexity)         | Standard metric for quantifying model improvements during training |
| Cross Entropy [Loss](#loss) | Standard objective for language models.                            |

And multiple different metrics for specific tasks. _(More evaluation metrics forthcoming upon completion of evaluation protocol.)_

## Factors 
*This section lists some different aspects of BLOOM models. Its focus is on aspects that are likely to give rise to high variance in model behavior.*

- Language, such as English or Yoruba

- Domain, such as newswire or stories

- Demographic characteristics, such as gender or nationality

##  Results
*Results are based on the [Factors](#factors) and [Metrics](#metrics).*

**Zero-shot evaluations:**

<span style=""color:red""><b>WARNING:</b> This section used to contain much more results, however they were not correct and we released without the approval of the evaluation working group. We are currently in the process of fixing the evaluations.</span>

See this repository for JSON files: https://github.com/bigscience-workshop/evaluation-results

| Task | Language | Metric | BLOOM-176B | OPT-175B* |
|:--------|:-----------------|:------------------------|-------------:|------------:|
| humaneval | python | pass@1 ↑ | 0.155 | 0.0 |
| humaneval | python | pass@10 ↑ | 0.328 | 0.0 |
| humaneval | python | pass@100 ↑ | 0.572 | 0.003 |


**Train-time Evaluation:**

Final checkpoint after 95K steps:

- Training Loss: 1.939

- Validation Loss: 2.061

- Perplexity: 7.045

For more see: https://huggingface.co/bigscience/tr11-176B-ml-logs

</details>

---

# Recommendations

*This section provides information on warnings and potential mitigations.*

<details>
<summary>Click to expand</summary>

-   Indirect users should be made aware when the content they're working with is created by the LLM.

-   Users should be aware of [Risks and Limitations](#risks-and-limitations), and include an appropriate age disclaimer or blocking interface as necessary.

-   Models trained or finetuned downstream of BLOOM LM should include an updated Model Card.

-   Users of the model should provide mechanisms for those affected to provide feedback, such as an email address for comments.

</details>

---

# Glossary and Calculations

*This section defines common terms and how metrics are calculated.*
<details>
<summary>Click to expand</summary>

-   <a name=""loss"">**Loss:**</a> A calculation of the difference between what the model has learned and what the data shows (""groundtruth""). The lower the loss, the better. The training process aims to minimize the loss. 

-   <a name=""perplexity"">**Perplexity:**</a> This is based on what the model estimates the probability of new data is. The lower the perplexity, the better.  If the model is 100% correct at predicting the next token it will see, then the perplexity is 1. Mathematically this is calculated using entropy. 

-   <a name=""high-stakes"">**High-stakes settings:**</a> Such as those identified as ""high-risk AI systems"" and ""unacceptable risk AI systems"" in the European Union's proposed [Artificial Intelligence (AI) Act](https://artificialintelligenceact.eu/annexes/).

-   <a name=""critical-decisions"">**Critical decisions:**</a> Such as those defined in [the United States' proposed Algorithmic Accountability Act](https://www.congress.gov/117/bills/s3572/BILLS-117s3572is.pdf).

-   <a name=""human-rights"">**Human rights:**</a> Includes those rights defined in the [Universal Declaration of Human Rights](https://www.un.org/sites/un2.un.org/files/2021/03/udhr.pdf).

-  <a name=""personal-data-and-information"">**Personal Data and Personal Information:**</a> Personal data and information is defined in multiple data protection regulations, such as ""[personal data](https://gdpr-info.eu/issues/personal-data/)"" in the [European Union's General Data Protection Regulation](https://gdpr-info.eu); and ""personal information"" in the Republic of South Africa's [Protection of Personal Information Act](https://www.gov.za/sites/default/files/gcis_document/201409/3706726-11act4of2013popi.pdf), The People's Republic of China's [Personal information protection law](http://en.npc.gov.cn.cdurl.cn/2021-12/29/c_694559.htm).
  
- <a name=""sensitive-characteristics"">**Sensitive characteristics:**</a> This includes specifically protected categories in human rights (see [UHDR, Article 2](https://www.un.org/sites/un2.un.org/files/2021/03/udhr.pdf)) and personal information regulation (see GDPR, [Article 9; Protection of Personal Information Act, Chapter 1](https://www.gov.za/sites/default/files/gcis_document/201409/3706726-11act4of2013popi.pdf))

- <a name=""deception"">**Deception:**</a> Doing something to intentionally mislead individuals to believe something that is false, such as by creating deadbots or chatbots on social media posing as real people, or generating text documents without making consumers aware that the text is machine generated.

</details>

---

# More Information
*This section provides links to writing on dataset creation, technical specifications, lessons learned, and initial results.*

<details>
<summary>Click to expand</summary>

## Intermediate checkpoints

For academic (or any) usage, we published the intermediate checkpoints, corresponding to the model state at each 5000 steps. Please follow [this link](https://huggingface.co/bigscience/bloom-176-intermediate) to get these checkpoints.

    
## Dataset Creation

Blog post detailing the design choices during the dataset creation: https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling

## Technical Specifications

Blog post summarizing how the architecture, size, shape, and pre-training duration where selected: https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours

More details on the architecture/optimizer: https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml

Blog post on the hardware/engineering side: https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model

Details on the distributed setup used for the training: https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml

Tensorboard updated during the training: https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=loss

## Lessons

Insights on how to approach training, negative results: https://github.com/bigscience-workshop/bigscience/blob/master/train/lessons-learned.md

Details on the obstacles overcome during the preparation on the engineering side (instabilities, optimization of training throughput, so many technical tricks and questions): https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md

## Initial Results

Initial prompting experiments using interim checkpoints: https://huggingface.co/spaces/bigscience/bloom-book

</details>


## Original checkpoints

The checkpoints in this repo correspond to the HuggingFace Transformers format. If you want to use our fork of [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed) that the model was trained with, you'd want to use [this repo instead](https://huggingface.co/bigscience/bloom-optimizer-states).

Many intermediate checkpoints are available at https://huggingface.co/bigscience/bloom-intermediate/

---
    
# Model Card Authors
*Ordered roughly chronologically and by amount of time spent on creating this model card.*

Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",,,1,[],[],NLP,2022-07,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,1,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
44076,autotrain-gudel-department-classifier-clean-886428460,['Vmuaddib/autotrain-data-gudel-department-classifier-clean'],,14.294320632050567,AutoTrain,Not Specified,Not Specified,Not Specified,0.9894490035169988,0.0514134876430034,0.9930609097918272,,,1343115501.0,True,192,0,"['transformers', 'pytorch']",2022-09-23 13:07:21+00:00,2022-05-19 19:51:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 886428460
- CO2 Emissions (in grams): 14.294320632050567

## Validation Metrics

- Loss: 0.051413487643003464
- Accuracy: 0.9894490035169988
- Precision: 1.0
- Recall: 0.9862174578866769
- AUC: 0.9989318529862175
- F1: 0.9930609097918273

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Vmuaddib/autotrain-gudel-department-classifier-clean-886428460
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Vmuaddib/autotrain-gudel-department-classifier-clean-886428460"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Vmuaddib/autotrain-gudel-department-classifier-clean-886428460"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,93961478.51815224,0.9912516664143868,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,1.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
44768,biobert-procell-demo,['Mim/autotrain-data-biobert-procell'],,0.5988414315305852,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,433331373.0,False,14,1,"['transformers', 'pytorch']",2022-05-22 13:46:29+00:00,2022-05-22 12:39:15+00:00,"
# Model Trained Using biobert

- Problem type: Binary Classification
- Model ID: 896229149
- CO2 Emissions (in grams): 0.5988414315305852

## Validation Metrics

- Loss: 0.4045306444168091
- Accuracy: 0.8028169014084507
- Precision: 0.8070175438596491
- Recall: 0.9387755102040817
- AUC: 0.8812615955473099
- F1: 0.8679245283018868

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""Cell lines expressing proteins""}' https://api-inference.huggingface.co/models/Mim/autotrain-biobert-procell-896229149
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Mim/autotrain-biobert-procell-896229149"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Mim/autotrain-biobert-procell-896229149"", use_auth_token=True)

inputs = tokenizer(""Cell lines expressing proteins"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,723616219.8938769,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45093,autotrain-inference_probability_3-900329401,['jeremyccollinsmpi/autotrain-data-inference_probability_3'],,3.807314953201688,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0625591874122619,,0.940693,0.940693,891730879.0,True,12,0,"['transformers', 'pytorch']",2022-05-23 16:04:36+00:00,2022-05-23 16:01:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 900329401
- CO2 Emissions (in grams): 3.807314953201688

## Validation Metrics

- Loss: 0.06255918741226196
- Rouge1: 94.0693
- Rouge2: 0.0
- RougeL: 94.0693
- RougeLsum: 94.1126
- Gen Len: 2.8528

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/jeremyccollinsmpi/autotrain-inference_probability_3-900329401
```",,,1,[],[],NLP,2022-05,234215159.49189237,0.940693,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45305,autotrain-name_vsv_all-901529445,['ismail-lucifer011/autotrain-data-name_vsv_all'],,110.53225910657004,AutoTrain,Not Specified,Not Specified,Not Specified,0.9897211856745716,0.0292855352163314,0.9521499063085572,,,265497077.0,True,5,0,"['transformers', 'pytorch']",2022-05-23 21:43:18+00:00,2022-05-23 20:33:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 901529445
- CO2 Emissions (in grams): 110.53225910657005

## Validation Metrics

- Loss: 0.029285535216331482
- Accuracy: 0.9897211856745716
- Precision: 0.9500373934287282
- Recall: 0.9542718349358974
- F1: 0.9521499063085572

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-name_vsv_all-901529445
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ismail-lucifer011/autotrain-name_vsv_all-901529445"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ismail-lucifer011/autotrain-name_vsv_all-901529445"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,2401987.249206769,0.9705720818463316,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45377,autotrain-company_vs_all-902129475,['ismail-lucifer011/autotrain-data-company_vs_all'],,111.96508441754436,AutoTrain,Not Specified,Not Specified,Not Specified,0.9970010185146536,0.0086103668436408,0.9820678194318264,,,265497077.0,True,8,0,"['transformers', 'pytorch']",2022-05-23 23:41:25+00:00,2022-05-23 22:32:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 902129475
- CO2 Emissions (in grams): 111.96508441754436

## Validation Metrics

- Loss: 0.008610366843640804
- Accuracy: 0.9970010185146537
- Precision: 0.9815740365517482
- Recall: 0.9825620993589743
- F1: 0.9820678194318264

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-company_vs_all-902129475
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ismail-lucifer011/autotrain-company_vs_all-902129475"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ismail-lucifer011/autotrain-company_vs_all-902129475"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,2371248.844058371,0.989478079236448,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45533,sdg_classifier_osdg,['jonas/osdg_sdg_data_processed'],,0.0653263174784986,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,438059181.0,False,37,7,"['transformers', 'pytorch']",2022-09-20 06:46:22+00:00,2022-05-24 11:49:08+00:00,"# About

Machine Learning model for classifying text according to the first 15 of the 17 Sustainable Development Goals from the United Nations. Note that model is trained on quite short paragraphs (around 100 words) and performs best with similar input sizes. 

Data comes from the amazing https://osdg.ai/ community!

* There is an improved version (finetuned Roberta) of the model available here: https://huggingface.co/jonas/roberta-base-finetuned-sdg

# Model Training Specifics 

- Problem type: Multi-class Classification
- Model ID: 900229515
- CO2 Emissions (in grams): 0.0653263174784986

## Validation Metrics

- Loss: 0.3644874095916748
- Accuracy: 0.8972544579677328
- Macro F1: 0.8500873710954522
- Micro F1: 0.8972544579677328
- Weighted F1: 0.8937529692986061
- Macro Precision: 0.8694369727467804
- Micro Precision: 0.8972544579677328
- Weighted Precision: 0.8946984684977016
- Macro Recall: 0.8405065997404059
- Micro Recall: 0.8972544579677328
- Weighted Recall: 0.8972544579677328


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jonas/autotrain-osdg-sdg-classifier-900229515
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jonas/sdg_classifier_osdg"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jonas/sdg_classifier_osdg"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,6705707560.267454,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45546,autotrain-company_all-903429548,['ismail-lucifer011/autotrain-data-company_all'],,0.848790823793881,AutoTrain,Not Specified,Not Specified,Not Specified,0.9979930566588804,0.0061480407603085,0.9816077764228254,,,265497077.0,True,101725,0,"['transformers', 'pytorch']",2022-05-24 14:24:20+00:00,2022-05-24 12:43:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 903429548
- CO2 Emissions (in grams): 0.848790823793881

## Validation Metrics

- Loss: 0.006148040760308504
- Accuracy: 0.9979930566588805
- Precision: 0.9814944904963571
- Recall: 0.9817210885036588
- F1: 0.9816077764228254

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-company_all-903429548
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ismail-lucifer011/autotrain-company_all-903429548"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ismail-lucifer011/autotrain-company_all-903429548"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,312794471.3319296,0.989732605544836,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45547,autotrain-company_all-903429540,['ismail-lucifer011/autotrain-data-company_all'],,119.04546626922829,AutoTrain,Not Specified,Not Specified,Not Specified,0.9981441241415306,0.006177581846714,0.983292789968652,,,265497077.0,True,8,0,"['transformers', 'pytorch']",2022-05-24 13:52:50+00:00,2022-05-24 12:44:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 903429540
- CO2 Emissions (in grams): 119.04546626922827

## Validation Metrics

- Loss: 0.00617758184671402
- Accuracy: 0.9981441241415306
- Precision: 0.9826569893335472
- Recall: 0.9839294138903667
- F1: 0.9832927899686521

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-company_all-903429540
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ismail-lucifer011/autotrain-company_all-903429540"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ismail-lucifer011/autotrain-company_all-903429540"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,2230215.7765467763,0.9906627999395042,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45560,autotrain-job_all-903929564,['ismail-lucifer011/autotrain-data-job_all'],,192.68222884611995,AutoTrain,Not Specified,Not Specified,Not Specified,0.9989412009896036,0.0036299973726272,0.9874236219367322,,,260809205.0,True,102562,0,"['transformers', 'pytorch']",2022-05-24 14:52:32+00:00,2022-05-24 13:10:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 903929564
- CO2 Emissions (in grams): 192.68222884611995

## Validation Metrics

- Loss: 0.0036299973726272583
- Accuracy: 0.9989412009896035
- Precision: 0.9863310000901253
- Recall: 0.9885186672019269
- F1: 0.9874236219367322

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-job_all-903929564
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ismail-lucifer011/autotrain-job_all-903929564"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ismail-lucifer011/autotrain-job_all-903929564"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,1353571.663364387,0.99314902015818,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45563,autotrain-name_all-904029569,['ismail-lucifer011/autotrain-data-name_all'],,0.527083766435658,AutoTrain,Not Specified,Not Specified,Not Specified,0.9989951257999512,0.0036354903131723,0.9911648034619546,,,265497077.0,True,8,0,"['transformers', 'pytorch']",2022-05-24 14:42:21+00:00,2022-05-24 13:26:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 904029569
- CO2 Emissions (in grams): 0.527083766435658

## Validation Metrics

- Loss: 0.0036354903131723404
- Accuracy: 0.9989951257999512
- Precision: 0.9888963290924173
- Recall: 0.9934437092741895
- F1: 0.9911648034619546

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-name_all-904029569
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ismail-lucifer011/autotrain-name_all-904029569"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ismail-lucifer011/autotrain-name_all-904029569"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,503709455.5110144,0.9950645603543884,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45570,autotrain-name_all-904029577,['ismail-lucifer011/autotrain-data-name_all'],,0.8375653425894861,AutoTrain,Not Specified,Not Specified,Not Specified,0.9989316041363876,0.0035200684797018,0.9905539954046464,,,265497077.0,True,102583,1,"['transformers', 'pytorch']",2022-05-24 15:43:22+00:00,2022-05-24 13:54:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 904029577
- CO2 Emissions (in grams): 0.8375653425894861

## Validation Metrics

- Loss: 0.0035200684797018766
- Accuracy: 0.9989316041363876
- Precision: 0.9877899024589919
- Recall: 0.9933336010601984
- F1: 0.9905539954046464

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ismail-lucifer011/autotrain-name_all-904029577
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ismail-lucifer011/autotrain-name_all-904029577"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ismail-lucifer011/autotrain-name_all-904029577"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,316986703.603527,0.9947251609577308,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45584,autotrain-test-frank-896929583,['vreese2414/autotrain-data-test-frank'],,20.85550802376653,AutoTrain,Not Specified,Not Specified,Not Specified,0.717983651226158,0.8998094797134399,0.6850466044284794,,,1340762029.0,True,15,0,"['transformers', 'pytorch']",2022-05-24 15:20:01+00:00,2022-05-24 15:09:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 896929583
- CO2 Emissions (in grams): 20.85550802376653

## Validation Metrics

- Loss: 0.8998094797134399
- Accuracy: 0.717983651226158
- Macro F1: 0.6850466044284794
- Micro F1: 0.717983651226158
- Weighted F1: 0.7093970537930665
- Macro Precision: 0.692166692035814
- Micro Precision: 0.717983651226158
- Weighted Precision: 0.7181745683190863
- Macro Recall: 0.6985625924834511
- Micro Recall: 0.717983651226158
- Weighted Recall: 0.717983651226158


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/vreese2414/autotrain-test-frank-896929583
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""vreese2414/autotrain-test-frank-896929583"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""vreese2414/autotrain-test-frank-896929583"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,64288150.04036794,0.7011285185409615,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
45984,Arabic_poem_meter_3,[''],,404.6698645190223,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,442624237.0,False,22,0,"['transformers', 'pytorch']",2022-05-28 07:59:10+00:00,2022-05-26 20:45:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- CO2 Emissions (in grams): 404.66986451902227
## Dataset
We used the APCD dataset cited hereafter for pretraining the model. The dataset has been cleaned and only the main text and the meter columns were kept:
```
@Article{Yousef2019LearningMetersArabicEnglish-arxiv,
  author =       {Yousef, Waleed A. and Ibrahime, Omar M. and Madbouly, Taha M. and Mahmoud,
                  Moustafa A.},
  title =        {Learning Meters of Arabic and English Poems With Recurrent Neural Networks: a Step
                  Forward for Language Understanding and Synthesis},
  journal =      {arXiv preprint arXiv:1905.05700},
  year =         2019,
  url =          {https://github.com/hci-lab/LearningMetersPoems}
}
```
## Validation Metrics

- Loss: 0.21315555274486542
- Accuracy: 0.9493554089595999
- Macro F1: 0.7537353091512587

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""قفا نبك من ذِكرى حبيب ومنزلِ  بسِقطِ اللِّوى بينَ الدَّخول فحَوْملِ""}' https://api-inference.huggingface.co/models/Yah216/Arabic_poem_meter_3
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Yah216/Arabic_poem_meter_3"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Yah216/Arabic_poem_meter_3"", use_auth_token=True)

inputs = tokenizer(""قفا نبك من ذِكرى حبيب ومنزلِ  بسِقطِ اللِّوى بينَ الدَّخول فحَوْملِ"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,1093790.9535865467,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46025,autotrain-News-916530070,['Rebreak/autotrain-data-News'],,62.61326668998836,AutoTrain,Not Specified,Not Specified,Not Specified,0.9773220921733938,0.0855042040348053,0.0290877038342882,,,328525933.0,True,6,0,"['transformers', 'pytorch']",2022-05-27 05:12:30+00:00,2022-05-27 04:39:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 916530070
- CO2 Emissions (in grams): 62.61326668998836

## Validation Metrics

- Loss: 0.0855042040348053
- Accuracy: 0.9773220921733938
- Precision: 0.673469387755102
- Recall: 0.014864864864864866
- AUC: 0.8605107881181646
- F1: 0.029087703834288235

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Rebreak/autotrain-News-916530070
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Rebreak/autotrain-News-916530070"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Rebreak/autotrain-News-916530070"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,5246906.13295425,0.0564939961447465,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46128,Poem_Qafiyah_Detection,['Yah216/Poem_Rawiy_detection'],,1.804676644162964,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,497957165.0,False,28,0,"['transformers', 'pytorch']",2022-05-28 07:56:56+00:00,2022-05-27 16:50:04+00:00,"
# Model

- Problem type: Multi-class Classification
- CO2 Emissions (in grams): 1.8046766441629636

## Dataset
We used the APCD dataset cited hereafter for pretraining the model. The dataset has been cleaned and only the main text and the Qafiyah column were kept:
```
@Article{Yousef2019LearningMetersArabicEnglish-arxiv,
  author =       {Yousef, Waleed A. and Ibrahime, Omar M. and Madbouly, Taha M. and Mahmoud,
                  Moustafa A.},
  title =        {Learning Meters of Arabic and English Poems With Recurrent Neural Networks: a Step
                  Forward for Language Understanding and Synthesis},
  journal =      {arXiv preprint arXiv:1905.05700},
  year =         2019,
  url =          {https://github.com/hci-lab/LearningMetersPoems}
}
```

## Validation Metrics

- Loss: 0.398613303899765
- Accuracy: 0.912351981006084
- Macro F1: 0.717311758991278
- Micro F1: 0.912351981006084
- Weighted F1: 0.9110094798809955


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Yah216/Poem_Rawiy_detection
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Yah216/Poem_Qafiyah_Detection"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Yah216/Poem_Qafiyah_Detection"", use_auth_token=True)

inputs = tokenizer(""text, return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,275925976.32965994,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46206,autotrain-sentiment_polarity-918130222,['Ritvik19/autotrain-data-sentiment_polarity'],,4.280488237750762,AutoTrain,Not Specified,Not Specified,Not Specified,0.9504804036293304,0.1360860466957092,0.9719076444852428,,,328525933.0,True,6,0,"['transformers', 'pytorch']",2022-05-28 14:18:46+00:00,2022-05-28 06:17:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 918130222
- CO2 Emissions (in grams): 4.280488237750762

## Validation Metrics

- Loss: 0.13608604669570923
- Accuracy: 0.9504804036293305
- Precision: 0.9792047060317863
- Recall: 0.9647185343057701
- AUC: 0.9791895292939061
- F1: 0.9719076444852428

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ritvik19/autotrain-sentiment_polarity-918130222
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ritvik19/autotrain-sentiment_polarity-918130222"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ritvik19/autotrain-sentiment_polarity-918130222"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,76749640.40378445,0.9610746083516104,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46214,autotrain-Arabic_Poetry_by_Subject-920730227,['zenkri/autotrain-data-Arabic_Poetry_by_Subject-1d8ba412'],,0.0617037401910781,AutoTrain,Not Specified,Not Specified,Not Specified,0.8687837028160575,0.5905918478965759,0.7777187122151491,,,442630381.0,True,13,0,"['transformers', 'pytorch']",2022-05-28 08:39:47+00:00,2022-05-28 08:32:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 920730227
- CO2 Emissions (in grams): 0.06170374019107819

## Validation Metrics

- Loss: 0.5905918478965759
- Accuracy: 0.8687837028160575
- Macro F1: 0.7777187122151491
- Micro F1: 0.8687837028160575
- Weighted F1: 0.8673230166815299
- Macro Precision: 0.796117563625016
- Micro Precision: 0.8687837028160575
- Weighted Precision: 0.8692944353097692
- Macro Recall: 0.7732013751753718
- Micro Recall: 0.8687837028160575
- Weighted Recall: 0.8687837028160575


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/zenkri/autotrain-Arabic_Poetry_by_Subject-920730227
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""zenkri/autotrain-Arabic_Poetry_by_Subject-920730227"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""zenkri/autotrain-Arabic_Poetry_by_Subject-920730227"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,7173477322.919242,0.8207328897659794,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46215,autotrain-Arabic_Poetry_by_Subject-920730230,['zenkri/autotrain-data-Arabic_Poetry_by_Subject-1d8ba412'],,0.0744521984740964,AutoTrain,Not Specified,Not Specified,Not Specified,0.8785200718993409,0.5806193351745605,0.8208042310550474,,,497926381.0,True,18,0,"['transformers', 'pytorch']",2022-05-28 08:41:57+00:00,2022-05-28 08:33:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 920730230
- CO2 Emissions (in grams): 0.07445219847409645

## Validation Metrics

- Loss: 0.5806193351745605
- Accuracy: 0.8785200718993409
- Macro F1: 0.8208042310550474
- Micro F1: 0.8785200718993409
- Weighted F1: 0.8783590365809876
- Macro Precision: 0.8486540338838363
- Micro Precision: 0.8785200718993409
- Weighted Precision: 0.8815185727115001
- Macro Recall: 0.8121110408113442
- Micro Recall: 0.8785200718993409
- Weighted Recall: 0.8785200718993409


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/zenkri/autotrain-Arabic_Poetry_by_Subject-920730230
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""zenkri/autotrain-Arabic_Poetry_by_Subject-920730230"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""zenkri/autotrain-Arabic_Poetry_by_Subject-920730230"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,6687866727.981706,0.8486820212340815,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46232,autotrain-TNC_Domain_WangchanBERTa-921730254,['CH0KUN/autotrain-data-TNC_Domain_WangchanBERTa'],,25.144394918865917,AutoTrain,Not Specified,Not Specified,Not Specified,0.7775925925925926,0.7080970406532288,0.7758012615987406,,,421087597.0,True,4,0,"['transformers', 'pytorch']",2022-05-28 12:04:53+00:00,2022-05-28 11:51:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 921730254
- CO2 Emissions (in grams): 25.144394918865913

## Validation Metrics

- Loss: 0.7080970406532288
- Accuracy: 0.7775925925925926
- Macro F1: 0.7758012615987406
- Micro F1: 0.7775925925925925
- Weighted F1: 0.7758012615987406
- Macro Precision: 0.7833307663368776
- Micro Precision: 0.7775925925925926
- Weighted Precision: 0.7833307663368777
- Macro Recall: 0.7775925925925926
- Micro Recall: 0.7775925925925926
- Weighted Recall: 0.7775925925925926


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/CH0KUN/autotrain-TNC_Domain_WangchanBERTa-921730254
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""CH0KUN/autotrain-TNC_Domain_WangchanBERTa-921730254"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""CH0KUN/autotrain-TNC_Domain_WangchanBERTa-921730254"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,16746777.894585831,0.7766958942388927,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46364,autotrain-hi_ner_xlmr_large-924630372,['pujaburman30/autotrain-data-hi_ner_xlmr_large'],,5.880084418778246,AutoTrain,Not Specified,Not Specified,Not Specified,0.7745009890307498,0.8206124901771545,0.6285289747399703,,,2235596465.0,True,4,0,"['transformers', 'pytorch']",2022-05-29 13:44:19+00:00,2022-05-29 13:39:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 924630372
- CO2 Emissions (in grams): 5.880084418778246

## Validation Metrics

- Loss: 0.8206124901771545
- Accuracy: 0.7745009890307498
- Precision: 0.6042857142857143
- Recall: 0.6547987616099071
- F1: 0.6285289747399703

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pujaburman30/autotrain-hi_ner_xlmr_large-924630372
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""pujaburman30/autotrain-hi_ner_xlmr_large-924630372"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pujaburman30/autotrain-hi_ner_xlmr_large-924630372"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,380198021.9638596,0.6939214772894776,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,1,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46466,autotrain-TNC_Data1000_wangchanBERTa-927730545,['CH0KUN/autotrain-data-TNC_Data1000_wangchanBERTa'],,0.0388231840613338,AutoTrain,Not Specified,Not Specified,Not Specified,0.9212962962962964,0.346664160490036,0.9193830593356196,,,421087597.0,True,4,0,"['transformers', 'pytorch']",2022-05-30 06:32:34+00:00,2022-05-30 06:27:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 927730545
- CO2 Emissions (in grams): 0.03882318406133382

## Validation Metrics

- Loss: 0.346664160490036
- Accuracy: 0.9212962962962963
- Macro F1: 0.9193830593356196
- Micro F1: 0.9212962962962963
- Weighted F1: 0.9213272351125573
- Macro Precision: 0.920255423800781
- Micro Precision: 0.9212962962962963
- Weighted Precision: 0.9231182355921642
- Macro Recall: 0.920208415963133
- Micro Recall: 0.9212962962962963
- Weighted Recall: 0.9212962962962963


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/CH0KUN/autotrain-TNC_Data1000_wangchanBERTa-927730545
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""CH0KUN/autotrain-TNC_Data1000_wangchanBERTa-927730545"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""CH0KUN/autotrain-TNC_Data1000_wangchanBERTa-927730545"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,10846292162.300638,0.9203386834886036,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46468,autotrain-TNC_Data2500_WangchanBERTa-928030564,['CH0KUN/autotrain-data-TNC_Data2500_WangchanBERTa'],,0.0729336291315811,AutoTrain,Not Specified,Not Specified,Not Specified,0.8445845697329377,0.4989683926105499,0.8407629450432429,,,421087597.0,True,5,0,"['transformers', 'pytorch']",2022-05-30 07:27:02+00:00,2022-05-30 07:16:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 928030564
- CO2 Emissions (in grams): 0.07293362913158113

## Validation Metrics

- Loss: 0.4989683926105499
- Accuracy: 0.8445845697329377
- Macro F1: 0.8407629450432429
- Micro F1: 0.8445845697329377
- Weighted F1: 0.8407629450432429
- Macro Precision: 0.8390327354531153
- Micro Precision: 0.8445845697329377
- Weighted Precision: 0.8390327354531154
- Macro Recall: 0.8445845697329377
- Micro Recall: 0.8445845697329377
- Weighted Recall: 0.8445845697329377


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/CH0KUN/autotrain-TNC_Data2500_WangchanBERTa-928030564
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""CH0KUN/autotrain-TNC_Data2500_WangchanBERTa-928030564"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""CH0KUN/autotrain-TNC_Data2500_WangchanBERTa-928030564"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-05,5773572520.850523,0.8426694245086276,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46858,autotrain-Adult-934630783,['rajistics/autotrain-data-Adult'],,38.42484725553464,AutoTrain,Not Specified,Not Specified,Not Specified,0.8628221244500315,0.2984429822985684,0.6751023446222553,,,,True,4,0,"['joblib', 'transformers']",2022-05-31 19:36:02+00:00,2022-05-31 17:54:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 934630783
- CO2 Emissions (in grams): 38.42484725553464

## Validation Metrics

- Loss: 0.2984429822985684
- Accuracy: 0.8628221244500315
- Precision: 0.7873263888888888
- Recall: 0.5908794788273616
- AUC: 0.9182195921357326
- F1: 0.6751023446222553

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-05,,0.7575056524844106,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
46978,masress-medcrit-camel,['cjbarrie/autotrain-data-masress-medcrit-binary-5'],,0.0101748763809847,AutoTrain,Not Specified,Not Specified,Not Specified,0.7551020408163265,0.757265031337738,0.7202470830473576,,,436418733.0,True,14,0,"['transformers', 'pytorch']",2022-06-01 13:23:54+00:00,2022-06-01 12:56:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 937130980
- CO2 Emissions (in grams): 0.01017487638098474

## Validation Metrics

- Loss: 0.757265031337738
- Accuracy: 0.7551020408163265
- Macro F1: 0.7202470830473576
- Micro F1: 0.7551020408163265
- Weighted F1: 0.7594301962377263
- Macro Precision: 0.718716577540107
- Micro Precision: 0.7551020408163265
- Weighted Precision: 0.7711448215649895
- Macro Recall: 0.7285714285714286
- Micro Recall: 0.7551020408163265
- Weighted Recall: 0.7551020408163265


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/cjbarrie/autotrain-masress-medcrit-binary-5-937130980
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""cjbarrie/autotrain-masress-medcrit-binary-5-937130980"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""cjbarrie/autotrain-masress-medcrit-binary-5-937130980"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,42891797075.35304,0.7372628396955843,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
47156,BERT-Banking77,['banking77'],919050.0,0.0333065101415592,AutoTrain,Not Specified,Not Specified,Not Specified,0.9264,0.3505457043647766,0.9268371013605567,,,438249901.0,True,5043,6,"['transformers', 'pytorch']",2022-12-05 13:36:09+00:00,2022-06-02 10:37:57+00:00,"#  `BERT-Banking77` Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 940131041
- CO2 Emissions (in grams): 0.03330651014155927

## Validation Metrics

- Loss: 0.3505457043647766
- Accuracy: 0.9263261296660118
- Macro F1: 0.9268371013605569
- Micro F1: 0.9263261296660118
- Weighted F1: 0.9259954221865809
- Macro Precision: 0.9305746406646502
- Micro Precision: 0.9263261296660118
- Weighted Precision: 0.929031563971418
- Macro Recall: 0.9263724620088746
- Micro Recall: 0.9263261296660118
- Weighted Recall: 0.9263261296660118


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/philschmid/autotrain-does-it-work-940131041
```

Or Python API:

```
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
model_id = 'philschmid/BERT-Banking77'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSequenceClassification.from_pretrained(model_id)
classifier = pipeline('text-classification', tokenizer=tokenizer, model=model)
classifier('What is the base of the exchange rates?')
```",,,1,[],[],NLP,2022-06,13158085285.349651,0.9266184991332856,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
47157,DistilBERT-Banking77,['banking77'],919050.0,5.632805352029529,AutoTrain,Not Specified,Not Specified,Not Specified,0.9199,0.3392622470855713,0.9199390885956756,,,268090737.0,True,38,0,"['transformers', 'pytorch']",2022-06-24 14:31:49+00:00,2022-06-02 10:38:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 940131045
- CO2 Emissions (in grams): 5.632805352029529

## Validation Metrics

- Loss: 0.3392622470855713
- Accuracy: 0.9199410609037328
- Macro F1: 0.9199390885956755
- Micro F1: 0.9199410609037327
- Weighted F1: 0.9198140295005729
- Macro Precision: 0.9235531521509113
- Micro Precision: 0.9199410609037328
- Weighted Precision: 0.9228777883152248
- Macro Recall: 0.919570805773292
- Micro Recall: 0.9199410609037328
- Weighted Recall: 0.9199410609037328


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/philschmid/autotrain-does-it-work-940131045
```

Or Python API:

```
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
model_id = 'philschmid/DistilBERT-Banking77'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSequenceClassification.from_pretrained(model_id)
classifier = pipeline('text-classification', tokenizer=tokenizer, model=model)
classifier('What is the base of the exchange rates?')
```",,,1,[],[],NLP,2022-06,47594532.43016919,0.9199195438826062,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
47903,autotrain-chat-bot-responses-949231426,['nitishkumargundapu793/autotrain-data-chat-bot-responses'],,0.0112353453775142,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.26922607421875,1.0,,,1340745645.0,True,15,0,"['transformers', 'pytorch']",2022-06-05 03:16:21+00:00,2022-06-05 03:13:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 949231426
- CO2 Emissions (in grams): 0.01123534537751425

## Validation Metrics

- Loss: 0.26922607421875
- Accuracy: 1.0
- Macro F1: 1.0
- Micro F1: 1.0
- Weighted F1: 1.0
- Macro Precision: 1.0
- Micro Precision: 1.0
- Weighted Precision: 1.0
- Macro Recall: 1.0
- Micro Recall: 1.0
- Weighted Recall: 1.0


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/nitishkumargundapu793/autotrain-chat-bot-responses-949231426
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""nitishkumargundapu793/autotrain-chat-bot-responses-949231426"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""nitishkumargundapu793/autotrain-chat-bot-responses-949231426"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,119332837571.9796,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
48040,autotrain-song_title_generate-939531516,['victorlifan/autotrain-data-song_title_generate'],,11.013963276910236,AutoTrain,Not Specified,Not Specified,Not Specified,,1.1184396743774414,,0.549539,0.548616,891730879.0,True,12,1,"['transformers', 'pytorch']",2022-06-06 15:36:11+00:00,2022-06-05 21:52:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 939531516
- CO2 Emissions (in grams): 11.013963276910237

## Validation Metrics

- Loss: 1.1184396743774414
- Rouge1: 54.9539
- Rouge2: 40.7878
- RougeL: 54.8616
- RougeLsum: 54.8682
- Gen Len: 5.1429

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/victorlifan/autotrain-song_title_generate-939531516
```",,,1,[],[],NLP,2022-06,80963669.16979212,0.5490771121089464,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
48144,autotrain-expand-928531583,['spy24/autotrain-data-expand'],,3.4552892403407167,AutoTrain,Not Specified,Not Specified,Not Specified,,2.1122372150421143,,0.687226,0.597235,2283825905.0,True,2,1,"['transformers', 'pytorch']",2022-06-06 16:04:02+00:00,2022-06-06 10:07:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 928531583
- CO2 Emissions (in grams): 3.4552892403407167

## Validation Metrics

- Loss: 2.1122372150421143
- Rouge1: 68.7226
- Rouge2: 50.1638
- RougeL: 59.7235
- RougeLsum: 62.3458
- Gen Len: 63.2505

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/spy24/autotrain-expand-928531583
```",,,1,[],[],NLP,2022-06,660965188.7709401,0.639078057037154,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
48315,2ch-text-classification,['BraveOni/autotrain-data-2ch-text-classification'],,0.0856428106791965,AutoTrain,Not Specified,Not Specified,Not Specified,0.8671983356449375,0.3410861194133758,0.8062721294891249,,,1334486957.0,True,15,0,"['transformers', 'pytorch']",2022-06-07 04:18:50+00:00,2022-06-07 04:08:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 955631800
- CO2 Emissions (in grams): 0.08564281067919652

## Validation Metrics

- Loss: 0.34108611941337585
- Accuracy: 0.8671983356449375
- Precision: 0.7883283877349159
- Recall: 0.8250517598343685
- AUC: 0.9236450689447471
- F1: 0.8062721294891249

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/BraveOni/autotrain-2ch-text-classification-955631800
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""BraveOni/autotrain-2ch-text-classification-955631800"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""BraveOni/autotrain-2ch-text-classification-955631800"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,15582007951.592836,0.8356261593345249,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
48348,autotrain-expand-parrot-956131825,['spy24/autotrain-data-expand-parrot'],,0.647019768976749,AutoTrain,Not Specified,Not Specified,Not Specified,,2.330639123916626,,0.533589,0.484928,891730879.0,True,12,0,"['transformers', 'pytorch']",2022-06-07 09:11:04+00:00,2022-06-07 07:59:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 956131825
- CO2 Emissions (in grams): 0.647019768976749

## Validation Metrics

- Loss: 2.330639123916626
- Rouge1: 53.3589
- Rouge2: 40.4273
- RougeL: 48.4928
- RougeLsum: 49.4952
- Gen Len: 18.8741

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/spy24/autotrain-expand-parrot-956131825
```",,,1,[],[],NLP,2022-06,1378212725.1077006,0.5080960781057164,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
48387,gpt2wilkinscoffee,['openwebtext'],39769491688.0,149200.0,Not Specified,Not Specified,East US,** 8 16GB V100,,,,,,,False,0,0,[],2022-06-07 11:01:22+00:00,2022-06-07 10:58:10+00:00,"
# DistilGPT2

DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of [GPT-2](https://huggingface.co/gpt2), And this is a Wilkins-ified Version.

## Model Details

- **Developed by:** Hugging Face
- **Model type:** Transformer-based Language Model
- **Language:** English
- **License:** Apache 2.0
- **Model Description:** DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters, was developed using [knowledge distillation](#knowledge-distillation) and was designed to be a faster, lighter version of GPT-2.
- **Resources for more information:** See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\* (a class of compressed models including Distilled-GPT2), [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure, and this page for more about [GPT-2](https://openai.com/blog/better-language-models/).

## Uses, Limitations and Risks

#### Limitations and Risks

<details>
<summary>Click to expand</summary>

**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**

As the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md), “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). 

DistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.

The impact of model compression techniques – such as knowledge distillation – on bias and fairness issues associated with language models is an active area of research. For example: 

- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.
- [Xu and Hu (2022)](https://arxiv.org/pdf/2201.08542.pdf) find that distilled versions of GPT-2 showed consistent reductions in toxicity and bias compared to the teacher model (see the paper for more detail on metrics used to define/measure toxicity and bias). 
- [Gupta et al. (2022)](https://arxiv.org/pdf/2203.12574.pdf) find that DistilGPT2 exhibits greater gender disparities than GPT-2 and propose a technique for mitigating gender bias in distilled language models like DistilGPT2. 

```python
>>> from transformers import pipeline, set_seed
>>> generator = pipeline('text-generation', model='distilgpt2')
>>> set_seed(48)
>>> generator(""The White man worked as a"", max_length=20, num_return_sequences=3)
[{'generated_text': ""The White man worked as a salesman at a McDonald's restaurant called Kia at the time of the""},
 {'generated_text': 'The White man worked as a contractor in the Army in the late 1990s. He became a ""'},
 {'generated_text': 'The White man worked as a police spokesman to the US Navy in the 1930s.'}]
 
>>> set_seed(48)
>>> generator(""The Black man worked as a"", max_length=20, num_return_sequences=3)
[{'generated_text': 'The Black man worked as a shop assistant for an hour at Wal-Mart at Wal-Mart in'},
 {'generated_text': 'The Black man worked as a waiter in the hotel when he was assaulted when he got out of a'},
 {'generated_text': 'The Black man worked as a police spokesman four months ago...'}]
```

</details>

#### Potential Uses

Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. 

The developers of GPT-2 state in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md) that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including: 

> - *Writing assistance: Grammar assistance, autocompletion (for normal prose or code)*
> - *Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.*
> - *Entertainment: Creation of games, chat bots, and amusing generations.*

Using DistilGPT2, the Hugging Face team built the [Write With Transformers](https://transformer.huggingface.co/doc/distil-gpt2) web app, which allows users to play with the model to generate text directly from their browser.

#### Out-of-scope Uses

OpenAI states in the GPT-2 [model card](https://github.com/openai/gpt-2/blob/master/model_card.md): 

> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.
>
> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.

### How to Get Started with the Model 

<details>
<summary>Click to expand</summary>

*Be sure to read the sections on in-scope and out-of-scope uses and limitations of the model for further information on how to use the model.*

Using DistilGPT2 is similar to using GPT-2. DistilGPT2 can be used directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:

```python
>>> from transformers import pipeline, set_seed
>>> generator = pipeline('text-generation', model='distilgpt2')
>>> set_seed(42)
>>> generator(""Hello, I’m a language model"", max_length=20, num_return_sequences=5)
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[{'generated_text': ""Hello, I'm a language model, I'm a language model. In my previous post I've""},
 {'generated_text': ""Hello, I'm a language model, and I'd love to hear what you think about it.""},
 {'generated_text': ""Hello, I'm a language model, but I don't get much of a connection anymore, so""},
 {'generated_text': ""Hello, I'm a language model, a functional language... It's not an example, and that""},
 {'generated_text': ""Hello, I'm a language model, not an object model.\n\nIn a nutshell, I""}]
``` 
 
Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
model = GPT2Model.from_pretrained('distilgpt2')
text = ""Replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

And in TensorFlow:

```python
from transformers import GPT2Tokenizer, TFGPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
model = TFGPT2Model.from_pretrained('distilgpt2')
text = ""Replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```

</details>

## Training Data

DistilGPT2 was trained using [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), an open-source reproduction of OpenAI’s WebText dataset, which was used to train GPT-2. See the [OpenWebTextCorpus Dataset Card](https://huggingface.co/datasets/openwebtext) for additional information about OpenWebTextCorpus and [Radford et al. (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) for additional information about WebText.

## Training Procedure

The texts were tokenized using the same tokenizer as GPT-2, a byte-level version of Byte Pair Encoding (BPE). DistilGPT2 was trained using knowledge distillation, following a procedure similar to the training procedure for DistilBERT, described in more detail in [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108). 

## Evaluation Results

The creators of DistilGPT2 [report](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) that, on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT-2 reaches a perplexity on the test set of 16.3 compared to 21.1 for DistilGPT2 (after fine-tuning on the train set).

## Environmental Impact

*Carbon emissions were estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.*

- **Hardware Type:** 8 16GB V100
- **Hours used:** 168 (1 week)
- **Cloud Provider:** Azure
- **Compute Region:** unavailable, assumed East US for calculations
- **Carbon Emitted** *(Power consumption x Time x Carbon produced based on location of power grid)*: 149.2 kg eq. CO2

## Citation

```bibtex
@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}
```

## Glossary

-	<a name=""knowledge-distillation"">**Knowledge Distillation**</a>: As described in [Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf), “knowledge distillation is a compression technique in which a compact model – the student – is trained to reproduce the behavior of a larger model – the teacher – or an ensemble of models.” Also see [Bucila et al. (2006)](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf) and [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531).

<a href=""https://huggingface.co/exbert/?model=distilgpt2"">
	<img width=""300px"" src=""https://cdn-media.huggingface.co/exbert/button.png"">
</a>

This is the Wilkins Coffee Version.",** 168 (1 week),** Azure,1,[],[],NLP,2022-06,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
48935,autotrain-car-review-project-966432120,['qualitydatalab/autotrain-data-car-review-project'],,0.061185706621337,AutoTrain,Not Specified,Not Specified,Not Specified,0.724822695035461,0.6066656112670898,0.7077087000886584,,,498677165.0,True,15,1,"['transformers', 'pytorch']",2022-06-09 12:36:14+00:00,2022-06-09 12:30:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 966432120
- CO2 Emissions (in grams): 0.061185706621337065

## Validation Metrics

- Loss: 0.6066656112670898
- Accuracy: 0.724822695035461
- Macro F1: 0.7077087000886584
- Micro F1: 0.7248226950354609
- Weighted F1: 0.7077087000886584
- Macro Precision: 0.7143184427227084
- Micro Precision: 0.724822695035461
- Weighted Precision: 0.7143184427227083
- Macro Recall: 0.7248226950354609
- Micro Recall: 0.724822695035461
- Weighted Recall: 0.724822695035461


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/qualitydatalab/autotrain-car-review-project-966432120
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""qualitydatalab/autotrain-car-review-project-966432120"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""qualitydatalab/autotrain-car-review-project-966432120"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,8150223189.97127,0.7161634698468291,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
48936,autotrain-car-review-project-966432121,['qualitydatalab/autotrain-data-car-review-project'],,0.2152988836837717,AutoTrain,Not Specified,Not Specified,Not Specified,0.737791286727457,0.6013365983963013,0.729171012281939,,,1421615405.0,True,14,1,"['transformers', 'pytorch']",2022-06-09 13:04:21+00:00,2022-06-09 12:30:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 966432121
- CO2 Emissions (in grams): 0.21529888368377176

## Validation Metrics

- Loss: 0.6013365983963013
- Accuracy: 0.737791286727457
- Macro F1: 0.729171012281939
- Micro F1: 0.737791286727457
- Weighted F1: 0.729171012281939
- Macro Precision: 0.7313770127538427
- Micro Precision: 0.737791286727457
- Weighted Precision: 0.7313770127538428
- Macro Recall: 0.737791286727457
- Micro Recall: 0.737791286727457
- Weighted Recall: 0.737791286727457


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love driving this car""}' https://api-inference.huggingface.co/models/qualitydatalab/autotrain-car-review-project-966432121
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""qualitydatalab/autotrain-car-review-project-966432121"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""qualitydatalab/autotrain-car-review-project-966432121"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,6602985490.106166,0.7334558219514384,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
49556,kekbot-mini,[''],,10.0,MLCO2,fine-tuning,"West Java, Indonesia",1 T4,,,,,,333969117.0,False,5,0,"['transformers', 'pytorch']",2022-06-12 05:53:59+00:00,2022-06-12 03:40:33+00:00,"> THIS MODEL IS INTENDED FOR RESEARCH PURPOSES ONLY
# Kekbot Mini

Based on a `distilgpt2` model, fine-tuned to a select subset (65k<= messages) of Art Union's general-chat channel chat history.

### Limits and biases
As this is trained on chat history, it is possible that discriminatory or even offensive materials to be outputted. 
Author holds his ground on the fact that ML models are mere statistical representation of the dataset used to train it, 
and that due to the nature of the dataset it is practically impossible to be certain of 
the degree of ""cleanliness"" that the data contained within holds.

Author can confirm, however, that from heuristical testing that the model was not found to be offensive 
to the author himself, hopefully this opinion stays true for everyone in the audience.
",,,1,[],[],NLP,2022-06,33396911.7,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,1,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
49858,autotrain-Twitter_Sentiment-975432358,['chradden/autotrain-data-Twitter_Sentiment'],,5.0728502681092005,AutoTrain,Not Specified,Not Specified,Not Specified,0.7271750805585392,0.5502054691314697,0.7350855235711306,,,267860081.0,True,6,0,"['transformers', 'pytorch']",2022-06-12 20:41:13+00:00,2022-06-12 20:38:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 975432358
- CO2 Emissions (in grams): 5.0728502681092005

## Validation Metrics

- Loss: 0.5502054691314697
- Accuracy: 0.7271750805585392
- Precision: 0.7143725927427529
- Recall: 0.7570354457572502
- AUC: 0.8047178634017913
- F1: 0.7350855235711306

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/chradden/autotrain-Twitter_Sentiment-975432358
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""chradden/autotrain-Twitter_Sentiment-975432358"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""chradden/autotrain-Twitter_Sentiment-975432358"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,52802678.345134616,0.7311089053628794,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
49869,kekbot-beta-4-medium,[''],,840.0,MLCO2,fine-tuning,"West Java, Indonesia",1 Tesla P100,,,,,,1444566873.0,False,5,0,"['transformers', 'pytorch']",2022-06-12 21:36:45+00:00,2022-06-12 21:21:05+00:00,"> THIS MODEL IS IN PUBLIC BETA, PLEASE DO NOT EXPECT ANY FORM OF STABILITY IN ITS CURRENT STATE.  
# Art Union server chatbot

Based on a DialoGPT-medium (`kekbot-beta-3-medium`) model, fine-tuned to a select subset (65k<= messages) of Art Union's general-chat channel chat history.

### Current issues  
(Which hopefully will be fixed in future iterations) Include, but not limited to:
- Limited turns, after ~20 turns output may break for no apparent reason.
- Inconsistent variance, acts like an overfitted model from time to time for no reason whatsoever.
",,,1,[],[],NLP,2022-06,1719722.467857143,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,1,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,1,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50010,autotrain-PAN-976832386,['masifayub/autotrain-data-PAN'],,7.17945527948844,AutoTrain,Not Specified,Not Specified,Not Specified,0.7591666666666667,0.4892439842224121,0.8470089994706194,,,328525933.0,True,6,0,"['transformers', 'pytorch']",2022-06-13 08:05:59+00:00,2022-06-13 08:02:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 976832386
- CO2 Emissions (in grams): 7.17945527948844

## Validation Metrics

- Loss: 0.4892439842224121
- Accuracy: 0.7591666666666667
- Precision: 0.8088978766430738
- Recall: 0.8888888888888888
- AUC: 0.7550212962962962
- F1: 0.8470089994706194

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/masifayub/autotrain-PAN-976832386
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""masifayub/autotrain-PAN-976832386"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""masifayub/autotrain-PAN-976832386"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,45759172.557086885,0.8006857684641537,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50026,autotrain-pan-977432399,['tayyaba/autotrain-data-pan'],,27.081173251466467,AutoTrain,Not Specified,Not Specified,Not Specified,0.8841666666666667,0.277687668800354,0.9231619679380874,,,498674093.0,True,4,0,"['transformers', 'pytorch']",2022-06-13 10:13:31+00:00,2022-06-13 10:00:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 977432399
- CO2 Emissions (in grams): 27.081173251466467

## Validation Metrics

- Loss: 0.277687668800354
- Accuracy: 0.8841666666666667
- Precision: 0.9185918591859186
- Recall: 0.9277777777777778
- AUC: 0.9422805555555556
- F1: 0.9231619679380874

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/tayyaba/autotrain-pan-977432399
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""tayyaba/autotrain-pan-977432399"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""tayyaba/autotrain-pan-977432399"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,18414050.542400204,0.903243631907332,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50030,autotrain-pan-977432388,['tayyaba/autotrain-data-pan'],,13.776410975057908,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-06-13 10:36:28+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50223,autotrain-dontknowwhatImdoing-980432459,['Jerimee/autotrain-data-dontknowwhatImdoing'],,0.0121473985779178,AutoTrain,Not Specified,Not Specified,Not Specified,0.9917355371900828,0.0469294898211956,0.9936708860759492,,,1334486957.0,True,18,1,"['transformers', 'pytorch']",2022-06-14 01:36:33+00:00,2022-06-13 22:23:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 980432459
- CO2 Emissions (in grams): 0.012147398577917884

## Validation Metrics

- Loss: 0.0469294898211956
- Accuracy: 0.9917355371900827
- Precision: 0.9936708860759493
- Recall: 0.9936708860759493
- AUC: 0.9990958408679927
- F1: 0.9936708860759493

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Jerimee/autotrain-dontknowwhatImdoing-980432459
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Jerimee/autotrain-dontknowwhatImdoing-980432459"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Jerimee/autotrain-dontknowwhatImdoing-980432459"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,109857838980.10011,0.9927022683562978,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50245,bert2gpt2SUMM,['Chemsseddine/autotrain-data-bertSummGpt2'],,0.1068550128808479,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,1079058281.0,False,3,0,"['transformers', 'pytorch']",2022-12-07 18:43:18+00:00,2022-06-14 00:34:06+00:00,"
<img src=""https://huggingface.co/Chemsseddine/bert2gpt2_med_ml_orange_summ-finetuned_med_sum_new-finetuned_med_sum_new/resolve/main/logobert2gpt2.png"" alt=""Map of positive probabilities per country."" width=""200""/>

## This model is used for french summarization
- Problem type: Summarization
- Model ID: 980832493
- CO2 Emissions (in grams): 0.10685501288084795

## Validation Metrics

- Loss: 4.03749418258667
- Rouge1: 28.8384
- Rouge2: 10.7511
- RougeL: 27.0842
- RougeLsum: 27.5118
- Gen Len: 22.0625

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Chemsseddine/autotrain-bertSummGpt2-980832493
```",,,1,[],[],NLP,2022-06,10098340282.858212,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,1.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50355,pegasus-pdm-news,['mshoaibsarwar/autotrain-data-pdm-news'],,258.9123940027299,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,1,"['transformers', 'pytorch']",2022-06-14 14:54:33+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50800,bert2gpt2Summy,['ouiame/autotrain-data-trainproject'],,894.9753853627794,AutoTrain,Not Specified,Not Specified,Not Specified,,1.969262838363648,,0.193642,0.1614799999999999,2329732045.0,True,2,0,"['transformers', 'pytorch']",2022-06-15 19:31:08+00:00,2022-06-15 13:08:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 985232782
- CO2 Emissions (in grams): 894.9753853627794

## Validation Metrics

- Loss: 1.9692628383636475
- Rouge1: 19.3642
- Rouge2: 7.3644
- RougeL: 16.148
- RougeLsum: 16.4988
- Gen Len: 18.9975

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/ouiame/autotrain-trainproject-985232782
```",,,1,[],[],NLP,2022-06,2603124.156376256,0.1761046072054111,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50808,T5_mlsum,['ouiame/autotrain-data-trainproject'],,976.8219757938544,AutoTrain,Not Specified,Not Specified,Not Specified,,1.7047555446624756,,0.2021079999999999,0.169554,4918578681.0,True,2,0,"['transformers', 'pytorch']",2022-06-16 05:31:30+00:00,2022-06-15 13:51:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 985232789
- CO2 Emissions (in grams): 976.8219757938544

## Validation Metrics

- Loss: 1.7047555446624756
- Rouge1: 20.2108
- Rouge2: 7.8633
- RougeL: 16.9554
- RougeLsum: 17.3178
- Gen Len: 18.9874

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/ouiame/autotrain-trainproject-985232789
```",,,1,[],[],NLP,2022-06,5035286.677495882,0.1844052920772099,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50898,autotrain-journals-covid-990032813,['liux3790/autotrain-data-journals-covid'],,1.52810485048449,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-06-15 19:09:50+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
50928,autotrain-Psychiatry_Article_Identifier-990132822,['justpyschitry/autotrain-data-Psychiatry_Article_Identifier'],,13.4308931494349,AutoTrain,Not Specified,Not Specified,Not Specified,0.9177471636953,0.3777158558368683,0.9082952086962772,,,438074605.0,True,15,0,"['transformers', 'pytorch']",2022-06-15 21:42:20+00:00,2022-06-15 21:36:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 990132822
- CO2 Emissions (in grams): 13.4308931494349

## Validation Metrics

- Loss: 0.3777158558368683
- Accuracy: 0.9177471636952999
- Macro F1: 0.9082952086962773
- Micro F1: 0.9177471636952999
- Weighted F1: 0.9175376430905807
- Macro Precision: 0.9175123149319843
- Micro Precision: 0.9177471636952999
- Weighted Precision: 0.9185452324503698
- Macro Recall: 0.9042000199743617
- Micro Recall: 0.9177471636952999
- Weighted Recall: 0.9177471636952999


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/justpyschitry/autotrain-Psychiatry_Article_Identifier-990132822
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""justpyschitry/autotrain-Psychiatry_Article_Identifier-990132822"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""justpyschitry/autotrain-Psychiatry_Article_Identifier-990132822"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,32616937.69177457,0.9129967236053652,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
51202,bertGpt2Summ,['ouiame/autotrain-data-Robertatogpt2'],,2.4722651844547827,AutoTrain,Not Specified,Not Specified,Not Specified,,3.5972988605499268,,0.161218,0.130085,1135171497.0,True,2,0,"['transformers', 'pytorch']",2022-06-17 00:38:07+00:00,2022-06-16 20:13:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 995132940
- CO2 Emissions (in grams): 2.4722651844547827

## Validation Metrics

- Loss: 3.5972988605499268
- Rouge1: 16.1218
- Rouge2: 2.9195
- RougeL: 13.0085
- RougeLsum: 13.2975
- Gen Len: 19.9962

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/ouiame/autotrain-Robertatogpt2-995132940
```",,,1,[],[],NLP,2022-06,459162513.8507718,0.1439878307466796,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,1.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
51203,autotrain-Robertatogpt2-995132944,['ouiame/autotrain-data-Robertatogpt2'],,611.0958349328379,AutoTrain,Not Specified,Not Specified,Not Specified,,3.8850467205047607,,0.166344,0.135872,1135171497.0,True,2,0,"['transformers', 'pytorch']",2022-06-17 01:09:06+00:00,2022-06-16 20:14:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 995132944
- CO2 Emissions (in grams): 611.0958349328379

## Validation Metrics

- Loss: 3.8850467205047607
- Rouge1: 16.6344
- Rouge2: 2.9899
- RougeL: 13.5872
- RougeLsum: 13.9042
- Gen Len: 20.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/ouiame/autotrain-Robertatogpt2-995132944
```",,,1,[],[],NLP,2022-06,1857599.7938600916,0.1495717762659819,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,1.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
51463,bert2gpt2frenchSumm,['ouiame/autotrain-data-orangesum'],,999.838587232387,AutoTrain,Not Specified,Not Specified,Not Specified,,2.4244203567504883,,0.257023,0.186776,1079058281.0,True,2,1,"['transformers', 'pytorch']",2022-06-18 06:31:16+00:00,2022-06-17 23:10:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1000833138
- CO2 Emissions (in grams): 999.838587232387

## Validation Metrics

- Loss: 2.4244203567504883
- Rouge1: 25.7023
- Rouge2: 8.5872
- RougeL: 18.6776
- RougeLsum: 19.821
- Gen Len: 39.732

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/ouiame/autotrain-orangesum-1000833138
```",,,1,[],[],NLP,2022-06,1079232.4829019632,0.2163399550156715,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,1.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
51765,biomedical-ner-all,[''],,0.0279399890043426,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,265743541.0,False,15519,40,"['transformers', 'safetensors', 'pytorch']",2023-03-21 10:37:49+00:00,2022-06-19 14:04:18+00:00,"
## About the Model
An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased

- Dataset: Maccrobat https://figshare.com/articles/dataset/MACCROBAT2018/9764942
- Carbon emission: 0.0279399890043426 Kg
- Training time: 30.16527 minutes
- GPU used : 1 x GeForce RTX 3060 Laptop GPU

Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18

## Usage
The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.
```python
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained(""d4data/biomedical-ner-all"")
model = AutoModelForTokenClassification.from_pretrained(""d4data/biomedical-ner-all"")

pipe = pipeline(""ner"", model=model, tokenizer=tokenizer, aggregation_strategy=""simple"") # pass device=0 if using gpu
pipe(""""""The patient reported no recurrence of palpitations at follow-up 6 months after the ablation."""""")
```

## Author
This model is part of the Research topic ""AI in Biomedical field"" conducted by Deepak John Reji, Shaina Raza. If you use this work (code, model or dataset), please star at:
> https://github.com/dreji18/Bio-Epidemiology-NER",,,1,[],[],NLP,2022-06,9511225682.969902,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,2,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
51811,autotrain-blaze_text_classification-1004733283,['twhitehurst3/autotrain-data-blaze_text_classification'],,10.8014599472142,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-06-19 19:06:24+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
51970,autotrain-yes-or-no-classifier-on-circa-1009033469,['Siddish/autotrain-data-yes-or-no-classifier-on-circa'],,0.1287915253247826,AutoTrain,Not Specified,Not Specified,Not Specified,0.8722054859679721,0.4084862470626831,0.6340608446004876,,,1421635885.0,True,54,0,"['transformers', 'pytorch']",2022-06-20 16:21:09+00:00,2022-06-20 16:06:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1009033469
- CO2 Emissions (in grams): 0.1287915253247826

## Validation Metrics

- Loss: 0.4084862470626831
- Accuracy: 0.8722054859679721
- Macro F1: 0.6340608446004876
- Micro F1: 0.8722054859679722
- Weighted F1: 0.8679846554644491
- Macro Precision: 0.645023001823007
- Micro Precision: 0.8722054859679721
- Weighted Precision: 0.8656545967138464
- Macro Recall: 0.6283763558287574
- Micro Recall: 0.8722054859679721
- Weighted Recall: 0.8722054859679721


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Siddish/autotrain-yes-or-no-classifier-on-circa-1009033469
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Siddish/autotrain-yes-or-no-classifier-on-circa-1009033469"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Siddish/autotrain-yes-or-no-classifier-on-circa-1009033469"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,11038271978.027756,0.734307520356402,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52055,autotrain-GlueModels-1010733562,['deepesh0x/autotrain-data-GlueModels'],,60.24263131580023,AutoTrain,Not Specified,Not Specified,Not Specified,0.9252564102564104,0.1812974065542221,0.923920135717082,,,438019245.0,True,24,0,"['transformers', 'pytorch']",2022-06-21 01:48:26+00:00,2022-06-21 01:21:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1010733562
- CO2 Emissions (in grams): 60.24263131580023

## Validation Metrics

- Loss: 0.1812974065542221
- Accuracy: 0.9252564102564103
- Precision: 0.9409888357256778
- Recall: 0.9074596257369905
- AUC: 0.9809618001947271
- F1: 0.923920135717082

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-GlueModels-1010733562
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-GlueModels-1010733562"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-GlueModels-1010733562"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,7270918.209130713,0.9245877901692324,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52274,autotrain-mlsec-1013333726,['deepesh0x/autotrain-data-mlsec'],,33.183779535405364,AutoTrain,Not Specified,Not Specified,Not Specified,0.9226923076923076,0.1998898833990097,0.9223238438747908,,,267860081.0,True,4,0,"['transformers', 'pytorch']",2022-06-21 20:49:59+00:00,2022-06-21 16:55:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1013333726
- CO2 Emissions (in grams): 33.183779535405364

## Validation Metrics

- Loss: 0.1998898833990097
- Accuracy: 0.9226923076923077
- Precision: 0.9269808389435525
- Recall: 0.9177134068187645
- AUC: 0.9785380985232148
- F1: 0.9223238438747907

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-mlsec-1013333726
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-mlsec-1013333726"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-mlsec-1013333726"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,8072018.460531515,0.9225080389910298,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52275,autotrain-mlsec-1013333734,['deepesh0x/autotrain-data-mlsec'],,308.7012650779217,AutoTrain,Not Specified,Not Specified,Not Specified,0.9396153846153846,0.2087773829698562,0.940357097632012,,,1421611309.0,True,6,0,"['transformers', 'pytorch']",2022-06-21 19:12:28+00:00,2022-06-21 16:56:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1013333734
- CO2 Emissions (in grams): 308.7012650779217

## Validation Metrics

- Loss: 0.20877738296985626
- Accuracy: 0.9396153846153846
- Precision: 0.9291791791791791
- Recall: 0.9518072289156626
- AUC: 0.9671522989580735
- F1: 0.9403570976320121

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-mlsec-1013333734
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-mlsec-1013333734"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-mlsec-1013333734"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,4605135.999818984,0.939986094808206,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52284,autotrain-GlueFineTunedModel-1013533786,['deepesh0x/autotrain-data-GlueFineTunedModel'],,57.79463560530838,AutoTrain,Not Specified,Not Specified,Not Specified,0.926153846153846,0.1825724393129348,0.92632386799693,,,438019245.0,True,14,1,"['transformers', 'pytorch']",2022-06-21 18:05:40+00:00,2022-06-21 17:38:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1013533786
- CO2 Emissions (in grams): 57.79463560530838

## Validation Metrics

- Loss: 0.18257243931293488
- Accuracy: 0.9261538461538461
- Precision: 0.9244319632371713
- Recall: 0.9282235324275827
- AUC: 0.9800523984255356
- F1: 0.92632386799693

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-GlueFineTunedModel-1013533786
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-GlueFineTunedModel-1013533786"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-GlueFineTunedModel-1013533786"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,7578891.023577427,0.9262388492730196,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52286,autotrain-GlueFineTunedModel-1013533798,['deepesh0x/autotrain-data-GlueFineTunedModel'],,56.65990763623749,AutoTrain,Not Specified,Not Specified,Not Specified,0.4998717948717949,0.693366527557373,0.0,,,438019245.0,True,15,0,"['transformers', 'pytorch']",2022-06-21 18:16:42+00:00,2022-06-21 17:49:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1013533798
- CO2 Emissions (in grams): 56.65990763623749

## Validation Metrics

- Loss: 0.693366527557373
- Accuracy: 0.4998717948717949
- Precision: 0.0
- Recall: 0.0
- AUC: 0.5
- F1: 0.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-GlueFineTunedModel-1013533798
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-GlueFineTunedModel-1013533798"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-GlueFineTunedModel-1013533798"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,7730673.473951443,0.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52311,autotrain-qn-classification-1015534072,['lucianpopa/autotrain-data-qn-classification'],,0.0131704400140432,AutoTrain,Not Specified,Not Specified,Not Specified,0.7333333333333333,1.493847370147705,0.6777777777777777,,,1421664557.0,True,6,0,"['transformers', 'pytorch']",2022-06-21 22:26:00+00:00,2022-06-21 22:23:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1015534072
- CO2 Emissions (in grams): 0.013170440014043236

## Validation Metrics

- Loss: 1.493847370147705
- Accuracy: 0.7333333333333333
- Macro F1: 0.6777777777777777
- Micro F1: 0.7333333333333333
- Weighted F1: 0.6777777777777777
- Macro Precision: 0.6555555555555554
- Micro Precision: 0.7333333333333333
- Weighted Precision: 0.6555555555555554
- Macro Recall: 0.7333333333333333
- Micro Recall: 0.7333333333333333
- Weighted Recall: 0.7333333333333333


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lucianpopa/autotrain-qn-classification-1015534072
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucianpopa/autotrain-qn-classification-1015534072"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucianpopa/autotrain-qn-classification-1015534072"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,107943588481.79124,0.7044619422572178,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52405,autotrain-vision_528a5bd60a4b4b1080538a6ede3f23c7-260265,['abhishek/autotrain-data-vision_528a5bd60a4b4b1080538a6ede3f23c7'],,8.217704896005591,AutoTrain,Not Specified,Not Specified,Not Specified,0.914,0.2458025217056274,0.912823674084623,,,110417455.0,True,8,0,"['transformers', 'pytorch']",2022-06-22 10:02:50+00:00,2022-06-22 09:50:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 260265
- CO2 Emissions (in grams): 8.217704896005591

## Validation Metrics

- Loss: 0.24580252170562744
- Accuracy: 0.914
- Macro F1: 0.912823674084623
- Micro F1: 0.914
- Weighted F1: 0.9128236740846232
- Macro Precision: 0.9135654150297885
- Micro Precision: 0.914
- Weighted Precision: 0.9135654150297884
- Macro Recall: 0.9139999999999999
- Micro Recall: 0.914
- Weighted Recall: 0.914",,,1,[],[],Computer Vision,2022-06,13436532.02412647,0.9134114583131876,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52408,autotrain-dog-vs-food,"['abhishek/autotrain-data-vision_652fee16113a4f07a2452e021a22a934', 'sasha/dog-food']",,2.050948967287266,AutoTrain,Not Specified,Not Specified,Not Specified,0.9976190476190476,0.009216072037816,0.9973261861865684,,,343266993.0,True,29118,1,"['transformers', 'pytorch']",2022-06-22 14:51:28+00:00,2022-06-22 10:33:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 264300
- CO2 Emissions (in grams): 2.050948967287266

## Validation Metrics

- Loss: 0.009216072037816048
- Accuracy: 0.9976190476190476
- Macro F1: 0.9973261861865685
- Micro F1: 0.9976190476190476
- Weighted F1: 0.997621154535828
- Macro Precision: 0.9964539007092199
- Micro Precision: 0.9976190476190476
- Weighted Precision: 0.9976359338061465
- Macro Recall: 0.9982142857142857
- Micro Recall: 0.9976190476190476
- Weighted Recall: 0.9976190476190476",,,1,[],[],Computer Vision,2022-06,167369836.34167644,0.997472595406524,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52441,autotrain-avar-1016534299,['Mizew/autotrain-data-avar'],,0.0781596601881881,AutoTrain,Not Specified,Not Specified,Not Specified,,0.9978321194648744,,,,4918480377.0,True,3,0,"['transformers', 'pytorch']",2022-06-22 12:12:07+00:00,2022-06-22 11:55:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1016534299
- CO2 Emissions (in grams): 0.07815966018818815

## Validation Metrics

- Loss: 0.9978321194648743
- SacreBLEU: 13.8459
- Gen len: 6.0588",,,1,[],[],NLP,2022-06,62928630512.94718,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52447,autotrain_cifar10_vit_base,"['abhishek/autotrain-data-vision_79ca848474e24ad3a520c09e36452e85', 'cifar10']",136627438.0,32.86964815711988,AutoTrain,Not Specified,Not Specified,Not Specified,0.9834,0.0507049970328807,0.9834026834840476,,,343291569.0,True,3,3,"['transformers', 'pytorch']",2022-06-28 02:56:22+00:00,2022-06-22 12:21:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 300303
- CO2 Emissions (in grams): 32.869648157119876

## Validation Metrics

- Loss: 0.05070499703288078
- Accuracy: 0.9834
- Macro F1: 0.9834026834840477
- Micro F1: 0.9834
- Weighted F1: 0.9834026834840479
- Macro Precision: 0.9834502145172822
- Micro Precision: 0.9834
- Weighted Precision: 0.9834502145172822
- Macro Recall: 0.9833999999999999
- Micro Recall: 0.9834
- Weighted Recall: 0.9834",,,1,[],[],Computer Vision,2022-06,10444029.317230152,0.9834013417401932,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52448,EN-RSK,['Mizew/autotrain-data-rusyn2'],,19.740487511182447,AutoTrain,Not Specified,Not Specified,Not Specified,,0.9978321194648744,,,,4918480377.0,True,3,0,"['transformers', 'pytorch']",2022-06-24 11:13:10+00:00,2022-06-22 12:39:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1018434345
- CO2 Emissions (in grams): 19.740487511182447

## Validation Metrics

- Loss: 0.9978321194648743
- SacreBLEU: 13.8459
- Gen len: 6.0588

## Description

This is a model for the Pannonian Rusyn language, Albeit the data i trained it on also had a bit of Carpathian Rusyn in it, so don't expect the translator give out pure pannonian. and also it's not very good.",,,1,[],[],NLP,2022-06,249156986.33145788,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52524,bert_wikipedia_sst2,['deepesh0x/autotrain-data-bert_wikipedia_sst2'],,16.368556687663705,AutoTrain,Not Specified,Not Specified,Not Specified,0.9503340757238308,0.1571264714002609,0.9556748161399324,,,438019245.0,True,14,0,"['transformers', 'pytorch']",2022-06-22 21:27:21+00:00,2022-06-22 21:18:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1021934687
- CO2 Emissions (in grams): 16.368556687663705

## Validation Metrics

- Loss: 0.15712647140026093
- Accuracy: 0.9503340757238308
- Precision: 0.9515767251616308
- Recall: 0.9598083577322332
- AUC: 0.9857179850355002
- F1: 0.9556748161399324

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-bert_wikipedia_sst2-1021934687
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-bert_wikipedia_sst2-1021934687"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-bert_wikipedia_sst2-1021934687"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,26759796.44131463,0.9529969634095506,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52554,autotrain-Wikipeida_Article_Classifier_by_Chap-1022634731,['justpyschitry/autotrain-data-Wikipeida_Article_Classifier_by_Chap'],,19.2150872382377,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-06-23 02:26:23+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52555,autotrain-Wikipeida_Article_Classifier_by_Chap-1022634735,['justpyschitry/autotrain-data-Wikipeida_Article_Classifier_by_Chap'],,16.816741650923202,AutoTrain,Not Specified,Not Specified,Not Specified,0.9027552674230146,0.4373569190502167,0.8938134766263609,,,1334560749.0,True,15,0,"['transformers', 'pytorch']",2022-06-23 02:24:51+00:00,2022-06-23 02:16:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1022634735
- CO2 Emissions (in grams): 16.816741650923202

## Validation Metrics

- Loss: 0.4373569190502167
- Accuracy: 0.9027552674230146
- Macro F1: 0.8938134766263609
- Micro F1: 0.9027552674230146
- Weighted F1: 0.9023653852553881
- Macro Precision: 0.8970541297231431
- Micro Precision: 0.9027552674230146
- Weighted Precision: 0.903514305510645
- Macro Recall: 0.892665778987219
- Micro Recall: 0.9027552674230146
- Weighted Recall: 0.9027552674230146


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/justpyschitry/autotrain-Wikipeida_Article_Classifier_by_Chap-1022634735
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""justpyschitry/autotrain-Wikipeida_Article_Classifier_by_Chap-1022634735"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""justpyschitry/autotrain-Wikipeida_Article_Classifier_by_Chap-1022634735"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,79359056.39168426,0.8982621197109603,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52556,autotrain-Wikipeida_Article_Classifier_by_Chap-1022634736,['justpyschitry/autotrain-data-Wikipeida_Article_Classifier_by_Chap'],,0.0759956950556571,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-06-23 02:25:13+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52590,autotrain-atc,['cjbarrie/autotrain-data-traintest-sentiment-split'],,2.288443953210163,AutoTrain,Not Specified,Not Specified,Not Specified,0.7619047619047619,0.5510443449020386,0.7041420118343196,,,267860081.0,True,5,0,"['transformers', 'pytorch']",2022-06-23 08:00:44+00:00,2022-06-23 07:59:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1024534822
- CO2 Emissions (in grams): 2.288443953210163

## Validation Metrics

- Loss: 0.5510443449020386
- Accuracy: 0.7619047619047619
- Precision: 0.6761363636363636
- Recall: 0.7345679012345679
- AUC: 0.7936883912336109
- F1: 0.7041420118343196

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/cjbarrie/autotrain-traintest-sentiment-split-1024534822
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""cjbarrie/autotrain-traintest-sentiment-split-1024534822"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""cjbarrie/autotrain-traintest-sentiment-split-1024534822"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,117049002.06284434,0.7318854507015184,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52591,autotrain-atc2,['cjbarrie/autotrain-data-traintest-sentiment-split'],,3.1566482249518177,AutoTrain,Not Specified,Not Specified,Not Specified,0.7523809523809524,0.5167999267578125,0.6338028169014086,,,328525933.0,True,6,0,"['transformers', 'pytorch']",2022-06-23 08:01:58+00:00,2022-06-23 07:59:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1024534825
- CO2 Emissions (in grams): 3.1566482249518177

## Validation Metrics

- Loss: 0.5167999267578125
- Accuracy: 0.7523809523809524
- Precision: 0.7377049180327869
- Recall: 0.5555555555555556
- AUC: 0.8142525600535937
- F1: 0.6338028169014086

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/cjbarrie/autotrain-traintest-sentiment-split-1024534825
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""cjbarrie/autotrain-traintest-sentiment-split-1024534825"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""cjbarrie/autotrain-traintest-sentiment-split-1024534825"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,104074293.23392996,0.6880201277336947,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52636,autotrain_fashion_mnist_vit_base,"['abhishek/autotrain-data-vision_877913e77fb94b7abd4dafc5ebf830b0', 'fashion_mnist']",36530473.0,0.2438639401641305,AutoTrain,Not Specified,Not Specified,Not Specified,0.9473,0.1677586734294891,0.9473921270228504,,,343291569.0,True,990,1,"['transformers', 'pytorch']",2022-06-23 13:48:56+00:00,2022-06-23 12:59:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 7024732
- CO2 Emissions (in grams): 0.2438639401641305

## Validation Metrics

- Loss: 0.16775867342948914
- Accuracy: 0.9473333333333334
- Macro F1: 0.9473921270228505
- Micro F1: 0.9473333333333334
- Weighted F1: 0.9473921270228505
- Macro Precision: 0.9478705813419325
- Micro Precision: 0.9473333333333334
- Weighted Precision: 0.9478705813419323
- Macro Recall: 0.9473333333333332
- Micro Recall: 0.9473333333333334
- Weighted Recall: 0.9473333333333334",,,1,[],[],Computer Vision,2022-06,1407717634.5504408,0.9473460612716448,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52643,agric-eng-lug,['hellennamulinda/autotrain-data-agric-eng-lug'],,0.0408791067153807,AutoTrain,Not Specified,Not Specified,Not Specified,,1.0871405601501465,,0.558225,0.544274,308202053.0,True,2,0,"['transformers', 'pytorch']",2022-06-29 06:40:17+00:00,2022-06-23 13:50:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1026034854
- CO2 Emissions (in grams): 0.04087910671538076

## Validation Metrics

- Loss: 1.0871405601501465
- Rouge1: 55.8225
- Rouge2: 34.1547
- RougeL: 54.4274
- RougeLsum: 54.408
- Gen Len: 23.178

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/hellennamulinda/autotrain-agric-eng-lug-1026034854
```",,,1,[],[],NLP,2022-06,7539353908.730081,0.551161232164383,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52652,autotrain-formality-1026434913,['404E/autotrain-data-formality'],,7.300283563922049,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5467672348022461,,,,433328301.0,True,23,0,"['transformers', 'pytorch']",2022-06-23 15:19:21+00:00,2022-06-23 15:15:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1026434913
- CO2 Emissions (in grams): 7.300283563922049

## Validation Metrics

- Loss: 0.5467672348022461
- MSE: 0.5467672944068909
- MAE: 0.5851736068725586
- R2: 0.6883510493648173
- RMSE: 0.7394371628761292
- Explained Variance: 0.6885714530944824

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/404E/autotrain-formality-1026434913
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""404E/autotrain-formality-1026434913"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""404E/autotrain-formality-1026434913"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,59357735.51886471,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52785,autotrain-acronym-identification-7324788,"['lewtun/autotrain-data-acronym-identification', 'acronym_identification']",9733236.0,10.435358044493652,AutoTrain,Not Specified,Not Specified,Not Specified,0.9708090976211484,0.08991389721632,0.9151284109149278,,,430964529.0,True,127,0,"['transformers', 'pytorch']",2022-08-25 13:34:54+00:00,2022-06-24 10:11:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 7324788
- CO2 Emissions (in grams): 10.435358044493652

## Validation Metrics

- Loss: 0.08991389721632004
- Accuracy: 0.9708090976211485
- Precision: 0.8998421675654347
- Recall: 0.9309429854401959
- F1: 0.9151284109149278

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lewtun/autotrain-acronym-identification-7324788
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""lewtun/autotrain-acronym-identification-7324788"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lewtun/autotrain-acronym-identification-7324788"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,41298489.91883933,0.9421467920190124,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52841,autotrain-bert_wikipedia_sst_2-1034235509,['deepesh0x/autotrain-data-bert_wikipedia_sst_2'],,17.051424016530056,AutoTrain,Not Specified,Not Specified,Not Specified,0.954046028210839,0.1441494077444076,0.9588293980711672,,,438019245.0,True,19,0,"['transformers', 'pytorch']",2022-06-24 17:25:50+00:00,2022-06-24 17:17:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1034235509
- CO2 Emissions (in grams): 17.051424016530056

## Validation Metrics

- Loss: 0.14414940774440765
- Accuracy: 0.954046028210839
- Precision: 0.9583831937242387
- Recall: 0.9592760180995475
- AUC: 0.9872623710421541
- F1: 0.9588293980711673

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-bert_wikipedia_sst_2-1034235509
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-bert_wikipedia_sst_2-1034235509"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-bert_wikipedia_sst_2-1034235509"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,25688132.82546805,0.9564317324516948,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52842,autotrain-bert_wikipedia_sst_2-1034235513,['deepesh0x/autotrain-data-bert_wikipedia_sst_2'],,16.686945384446037,AutoTrain,Not Specified,Not Specified,Not Specified,0.952783964365256,0.1445064395666122,0.9577296291373122,,,438019245.0,True,15,0,"['transformers', 'pytorch']",2022-06-24 17:25:28+00:00,2022-06-24 17:17:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1034235513
- CO2 Emissions (in grams): 16.686945384446037

## Validation Metrics

- Loss: 0.14450643956661224
- Accuracy: 0.9527839643652561
- Precision: 0.9565852363250132
- Recall: 0.9588767633750332
- AUC: 0.9872179498202862
- F1: 0.9577296291373122

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-bert_wikipedia_sst_2-1034235513
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-bert_wikipedia_sst_2-1034235513"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-bert_wikipedia_sst_2-1034235513"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,26249216.67259002,0.9552503954359204,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52848,autotrain-finetunedmodelbert-1034335535,['deepesh0x/autotrain-data-finetunedmodelbert'],,7.180506910995884,AutoTrain,Not Specified,Not Specified,Not Specified,0.9793615441722346,0.0586655363440513,0.9815085805507516,,,267860081.0,True,4,0,"['transformers', 'pytorch']",2022-06-24 18:00:54+00:00,2022-06-24 17:56:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1034335535
- CO2 Emissions (in grams): 7.1805069109958835

## Validation Metrics

- Loss: 0.05866553634405136
- Accuracy: 0.9793615441722346
- Precision: 0.9811170212765957
- Recall: 0.9819004524886877
- AUC: 0.9976735725727466
- F1: 0.9815085805507516

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-finetunedmodelbert-1034335535
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-finetunedmodelbert-1034335535"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-finetunedmodelbert-1034335535"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,37303784.303836815,0.9804338869228058,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52855,autotrain-finetunedmodel1-1034535555,['deepesh0x/autotrain-data-finetunedmodel1'],,29.194903746653303,AutoTrain,Not Specified,Not Specified,Not Specified,0.9402375649591684,0.1642388701438903,0.9462939488958568,,,267860081.0,True,4,0,"['transformers', 'pytorch']",2022-06-24 18:57:34+00:00,2022-06-24 18:43:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1034535555
- CO2 Emissions (in grams): 29.194903746653306

## Validation Metrics

- Loss: 0.16423887014389038
- Accuracy: 0.9402375649591685
- Precision: 0.94876254180602
- Recall: 0.9438381687516636
- AUC: 0.9843968335444757
- F1: 0.9462939488958569

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-finetunedmodel1-1034535555
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-finetunedmodel1-1034535555"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-finetunedmodel1-1034535555"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,9174891.731941592,0.9432560354396606,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
52881,autotrain-finetuned1-1035435583,['AI-Prize-Challenges/autotrain-data-finetuned1'],,0.0360866056291979,AutoTrain,Not Specified,Not Specified,Not Specified,0.8816629547141797,0.315512865781784,0.8935772466283884,,,16339473.0,True,4,0,"['transformers', 'pytorch']",2022-06-24 23:26:04+00:00,2022-06-24 23:19:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1035435583
- CO2 Emissions (in grams): 0.03608660562919794

## Validation Metrics

- Loss: 0.31551286578178406
- Accuracy: 0.8816629547141797
- Precision: 0.8965702036441586
- Recall: 0.8906042054830983
- AUC: 0.9449180200540812
- F1: 0.8935772466283884

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/AI-Prize-Challenges/autotrain-finetuned1-1035435583
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""AI-Prize-Challenges/autotrain-finetuned1-1035435583"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""AI-Prize-Challenges/autotrain-finetuned1-1035435583"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,452784979.7759757,0.8875801200670511,0.0,1.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53048,autotrain-my-sum-1040935781,['p123/autotrain-data-my-sum'],,326.52733725745725,AutoTrain,Not Specified,Not Specified,Not Specified,,1.915754318237305,,0.004843,0.004843,4918578681.0,True,2,0,"['transformers', 'pytorch']",2022-06-26 18:02:45+00:00,2022-06-26 15:19:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1040935781
- CO2 Emissions (in grams): 326.52733725745725

## Validation Metrics

- Loss: 1.9157543182373047
- Rouge1: 0.4843
- Rouge2: 0.0
- RougeL: 0.4843
- RougeLsum: 0.4843
- Gen Len: 10.9718

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/p123/autotrain-my-sum-1040935781
```",,,1,[],[],NLP,2022-06,15063298.290157693,0.004843,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53099,autotrain-sum-1042335811,['zyxzyx/autotrain-data-sum'],,426.15271368095927,AutoTrain,Not Specified,Not Specified,Not Specified,,1.7748287916183472,,0.00536,0.00536,4918578681.0,True,2,0,"['transformers', 'pytorch']",2022-06-27 05:15:17+00:00,2022-06-27 01:25:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1042335811
- CO2 Emissions (in grams): 426.15271368095927

## Validation Metrics

- Loss: 1.7748287916183472
- Rouge1: 0.536
- Rouge2: 0.0
- RougeL: 0.536
- RougeLsum: 0.536
- Gen Len: 10.9089

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zyxzyx/autotrain-sum-1042335811
```",,,1,[],[],NLP,2022-06,11541821.800253304,0.00536,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53162,autotrain-oms-ner-bi-1044135953,['danielmantisnlp/autotrain-data-oms-ner-bi'],,1.425282392185522,AutoTrain,Not Specified,Not Specified,Not Specified,0.8957797220792589,0.4587894678115845,0.6102610261026103,,,435729969.0,True,8,0,"['transformers', 'pytorch']",2022-06-27 09:39:42+00:00,2022-06-27 09:38:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1044135953
- CO2 Emissions (in grams): 1.425282392185522

## Validation Metrics

- Loss: 0.4587894678115845
- Accuracy: 0.8957797220792589
- Precision: 0.553921568627451
- Recall: 0.6793587174348698
- F1: 0.6102610261026103

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/danielmantisnlp/autotrain-oms-ner-bi-1044135953
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""danielmantisnlp/autotrain-oms-ner-bi-1044135953"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""danielmantisnlp/autotrain-oms-ner-bi-1044135953"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,305714833.3474137,0.7259557259894075,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53247,bart_cnn_auto,['nizamudma/autotrain-data-text1'],,4581.794954519826,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-06-29 14:15:25+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53255,autotrain-glue1-1046836019,['deepesh0x/autotrain-data-glue1'],,3.869994913020229,AutoTrain,Not Specified,Not Specified,Not Specified,0.6606574761399788,0.626447856426239,0.750390015600624,,,438019245.0,True,14,0,"['transformers', 'pytorch']",2022-06-27 23:59:33+00:00,2022-06-27 23:57:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1046836019
- CO2 Emissions (in grams): 3.869994913020229

## Validation Metrics

- Loss: 0.626447856426239
- Accuracy: 0.6606574761399788
- Precision: 0.6925845932325414
- Recall: 0.8187234042553192
- AUC: 0.656404823892031
- F1: 0.750390015600624

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepesh0x/autotrain-glue1-1046836019
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""deepesh0x/autotrain-glue1-1046836019"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepesh0x/autotrain-glue1-1046836019"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,113183416.21750613,0.7026705716556888,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53351,dalle-mega,[''],,450300.0,Not Specified,Not Specified,East US,TTPU v3-256,,,,,,,False,74,124,"['transformers', 'jax']",2023-01-11 08:53:53+00:00,2022-06-28 14:07:04+00:00,"
# DALL·E Mega Model Card
This model card focuses on the DALL·E Mega model associated with the DALL·E mini space on Hugging Face, available [here](https://huggingface.co/spaces/dalle-mini/dalle-mini). The app is called “dalle-mini”, but  incorporates “[DALL·E Mini](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy)” and “[DALL·E Mega](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2)” models. The DALL·E Mega model is the largest version of DALLE Mini. For more information specific to DALL·E Mini, see the [DALL·E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).

## Model Details

* **Developed by:** Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc Lê, Luke, Luke Melas, Ritobrata Ghosh
* **Model type:** Transformer-based text-to-image generation model
* **Language(s):** English
* **License:** Apache 2.0
* **Model Description:** This is a model that can be used to generate images based on text prompts. As the model developers wrote in the [project report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy) about DALL·E mini, “OpenAI had the first impressive model for generating images with [DALL·E](https://openai.com/blog/dall-e/). DALL·E mini is an attempt at reproducing those results with an open-source model.”
* **Resources for more information:**
  - See OpenAI’s website for more information about [DALL·E](https://openai.com/blog/dall-e/), including the [DALL·E model card](https://github.com/openai/DALL-E/blob/master/model_card.md). 
  - See the DALL·E Mini [project report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy) for more information from the model’s developers about DALL·E Mini. 
  - To learn more about DALL·E Mega, see the DALL·E Mega [training journal](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training--VmlldzoxODMxMDI2#training-parameters).

* **Cite as:** 
```bib text
@misc{Dayma_DALL·E_Mini_2021,
      author = {Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and Lê Khắc, Phúc and Melas, Luke and Ghosh, Ritobrata},
      doi = {10.5281/zenodo.5146400},
      month = {7},
      title = {DALL·E Mini},
      url = {https://github.com/borisdayma/dalle-mini},
      year = {2021}
}
```

## Uses

### Direct Use

The model is intended to be used to generate images based on text prompts for research and personal consumption. Intended uses include supporting creativity, creating humorous content, and providing generations for people curious about the model’s behavior. Intended uses exclude those described in the [Misuse and Out-of-Scope Use](#misuse-malicious-use-and-out-of-scope-use) section.

### Downstream Use
The model could also be used for downstream use cases, including:
* Research efforts, such as probing and better understanding the limitations and biases of generative models to further improve the state of science
* Development of educational or creative tools
* Generation of artwork and use in design and artistic processes. 
* Other uses that are newly discovered by users. This currently includes poetry illustration (give a poem as prompt), fan art (putting a character in various other visual universes), visual puns, fairy tale illustrations (give a fantasy situation as prompt), concept mashups (applying a texture to something completely different), style transfers (portraits in the style of), … We hope you will find your own application!


Downstream uses exclude the uses described in [Misuse and Out-of-Scope Use](#misuse-malicious-use-and-out-of-scope-use).

### Misuse, Malicious Use, and Out-of-Scope Use

The model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes. 

#### Out-of-Scope Use

The model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.

#### Misuse and Malicious Use

Using the model to generate content that is cruel to individuals is a misuse of this model.
This includes:
* Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.
* Intentionally promoting or propagating discriminatory content or harmful stereotypes.
* Impersonating individuals without their consent.
* Sexual content without consent of the people who might see it.
* Mis- and disinformation
* Representations of egregious violence and gore
* Sharing of copyrighted or licensed material in violation of its terms of use.
* Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.


## Limitations and Bias

### Limitations

The model developers discuss the limitations of the model further in the DALL·E Mini [technical report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA):
* Faces and people in general are not generated properly.
* Animals are usually unrealistic.
* It is hard to predict where the model excels or falls short…Good prompt engineering will lead to the best results.
* The model has only been trained with English descriptions and will not perform as well in other languages


### Bias 
**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.** 

The model was trained on unfiltered data from the Internet, limited to pictures with English descriptions. Text and images from communities and cultures using other languages were not utilized. This affects all output of the model, with white and Western culture asserted as a default, and the model’s ability to generate content using non-English prompts is observably lower quality than prompts in English.

While the capabilities of image generation models are impressive, they may also reinforce or exacerbate societal biases. The extent and nature of the biases of DALL·E Mini and DALL·E Mega models have yet to be fully documented, but initial testing demonstrates that they may generate images that contain negative stereotypes against minoritized groups. Work to analyze the nature and extent of the models’ biases and limitations is ongoing.


Our current analyses demonstrate that:
* Images generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.
* When the model generates images with people in them, it tends to output people who we perceive to be white, while people of color are underrepresented. 
* Images generated by the model can contain biased content that depicts power differentials between people of color and people who are white, with white people in positions of privilege.
* The model is generally only usable for generating images based on text in English, limiting accessibility of the model for non-English speakers and potentially contributing to the biases in images generated by the model.

The [technical report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA) discusses these issues in more detail, and also highlights potential sources of bias in the model development process.


### Limitations and Bias Recommendations

* Users (both direct and downstream) should be made aware of the biases and limitations.
* Content that is potentially problematic should be filtered out, e.g., via automated models that detect violence or pornography.
* Further work on this model should include methods for balanced and just representations of people and cultures, for example, by curating the training dataset to be both diverse and inclusive.


## Training

### Training Data

For details on the DALL·E Mega training data, see the [DALL·E Mega training journal](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2#dall·e-mega---training).

## Training Procedure

The simplified training procedure for DALL·E Mega is as follows: 

* **Hardware:** 1 pod TPU v3-256 = 32 nodes of TPU VM v3-8 (8 TPU per node) = 256 TPU v3
* **Optimizer:** Distributed Shampoo
* **Model Partition Specificiations:** 8 model parallel x 32 data parallel
* **Batch:** 44 samples per model x 32 data parallel x 3 gradient accumulation steps =  4224 increasing samples per update
* **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant until plateau
* Gradient checkpointing used on each Encoder/Decoder layer (ie, MHA + FFN)
* Distributed Shampoo + Normformer Optimizations have proved to be effective and efficiently scaling this model. 
* It should also be noted that the learning rate and other parameters are sometimes adjusted on the fly, and batch size increased over time as well.

There is more information about the full procedure and technical material in the DALL·E Mega [training journal](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training--VmlldzoxODMxMDI2#training-parameters).

## Evaluation Results
For evaluation results related to DALL·E Mega, see this [technical report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy) and the [DALL·E Mega training journal](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2#dall·e-mega---training).


## Environmental Impact

DALL·E Mega is still training. So far, as of June 28, 2022, the model developers report that DALL·E Mega has been training for about 40-45 days on a TPU v3-256. Using those numbers, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.

* **Hardware Type:** TPU v3-256
* **Hours used:** 1344 hours (56 days)
* **Cloud Provider:** GCP
* **Compute Region:** us-east1
* **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid)**:  18013.47 kg CO2 eq.

## Citation

```bibtext
@misc{Dayma_DALL·E_Mini_2021,
      author = {Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and Lê Khắc, Phúc and Melas, Luke and Ghosh, Ritobrata},
      doi = {10.5281/zenodo.5146400},
      month = {7},
      title = {DALL·E Mini},
      url = {https://github.com/borisdayma/dalle-mini},
      year = {2021}
}
```

*This model card was written by: Boris Dayma, Margaret Mitchell, Ezi Ozoani, Marissa Gerchick, Irene Solaiman, Clémentine Fourrier, Sasha Luccioni, Emily Witko, Nazneen Rajani, and Julian Herrera.*


",** 1344 hours (56 days),** GCP,1,[],[],Multimodal,2022-06,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53674,autotrain-tweet-sentiment-classifier-1055036381,['kakashi210/autotrain-data-tweet-sentiment-classifier'],,17.43982800509071,AutoTrain,Not Specified,Not Specified,Not Specified,0.7306006137658921,0.6177256107330322,0.719534854339415,,,267863153.0,True,4,0,"['transformers', 'pytorch']",2022-06-29 17:54:00+00:00,2022-06-29 17:45:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1055036381
- CO2 Emissions (in grams): 17.43982800509071

## Validation Metrics

- Loss: 0.6177256107330322
- Accuracy: 0.7306006137658921
- Macro F1: 0.719534854339415
- Micro F1: 0.730600613765892
- Weighted F1: 0.7302204676842725
- Macro Precision: 0.714938066281146
- Micro Precision: 0.7306006137658921
- Weighted Precision: 0.7316651970219867
- Macro Recall: 0.7258484087500343
- Micro Recall: 0.7306006137658921
- Weighted Recall: 0.7306006137658921


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/kakashi210/autotrain-tweet-sentiment-classifier-1055036381
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kakashi210/autotrain-tweet-sentiment-classifier-1055036381"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kakashi210/autotrain-tweet-sentiment-classifier-1055036381"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,15359277.220039692,0.725025513503478,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53791,autotrain-country-recognition-1059336697,['mhaegeman/autotrain-data-country-recognition'],,0.0295218822349136,AutoTrain,Not Specified,Not Specified,Not Specified,0.9879569162920872,0.0610814839601516,0.9765004449554612,,,267918513.0,True,6,0,"['transformers', 'pytorch']",2022-06-30 08:35:28+00:00,2022-06-30 08:31:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1059336697
- CO2 Emissions (in grams): 0.02952188223491361

## Validation Metrics

- Loss: 0.06108148396015167
- Accuracy: 0.9879569162920872
- Macro F1: 0.9765004449554612
- Micro F1: 0.9879569162920872
- Weighted F1: 0.9879450113590053
- Macro Precision: 0.9784321161207384
- Micro Precision: 0.9879569162920872
- Weighted Precision: 0.9880404765946114
- Macro Recall: 0.9748417542427885
- Micro Recall: 0.9879569162920872
- Weighted Recall: 0.9879569162920872


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/mhaegeman/autotrain-country-recognition-1059336697
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mhaegeman/autotrain-country-recognition-1059336697"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mhaegeman/autotrain-country-recognition-1059336697"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,9075251735.919134,0.9821952742648284,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53826,autotrain-chinese-title-summarization-1060936832,['zhifei/autotrain-data-chinese-title-summarization'],,3.841483701875158,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5115200877189636,,0.273016,0.273016,1200743045.0,True,8,0,"['transformers', 'pytorch']",2022-06-30 12:23:58+00:00,2022-06-30 12:20:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1060936832
- CO2 Emissions (in grams): 3.841483701875158

## Validation Metrics

- Loss: 0.5115200877189636
- Rouge1: 27.3016
- Rouge2: 10.4762
- RougeL: 27.3016
- RougeLsum: 27.1111
- Gen Len: 14.3619

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zhifei/autotrain-chinese-title-summarization-1060936832
```",,,1,[],[],NLP,2022-06,312572729.2332066,0.273016,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53827,title_generator,['dddb/autotrain-data-mt5_chinese_small_finetune'],,0.2263611804615655,AutoTrain,Not Specified,Not Specified,Not Specified,,2.3939340114593506,,0.003375,0.003375,1200743045.0,True,6,0,"['transformers', 'pytorch']",2022-06-30 13:27:17+00:00,2022-06-30 13:00:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1060836848
- CO2 Emissions (in grams): 0.2263611804615655

## Validation Metrics

- Loss: 2.3939340114593506
- Rouge1: 0.3375
- Rouge2: 0.0
- RougeL: 0.3375
- RougeLsum: 0.3375
- Gen Len: 11.4395

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/dddb/autotrain-mt5_chinese_small_finetune-1060836848
```",,,1,[],[],NLP,2022-06,5304544898.34169,0.003375,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53829,autotrain-imdbtestmodel-9215210,['abhishek/autotrain-data-imdbtestmodel'],,0.2757084122251468,AutoTrain,Not Specified,Not Specified,Not Specified,0.9372,0.1699502319097519,0.9378857414147808,,,438006125.0,True,23,0,"['transformers', 'pytorch']",2022-06-30 13:36:05+00:00,2022-06-30 13:07:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 9215210
- CO2 Emissions (in grams): 0.2757084122251468

## Validation Metrics

- Loss: 0.1699502319097519
- Accuracy: 0.9372
- Precision: 0.9277551659361303
- Recall: 0.94824
- AUC: 0.9837227744
- F1: 0.9378857414147808

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/abhishek/autotrain-imdbtestmodel-9215210
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""abhishek/autotrain-imdbtestmodel-9215210"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""abhishek/autotrain-imdbtestmodel-9215210"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,1588657094.1561222,0.9375427453154476,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53845,country-recognition,['Maxbnza/autotrain-data-address-training'],,141.11976199388627,AutoTrain,Not Specified,Not Specified,Not Specified,0.9859325979151908,0.1014710962772369,0.9715036017680622,,,2239816045.0,True,11,1,"['transformers', 'pytorch']",2022-06-30 16:03:34+00:00,2022-06-30 14:59:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1062136864
- CO2 Emissions (in grams): 141.11976199388627

## Validation Metrics

- Loss: 0.10147109627723694
- Accuracy: 0.9859325979151907
- Macro F1: 0.9715036017680622
- Micro F1: 0.9859325979151907
- Weighted F1: 0.9859070541468058
- Macro Precision: 0.9732956651937184
- Micro Precision: 0.9859325979151907
- Weighted Precision: 0.9860574596777458
- Macro Recall: 0.970199341807239
- Micro Recall: 0.9859325979151907
- Weighted Recall: 0.9859325979151907


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Maxbnza/autotrain-address-training-1062136864
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Maxbnza/autotrain-address-training-1062136864"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Maxbnza/autotrain-address-training-1062136864"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-06,15871739.105519718,0.9786649190713292,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53932,autotrain-livedoor_news_summarization-1065437005,['kzkymn/autotrain-data-livedoor_news_summarization'],,1.854603770877255,AutoTrain,Not Specified,Not Specified,Not Specified,,2.017435312271118,,0.234405,0.231304,4918578681.0,True,4,0,"['transformers', 'pytorch']",2022-07-01 08:34:06+00:00,2022-07-01 04:52:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1065437005
- CO2 Emissions (in grams): 1.854603770877255

## Validation Metrics

- Loss: 2.017435312271118
- Rouge1: 23.4405
- Rouge2: 10.6415
- RougeL: 23.1304
- RougeLsum: 23.0871
- Gen Len: 16.8351

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/kzkymn/autotrain-livedoor_news_summarization-1065437005
```",,,1,[],[],NLP,2022-07,2652091383.7425447,0.2328441757406449,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53936,EN-ROM,['Tritkoman/autotrain-data-rusynpann'],,30.06853713677673,AutoTrain,Not Specified,Not Specified,Not Specified,,2.461327075958252,,,,4918480377.0,True,2,0,"['transformers', 'pytorch']",2022-07-01 06:07:37+00:00,2022-07-01 05:43:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1066237031
- CO2 Emissions (in grams): 30.068537136776726

## Validation Metrics

- Loss: 2.461327075958252
- SacreBLEU: 13.8452
- Gen len: 13.2313",,,1,[],[],NLP,2022-07,163575645.68660787,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53946,autotrain-60-50-1067437104,['scaccomatto/autotrain-data-60-50'],,29.54716889998106,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5487185120582581,,0.774054,0.771503,2283825905.0,True,2,0,"['transformers', 'pytorch']",2022-07-01 08:19:35+00:00,2022-07-01 08:04:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1067437104
- CO2 Emissions (in grams): 29.54716889998106

## Validation Metrics

- Loss: 0.5487185120582581
- Rouge1: 77.4054
- Rouge2: 74.6166
- RougeL: 77.1503
- RougeLsum: 76.8399
- Gen Len: 42.0326

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/scaccomatto/autotrain-60-50-1067437104
```",,,1,[],[],NLP,2022-07,77294237.9938629,0.772776394739243,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53948,autotrain-120-50-1067537149,['scaccomatto/autotrain-data-120-50'],,0.1197363010890659,AutoTrain,Not Specified,Not Specified,Not Specified,,0.4912683367729187,,0.80211,0.7953589999999999,2283825905.0,True,2,0,"['transformers', 'pytorch']",2022-07-01 08:48:11+00:00,2022-07-01 08:34:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1067537149
- CO2 Emissions (in grams): 0.11973630108906597

## Validation Metrics

- Loss: 0.4912683367729187
- Rouge1: 80.211
- Rouge2: 77.7552
- RougeL: 79.5359
- RougeLsum: 79.7243
- Gen Len: 87.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/scaccomatto/autotrain-120-50-1067537149
```",,,1,[],[],NLP,2022-07,19073797037.55149,0.7987202349341365,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53951,autotrain-120-0-1067937173,['scaccomatto/autotrain-data-120-0'],,0.0862544284419052,AutoTrain,Not Specified,Not Specified,Not Specified,,0.502437174320221,,0.837457,0.832649,2283825905.0,True,2,0,"['transformers', 'pytorch']",2022-07-01 09:09:50+00:00,2022-07-01 08:59:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1067937173
- CO2 Emissions (in grams): 0.08625442844190523

## Validation Metrics

- Loss: 0.502437174320221
- Rouge1: 83.7457
- Rouge2: 81.1714
- RougeL: 83.2649
- RougeLsum: 83.3018
- Gen Len: 78.7059

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/scaccomatto/autotrain-120-0-1067937173
```",,,1,[],[],NLP,2022-07,26477781445.60103,0.8350460792225164,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53960,autotrain-260-0-1068537269,['scaccomatto/autotrain-data-260-0'],,19.045065953636296,AutoTrain,Not Specified,Not Specified,Not Specified,,0.4295164048671722,,0.8543219999999999,0.848782,2283825905.0,True,2,0,"['transformers', 'pytorch']",2022-07-01 10:05:22+00:00,2022-07-01 09:53:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1068537269
- CO2 Emissions (in grams): 19.045065953636296

## Validation Metrics

- Loss: 0.42951640486717224
- Rouge1: 85.4322
- Rouge2: 82.999
- RougeL: 84.8782
- RougeLsum: 85.1256
- Gen Len: 169.2895

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/scaccomatto/autotrain-260-0-1068537269
```",,,1,[],[],NLP,2022-07,119916933.37055346,0.8515429895109166,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
53986,eng-lug,[''],,0.0408791067153807,AutoTrain,Not Specified,Not Specified,Not Specified,,1.0871405601501465,,0.558225,0.544274,308202053.0,True,2,0,"['transformers', 'pytorch']",2022-07-11 06:45:00+00:00,2022-07-01 13:10:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1026034854
- CO2 Emissions (in grams): 0.04087910671538076

## Validation Metrics

- Loss: 1.0871405601501465
- Rouge1: 55.8225
- Rouge2: 34.1547
- RougeL: 54.4274
- RougeLsum: 54.408
- Gen Len: 23.178


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/hellennamulinda/autotrain-eng-lug-1070637495
```",,,1,[],[],NLP,2022-07,7539353908.730081,0.551161232164383,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54006,autotrain-test_3-1071537591,['Luojike/autotrain-data-test_3'],,0.0398540179893401,AutoTrain,Not Specified,Not Specified,Not Specified,0.7389705882352942,0.5283975601196289,0.4180327868852458,,,409160877.0,True,14,0,"['transformers', 'pytorch']",2022-07-01 15:04:07+00:00,2022-07-01 14:59:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1071537591
- CO2 Emissions (in grams): 0.03985401798934018

## Validation Metrics

- Loss: 0.5283975601196289
- Accuracy: 0.7389705882352942
- Precision: 0.5032894736842105
- Recall: 0.3574766355140187
- AUC: 0.7135599403856304
- F1: 0.41803278688524587

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Luojike/autotrain-test_3-1071537591
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Luojike/autotrain-test_3-1071537591"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Luojike/autotrain-test_3-1071537591"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,10266489996.301975,0.53398968588842,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54015,autotrain-test-4-macbert-1071837613,['Luojike/autotrain-data-test-4-macbert'],,0.0122251179073363,AutoTrain,Not Specified,Not Specified,Not Specified,0.7408088235294118,0.533202052116394,0.4527813712807245,,,409160877.0,True,14,0,"['transformers', 'pytorch']",2022-07-01 15:45:50+00:00,2022-07-01 15:43:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1071837613
- CO2 Emissions (in grams): 0.012225117907336358

## Validation Metrics

- Loss: 0.533202052116394
- Accuracy: 0.7408088235294118
- Precision: 0.5072463768115942
- Recall: 0.4088785046728972
- AUC: 0.710585043624057
- F1: 0.4527813712807245

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Luojike/autotrain-test-4-macbert-1071837613
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Luojike/autotrain-test-4-macbert-1071837613"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Luojike/autotrain-test-4-macbert-1071837613"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,33468869593.02539,0.5620428794287525,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54148,autotrain-dataset-en-5-mini-1-50-truncate-1076038122,['scaccomatto/autotrain-data-dataset-en-5-mini-1-50-truncate'],,6.1987408118248375,AutoTrain,Not Specified,Not Specified,Not Specified,,0.5054866671562195,,0.764469,0.7631279999999999,1625557313.0,True,2,0,"['transformers', 'pytorch']",2022-07-02 14:59:36+00:00,2022-07-02 14:55:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1076038122
- CO2 Emissions (in grams): 6.1987408118248375

## Validation Metrics

- Loss: 0.5054866671562195
- Rouge1: 76.4469
- Rouge2: 72.6874
- RougeL: 76.3128
- RougeLsum: 76.2952
- Gen Len: 19.3856

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/scaccomatto/autotrain-dataset-en-5-mini-1-50-truncate-1076038122
```",,,1,[],[],NLP,2022-07,262239922.9693643,0.7637979114020255,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54150,autotrain-dataset-en-5-mini-1-50-num-1076338146,['scaccomatto/autotrain-data-dataset-en-5-mini-1-50-num'],,5.239170170576799,AutoTrain,Not Specified,Not Specified,Not Specified,,0.6177766919136047,,0.7640340000000001,0.7623300000000001,1625557313.0,True,2,0,"['transformers', 'pytorch']",2022-07-02 15:13:42+00:00,2022-07-02 15:10:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1076338146
- CO2 Emissions (in grams): 5.239170170576799

## Validation Metrics

- Loss: 0.6177766919136047
- Rouge1: 76.4034
- Rouge2: 72.6118
- RougeL: 76.233
- RougeLsum: 76.2601
- Gen Len: 18.6275

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/scaccomatto/autotrain-dataset-en-5-mini-1-50-num-1076338146
```",,,1,[],[],NLP,2022-07,310269996.9795096,0.7631810488454916,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54309,autotrain-sum-200-random-1082438930,['mf99/autotrain-data-sum-200-random'],,4.994502035089263,AutoTrain,Not Specified,Not Specified,Not Specified,,0.4404382705688476,,0.7845340000000001,0.782595,557979193.0,True,2,0,"['transformers', 'pytorch']",2022-07-04 07:26:22+00:00,2022-07-03 20:56:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1082438930
- CO2 Emissions (in grams): 4.994502035089263

## Validation Metrics

- Loss: 0.44043827056884766
- Rouge1: 78.4534
- Rouge2: 73.6511
- RougeL: 78.2595
- RougeLsum: 78.2561
- Gen Len: 17.2448

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/mf99/autotrain-sum-200-random-1082438930
```",,,1,[],[],NLP,2022-07,111718683.68054987,0.7835633004430395,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54371,distilbart-wikilingua-autotrain,['datien228/autotrain-data-summary-text'],,1850.790132860878,AutoTrain,Not Specified,Not Specified,Not Specified,,1.8720897436141968,,0.403451,0.309608,1222374713.0,True,3,0,"['transformers', 'pytorch']",2022-07-05 00:53:41+00:00,2022-07-04 08:45:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1079039131
- CO2 Emissions (in grams): 1850.790132860878

## Validation Metrics

- Loss: 1.8720897436141968
- Rouge1: 40.3451
- Rouge2: 17.4156
- RougeL: 30.9608
- RougeLsum: 38.8329
- Gen Len: 67.0434

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/datien228/autotrain-summary-text-1079039131
```",,,1,[],[],NLP,2022-07,660461.005976135,0.350354338723724,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54373,autotrain-chinese-title-summarization-1-1084539138,['zhifei/autotrain-data-chinese-title-summarization-1'],,0.004484038360707,AutoTrain,Not Specified,Not Specified,Not Specified,,0.7330857515335083,,0.222222,0.222222,1200743045.0,True,2,0,"['transformers', 'pytorch']",2022-07-04 08:49:18+00:00,2022-07-04 08:48:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1084539138
- CO2 Emissions (in grams): 0.004484038360707097

## Validation Metrics

- Loss: 0.7330857515335083
- Rouge1: 22.2222
- Rouge2: 10.0
- RougeL: 22.2222
- RougeLsum: 22.2222
- Gen Len: 13.7333

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zhifei/autotrain-chinese-title-summarization-1-1084539138
```",,,1,[],[],NLP,2022-07,267781617463.8284,0.222222,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54513,autotrain-kaggle-effective-arguments-1086739296,['Danitg95/autotrain-data-kaggle-effective-arguments'],,5.249720686430607,AutoTrain,Not Specified,Not Specified,Not Specified,0.6719238613188308,0.744236171245575,0.5450301061253738,,,263175281.0,True,4,0,"['transformers', 'pytorch']",2022-07-04 21:53:10+00:00,2022-07-04 21:49:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1086739296
- CO2 Emissions (in grams): 5.2497206864306065

## Validation Metrics

- Loss: 0.744236171245575
- Accuracy: 0.6719238613188308
- Macro F1: 0.5450301061253738
- Micro F1: 0.6719238613188308
- Weighted F1: 0.6349879540623229
- Macro Precision: 0.6691326843926052
- Micro Precision: 0.6719238613188308
- Weighted Precision: 0.6706209016443158
- Macro Recall: 0.5426627824078865
- Micro Recall: 0.6719238613188308
- Weighted Recall: 0.6719238613188308


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Danitg95/autotrain-kaggle-effective-arguments-1086739296
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Danitg95/autotrain-kaggle-effective-arguments-1086739296"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Danitg95/autotrain-kaggle-effective-arguments-1086739296"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,50131292.06669056,0.6018612753478103,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54532,autotrain-chineses-title-summarization-3-1087939403,['zhifei/autotrain-data-chineses-title-summarization-3'],,0.0049000878426465,AutoTrain,Not Specified,Not Specified,Not Specified,,0.1637328416109085,,0.238095,0.238095,1200743045.0,True,2,0,"['transformers', 'pytorch']",2022-07-05 02:45:16+00:00,2022-07-05 02:44:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1087939403
- CO2 Emissions (in grams): 0.004900087842646563

## Validation Metrics

- Loss: 0.1637328416109085
- Rouge1: 23.8095
- Rouge2: 15.0794
- RougeL: 23.8095
- RougeLsum: 23.8095
- Gen Len: 16.7143

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zhifei/autotrain-chineses-title-summarization-3-1087939403
```",,,1,[],[],NLP,2022-07,245045208077.6316,0.238095,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54549,autotrain-test-1088139436,['dddb/autotrain-data-test'],,0.122040594036971,AutoTrain,Not Specified,Not Specified,Not Specified,,2.2693707942962646,,0.004566,0.004566,1200743045.0,True,2,0,"['transformers', 'pytorch']",2022-07-05 05:34:17+00:00,2022-07-05 05:20:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1088139436
- CO2 Emissions (in grams): 0.12204059403697107

## Validation Metrics

- Loss: 2.2693707942962646
- Rouge1: 0.4566
- Rouge2: 0.0
- RougeL: 0.4566
- RougeLsum: 0.4566
- Gen Len: 11.5092

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/dddb/autotrain-test-1088139436
```",,,1,[],[],NLP,2022-07,9838882336.448198,0.004566,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54596,autotrain-iris-logistic-regression,"['abhishek/autotrain-data-iris-train', 'scikit-learn/iris']",,0.0006300767567816,AutoTrain,Not Specified,Not Specified,Not Specified,0.9,0.1598750532585615,0.899749373433584,,,,True,14,0,"['joblib', 'transformers']",2022-07-05 11:58:57+00:00,2022-07-05 11:36:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 9705273
- CO2 Emissions (in grams): 0.0006300767567816624

## Validation Metrics

- Loss: 0.15987505325856152
- Accuracy: 0.9
- Macro F1: 0.899749373433584
- Micro F1: 0.9
- Weighted F1: 0.8997493734335841
- Macro Precision: 0.9023569023569024
- Micro Precision: 0.9
- Weighted Precision: 0.9023569023569025
- Macro Recall: 0.9
- Micro Recall: 0.9
- Weighted Recall: 0.9

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-07,,0.8998746692661189,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54598,autotrain-iris-knn,"['abhishek/autotrain-data-iris-train', 'scikit-learn/iris']",,0.1502870119905602,AutoTrain,Not Specified,Not Specified,Not Specified,0.9,0.1562271391676219,0.899749373433584,,,,True,4,0,"['joblib', 'transformers']",2022-07-05 11:59:16+00:00,2022-07-05 11:37:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 9705277
- CO2 Emissions (in grams): 0.15028701199056024

## Validation Metrics

- Loss: 0.15622713916762193
- Accuracy: 0.9
- Macro F1: 0.899749373433584
- Micro F1: 0.9
- Weighted F1: 0.8997493734335841
- Macro Precision: 0.9023569023569024
- Micro Precision: 0.9
- Weighted Precision: 0.9023569023569024
- Macro Recall: 0.9
- Micro Recall: 0.9
- Weighted Recall: 0.9

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-07,,0.8998746692661189,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54600,autotrain-iris-xgboost,"['abhishek/autotrain-data-iris-train', 'scikit-learn/iris']",,1.9138035947108896,AutoTrain,Not Specified,Not Specified,Not Specified,0.8666666666666667,0.2559724063922962,0.8666666666666668,,,,True,5,1,"['joblib', 'transformers']",2022-07-05 12:00:49+00:00,2022-07-05 11:37:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 9705278
- CO2 Emissions (in grams): 1.9138035947108896

## Validation Metrics

- Loss: 0.2559724063922962
- Accuracy: 0.8666666666666667
- Macro F1: 0.8666666666666668
- Micro F1: 0.8666666666666667
- Weighted F1: 0.8666666666666667
- Macro Precision: 0.8666666666666667
- Micro Precision: 0.8666666666666667
- Weighted Precision: 0.8666666666666667
- Macro Recall: 0.8666666666666667
- Micro Recall: 0.8666666666666667
- Weighted Recall: 0.8666666666666667

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-07,,0.8666666666666667,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54605,autotrain-adult-census-xgboost,"['abhishek/autotrain-data-adult-train', 'scikit-learn/adult-census-income']",,0.1269359057786197,AutoTrain,Not Specified,Not Specified,Not Specified,0.8750191923844618,0.267161820562134,0.7191166321601105,,,,True,48,0,"['joblib', 'transformers']",2022-07-05 17:14:07+00:00,2022-07-05 12:06:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 9725286
- CO2 Emissions (in grams): 0.12693590577861977

## Validation Metrics

- Loss: 0.26716182056213406
- Accuracy: 0.8750191923844618
- Precision: 0.7840481565086531
- Recall: 0.6641172721478649
- AUC: 0.9345322809861784
- F1: 0.7191166321601105

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-07,,0.7894444689275351,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54624,autotrain-Text-Generate-1089139622,['tho-clare/autotrain-data-Text-Generate'],,7.256654556879194,AutoTrain,Not Specified,Not Specified,Not Specified,,2.43980360031128,,0.154155,0.1232569999999999,2950904711.0,True,5,0,"['transformers', 'pytorch']",2022-07-05 14:47:38+00:00,2022-07-05 14:42:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1089139622
- CO2 Emissions (in grams): 7.2566545568791945

## Validation Metrics

- Loss: 2.4398036003112793
- Rouge1: 15.4155
- Rouge2: 6.5786
- RougeL: 12.3257
- RougeLsum: 13.9424
- Gen Len: 19.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/tho-clare/autotrain-Text-Generate-1089139622
```",,,1,[],[],NLP,2022-07,406648089.4012777,0.1369852986532666,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54737,autotrain-deephate2-1093539673,['dee4hf/autotrain-data-deephate2'],,7.663051290039914,AutoTrain,Not Specified,Not Specified,Not Specified,0.8843120070113936,0.3440411984920501,0.8771237753798016,,,71791313.0,True,4,0,"['transformers', 'pytorch']",2022-07-06 04:28:59+00:00,2022-07-06 04:25:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1093539673
- CO2 Emissions (in grams): 7.663051290039914

## Validation Metrics

- Loss: 0.34404119849205017
- Accuracy: 0.8843120070113936
- Macro F1: 0.8771237753798016
- Micro F1: 0.8843120070113936
- Weighted F1: 0.8843498914288083
- Macro Precision: 0.8745249813256932
- Micro Precision: 0.8843120070113936
- Weighted Precision: 0.8854719661321065
- Macro Recall: 0.8812563739901838
- Micro Recall: 0.8843120070113936
- Weighted Recall: 0.8843120070113936


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dee4hf/autotrain-deephate2-1093539673
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dee4hf/autotrain-deephate2-1093539673"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dee4hf/autotrain-deephate2-1093539673"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,9368502.217036065,0.8807032239921417,0.0,1.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54926,autotrain-ZuoZhuan-1100540141,['ScarlettSun9/autotrain-data-ZuoZhuan'],,8.343592303925112,AutoTrain,Not Specified,Not Specified,Not Specified,0.8795777325860159,0.3809488415718078,0.8292385373953709,,,496390961.0,True,15,0,"['transformers', 'pytorch']",2022-07-07 07:08:04+00:00,2022-07-07 07:02:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1100540141
- CO2 Emissions (in grams): 8.343592303925112

## Validation Metrics

- Loss: 0.38094884157180786
- Accuracy: 0.8795777325860159
- Precision: 0.8171375141922127
- Recall: 0.8417033571821684
- F1: 0.8292385373953709

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ScarlettSun9/autotrain-ZuoZhuan-1100540141
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ScarlettSun9/autotrain-ZuoZhuan-1100540141"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ScarlettSun9/autotrain-ZuoZhuan-1100540141"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,59493674.05769343,0.853666675941831,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54927,autotrain-ZuoZhuan-1100540143,['ScarlettSun9/autotrain-data-ZuoZhuan'],,14.50120424968173,AutoTrain,Not Specified,Not Specified,Not Specified,0.8799234894798035,0.3792617619037628,0.8273035872656656,,,496390961.0,True,14,0,"['transformers', 'pytorch']",2022-07-07 07:11:00+00:00,2022-07-07 07:03:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1100540143
- CO2 Emissions (in grams): 14.50120424968173

## Validation Metrics

- Loss: 0.3792617619037628
- Accuracy: 0.8799234894798035
- Precision: 0.8133982801130555
- Recall: 0.8416925948973242
- F1: 0.8273035872656656

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ScarlettSun9/autotrain-ZuoZhuan-1100540143
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ScarlettSun9/autotrain-ZuoZhuan-1100540143"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ScarlettSun9/autotrain-ZuoZhuan-1100540143"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,34231016.43512777,0.85280261692393,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54966,autotrain-chinese-title-summarization-8-1101140174,['zhifei/autotrain-data-chinese-title-summarization-8'],,1.4118255120710663,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0049639358185231,,0.493333,0.493333,1200743045.0,True,2,0,"['transformers', 'pytorch']",2022-07-07 10:21:29+00:00,2022-07-07 10:19:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1101140174
- CO2 Emissions (in grams): 1.4118255120710663

## Validation Metrics

- Loss: 0.0049639358185231686
- Rouge1: 49.3333
- Rouge2: 26.6667
- RougeL: 49.3333
- RougeLsum: 49.3333
- Gen Len: 15.12

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zhifei/autotrain-chinese-title-summarization-8-1101140174
```",,,1,[],[],NLP,2022-07,850489692.0573275,0.4933330000000001,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
54972,autotrain-autotrain-chinese-title-summarization-9-1101340178,['zhifei/autotrain-data-autotrain-chinese-title-summarization-9'],,1.565396518204961,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0001277882111025,,0.292308,0.292308,1200743045.0,True,2,0,"['transformers', 'pytorch']",2022-07-07 10:49:19+00:00,2022-07-07 10:48:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1101340178
- CO2 Emissions (in grams): 1.565396518204961

## Validation Metrics

- Loss: 0.00012778821110259742
- Rouge1: 29.2308
- Rouge2: 0.0
- RougeL: 29.2308
- RougeLsum: 29.2308
- Gen Len: 18.4462

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zhifei/autotrain-autotrain-chinese-title-summarization-9-1101340178
```",,,1,[],[],NLP,2022-07,767053606.5692104,0.292308,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
55053,autotrain-amazon-shoe-reviews-classification-1104340243,['mbyanfei/autotrain-data-amazon-shoe-reviews-classification'],,27.982443349742287,AutoTrain,Not Specified,Not Specified,Not Specified,0.5843,0.9584922790527344,0.5801009597024507,,,498683309.0,True,17,0,"['transformers', 'pytorch']",2022-07-07 20:02:39+00:00,2022-07-07 19:48:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1104340243
- CO2 Emissions (in grams): 27.982443349742287

## Validation Metrics

- Loss: 0.9584922790527344
- Accuracy: 0.5843
- Macro F1: 0.5801009597024507
- Micro F1: 0.5843
- Weighted F1: 0.5792137097243996
- Macro Precision: 0.5897236028586046
- Micro Precision: 0.5843
- Weighted Precision: 0.5896188517045103
- Macro Recall: 0.5857983081566331
- Micro Recall: 0.5843
- Weighted Recall: 0.5843


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/mbyanfei/autotrain-amazon-shoe-reviews-classification-1104340243
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mbyanfei/autotrain-amazon-shoe-reviews-classification-1104340243"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mbyanfei/autotrain-amazon-shoe-reviews-classification-1104340243"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,17821292.542868413,0.5821929086021321,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
55227,autotrain-jk123-1105140277,['jk-gjom/autotrain-data-jk123'],,0.1863935648335355,AutoTrain,Not Specified,Not Specified,Not Specified,0.9808,0.0680043175816536,0.9808013970263608,,,409156973.0,True,14,0,"['transformers', 'pytorch']",2022-07-08 13:22:03+00:00,2022-07-08 12:59:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1105140277
- CO2 Emissions (in grams): 0.1863935648335355

## Validation Metrics

- Loss: 0.0680043175816536
- Accuracy: 0.9808
- Macro F1: 0.9808013970263609
- Micro F1: 0.9808
- Weighted F1: 0.9808013970263609
- Macro Precision: 0.9808207901614748
- Micro Precision: 0.9808
- Weighted Precision: 0.9808207901614749
- Macro Recall: 0.9808
- Micro Recall: 0.9808
- Weighted Recall: 0.9808


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jk-gjom/autotrain-jk123-1105140277
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jk-gjom/autotrain-jk123-1105140277"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jk-gjom/autotrain-jk123-1105140277"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,2195123921.6086144,0.9808006985126828,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
57186,autotrain-first-test-html-1136241676,['aalbertini1990/autotrain-data-first-test-html'],,684.7105644305452,AutoTrain,Not Specified,Not Specified,Not Specified,,0.2270897775888443,,0.634452,0.633343,1625537793.0,True,5,0,"['transformers', 'pytorch']",2022-07-15 17:59:11+00:00,2022-07-15 12:45:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1136241676
- CO2 Emissions (in grams): 684.7105644305452

## Validation Metrics

- Loss: 0.2270897775888443
- Rouge1: 63.4452
- Rouge2: 60.0038
- RougeL: 63.3343
- RougeLsum: 63.321
- Gen Len: 19.1562

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aalbertini1990/autotrain-first-test-html-1136241676
```",,,1,[],[],NLP,2022-07,2374050.989489135,0.6338970149527329,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
57187,autotrain-first-test-html-1136241677,['aalbertini1990/autotrain-data-first-test-html'],,19.49742293318862,AutoTrain,Not Specified,Not Specified,Not Specified,,0.1886099278926849,,0.842283,0.839066,2283800049.0,True,2,0,"['transformers', 'pytorch']",2022-07-16 21:16:30+00:00,2022-07-15 12:46:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1136241677
- CO2 Emissions (in grams): 19.49742293318862

## Validation Metrics

- Loss: 0.18860992789268494
- Rouge1: 84.2283
- Rouge2: 80.2825
- RougeL: 83.9066
- RougeLsum: 83.9129
- Gen Len: 58.3175

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aalbertini1990/autotrain-first-test-html-1136241677
```",,,1,[],[],NLP,2022-07,117133431.26554962,0.8406714223852394,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
57383,autotrain-article_pred-1142742075,['Johny201/autotrain-data-article_pred'],,3.973071565343572,AutoTrain,Not Specified,Not Specified,Not Specified,0.7227722772277227,0.6098461151123047,0.7777777777777779,,,1421584365.0,True,6,0,"['transformers', 'pytorch']",2022-07-17 10:31:21+00:00,2022-07-17 10:28:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1142742075
- CO2 Emissions (in grams): 3.973071565343572

## Validation Metrics

- Loss: 0.6098461151123047
- Accuracy: 0.7227722772277227
- Precision: 0.6805555555555556
- Recall: 0.9074074074074074
- AUC: 0.7480299448384554
- F1: 0.7777777777777779

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Johny201/autotrain-article_pred-1142742075
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Johny201/autotrain-article_pred-1142742075"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Johny201/autotrain-article_pred-1142742075"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,357804872.5324353,0.7492668621700881,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
57549,gs3n-roberta-model,['erixxdp/autotrain-data-gsemodel'],,0.0278462829709136,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,435226605.0,False,14,0,"['transformers', 'pytorch']",2022-07-18 16:46:02+00:00,2022-07-18 16:34:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1148842296
- CO2 Emissions (in grams): 0.027846282970913613

## Validation Metrics

- Loss: 0.4816772937774658
- Accuracy: 0.864
- Macro F1: 0.865050349743783
- Micro F1: 0.864
- Weighted F1: 0.865050349743783
- Macro Precision: 0.8706266090178479
- Micro Precision: 0.864
- Weighted Precision: 0.8706266090178482
- Macro Recall: 0.864
- Micro Recall: 0.864
- Weighted Recall: 0.864


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/erixxdp/autotrain-gsemodel-1148842296
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""erixxdp/autotrain-gsemodel-1148842296"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""erixxdp/autotrain-gsemodel-1148842296"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,15629612234.229212,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
57620,autotrain-CompanyDescription-1149642380,['vencortexTeam/autotrain-data-CompanyDescription'],,4.803822525731932,AutoTrain,Not Specified,Not Specified,Not Specified,,1.1474181413650513,,0.578827,0.5642090000000001,557969145.0,True,2,0,"['transformers', 'pytorch']",2022-07-19 15:24:12+00:00,2022-07-19 05:44:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1149642380
- CO2 Emissions (in grams): 4.803822525731932

## Validation Metrics

- Loss: 1.1474181413650513
- Rouge1: 57.8827
- Rouge2: 46.6881
- RougeL: 56.4209
- RougeLsum: 56.4665
- Gen Len: 18.0731

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/vencortexTeam/autotrain-CompanyDescription-1149642380
```",,,1,[],[],NLP,2022-07,116151073.86070332,0.5714245270367688,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
57746,autotrain-argument-feedback-1154042510,['snap/autotrain-data-argument-feedback'],,39.98165454365982,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-07-19 19:09:36+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
57747,autotrain-argument-feedback-1154042511,['snap/autotrain-data-argument-feedback'],,50.94211122225772,AutoTrain,Not Specified,Not Specified,Not Specified,0.672331747110809,0.7391096353530884,0.5302988889038903,,,1340715821.0,True,15,0,"['transformers', 'pytorch']",2022-07-19 19:17:44+00:00,2022-07-19 18:53:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1154042511
- CO2 Emissions (in grams): 50.942111222257715

## Validation Metrics

- Loss: 0.7391096353530884
- Accuracy: 0.672331747110809
- Macro F1: 0.5302988889038903
- Micro F1: 0.672331747110809
- Weighted F1: 0.628333699928783
- Macro Precision: 0.6916740835116003
- Micro Precision: 0.672331747110809
- Weighted Precision: 0.6810625595246631
- Macro Recall: 0.5264405918447705
- Micro Recall: 0.672331747110809
- Weighted Recall: 0.672331747110809


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/snap/autotrain-argument-feedback-1154042511
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""snap/autotrain-argument-feedback-1154042511"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""snap/autotrain-argument-feedback-1154042511"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,26318418.86470956,0.592928148993729,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
57835,autotrain-Flexport_Classification_Desc-1155542601,['ar2rpapian/autotrain-data-Flexport_Classification_Desc'],,206.60369255723003,AutoTrain,Not Specified,Not Specified,Not Specified,0.9578838092484788,0.2210556864738464,0.9360695960738428,,,439645613.0,True,23,0,"['transformers', 'pytorch']",2022-07-20 10:12:11+00:00,2022-07-20 08:32:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1155542601
- CO2 Emissions (in grams): 206.60369255723003

## Validation Metrics

- Loss: 0.22105568647384644
- Accuracy: 0.9578838092484789
- Macro F1: 0.9360695960738429
- Micro F1: 0.9578838092484788
- Weighted F1: 0.957863360811612
- Macro Precision: 0.9415730549729362
- Micro Precision: 0.9578838092484789
- Weighted Precision: 0.9586754512711492
- Macro Recall: 0.9329742157218464
- Micro Recall: 0.9578838092484789
- Weighted Recall: 0.9578838092484789


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ar2rpapian/autotrain-Flexport_Classification_Desc-1155542601
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ar2rpapian/autotrain-Flexport_Classification_Desc-1155542601"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ar2rpapian/autotrain-Flexport_Classification_Desc-1155542601"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,2127965.902052871,0.9468510765778868,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58035,autotrain-summtest1-11405516,['abhishek/autotrain-data-summtest1'],,28.375764585180136,AutoTrain,Not Specified,Not Specified,Not Specified,,1.5257819890975952,,0.419534,0.347507,990450547.0,True,2,0,"['transformers', 'pytorch']",2022-07-21 12:55:20+00:00,2022-07-21 12:33:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 11405516
- CO2 Emissions (in grams): 28.375764585180136

## Validation Metrics

- Loss: 1.5257819890975952
- Rouge1: 41.9534
- Rouge2: 18.5044
- RougeL: 34.7507
- RougeLsum: 38.6091
- Gen Len: 15.1037

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/abhishek/autotrain-summtest1-11405516
```",,,1,[],[],NLP,2022-07,34904805.614199534,0.3801387454855738,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58203,autotrain-imdb-1166543171,['ameerazam08/autotrain-data-imdb'],,0.0730830214040682,AutoTrain,Not Specified,Not Specified,Not Specified,0.9138,0.2211569994688034,0.9150404100137984,,,267854321.0,True,4,0,"['transformers', 'pytorch']",2022-07-22 11:56:54+00:00,2022-07-22 11:46:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1166543171
- CO2 Emissions (in grams): 0.07308302140406821

## Validation Metrics

- Loss: 0.2211569994688034
- Accuracy: 0.9138
- Precision: 0.9020598523124758
- Recall: 0.9284
- AUC: 0.9711116000000001
- F1: 0.9150404100137985

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ameerazam08/autotrain-imdb-1166543171
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ameerazam08/autotrain-imdb-1166543171"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ameerazam08/autotrain-imdb-1166543171"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,3665069066.029196,0.9144197843531904,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58245,auditor_sentiment_finetuned,"['rajistics/autotrain-data-auditor-sentiment', 'FinanceInc/auditor_sentiment']",,3.165771608457648,AutoTrain,Not Specified,Not Specified,Not Specified,0.848937,0.3418470025062561,0.8448284352912685,,,439087469.0,True,76,8,"['transformers', 'pytorch']",2022-12-12 17:30:43+00:00,2022-07-22 16:42:57+00:00,"
# Auditor Review Sentiment Model

This model has been finetuned from the proprietary version of [FinBERT](https://huggingface.co/FinanceInc/finbert-pretrain) trained internally using demo.org proprietary dataset of auditor evaluation of sentiment.  

FinBERT is a BERT model pre-trained on a large corpora of financial texts. The purpose is to enhance financial NLP research and practice in the financial domain, hoping that financial practitioners and researchers can benefit from this model without the necessity of the significant computational resources required to train the model.

# Training Data

This model was fine-tuned using [Autotrain](https://ui.autotrain.huggingface.co/11671/metrics) from the demo-org/auditor_review review dataset.  

# Model Status
This model is currently being evaluated in development until the end of the quarter.  Based on the results, it may be elevated to production.


### Training hyperparameters
The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 16
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 4
- mixed_precision_training: Native AMP


# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: [1167143226](https://huggingface.co/rajistics/autotrain-auditor-sentiment-1167143226)
- CO2 Emissions (in grams): 3.165771608457648

## Validation Metrics

- Loss: 0.3418470025062561
- Accuracy: 0.8617131062951496
- Macro F1: 0.8448284352912685
- Micro F1: 0.8617131062951496
- Weighted F1: 0.8612696670395574
- Macro Precision: 0.8440532616584138
- Micro Precision: 0.8617131062951496
- Weighted Precision: 0.8612762332366959
- Macro Recall: 0.8461980005490884
- Micro Recall: 0.8617131062951496
- Weighted Recall: 0.8617131062951496


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/rajistics/autotrain-auditor-sentiment-1167143226
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""rajistics/autotrain-auditor-sentiment-1167143226"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""rajistics/autotrain-auditor-sentiment-1167143226"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,138698403.8352412,0.8468777345754837,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58310,Sentence_Classification4DesignTutor,['Shenzy/autotrain-data-sentence_classification'],,0.0098649438704349,AutoTrain,Not Specified,Not Specified,Not Specified,0.8263473053892215,0.6447726488113403,0.7776555055392036,,,1421588461.0,True,12,0,"['transformers', 'pytorch']",2022-07-26 03:25:26+00:00,2022-07-23 03:58:41+00:00,"
## Validation Metrics

- Loss: 0.6447726488113403
- Accuracy: 0.8263473053892215
- Macro F1: 0.7776555055392036
- Micro F1: 0.8263473053892215
- Weighted F1: 0.8161511591973788
- Macro Precision: 0.8273504273504274
- Micro Precision: 0.8263473053892215
- Weighted Precision: 0.8266697374481806
- Macro Recall: 0.7615518744551003
- Micro Recall: 0.8263473053892215
- Weighted Recall: 0.8263473053892215


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""An unusual hierarchy in the section near the top where the design seems to prioritise running time over a compacted artist name.""}' https://api-inference.huggingface.co/models/Shenzy/Sentence_Classification4DesignTutor
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

import numpy as np

labdic ={ 0: ""rationale"", 1: ""suggestion"", 2: ""specific_critique""}

model = AutoModelForSequenceClassification.from_pretrained(""Shenzy/Sentence_Classification4DesignTutor"")

tokenizer = AutoTokenizer.from_pretrained(""Shenzy/Sentence_Classification4DesignTutor"")

inputs = tokenizer(""An unusual hierarchy in the section near the top where the design seems to prioritise running time over a compacted artist name."", return_tensors=""pt"")

outputs = model(**inputs)

print(labdic[np.argmax(outputs)])
```",,,1,[],[],NLP,2022-07,144105073447.0439,0.8012623508452055,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58323,NER4DesignTutor,['Shenzy2/autotrain-data-NER4DesignTutor'],,0.0040326569882286,AutoTrain,Not Specified,Not Specified,Not Specified,0.8129095674967235,0.677674412727356,0.4625346901017577,,,430970673.0,True,34,0,"['transformers', 'pytorch']",2022-07-26 03:23:50+00:00,2022-07-23 06:04:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1169643336
- CO2 Emissions (in grams): 0.004032656988228696

## Validation Metrics

- Loss: 0.677674412727356
- Accuracy: 0.8129095674967235
- Precision: 0.4424778761061947
- Recall: 0.4844961240310077
- F1: 0.4625346901017577

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""Why is the username the largest part of each card?""}' https://api-inference.huggingface.co/models/Shenzy2/NER4DesignTutor
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Shenzy2/NER4DesignTutor"")

tokenizer = AutoTokenizer.from_pretrained(""Shenzy2/NER4DesignTutor"")

inputs = tokenizer(""Why is the username the largest part of each card?"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,106870153910.43698,0.5895967191711141,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58379,autotrain-amazon-summarization-1170943400,['jcashmoney123/autotrain-data-amazon-summarization'],,25.718350806012065,AutoTrain,Not Specified,Not Specified,Not Specified,,2.569204092025757,,0.21072,0.189156,2283800049.0,True,2,0,"['transformers', 'pytorch']",2022-07-23 18:06:12+00:00,2022-07-23 17:53:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1170943400
- CO2 Emissions (in grams): 25.718350806012065

## Validation Metrics

- Loss: 2.569204092025757
- Rouge1: 21.072
- Rouge2: 6.2072
- RougeL: 18.9156
- RougeLsum: 18.8997
- Gen Len: 10.7165

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/jcashmoney123/autotrain-amazon-summarization-1170943400
```",,,1,[],[],NLP,2022-07,88800408.16871221,0.1993565621342616,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58383,autotrain-amz-1171143428,['jcashmoney123/autotrain-data-amz'],,5.4331208624177245,AutoTrain,Not Specified,Not Specified,Not Specified,,2.5859596729278564,,0.193601,0.174309,1625533697.0,True,2,0,"['transformers', 'pytorch']",2022-07-23 18:31:20+00:00,2022-07-23 18:27:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1171143428
- CO2 Emissions (in grams): 5.4331208624177245

## Validation Metrics

- Loss: 2.5859596729278564
- Rouge1: 19.3601
- Rouge2: 4.6055
- RougeL: 17.4309
- RougeLsum: 17.4621
- Gen Len: 15.2938

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/jcashmoney123/autotrain-amz-1171143428
```",,,1,[],[],NLP,2022-07,299189680.87830126,0.1834491952325297,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58540,autotrain-MS2-1173943517,['ben-yu/autotrain-data-MS2'],,0.687008092853648,AutoTrain,Not Specified,Not Specified,Not Specified,,2.806302070617676,,0.000342,0.000242,1789277169.0,True,2,0,"['transformers', 'pytorch']",2022-07-25 01:31:42+00:00,2022-07-25 00:06:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1173943517
- CO2 Emissions (in grams): 0.687008092853648

## Validation Metrics

- Loss: 2.806302070617676
- Rouge1: 0.0342
- Rouge2: 0.006
- RougeL: 0.0242
- RougeLsum: 0.0283
- Gen Len: 19.9989

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/ben-yu/autotrain-MS2-1173943517
```",,,1,[],[],NLP,2022-07,2604448459.359221,0.0002834383561643,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58579,autotrain-ms-2-1174443640,['benjamyu/autotrain-data-ms-2'],,4.619328856849087,AutoTrain,Not Specified,Not Specified,Not Specified,,2.689530849456787,,0.159713,0.121778,891700799.0,True,2,0,"['transformers', 'pytorch']",2022-07-25 13:26:05+00:00,2022-07-25 03:54:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1174443640
- CO2 Emissions (in grams): 4.619328856849087

## Validation Metrics

- Loss: 2.689530849456787
- Rouge1: 15.9713
- Rouge2: 2.1067
- RougeL: 12.1778
- RougeLsum: 13.5772
- Gen Len: 18.9798

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/benjamyu/autotrain-ms-2-1174443640
```",,,1,[],[],NLP,2022-07,193036873.24141768,0.138189353933163,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58832,autotrain-summarization-test-1177043812,['singhajeet13/autotrain-data-summarization-test'],,1166.308824861558,AutoTrain,Not Specified,Not Specified,Not Specified,,1.6226013898849487,,0.395734,0.33257,1625537793.0,True,2,0,"['transformers', 'pytorch']",2022-07-26 02:15:55+00:00,2022-07-25 16:04:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1177043812
- CO2 Emissions (in grams): 1166.308824861558

## Validation Metrics

- Loss: 1.6226013898849487
- Rouge1: 39.5734
- Rouge2: 18.9817
- RougeL: 33.257
- RougeLsum: 33.2571
- Gen Len: 19.84

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/singhajeet13/autotrain-summarization-test-1177043812
```",,,1,[],[],NLP,2022-07,1393745.600092628,0.3614129714514817,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58835,test-model,['jcashmoney123/autotrain-data-test-summarization'],,6.160395825083539,AutoTrain,Not Specified,Not Specified,Not Specified,,2.9017226696014404,,0.216224,0.190725,1625533697.0,True,2,0,"['transformers', 'pytorch']",2022-07-25 16:16:07+00:00,2022-07-25 16:12:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1177143826
- CO2 Emissions (in grams): 6.160395825083539

## Validation Metrics

- Loss: 2.9017226696014404
- Rouge1: 21.6224
- Rouge2: 5.6481
- RougeL: 19.0725
- RougeLsum: 19.1428
- Gen Len: 12.0

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/jcashmoney123/autotrain-test-summarization-1177143826
```",,,1,[],[],NLP,2022-07,263868384.94715667,0.2026756296243509,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
58924,stress_twitter,['hsaglamlar/autotrain-data-stress_v2'],,2.7282806494855265,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,1421584365.0,False,6,1,"['transformers', 'pytorch']",2022-09-08 07:16:25+00:00,2022-07-25 20:53:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1178743973
- CO2 Emissions (in grams): 2.7282806494855265

## Validation Metrics

- Loss: 0.431733638048172
- Accuracy: 0.7976190476190477
- Precision: 0.6918918918918919
- Recall: 0.8205128205128205
- AUC: 0.8952141608391608
- F1: 0.7507331378299119

## Usage
This model finds self-reported stress from txt.


You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/hsaglamlar/autotrain-stress_v2-1178743973
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""hsaglamlar/autotrain-stress_v2-1178743973"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hsaglamlar/autotrain-stress_v2-1178743973"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,521055033.4211655,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
59053,autotrain-j-multi-classification-1181044057,['azizkh/autotrain-data-j-multi-classification'],,1.2309703499286415,AutoTrain,Not Specified,Not Specified,Not Specified,0.7192982456140351,0.896309494972229,0.5870079610791685,,,540868973.0,True,23,0,"['transformers', 'pytorch']",2022-07-26 11:34:22+00:00,2022-07-26 11:33:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1181044057
- CO2 Emissions (in grams): 1.2309703499286417

## Validation Metrics

- Loss: 0.896309494972229
- Accuracy: 0.7192982456140351
- Macro F1: 0.5870079610791685
- Micro F1: 0.7192982456140351
- Weighted F1: 0.719743631524632
- Macro Precision: 0.6779761904761905
- Micro Precision: 0.7192982456140351
- Weighted Precision: 0.8012949039264828
- Macro Recall: 0.5941468253968254
- Micro Recall: 0.7192982456140351
- Weighted Recall: 0.7192982456140351


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/azizkh/autotrain-j-multi-classification-1181044057
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""azizkh/autotrain-j-multi-classification-1181044057"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""azizkh/autotrain-j-multi-classification-1181044057"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,439384241.08375454,0.646454551623948,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
59060,autotrain-tk-1181244086,['Shenzy2/autotrain-data-tk'],,0.0046630444734851,AutoTrain,Not Specified,Not Specified,Not Specified,0.8263097949886105,0.5532978773117065,0.4883720930232558,,,430970673.0,True,8,0,"['transformers', 'pytorch']",2022-07-26 13:03:18+00:00,2022-07-26 13:02:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1181244086
- CO2 Emissions (in grams): 0.004663044473485149

## Validation Metrics

- Loss: 0.5532978773117065
- Accuracy: 0.8263097949886105
- Precision: 0.5104166666666666
- Recall: 0.4681528662420382
- F1: 0.4883720930232558

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Shenzy2/autotrain-tk-1181244086
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Shenzy2/autotrain-tk-1181244086"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Shenzy2/autotrain-tk-1181244086"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-07,92422595463.2368,0.6139076650313793,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
59067,distilgpt2,['openwebtext'],39769491688.0,149200.0,Not Specified,Not Specified,East US,** 8 16GB V100,,,,,,352833716.0,False,568,1,"['jax', 'transformers', 'pytorch', 'rust', 'tf']",2022-09-05 17:45:18+00:00,2019-10-03 14:08:13+00:00,"
# DistilGPT2

DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of [GPT-2](https://huggingface.co/gpt2).

## Model Details

- **Developed by:** Hugging Face
- **Model type:** Transformer-based Language Model
- **Language:** English
- **License:** Apache 2.0
- **Model Description:** DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters, was developed using [knowledge distillation](#knowledge-distillation) and was designed to be a faster, lighter version of GPT-2.
- **Resources for more information:** See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\* (a class of compressed models including Distilled-GPT2), [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure, and this page for more about [GPT-2](https://openai.com/blog/better-language-models/).

## Uses, Limitations and Risks

#### Limitations and Risks

<details>
<summary>Click to expand</summary>

**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**

As the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md), “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). 

DistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.

The impact of model compression techniques – such as knowledge distillation – on bias and fairness issues associated with language models is an active area of research. For example: 

- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.
- [Xu and Hu (2022)](https://arxiv.org/pdf/2201.08542.pdf) find that distilled versions of GPT-2 showed consistent reductions in toxicity and bias compared to the teacher model (see the paper for more detail on metrics used to define/measure toxicity and bias). 
- [Gupta et al. (2022)](https://arxiv.org/pdf/2203.12574.pdf) find that DistilGPT2 exhibits greater gender disparities than GPT-2 and propose a technique for mitigating gender bias in distilled language models like DistilGPT2. 

```python
>>> from transformers import pipeline, set_seed
>>> generator = pipeline('text-generation', model='distilgpt2')
>>> set_seed(48)
>>> generator(""The White man worked as a"", max_length=20, num_return_sequences=3)
[{'generated_text': ""The White man worked as a salesman at a McDonald's restaurant called Kia at the time of the""},
 {'generated_text': 'The White man worked as a contractor in the Army in the late 1990s. He became a ""'},
 {'generated_text': 'The White man worked as a police spokesman to the US Navy in the 1930s.'}]
 
>>> set_seed(48)
>>> generator(""The Black man worked as a"", max_length=20, num_return_sequences=3)
[{'generated_text': 'The Black man worked as a shop assistant for an hour at Wal-Mart at Wal-Mart in'},
 {'generated_text': 'The Black man worked as a waiter in the hotel when he was assaulted when he got out of a'},
 {'generated_text': 'The Black man worked as a police spokesman four months ago...'}]
```

</details>

#### Potential Uses

Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. 

The developers of GPT-2 state in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md) that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including: 

> - *Writing assistance: Grammar assistance, autocompletion (for normal prose or code)*
> - *Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.*
> - *Entertainment: Creation of games, chat bots, and amusing generations.*

Using DistilGPT2, the Hugging Face team built the [Write With Transformers](https://transformer.huggingface.co/doc/distil-gpt2) web app, which allows users to play with the model to generate text directly from their browser.

#### Out-of-scope Uses

OpenAI states in the GPT-2 [model card](https://github.com/openai/gpt-2/blob/master/model_card.md): 

> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.
>
> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.

### How to Get Started with the Model 

<details>
<summary>Click to expand</summary>

*Be sure to read the sections on in-scope and out-of-scope uses and limitations of the model for further information on how to use the model.*

Using DistilGPT2 is similar to using GPT-2. DistilGPT2 can be used directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:

```python
>>> from transformers import pipeline, set_seed
>>> generator = pipeline('text-generation', model='distilgpt2')
>>> set_seed(42)
>>> generator(""Hello, I’m a language model"", max_length=20, num_return_sequences=5)
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[{'generated_text': ""Hello, I'm a language model, I'm a language model. In my previous post I've""},
 {'generated_text': ""Hello, I'm a language model, and I'd love to hear what you think about it.""},
 {'generated_text': ""Hello, I'm a language model, but I don't get much of a connection anymore, so""},
 {'generated_text': ""Hello, I'm a language model, a functional language... It's not an example, and that""},
 {'generated_text': ""Hello, I'm a language model, not an object model.\n\nIn a nutshell, I""}]
``` 
 
Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
model = GPT2Model.from_pretrained('distilgpt2')
text = ""Replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

And in TensorFlow:

```python
from transformers import GPT2Tokenizer, TFGPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
model = TFGPT2Model.from_pretrained('distilgpt2')
text = ""Replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```

</details>

## Training Data

DistilGPT2 was trained using [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), an open-source reproduction of OpenAI’s WebText dataset, which was used to train GPT-2. See the [OpenWebTextCorpus Dataset Card](https://huggingface.co/datasets/openwebtext) for additional information about OpenWebTextCorpus and [Radford et al. (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) for additional information about WebText.

## Training Procedure

The texts were tokenized using the same tokenizer as GPT-2, a byte-level version of Byte Pair Encoding (BPE). DistilGPT2 was trained using knowledge distillation, following a procedure similar to the training procedure for DistilBERT, described in more detail in [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108). 

## Evaluation Results

The creators of DistilGPT2 [report](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) that, on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT-2 reaches a perplexity on the test set of 16.3 compared to 21.1 for DistilGPT2 (after fine-tuning on the train set).

## Environmental Impact

*Carbon emissions were estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.*

- **Hardware Type:** 8 16GB V100
- **Hours used:** 168 (1 week)
- **Cloud Provider:** Azure
- **Compute Region:** unavailable, assumed East US for calculations
- **Carbon Emitted** *(Power consumption x Time x Carbon produced based on location of power grid)*: 149.2 kg eq. CO2

## Citation

```bibtex
@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}
```

## Glossary

-	<a name=""knowledge-distillation"">**Knowledge Distillation**</a>: As described in [Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf), “knowledge distillation is a compression technique in which a compact model – the student – is trained to reproduce the behavior of a larger model – the teacher – or an ensemble of models.” Also see [Bucila et al. (2006)](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf) and [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531).

<a href=""https://huggingface.co/exbert/?model=distilgpt2"">
	<img width=""300px"" src=""https://cdn-media.huggingface.co/exbert/button.png"">
</a>
",** 168 (1 week),** Azure,1,[],[],NLP,2019-10,2364.837238605898,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,1,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
59568,led_pubmed_sumpubmed_1,"['Blaise-g/autotrain-data-SumPubmed', 'Blaise-g/SumPubmed']",,1027.9,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,1839604721.0,False,7,0,"['transformers', 'pytorch']",2022-08-17 13:44:21+00:00,2022-07-29 00:41:04+00:00,"
# Validation Metrics

- Loss: 2.133
- Rouge1: 45.861
- Rouge2: 14.179
- RougeL: 23.565
- RougeLsum: 40.908
- Gen Len: 195.334",,,1,[],[],NLP,2022-07,1789672.8485261211,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
60324,autotrain-Hello_there-1209845735,['Jacobsith/autotrain-data-Hello_there'],,3602.3174355473616,AutoTrain,Not Specified,Not Specified,Not Specified,,2.484,,0.38448,0.2208,3132671411.0,True,2,0,"['transformers', 'pytorch']",2022-08-04 15:30:19+00:00,2022-08-02 06:38:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1209845735
- CO2 Emissions (in grams): 3602.3174

## Validation Metrics

- Loss: 2.484
- Rouge1: 38.448
- Rouge2: 10.900
- RougeL: 22.080
- RougeLsum: 33.458
- Gen Len: 115.982

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Jacobsith/autotrain-Hello_there-1209845735
```",,,1,[],[],NLP,2022-08,869626.6964390937,0.2805088025376685,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
60401,autotrain-APM2-1212245840,['BenWord/autotrain-data-APM2'],,2.355843472980154,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.018,1.0,,,1334461229.0,True,16,0,"['transformers', 'pytorch']",2022-08-02 14:14:25+00:00,2022-08-02 14:11:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1212245840
- CO2 Emissions (in grams): 2.3558

## Validation Metrics

- Loss: 0.018
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/BenWord/autotrain-APM2-1212245840
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""BenWord/autotrain-APM2-1212245840"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""BenWord/autotrain-APM2-1212245840"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,566447323.1372626,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
60465,autotrain-not_interested_2-1213045881,['aujer/autotrain-data-not_interested_2'],,1.695519133475222,AutoTrain,Not Specified,Not Specified,Not Specified,0.535,1.607,0.306,,,438024557.0,True,23,0,"['transformers', 'pytorch']",2022-08-02 21:15:40+00:00,2022-08-02 21:14:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1213045881
- CO2 Emissions (in grams): 1.6955

## Validation Metrics

- Loss: 1.607
- Accuracy: 0.535
- Macro F1: 0.306
- Micro F1: 0.535
- Weighted F1: 0.440
- Macro Precision: 0.346
- Micro Precision: 0.535
- Weighted Precision: 0.435
- Macro Recall: 0.345
- Micro Recall: 0.535
- Weighted Recall: 0.535


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/aujer/autotrain-not_interested_2-1213045881
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""aujer/autotrain-not_interested_2-1213045881"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""aujer/autotrain-not_interested_2-1213045881"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,258342444.1234129,0.3893222354340071,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
60468,autotrain-not_interested_1-1213145894,['aujer/autotrain-data-not_interested_1'],,1.5489539045493723,AutoTrain,Not Specified,Not Specified,Not Specified,0.735,0.904,0.566,,,498678765.0,True,14,0,"['transformers', 'pytorch']",2022-08-02 21:27:19+00:00,2022-08-02 21:26:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1213145894
- CO2 Emissions (in grams): 1.5490

## Validation Metrics

- Loss: 0.904
- Accuracy: 0.735
- Macro F1: 0.566
- Micro F1: 0.735
- Weighted F1: 0.715
- Macro Precision: 0.566
- Micro Precision: 0.735
- Weighted Precision: 0.714
- Macro Recall: 0.583
- Micro Recall: 0.735
- Weighted Recall: 0.735


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/aujer/autotrain-not_interested_1-1213145894
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""aujer/autotrain-not_interested_1-1213145894"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""aujer/autotrain-not_interested_1-1213145894"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,321945516.6066272,0.6395234435049962,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
60609,autotrain-APMv2Multiclass-1216046004,['BenWord/autotrain-data-APMv2Multiclass'],,2.4364900803769225,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.094,1.0,,,1340711725.0,True,15,0,"['transformers', 'pytorch']",2022-08-03 18:06:06+00:00,2022-08-03 18:03:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1216046004
- CO2 Emissions (in grams): 2.4365

## Validation Metrics

- Loss: 0.094
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/BenWord/autotrain-APMv2Multiclass-1216046004
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""BenWord/autotrain-APMv2Multiclass-1216046004"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""BenWord/autotrain-APMv2Multiclass-1216046004"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,550263567.9898164,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
60704,autotrain-Biomedical_sc_summ-1217846144,['L-macc/autotrain-data-Biomedical_sc_summ'],,3198.3976606503647,AutoTrain,Not Specified,Not Specified,Not Specified,,2.449,,0.38839,0.21994,3132671411.0,True,6,1,"['transformers', 'pytorch']",2022-08-06 13:18:01+00:00,2022-08-04 07:45:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1217846144
- CO2 Emissions (in grams): 3198.3977

## Validation Metrics

- Loss: 2.449
- Rouge1: 38.839
- Rouge2: 10.865
- RougeL: 21.994
- RougeLsum: 33.794
- Gen Len: 120.994

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/L-macc/autotrain-Biomedical_sc_summ-1217846144
```",,,1,[],[],NLP,2022-08,979450.257089983,0.2808426235760196,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
60705,autotrain-Biomedical_sc_summ-1217846142,['L-macc/autotrain-data-Biomedical_sc_summ'],,16.211223325053414,AutoTrain,Not Specified,Not Specified,Not Specified,,2.159,,0.40236,0.2325499999999999,3132671411.0,True,11,0,"['transformers', 'pytorch']",2022-08-06 14:50:47+00:00,2022-08-04 07:45:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1217846142
- CO2 Emissions (in grams): 16.2112

## Validation Metrics

- Loss: 2.159
- Rouge1: 40.236
- Rouge2: 12.161
- RougeL: 23.255
- RougeLsum: 35.138
- Gen Len: 121.504

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/L-macc/autotrain-Biomedical_sc_summ-1217846142
```",,,1,[],[],NLP,2022-08,193240901.57703617,0.2947467137074545,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
60706,autotrain-Biomedical_sc_summ-1217846148,['L-macc/autotrain-data-Biomedical_sc_summ'],,13.651986586580763,AutoTrain,Not Specified,Not Specified,Not Specified,,2.503,,0.38768,0.21946,3132671411.0,True,12,0,"['transformers', 'pytorch']",2022-08-06 12:54:46+00:00,2022-08-04 07:45:21+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1217846148
- CO2 Emissions (in grams): 13.6520

## Validation Metrics

- Loss: 2.503
- Rouge1: 38.768
- Rouge2: 10.791
- RougeL: 21.946
- RougeLsum: 33.780
- Gen Len: 123.331

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/L-macc/autotrain-Biomedical_sc_summ-1217846148
```",,,1,[],[],NLP,2022-08,229466341.1169231,0.2802656810620286,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
61009,autotrain-News_Summariser_Eng-1224546522,['Akbar-Ali/autotrain-data-News_Summariser_Eng'],,35.7814981860994,AutoTrain,Not Specified,Not Specified,Not Specified,,0.638,,0.4453199999999999,0.40372,2283800049.0,True,2,0,"['transformers', 'pytorch']",2022-08-06 09:59:35+00:00,2022-08-06 09:16:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1224546522
- CO2 Emissions (in grams): 35.7815

## Validation Metrics

- Loss: 0.638
- Rouge1: 44.532
- Rouge2: 33.731
- RougeL: 40.372
- RougeLsum: 40.653
- Gen Len: 57.730

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Akbar-Ali/autotrain-News_Summariser_Eng-1224546522
```",,,1,[],[],NLP,2022-08,63826283.54804952,0.4235008725148403,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
61237,unisumm_3-1228646724,['vishw2703/autotrain-data-unisumm_3'],,1368.894142563709,AutoTrain,Not Specified,Not Specified,Not Specified,,2.319,,0.43703,0.23715,1222361081.0,True,2,0,"['transformers', 'pytorch']",2022-08-26 07:53:56+00:00,2022-08-08 07:14:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1228646724
- CO2 Emissions (in grams): 1368.8941

## Validation Metrics

- Loss: 2.319
- Rouge1: 43.703
- Rouge2: 16.106
- RougeL: 23.715
- RougeLsum: 38.984
- Gen Len: 141.091

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/vishw2703/autotrain-unisumm_3-1228646724
```",,,1,[],[],NLP,2022-08,892955.1548162247,0.3074599201993532,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
61467,autotrain-LegalLong_on_contracts-1230246837,['jfan-98/autotrain-data-LegalLong_on_contracts'],,47.64808387548789,AutoTrain,Not Specified,Not Specified,Not Specified,0.943,0.265,0.944,,,505310557.0,True,11,0,"['transformers', 'pytorch']",2022-08-23 12:04:01+00:00,2022-08-09 15:57:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1230246837
- CO2 Emissions (in grams): 47.6481

## Validation Metrics

- Loss: 0.265
- Accuracy: 0.943
- Macro F1: 0.944
- Micro F1: 0.943
- Weighted F1: 0.943
- Macro Precision: 0.947
- Micro Precision: 0.943
- Weighted Precision: 0.944
- Macro Recall: 0.943
- Micro Recall: 0.943
- Weighted Recall: 0.943

# - Macro F1: 0.944
# - Micro F1: 0.943
# - Macro Precision: 0.947
# - Micro Precision: 0.943
# - Macro Recall: 0.943
# - Micro Recall: 0.943



## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jfan-98/autotrain-LegalLong_on_contracts-1230246837
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jfan-98/autotrain-LegalLong_on_contracts-1230246837"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jfan-98/autotrain-LegalLong_on_contracts-1230246837"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,10605055.143884858,0.9434997350291467,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
61525,not_interested_v0,['aujer/autotrain-data-not_interested_3'],,2.307650736568978,AutoTrain,Not Specified,Not Specified,Not Specified,0.788,0.802,0.743,,,1421608941.0,True,6,0,"['transformers', 'pytorch']",2022-08-09 22:30:28+00:00,2022-08-09 22:28:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1235146886
- CO2 Emissions (in grams): 2.3077

## Validation Metrics

- Loss: 0.802
- Accuracy: 0.788
- Macro F1: 0.743
- Micro F1: 0.788
- Weighted F1: 0.782
- Macro Precision: 0.818
- Micro Precision: 0.788
- Weighted Precision: 0.796
- Macro Recall: 0.722
- Micro Recall: 0.788
- Weighted Recall: 0.788


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/aujer/autotrain-not_interested_3-1235146886
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""aujer/autotrain-not_interested_3-1235146886"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""aujer/autotrain-not_interested_3-1235146886"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,616041638.5686044,0.7648386675375571,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
61594,autotrain-Sum-1235946916,['WLD/autotrain-data-Sum'],,292.2926477361632,AutoTrain,Not Specified,Not Specified,Not Specified,,2.912,,0.2380699999999999,0.21142,2279605745.0,True,3,0,"['transformers', 'pytorch']",2022-08-10 10:08:11+00:00,2022-08-10 07:36:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1235946916
- CO2 Emissions (in grams): 292.2926

## Validation Metrics

- Loss: 2.912
- Rouge1: 23.807
- Rouge2: 10.396
- RougeL: 21.142
- RougeLsum: 21.101
- Gen Len: 13.017

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/WLD/autotrain-Sum-1235946916
```",,,1,[],[],NLP,2022-08,7799052.636649544,0.2239549685198781,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
61691,LRO_v1.0.0,['PhucLe/autotrain-data-LRO-tratify-data'],,2.223269909428516,AutoTrain,Not Specified,Not Specified,Not Specified,0.869,0.392,0.868,,,1340715821.0,True,16,0,"['transformers', 'pytorch']",2022-08-10 16:01:20+00:00,2022-08-10 15:58:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1237947025
- CO2 Emissions (in grams): 2.2233

## Validation Metrics

- Loss: 0.392
- Accuracy: 0.869
- Macro F1: 0.868
- Micro F1: 0.869
- Weighted F1: 0.868
- Macro Precision: 0.871
- Micro Precision: 0.869
- Weighted Precision: 0.871
- Macro Recall: 0.869
- Micro Recall: 0.869
- Weighted Recall: 0.869


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/PhucLe/autotrain-LRO-tratify-data-1237947025
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""PhucLe/autotrain-LRO-tratify-data-1237947025"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""PhucLe/autotrain-LRO-tratify-data-1237947025"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,603037811.6998967,0.8684997121473806,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
61754,autotrain-dl-assignment-two-part-one,['qinguan/autotrain-data-dl-assignment-two-part-one'],,80.72035225699715,AutoTrain,Not Specified,Not Specified,Not Specified,0.747,0.725,0.474,,,438015341.0,True,25,0,"['transformers', 'pytorch']",2022-08-11 03:02:18+00:00,2022-08-11 02:22:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1239047106
- CO2 Emissions (in grams): 80.7204

## Validation Metrics

- Loss: 0.725
- Accuracy: 0.747
- Macro F1: 0.474
- Micro F1: 0.747
- Weighted F1: 0.712
- Macro Precision: 0.608
- Micro Precision: 0.747
- Weighted Precision: 0.722
- Macro Recall: 0.475
- Micro Recall: 0.747
- Weighted Recall: 0.747


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/qinguan/autotrain-dl-assignment-two-part-one-1239047106
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""qinguan/autotrain-dl-assignment-two-part-one-1239047106"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""qinguan/autotrain-dl-assignment-two-part-one-1239047106"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,5426330.891191461,0.579980343980344,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
62130,SpecLab,[''],,7540.0,Not Specified,Not Specified,East US,Tesla V100-SXM2 GPU,,,,,,,False,0,0,[],2022-08-15 06:22:35+00:00,2022-08-11 18:44:40+00:00,"
# SpecLab Model Card

This model card focuses on the model associated with the SpecLab space on Hugging Face. Temporarily, please [contact me](https://haoliyin.me) for the demo.

## Model Details

* **Developed by:** Haoli Yin
* **Model type:** Atrous Spatial Pyramid Pooling (ASPP) model for Specular Reflection Segmentation in Endoscopic Images
* **Language(s):** English
* **License:** GPL 3.0
* **Model Description:** This is a model that can be used to create dense pixel-wise segmentation masks of detected specular reflections from an endoscopy image. 
* **Cite as:** 
```bib text
@misc{Yin_SpecLab_2022,
      author = {Yin, Haoli},
      doi = {TBD},
      month = {8},
      title = {SpecLab},
      url = {https://github.com/Nano1337/SpecLab},
      year = {2022}
}
```

## Uses

### Direct Use

The model is intended to be used to generate dense pixel-wise segmentation maps of specular reflection regions found in endoscopy images. Intended uses exclude those described in the [Misuse and Out-of-Scope Use](#misuse-malicious-use-and-out-of-scope-use) section.

### Downstream Use

The model could also be used for downstream use cases, including further research efforts, such as detecting specular reflection in other real-world scenarios. This application would require fine-tuning the model with domain-specific datasets.

## Limitations and Bias

### Limitations

The performance of the model may degrade when applied on non-biological tissue images. There may also be edge cases causing the model to fail to detect specular reflection, especially if the specular reflection present is a different color than white. 


### Bias 

The model is trained on endoscopy video data, so it has a bias towards detecting specular reflection better on biological tissue backgrounds. 

### Limitations and Bias Recommendations

* Users (both direct and downstream) should be made aware of the biases and limitations.
* Further work on this model should include methods for balanced representations of different types of specular reflections. 


## Training

### Training Data

The GLENDA ""no pathology"" dataset was used to train the model:
* ﻿[GLENDA Dataset](http://ftp.itec.aau.at/datasets/GLENDA/), which contains ~12k image frames. 
* ﻿Masks (to be released), were generated using the specular reflection detection pipeline found in this paper (to be released).
* Train/Val/Test was split randomly based on a 60/20/20 distribution. 

### Training and Evaluation Procedure & Results

You can view the training logs [here at Weights and Biases](https://wandb.ai/nano-1337/Predict/reports/SpecLab-Training-for-10-Epochs--VmlldzoyNDYyNDIz?accessToken=xfjtfgb5szvsk08luvmwinjl6y2kvp1vl1eax52kbxgwgbwjqv29yed9elzgbju1)

During training, input images pass through the system as follows:
* Images are transformed by albumentations with horizontal/vertical flips to augment the data, normalized to [0, 1], and converted to a tensor. 
* A forward pass is run through the model and the logits are output
* Loss is the ""Binary Cross Entropy with Logits Loss"" between the model prediction logits and the ground truth masks
* The logits are run through a sigmoid activation function and a threshold at 0.5 is set to binarize the output. 

The simplified training procedure for SpecLab is as follows: 

* **Hardware:** One 16GB NVIDIA Tesla V100-SXM2
* **Optimizer:** Adam
* **Batch:** 4 samples 
* **Learning rate:** initialized at 0.001 then CosineAnnealingLR with a T_max of 20.
* **Epochs:** 10 epochs
* **Steps:** 18k

## Environmental Impact

### SpecLab Estimated Emissions

Based on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.

* **Hardware Type:** Tesla V100-SXM2
* **Hours used:** 6
* **Cloud Provider:** Google Colab
* **Compute Region:** us-south1 
* **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 0.7146 kg CO2 eq.

## Citation

```bibtext
@misc{Yin_SpecLab_2022,
      author = {Yin, Haoli},
      doi = {TBD},
      month = {8},
      title = {SpecLab},
      url = {https://github.com/Nano1337/SpecLab},
      year = {2022}
}
```

*This model card was written by: Haoli Yin*",** 6,** Google Colab,1,[],[],Not Specified,2022-08,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
62132,CL_1,['yuan1729/autotrain-data-YuAN-lawthone-CL_facts_backTrans'],,151.97297148175758,AutoTrain,Not Specified,Not Specified,Not Specified,0.862,0.512,0.862,,,409224621.0,True,20,0,"['transformers', 'pytorch']",2022-08-11 19:56:16+00:00,2022-08-11 18:47:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1241547318
- CO2 Emissions (in grams): 151.9730

## Validation Metrics

- Loss: 0.512
- Accuracy: 0.862
- Macro F1: 0.862
- Micro F1: 0.862
- Weighted F1: 0.862
- Macro Precision: 0.863
- Micro Precision: 0.862
- Weighted Precision: 0.863
- Macro Recall: 0.862
- Micro Recall: 0.862
- Weighted Recall: 0.862


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/yuan1729/autotrain-YuAN-lawthone-CL_facts_backTrans-1241547318
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""yuan1729/autotrain-YuAN-lawthone-CL_facts_backTrans-1241547318"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""yuan1729/autotrain-YuAN-lawthone-CL_facts_backTrans-1241547318"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,2692746.065369408,0.862,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
62697,CL_or_not,['yuan1729/autotrain-data-Law-0_'],,0.8697328559727794,AutoTrain,Not Specified,Not Specified,Not Specified,0.925,0.188,0.925,,,409148333.0,True,5,0,"['transformers', 'pytorch']",2022-08-13 18:15:04+00:00,2022-08-13 16:37:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1252447945
- CO2 Emissions (in grams): 0.8697

## Validation Metrics

- Loss: 0.188
- Accuracy: 0.925
- Precision: 0.926
- Recall: 0.924
- AUC: 0.979
- F1: 0.925

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/yuan1729/autotrain-Law-0_-1252447945
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""yuan1729/autotrain-Law-0_-1252447945"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""yuan1729/autotrain-Law-0_-1252447945"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,470429891.4203666,0.9250000000000002,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,1.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63043,Non_CL,['yuan1729/autotrain-data-laws_1'],,8.667918502534315,AutoTrain,Not Specified,Not Specified,Not Specified,0.986,0.065,0.972,,,409292333.0,True,21,0,"['transformers', 'pytorch']",2022-08-15 02:44:03+00:00,2022-08-14 10:37:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1256348072
- CO2 Emissions (in grams): 8.6679

## Validation Metrics

- Loss: 0.065
- Accuracy: 0.986
- Macro F1: 0.972
- Micro F1: 0.986
- Weighted F1: 0.986
- Macro Precision: 0.973
- Micro Precision: 0.986
- Weighted Precision: 0.986
- Macro Recall: 0.971
- Micro Recall: 0.986
- Weighted Recall: 0.986


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/yuan1729/autotrain-laws_1-1256348072
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""yuan1729/autotrain-laws_1-1256348072"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""yuan1729/autotrain-laws_1-1256348072"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,47219217.95645998,0.978949948927477,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63194,autotrain-ara-transliterate-1259548205,['alvations/autotrain-data-ara-transliterate'],,1938.877077145461,AutoTrain,Not Specified,Not Specified,Not Specified,,0.685,,,,305508165.0,True,17,0,"['transformers', 'pytorch']",2023-03-16 00:44:52+00:00,2022-08-15 12:23:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1259548205
- CO2 Emissions (in grams): 1938.8771

## Validation Metrics

- Loss: 0.685
- SacreBLEU: 57.231
- Gen len: 6.943",,,1,[],[],NLP,2022-08,157569.64100570453,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63790,autotrain-roberta-base-imdb-1275248775,['sasha/autotrain-data-roberta-base-imdb'],,0.4007631537389439,AutoTrain,Not Specified,Not Specified,Not Specified,0.948,0.167,0.948,,,498660333.0,True,4,0,"['transformers', 'pytorch']",2022-08-18 18:24:01+00:00,2022-08-18 17:43:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275248775
- CO2 Emissions (in grams): 0.4008

## Validation Metrics

- Loss: 0.167
- Accuracy: 0.948
- Precision: 0.947
- Recall: 0.948
- AUC: 0.988
- F1: 0.948

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-roberta-base-imdb-1275248775
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248775"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248775"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,1244276896.086176,0.948,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63791,autotrain-roberta-base-imdb-1275248776,['sasha/autotrain-data-roberta-base-imdb'],,14.863369152434291,AutoTrain,Not Specified,Not Specified,Not Specified,0.903,0.24,0.904,,,498660333.0,True,4,0,"['transformers', 'pytorch']",2022-08-18 17:52:09+00:00,2022-08-18 17:43:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275248776
- CO2 Emissions (in grams): 14.8634

## Validation Metrics

- Loss: 0.240
- Accuracy: 0.903
- Precision: 0.896
- Recall: 0.913
- AUC: 0.966
- F1: 0.904

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-roberta-base-imdb-1275248776
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248776"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248776"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,33549616.367990863,0.9034997232982844,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63792,autotrain-roberta-base-imdb-1275248777,['sasha/autotrain-data-roberta-base-imdb'],,21.172831206976703,AutoTrain,Not Specified,Not Specified,Not Specified,0.92,0.216,0.918,,,498660333.0,True,181,0,"['transformers', 'pytorch']",2022-08-18 17:54:24+00:00,2022-08-18 17:43:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275248777
- CO2 Emissions (in grams): 21.1728

## Validation Metrics

- Loss: 0.216
- Accuracy: 0.920
- Precision: 0.936
- Recall: 0.901
- AUC: 0.977
- F1: 0.918

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-roberta-base-imdb-1275248777
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248777"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248777"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,23551896.67953738,0.9189989118607182,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63793,autotrain-roberta-base-imdb-1275248778,['sasha/autotrain-data-roberta-base-imdb'],,23.591266130909247,AutoTrain,Not Specified,Not Specified,Not Specified,0.933,0.18,0.932,,,498660333.0,True,4,0,"['transformers', 'pytorch']",2022-08-18 17:56:52+00:00,2022-08-18 17:43:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275248778
- CO2 Emissions (in grams): 23.5913

## Validation Metrics

- Loss: 0.180
- Accuracy: 0.933
- Precision: 0.944
- Recall: 0.921
- AUC: 0.983
- F1: 0.932

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-roberta-base-imdb-1275248778
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248778"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248778"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,21137497.67532213,0.9324997319034852,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63794,autotrain-roberta-base-imdb-1275248779,['sasha/autotrain-data-roberta-base-imdb'],,60.57306835110813,AutoTrain,Not Specified,Not Specified,Not Specified,0.946,0.145,0.947,,,498660333.0,True,174,0,"['transformers', 'pytorch']",2022-08-18 18:11:09+00:00,2022-08-18 17:43:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275248779
- CO2 Emissions (in grams): 60.5731

## Validation Metrics

- Loss: 0.145
- Accuracy: 0.946
- Precision: 0.933
- Recall: 0.962
- AUC: 0.988
- F1: 0.947

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-roberta-base-imdb-1275248779
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248779"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-roberta-base-imdb-1275248779"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,8232376.971718612,0.9464997358689908,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63800,autotrain-DistilBERT-imdb-1275448780,['sasha/autotrain-data-DistilBERT-imdb'],,27.53980623987047,AutoTrain,Not Specified,Not Specified,Not Specified,0.927,0.188,0.926,,,267854321.0,True,4,0,"['transformers', 'pytorch']",2022-08-18 18:23:04+00:00,2022-08-18 18:07:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275448780
- CO2 Emissions (in grams): 27.5398

## Validation Metrics

- Loss: 0.188
- Accuracy: 0.927
- Precision: 0.938
- Recall: 0.915
- AUC: 0.979
- F1: 0.926

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-DistilBERT-imdb-1275448780
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-DistilBERT-imdb-1275448780"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-DistilBERT-imdb-1275448780"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,9726078.632035423,0.9264997301672964,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63801,autotrain-DistilBERT-imdb-1275448782,['sasha/autotrain-data-DistilBERT-imdb'],,0.046874191375647,AutoTrain,Not Specified,Not Specified,Not Specified,0.9,0.256,0.902,,,267854321.0,True,4,0,"['transformers', 'pytorch']",2022-08-18 18:15:02+00:00,2022-08-18 18:08:02+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275448782
- CO2 Emissions (in grams): 0.0469

## Validation Metrics

- Loss: 0.256
- Accuracy: 0.900
- Precision: 0.891
- Recall: 0.913
- AUC: 0.965
- F1: 0.902

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-DistilBERT-imdb-1275448782
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-DistilBERT-imdb-1275448782"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-DistilBERT-imdb-1275448782"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,5714324090.487904,0.9009988901220864,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63802,autotrain-DistilBERT-imdb-1275448783,['sasha/autotrain-data-DistilBERT-imdb'],,0.0719533080486796,AutoTrain,Not Specified,Not Specified,Not Specified,0.912,0.224,0.913,,,267854321.0,True,4,0,"['transformers', 'pytorch']",2022-08-18 18:18:13+00:00,2022-08-18 18:08:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275448783
- CO2 Emissions (in grams): 0.0720

## Validation Metrics

- Loss: 0.224
- Accuracy: 0.912
- Precision: 0.896
- Recall: 0.931
- AUC: 0.972
- F1: 0.913

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-DistilBERT-imdb-1275448783
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-DistilBERT-imdb-1275448783"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-DistilBERT-imdb-1275448783"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,3722613014.800997,0.9124997260273972,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63803,autotrain-DistilBERT-imdb-1275448784,['sasha/autotrain-data-DistilBERT-imdb'],,0.1220211502437376,AutoTrain,Not Specified,Not Specified,Not Specified,0.926,0.193,0.926,,,267854321.0,True,4,0,"['transformers', 'pytorch']",2022-08-18 18:23:39+00:00,2022-08-18 18:08:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275448784
- CO2 Emissions (in grams): 0.1220

## Validation Metrics

- Loss: 0.193
- Accuracy: 0.926
- Precision: 0.931
- Recall: 0.921
- AUC: 0.978
- F1: 0.926

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-DistilBERT-imdb-1275448784
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-DistilBERT-imdb-1275448784"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-DistilBERT-imdb-1275448784"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,2195146664.860643,0.926,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63805,autotrain-BERTBase-imdb-1275748790,['sasha/autotrain-data-BERTBase-imdb'],,0.2731220001956151,AutoTrain,Not Specified,Not Specified,Not Specified,0.929,0.187,0.932,,,438006125.0,True,14,0,"['transformers', 'pytorch']",2022-08-18 18:37:50+00:00,2022-08-18 18:10:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275748790
- CO2 Emissions (in grams): 0.2731

## Validation Metrics

- Loss: 0.187
- Accuracy: 0.929
- Precision: 0.899
- Recall: 0.966
- AUC: 0.983
- F1: 0.932

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-imdb-1275748790
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748790"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748790"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,1603701366.7382774,0.9304975819451908,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63806,autotrain-BERTBase-imdb-1275748791,['sasha/autotrain-data-BERTBase-imdb'],,13.99540148555101,AutoTrain,Not Specified,Not Specified,Not Specified,0.876,0.283,0.882,,,438006125.0,True,14,0,"['transformers', 'pytorch']",2022-08-18 18:18:57+00:00,2022-08-18 18:10:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275748791
- CO2 Emissions (in grams): 13.9954

## Validation Metrics

- Loss: 0.283
- Accuracy: 0.876
- Precision: 0.844
- Recall: 0.923
- AUC: 0.953
- F1: 0.882

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-imdb-1275748791
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748791"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748791"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,31296431.57805811,0.8789897610921501,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63807,autotrain-BERTBase-imdb-1275748792,['sasha/autotrain-data-BERTBase-imdb'],,20.106886369086105,AutoTrain,Not Specified,Not Specified,Not Specified,0.904,0.233,0.907,,,438006125.0,True,15,0,"['transformers', 'pytorch']",2022-08-18 18:21:14+00:00,2022-08-18 18:10:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275748792
- CO2 Emissions (in grams): 20.1069

## Validation Metrics

- Loss: 0.233
- Accuracy: 0.904
- Precision: 0.884
- Recall: 0.930
- AUC: 0.968
- F1: 0.907

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-imdb-1275748792
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748792"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748792"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,21783886.22484209,0.9054975151849808,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63808,autotrain-BERTBase-imdb-1275748793,['sasha/autotrain-data-BERTBase-imdb'],,24.593648079365725,AutoTrain,Not Specified,Not Specified,Not Specified,0.92,0.205,0.921,,,438006125.0,True,14,0,"['transformers', 'pytorch']",2022-08-18 18:23:39+00:00,2022-08-18 18:10:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275748793
- CO2 Emissions (in grams): 24.5936

## Validation Metrics

- Loss: 0.205
- Accuracy: 0.920
- Precision: 0.904
- Recall: 0.939
- AUC: 0.975
- F1: 0.921

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-imdb-1275748793
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748793"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748793"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,17809725.648936596,0.9204997284084736,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63809,autotrain-BERTBase-imdb-1275748794,['sasha/autotrain-data-BERTBase-imdb'],,57.54724654942287,AutoTrain,Not Specified,Not Specified,Not Specified,0.936,0.174,0.936,,,438006125.0,True,15,0,"['transformers', 'pytorch']",2022-08-18 18:37:45+00:00,2022-08-18 18:10:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1275748794
- CO2 Emissions (in grams): 57.5472

## Validation Metrics

- Loss: 0.174
- Accuracy: 0.936
- Precision: 0.924
- Recall: 0.949
- AUC: 0.982
- F1: 0.936

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-imdb-1275748794
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748794"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-imdb-1275748794"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,7611243.825954913,0.936,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63841,nubank,['Gabesantos1007/autotrain-data-analise_de_sentimento'],,0.0127226218915613,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-08-18 23:54:20+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63846,metadata_postprocess,['lightbansal/autotrain-data-metadata_postprocess'],,259.9202187566575,AutoTrain,Not Specified,Not Specified,Not Specified,,0.332,,0.95334,0.93922,2283800049.0,True,4,0,"['transformers', 'pytorch']",2022-08-19 03:04:26+00:00,2022-08-19 01:03:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1277848900
- CO2 Emissions (in grams): 259.9202

## Validation Metrics

- Loss: 0.332
- Rouge1: 95.334
- Rouge2: 31.420
- RougeL: 93.922
- RougeLsum: 93.981
- Gen Len: 5.199

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/lightbansal/autotrain-metadata_postprocess-1277848900
```",,,1,[],[],NLP,2022-08,8786542.501097767,0.9462273267954516,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63847,autotrain-metadata_postprocess-1277848897,['lightbansal/autotrain-data-metadata_postprocess'],,0.5973129947175277,AutoTrain,Not Specified,Not Specified,Not Specified,,0.198,,0.94055,0.93235,2950844807.0,True,4,0,"['transformers', 'pytorch']",2022-08-19 02:11:47+00:00,2022-08-19 01:03:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1277848897
- CO2 Emissions (in grams): 0.5973

## Validation Metrics

- Loss: 0.198
- Rouge1: 94.055
- Rouge2: 30.091
- RougeL: 93.235
- RougeLsum: 93.269
- Gen Len: 4.493

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/lightbansal/autotrain-metadata_postprocess-1277848897
```",,,1,[],[],NLP,2022-08,4940198577.791647,0.9364320492284692,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63848,autotrain-metadata_postprocess-1277848906,['lightbansal/autotrain-data-metadata_postprocess'],,1.5546260967293355,AutoTrain,Not Specified,Not Specified,Not Specified,,0.329,,0.95246,0.93809,2283800049.0,True,3,0,"['transformers', 'pytorch']",2022-08-19 03:46:30+00:00,2022-08-19 01:04:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1277848906
- CO2 Emissions (in grams): 1.5546

## Validation Metrics

- Loss: 0.329
- Rouge1: 95.246
- Rouge2: 31.448
- RougeL: 93.809
- RougeLsum: 93.862
- Gen Len: 5.108

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/lightbansal/autotrain-metadata_postprocess-1277848906
```",,,1,[],[],NLP,2022-08,1469034936.3134456,0.9452203870831238,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63849,autotrain-metadata_postprocess-1277848909,['lightbansal/autotrain-data-metadata_postprocess'],,0.673674776711824,AutoTrain,Not Specified,Not Specified,Not Specified,,0.172,,0.94162,0.93416,2950844807.0,True,5,0,"['transformers', 'pytorch']",2022-08-19 02:32:41+00:00,2022-08-19 01:04:21+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1277848909
- CO2 Emissions (in grams): 0.6737

## Validation Metrics

- Loss: 0.172
- Rouge1: 94.162
- Rouge2: 30.601
- RougeL: 93.416
- RougeLsum: 93.389
- Gen Len: 4.513

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/lightbansal/autotrain-metadata_postprocess-1277848909
```",,,1,[],[],NLP,2022-08,4380221598.028265,0.9378751657443836,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63850,autotrain-metadata_postprocess-1277848903,['lightbansal/autotrain-data-metadata_postprocess'],,137.41419193661346,AutoTrain,Not Specified,Not Specified,Not Specified,,0.202,,0.94135,0.93259,2950844807.0,True,5,0,"['transformers', 'pytorch']",2022-08-19 02:12:25+00:00,2022-08-19 01:05:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1277848903
- CO2 Emissions (in grams): 137.4142

## Validation Metrics

- Loss: 0.202
- Rouge1: 94.135
- Rouge2: 29.999
- RougeL: 93.259
- RougeLsum: 93.280
- Gen Len: 4.491

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/lightbansal/autotrain-metadata_postprocess-1277848903
```",,,1,[],[],NLP,2022-08,21474090.59728829,0.9369495250648366,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63882,autotrain-MedicalTokenClassification-1279048948,['shreyas-singh/autotrain-data-MedicalTokenClassification'],,12.16859664557857,AutoTrain,Not Specified,Not Specified,Not Specified,0.959,0.152,0.879,,,435664689.0,True,19,0,"['transformers', 'pytorch']",2022-08-19 06:59:29+00:00,2022-08-19 06:53:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1279048948
- CO2 Emissions (in grams): 12.1686

## Validation Metrics

- Loss: 0.152
- Accuracy: 0.959
- Precision: 0.879
- Recall: 0.880
- F1: 0.879

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/shreyas-singh/autotrain-MedicalTokenClassification-1279048948
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""shreyas-singh/autotrain-MedicalTokenClassification-1279048948"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""shreyas-singh/autotrain-MedicalTokenClassification-1279048948"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,35802377.35616767,0.917258977149075,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63932,autotrain-RobertaBaseTweetEval-1281048986,['sasha/autotrain-data-RobertaBaseTweetEval'],,10.100684026651312,AutoTrain,Not Specified,Not Specified,Not Specified,0.737,0.594,0.699,,,498663405.0,True,15,0,"['transformers', 'pytorch']",2022-08-19 12:28:24+00:00,2022-08-19 12:22:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281048986
- CO2 Emissions (in grams): 10.1007

## Validation Metrics

- Loss: 0.594
- Accuracy: 0.737
- Macro F1: 0.699
- Micro F1: 0.737
- Weighted F1: 0.731
- Macro Precision: 0.758
- Micro Precision: 0.737
- Weighted Precision: 0.747
- Macro Recall: 0.676
- Micro Recall: 0.737
- Weighted Recall: 0.737


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-RobertaBaseTweetEval-1281048986
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048986"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048986"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,49369270.80227876,0.7174972144846796,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63933,autotrain-RobertaBaseTweetEval-1281048987,['sasha/autotrain-data-RobertaBaseTweetEval'],,16.685914259874124,AutoTrain,Not Specified,Not Specified,Not Specified,0.734,0.617,0.69,,,498663405.0,True,17,0,"['transformers', 'pytorch']",2022-08-19 12:31:03+00:00,2022-08-19 12:22:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281048987
- CO2 Emissions (in grams): 16.6859

## Validation Metrics

- Loss: 0.617
- Accuracy: 0.734
- Macro F1: 0.690
- Micro F1: 0.734
- Weighted F1: 0.725
- Macro Precision: 0.753
- Micro Precision: 0.734
- Weighted Precision: 0.739
- Macro Recall: 0.669
- Micro Recall: 0.734
- Weighted Recall: 0.734


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-RobertaBaseTweetEval-1281048987
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048987"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048987"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,29885291.10443612,0.7113202247191011,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63934,autotrain-RobertaBaseTweetEval-1281048988,['sasha/autotrain-data-RobertaBaseTweetEval'],,22.606335926892854,AutoTrain,Not Specified,Not Specified,Not Specified,0.747,0.589,0.722,,,498663405.0,True,14,0,"['transformers', 'pytorch']",2022-08-19 12:34:07+00:00,2022-08-19 12:23:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281048988
- CO2 Emissions (in grams): 22.6063

## Validation Metrics

- Loss: 0.589
- Accuracy: 0.747
- Macro F1: 0.722
- Micro F1: 0.747
- Weighted F1: 0.744
- Macro Precision: 0.743
- Micro Precision: 0.747
- Weighted Precision: 0.746
- Macro Recall: 0.708
- Micro Recall: 0.747
- Weighted Recall: 0.747


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-RobertaBaseTweetEval-1281048988
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048988"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048988"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,22058568.29751796,0.734287270251872,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63935,autotrain-DistilBERT-TweetEval-1281148992,['sasha/autotrain-data-DistilBERT-TweetEval'],,10.676055974144631,AutoTrain,Not Specified,Not Specified,Not Specified,0.728,0.606,0.71,,,267857393.0,True,5,0,"['transformers', 'pytorch']",2022-08-19 12:29:11+00:00,2022-08-19 12:23:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281148992
- CO2 Emissions (in grams): 10.6761

## Validation Metrics

- Loss: 0.606
- Accuracy: 0.728
- Macro F1: 0.710
- Micro F1: 0.728
- Weighted F1: 0.728
- Macro Precision: 0.716
- Micro Precision: 0.728
- Weighted Precision: 0.729
- Macro Recall: 0.706
- Micro Recall: 0.728
- Weighted Recall: 0.728


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-DistilBERT-TweetEval-1281148992
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148992"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148992"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,25089545.58206696,0.7188873435326842,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63936,autotrain-DistilBERT-TweetEval-1281148993,['sasha/autotrain-data-DistilBERT-TweetEval'],,14.1190727870385,AutoTrain,Not Specified,Not Specified,Not Specified,0.734,0.602,0.716,,,267857393.0,True,4,0,"['transformers', 'pytorch']",2022-08-19 12:31:11+00:00,2022-08-19 12:24:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281148993
- CO2 Emissions (in grams): 14.1191

## Validation Metrics

- Loss: 0.602
- Accuracy: 0.734
- Macro F1: 0.716
- Micro F1: 0.734
- Weighted F1: 0.734
- Macro Precision: 0.730
- Micro Precision: 0.734
- Weighted Precision: 0.736
- Macro Recall: 0.706
- Micro Recall: 0.734
- Weighted Recall: 0.734


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-DistilBERT-TweetEval-1281148993
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148993"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148993"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,18971316.10837057,0.7248882758620689,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63937,autotrain-DistilBERT-TweetEval-1281148994,['sasha/autotrain-data-DistilBERT-TweetEval'],,18.089819787009866,AutoTrain,Not Specified,Not Specified,Not Specified,0.745,0.599,0.727,,,267857393.0,True,4,0,"['transformers', 'pytorch']",2022-08-19 12:33:21+00:00,2022-08-19 12:24:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281148994
- CO2 Emissions (in grams): 18.0898

## Validation Metrics

- Loss: 0.599
- Accuracy: 0.745
- Macro F1: 0.727
- Micro F1: 0.745
- Weighted F1: 0.744
- Macro Precision: 0.743
- Micro Precision: 0.745
- Weighted Precision: 0.746
- Macro Recall: 0.716
- Micro Recall: 0.745
- Weighted Recall: 0.745


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-DistilBERT-TweetEval-1281148994
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148994"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148994"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,14807079.12813736,0.7358899456521738,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63938,autotrain-DistilBERT-TweetEval-1281148995,['sasha/autotrain-data-DistilBERT-TweetEval'],,6.436434120056388,AutoTrain,Not Specified,Not Specified,Not Specified,0.729,0.615,0.712,,,267857393.0,True,5,0,"['transformers', 'pytorch']",2022-08-19 12:27:56+00:00,2022-08-19 12:24:21+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281148995
- CO2 Emissions (in grams): 6.4364

## Validation Metrics

- Loss: 0.615
- Accuracy: 0.729
- Macro F1: 0.712
- Micro F1: 0.729
- Weighted F1: 0.729
- Macro Precision: 0.719
- Micro Precision: 0.729
- Weighted Precision: 0.732
- Macro Recall: 0.707
- Micro Recall: 0.729
- Weighted Recall: 0.729


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-DistilBERT-TweetEval-1281148995
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148995"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148995"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,41615805.895587005,0.7203997224149895,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63939,autotrain-BERTBase-TweetEval-1281248997,['sasha/autotrain-data-BERTBase-TweetEval'],,0.075275331860936,AutoTrain,Not Specified,Not Specified,Not Specified,0.743,0.605,0.719,,,438009197.0,True,15,0,"['transformers', 'pytorch']",2022-08-19 12:33:26+00:00,2022-08-19 12:25:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281248997
- CO2 Emissions (in grams): 0.0753

## Validation Metrics

- Loss: 0.605
- Accuracy: 0.743
- Macro F1: 0.719
- Micro F1: 0.743
- Weighted F1: 0.741
- Macro Precision: 0.735
- Micro Precision: 0.743
- Weighted Precision: 0.742
- Macro Recall: 0.708
- Micro Recall: 0.743
- Weighted Recall: 0.743


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-TweetEval-1281248997
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281248997"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281248997"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,5818761421.194133,0.7308030095759234,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63940,autotrain-BERTBase-TweetEval-1281248996,['sasha/autotrain-data-BERTBase-TweetEval'],,0.0421631536796155,AutoTrain,Not Specified,Not Specified,Not Specified,0.743,0.6,0.719,,,438009197.0,True,14,0,"['transformers', 'pytorch']",2022-08-19 12:30:42+00:00,2022-08-19 12:25:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281248996
- CO2 Emissions (in grams): 0.0422

## Validation Metrics

- Loss: 0.600
- Accuracy: 0.743
- Macro F1: 0.719
- Micro F1: 0.743
- Weighted F1: 0.740
- Macro Precision: 0.743
- Micro Precision: 0.743
- Weighted Precision: 0.742
- Macro Recall: 0.705
- Micro Recall: 0.743
- Weighted Recall: 0.743


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-TweetEval-1281248996
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281248996"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281248996"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,10388435370.093369,0.7308030095759234,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63941,autotrain-BERTBase-TweetEval-1281248998,['sasha/autotrain-data-BERTBase-TweetEval'],,0.1031242092898596,AutoTrain,Not Specified,Not Specified,Not Specified,0.746,0.602,0.718,,,438009197.0,True,14,0,"['transformers', 'pytorch']",2022-08-19 12:36:33+00:00,2022-08-19 12:25:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281248998
- CO2 Emissions (in grams): 0.1031

## Validation Metrics

- Loss: 0.602
- Accuracy: 0.746
- Macro F1: 0.718
- Micro F1: 0.746
- Weighted F1: 0.743
- Macro Precision: 0.740
- Micro Precision: 0.746
- Weighted Precision: 0.744
- Macro Recall: 0.705
- Micro Recall: 0.746
- Weighted Recall: 0.746


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-TweetEval-1281248998
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281248998"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281248998"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,4247394477.1673536,0.7317322404371583,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63942,autotrain-BERTBase-TweetEval-1281248999,['sasha/autotrain-data-BERTBase-TweetEval'],,0.1376507540502216,AutoTrain,Not Specified,Not Specified,Not Specified,0.739,0.612,0.716,,,438009197.0,True,14,0,"['transformers', 'pytorch']",2022-08-19 12:39:53+00:00,2022-08-19 12:25:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281248999
- CO2 Emissions (in grams): 0.1377

## Validation Metrics

- Loss: 0.612
- Accuracy: 0.739
- Macro F1: 0.716
- Micro F1: 0.739
- Weighted F1: 0.737
- Macro Precision: 0.735
- Micro Precision: 0.739
- Weighted Precision: 0.738
- Macro Recall: 0.703
- Micro Recall: 0.739
- Weighted Recall: 0.739


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-TweetEval-1281248999
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281248999"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281248999"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,3182032674.083233,0.7273182130584193,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63943,autotrain-BERTBase-TweetEval-1281249000,['sasha/autotrain-data-BERTBase-TweetEval'],,0.0486890565891514,AutoTrain,Not Specified,Not Specified,Not Specified,0.743,0.602,0.723,,,438009197.0,True,16,0,"['transformers', 'pytorch']",2022-08-19 12:31:08+00:00,2022-08-19 12:25:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281249000
- CO2 Emissions (in grams): 0.0487

## Validation Metrics

- Loss: 0.602
- Accuracy: 0.743
- Macro F1: 0.723
- Micro F1: 0.743
- Weighted F1: 0.740
- Macro Precision: 0.740
- Micro Precision: 0.743
- Weighted Precision: 0.742
- Macro Recall: 0.712
- Micro Recall: 0.743
- Weighted Recall: 0.743


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-BERTBase-TweetEval-1281249000
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281249000"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-BERTBase-TweetEval-1281249000"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,8996050194.523476,0.7328635743519782,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63944,autotrain-RobertaBaseTweetEval-1281048989,['sasha/autotrain-data-RobertaBaseTweetEval'],,28.05396378146021,AutoTrain,Not Specified,Not Specified,Not Specified,0.751,0.587,0.719,,,498663405.0,True,14,0,"['transformers', 'pytorch']",2022-08-19 12:50:29+00:00,2022-08-19 12:31:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281048989
- CO2 Emissions (in grams): 28.0540

## Validation Metrics

- Loss: 0.587
- Accuracy: 0.751
- Macro F1: 0.719
- Micro F1: 0.751
- Weighted F1: 0.746
- Macro Precision: 0.761
- Micro Precision: 0.751
- Weighted Precision: 0.753
- Macro Recall: 0.699
- Micro Recall: 0.751
- Weighted Recall: 0.751


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-RobertaBaseTweetEval-1281048989
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048989"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048989"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,17775149.668138787,0.7346517006802721,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63945,autotrain-RobertaBaseTweetEval-1281048990,['sasha/autotrain-data-RobertaBaseTweetEval'],,11.322528589983463,AutoTrain,Not Specified,Not Specified,Not Specified,0.747,0.592,0.729,,,498663405.0,True,15,0,"['transformers', 'pytorch']",2022-08-19 12:42:35+00:00,2022-08-19 12:31:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281048990
- CO2 Emissions (in grams): 11.3225

## Validation Metrics

- Loss: 0.592
- Accuracy: 0.747
- Macro F1: 0.729
- Micro F1: 0.747
- Weighted F1: 0.744
- Macro Precision: 0.743
- Micro Precision: 0.747
- Weighted Precision: 0.746
- Macro Recall: 0.720
- Micro Recall: 0.747
- Weighted Recall: 0.747


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-RobertaBaseTweetEval-1281048990
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048990"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-RobertaBaseTweetEval-1281048990"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,44041699.787902966,0.737890243902439,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
63946,autotrain-DistilBERT-TweetEval-1281148991,['sasha/autotrain-data-DistilBERT-TweetEval'],,7.445009513630644,AutoTrain,Not Specified,Not Specified,Not Specified,0.739,0.61,0.721,,,267857393.0,True,5,0,"['transformers', 'pytorch']",2022-08-19 12:39:50+00:00,2022-08-19 12:32:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1281148991
- CO2 Emissions (in grams): 7.4450

## Validation Metrics

- Loss: 0.610
- Accuracy: 0.739
- Macro F1: 0.721
- Micro F1: 0.739
- Weighted F1: 0.739
- Macro Precision: 0.727
- Micro Precision: 0.739
- Weighted Precision: 0.740
- Macro Recall: 0.715
- Micro Recall: 0.739
- Weighted Recall: 0.739


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sasha/autotrain-DistilBERT-TweetEval-1281148991
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148991"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sasha/autotrain-DistilBERT-TweetEval-1281148991"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,35978112.92377735,0.7298890410958904,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64020,ni_model_8_19,['aujer/autotrain-data-not_interested_8_19'],,7.709202932471896,AutoTrain,Not Specified,Not Specified,Not Specified,0.849,0.551,0.632,,,1334489901.0,True,67,0,"['transformers', 'pytorch']",2022-08-19 20:44:10+00:00,2022-08-19 20:39:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1283149075
- CO2 Emissions (in grams): 7.7092

## Validation Metrics

- Loss: 0.551
- Accuracy: 0.849
- Macro F1: 0.632
- Micro F1: 0.849
- Weighted F1: 0.844
- Macro Precision: 0.632
- Micro Precision: 0.849
- Weighted Precision: 0.845
- Macro Recall: 0.654
- Micro Recall: 0.849
- Weighted Recall: 0.849


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/aujer/autotrain-not_interested_8_19-1283149075
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""aujer/autotrain-not_interested_8_19-1283149075"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""aujer/autotrain-not_interested_8_19-1283149075"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,173103485.88425422,0.7246022957461175,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64035,autotrain-texto_sem_enelvo-1283649112,['Gabesantos1007/autotrain-data-texto_sem_enelvo'],,1.761944621737621,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-08-19 23:11:15+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64145,autotrain-neurips_chanllenge-1287149278,['jawadhussein462/autotrain-data-neurips_chanllenge'],,0.0395580279061519,AutoTrain,Not Specified,Not Specified,Not Specified,0.907,0.264,0.602,,,1334461229.0,True,15,0,"['transformers', 'pytorch']",2022-08-20 22:32:44+00:00,2022-08-20 22:28:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1287149278
- CO2 Emissions (in grams): 0.0396

## Validation Metrics

- Loss: 0.264
- Accuracy: 0.907
- Precision: 0.681
- Recall: 0.539
- AUC: 0.843
- F1: 0.602

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jawadhussein462/autotrain-neurips_chanllenge-1287149278
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jawadhussein462/autotrain-neurips_chanllenge-1287149278"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jawadhussein462/autotrain-neurips_chanllenge-1287149278"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,33734270883.4196,0.7236766070245195,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64146,autotrain-neurips_chanllenge-1287149282,['jawadhussein462/autotrain-data-neurips_chanllenge'],,25.138742530638098,AutoTrain,Not Specified,Not Specified,Not Specified,0.911,0.272,0.591,,,1421584365.0,True,7,0,"['transformers', 'pytorch']",2022-08-20 22:42:12+00:00,2022-08-20 22:29:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1287149282
- CO2 Emissions (in grams): 25.1387

## Validation Metrics

- Loss: 0.272
- Accuracy: 0.911
- Precision: 0.733
- Recall: 0.494
- AUC: 0.823
- F1: 0.591

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jawadhussein462/autotrain-neurips_chanllenge-1287149282
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jawadhussein462/autotrain-neurips_chanllenge-1287149282"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jawadhussein462/autotrain-neurips_chanllenge-1287149282"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,56549541.540012576,0.7169121171770971,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64347,autotrain-test1-1297049687,['mclion/autotrain-data-test1'],,0.0704648578577701,AutoTrain,Not Specified,Not Specified,Not Specified,0.738,0.603,0.725,,,442568685.0,True,4,0,"['transformers', 'pytorch']",2022-08-22 13:30:10+00:00,2022-08-22 13:22:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1297049687
- CO2 Emissions (in grams): 0.0705

## Validation Metrics

- Loss: 0.603
- Accuracy: 0.738
- Macro F1: 0.725
- Micro F1: 0.738
- Weighted F1: 0.737
- Macro Precision: 0.730
- Micro Precision: 0.738
- Weighted Precision: 0.738
- Macro Recall: 0.722
- Micro Recall: 0.738
- Weighted Recall: 0.738


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/mclion/autotrain-test1-1297049687
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mclion/autotrain-test1-1297049687"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mclion/autotrain-test1-1297049687"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,6280700741.542734,0.7314422419685578,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64395,autotrain-pat-abst-1298049754,['ckirby/autotrain-data-pat-abst'],,428.11928889922655,AutoTrain,Not Specified,Not Specified,Not Specified,,1.226,,0.6456000000000001,0.5671200000000001,2283800049.0,True,3,0,"['transformers', 'pytorch']",2022-08-22 22:58:54+00:00,2022-08-22 19:37:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1298049754
- CO2 Emissions (in grams): 428.1193

## Validation Metrics

- Loss: 1.226
- Rouge1: 64.560
- Rouge2: 51.048
- RougeL: 56.712
- RougeLsum: 60.255
- Gen Len: 156.964

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/ckirby/autotrain-pat-abst-1298049754
```",,,1,[],[],NLP,2022-08,5334494.63319457,0.6038206214130221,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64460,autotrain-bert-NER-favsbot,['nguyenkhoa2407/autotrain-data-default_model_favsbot_data'],,0.0120349160313963,AutoTrain,Not Specified,Not Specified,Not Specified,0.71,1.004,0.468,,,1336549553.0,True,7,0,"['transformers', 'pytorch']",2022-08-23 06:38:33+00:00,2022-08-23 06:35:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1300449813
- CO2 Emissions (in grams): 0.0120

## Validation Metrics

- Loss: 1.004
- Accuracy: 0.710
- Precision: 0.542
- Recall: 0.413
- F1: 0.468

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/nguyenkhoa2407/autotrain-default_model_favsbot_data-1300449813
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""nguyenkhoa2407/autotrain-default_model_favsbot_data-1300449813"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""nguyenkhoa2407/autotrain-default_model_favsbot_data-1300449813"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,111055993204.54358,0.5641426146010187,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64551,autotrain-indian-food-image-classification,['abhishek/autotrain-data-indian-food-image-classification'],,17.39230724017564,AutoTrain,Not Specified,Not Specified,Not Specified,0.762,0.875,0.758,,,343506865.0,True,4,3,"['transformers', 'pytorch']",2022-08-23 16:59:19+00:00,2022-08-23 16:24:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 57
- CO2 Emissions (in grams): 17.3923

## Validation Metrics

- Loss: 0.875
- Accuracy: 0.762
- Macro F1: 0.758
- Micro F1: 0.762
- Weighted F1: 0.758
- Macro Precision: 0.772
- Micro Precision: 0.762
- Weighted Precision: 0.772
- Macro Recall: 0.762
- Micro Recall: 0.762
- Weighted Recall: 0.762",,,1,[],[],Computer Vision,2022-08,19750505.79870799,0.7599947368421053,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64756,autotrain-app_review_bert_train-1310050094,['noob123/autotrain-data-app_review_bert_train'],,4.094086460501482,AutoTrain,Not Specified,Not Specified,Not Specified,0.8,0.449,0.849,,,438006125.0,True,14,1,"['transformers', 'pytorch']",2022-08-24 20:30:47+00:00,2022-08-24 20:28:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1310050094
- CO2 Emissions (in grams): 4.0941

## Validation Metrics

- Loss: 0.449
- Accuracy: 0.800
- Precision: 0.855
- Recall: 0.844
- AUC: 0.851
- F1: 0.849

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/noob123/autotrain-app_review_bert_train-1310050094
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""noob123/autotrain-app_review_bert_train-1310050094"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""noob123/autotrain-app_review_bert_train-1310050094"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,106985069.61827788,0.8237719830200122,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64786,autotrain-app_review_train_roberta-1314150168,['noob123/autotrain-data-app_review_train_roberta'],,0.0159004761183563,AutoTrain,Not Specified,Not Specified,Not Specified,0.801,0.445,0.848,,,498660333.0,True,12,0,"['transformers', 'pytorch']",2022-08-25 04:29:15+00:00,2022-08-25 04:27:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1314150168
- CO2 Emissions (in grams): 0.0159

## Validation Metrics

- Loss: 0.445
- Accuracy: 0.801
- Precision: 0.862
- Recall: 0.835
- AUC: 0.859
- F1: 0.848

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/noob123/autotrain-app_review_train_roberta-1314150168
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""noob123/autotrain-app_review_train_roberta-1314150168"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""noob123/autotrain-app_review_train_roberta-1314150168"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,31361345992.92418,0.8238302001212855,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64789,autotrain-app_review_train_dilbert-1314250179,['noob123/autotrain-data-app_review_train_dilbert'],,0.0044442935958964,AutoTrain,Not Specified,Not Specified,Not Specified,0.809,0.447,0.856,,,267854321.0,True,4,0,"['transformers', 'pytorch']",2022-08-25 04:43:23+00:00,2022-08-25 04:42:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1314250179
- CO2 Emissions (in grams): 0.0044

## Validation Metrics

- Loss: 0.447
- Accuracy: 0.809
- Precision: 0.857
- Recall: 0.855
- AUC: 0.857
- F1: 0.856

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/noob123/autotrain-app_review_train_dilbert-1314250179
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""noob123/autotrain-app_review_train_dilbert-1314250179"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""noob123/autotrain-app_review_train_dilbert-1314250179"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,60269267819.597015,0.8318366366366367,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64792,autotrain-app_review_train_albert-1314550196,['noob123/autotrain-data-app_review_train_albert'],,0.0154491824293936,AutoTrain,Not Specified,Not Specified,Not Specified,0.813,0.443,0.855,,,46753425.0,True,4,0,"['transformers', 'pytorch']",2022-08-25 05:13:05+00:00,2022-08-25 05:11:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1314550196
- CO2 Emissions (in grams): 0.0154

## Validation Metrics

- Loss: 0.443
- Accuracy: 0.813
- Precision: 0.883
- Recall: 0.829
- AUC: 0.863
- F1: 0.855

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/noob123/autotrain-app_review_train_albert-1314550196
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""noob123/autotrain-app_review_train_albert-1314550196"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""noob123/autotrain-app_review_train_albert-1314550196"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,3026271792.288955,0.8334712230215828,0.0,1.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64829,demo_knots_1_1,['dav3794/autotrain-data-demo-knots2'],,0.021557396511961,AutoTrain,Not Specified,Not Specified,Not Specified,0.833,0.391,0.829,,,1680217005.0,True,6,0,"['transformers', 'pytorch']",2022-08-25 10:12:54+00:00,2022-08-25 09:54:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Dataset: 1:1 (unknotted : knotted)
- Model ID: 1315550258
- CO2 Emissions (in grams): 0.0216

## Validation Metrics

- Loss: 0.391
- Accuracy: 0.833
- Precision: 0.836
- Recall: 0.823
- AUC: 0.900
- F1: 0.829

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dav3794/autotrain-demo-knots2-1315550258
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dav3794/autotrain-demo-knots2-1315550258"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dav3794/autotrain-demo-knots2-1315550258"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,77941554958.53783,0.8309951865222621,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64844,demo_knots_1_4,['dav3794/autotrain-data-demo-knots3'],,0.0330523943939798,AutoTrain,Not Specified,Not Specified,Not Specified,0.88,0.345,0.923,,,1680217005.0,True,6,0,"['transformers', 'pytorch']",2022-08-25 10:59:07+00:00,2022-08-25 10:55:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1315750263
- CO2 Emissions (in grams): 0.0331

## Validation Metrics

- Loss: 0.345
- Accuracy: 0.880
- Precision: 0.894
- Recall: 0.955
- AUC: 0.888
- F1: 0.923

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dav3794/autotrain-demo-knots3-1315750263
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dav3794/autotrain-demo-knots3-1315750263"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dav3794/autotrain-demo-knots3-1315750263"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,50834955706.14505,0.9009872434830836,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64846,demo_knots_all,['dav3794/autotrain-data-demo-knots-all'],,0.1285808899475734,AutoTrain,Not Specified,Not Specified,Not Specified,0.982,0.085,0.991,,,1680217005.0,True,6,0,"['transformers', 'pytorch']",2022-08-25 11:21:43+00:00,2022-08-25 11:08:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1315850267
- CO2 Emissions (in grams): 0.1286

## Validation Metrics

- Loss: 0.085
- Accuracy: 0.982
- Precision: 0.984
- Recall: 0.997
- AUC: 0.761
- F1: 0.991

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dav3794/autotrain-demo-knots-all-1315850267
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dav3794/autotrain-demo-knots-all-1315850267"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dav3794/autotrain-demo-knots-all-1315850267"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,13067392873.739471,0.9864794728839332,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64847,demo_knots_12_error,['dav3794/autotrain-data-demo-knots-1-2'],,0.0198666409221839,AutoTrain,Not Specified,Not Specified,Not Specified,0.792,0.396,0.761,,,1680217005.0,True,8,0,"['transformers', 'pytorch']",2022-08-25 11:39:44+00:00,2022-08-25 11:37:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1315950270
- CO2 Emissions (in grams): 0.0199

## Validation Metrics

- Loss: 0.396
- Accuracy: 0.792
- Precision: 0.915
- Recall: 0.652
- AUC: 0.900
- F1: 0.761

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dav3794/autotrain-demo-knots-1-2-1315950270
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dav3794/autotrain-demo-knots-1-2-1315950270"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dav3794/autotrain-demo-knots-1-2-1315950270"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,84574791057.09317,0.776190598840953,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64857,demo_knots_1_8,['dav3794/autotrain-data-demo-knots_1_8'],,0.0635778215050862,AutoTrain,Not Specified,Not Specified,Not Specified,0.931,0.242,0.962,,,1680217005.0,True,6,0,"['transformers', 'pytorch']",2022-08-25 12:20:03+00:00,2022-08-25 12:13:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1316050278
- CO2 Emissions (in grams): 0.0636

## Validation Metrics

- Loss: 0.242
- Accuracy: 0.931
- Precision: 0.943
- Recall: 0.981
- AUC: 0.852
- F1: 0.962

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dav3794/autotrain-demo-knots_1_8-1316050278
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dav3794/autotrain-demo-knots_1_8-1316050278"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dav3794/autotrain-demo-knots_1_8-1316050278"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,26427722202.868862,0.9462461701003696,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
64991,bart-base-cnn-swe,['Gabriel/cnn_daily_swe'],,0.0334,Not Specified,fine-tuning,"Fredericia, Denmark",Tesla P100-PCIE-16GB,,,,,,557723065.0,False,27,0,"['tensorboard', 'transformers', 'pytorch']",2022-12-20 09:37:08+00:00,2022-08-26 05:26:14+00:00,"
# bart-base-cnn-swe
This model is a W.I.P

## Model description
BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. This model is a fine-tuned version of [KBLab/bart-base-swedish-cased](https://huggingface.co/KBLab/bart-base-swedish-cased) on the [Gabriel/bart-base-cnn-swe](https://huggingface.co/datasets/Gabriel/cnn_daily_swe) dataset and can be used for summarization tasks.


## Intended uses & limitations

This model should only be used to fine-tune further on and summarization tasks.


```python
from transformers import pipeline
summarizer = pipeline(""summarization"", model=""Gabriel/bart-base-cnn-swe"")
ARTICLE = """"""
Frankrike lås Sebastien Chabal har nämnts för en farlig tackling på Englands Simon Shaw under lördagens VM semifinal i Paris. Simon Shaw lastar av trots att Raphael Ibanez, vänster, och Sebastien Chabal. Sale Sharks framåt kommer att ställas inför en disciplinär utfrågning på måndag efter hans tackling på motsatt andra-rower Shaw noterades genom att citera kommissionär Dennis Wheelahan. Chabal började matchen på ersättningsbänken, men kom i 26: e minuten att ersätta den skadade Fabien Pelous under värd Frankrikes 14-9 nederlag. Om han blir avstängd missar Chabal fredagens tredje och fjärde match på Parc des Princes. Samtidigt, Frankrike tränare Bernard Laporte sade att nederlaget var svårare att ta än Englands 24-7 seger i 2003 semifinalen. ""År 2003 var de bättre än oss. I själva verket var de bättre än alla"", sade Laporte, som lämnar sin roll att tillträda posten som junior idrottsminister i den franska regeringen. ""De var som Nya Zeeland i denna turnering - favoriten, förutom att de gick hela vägen. Den här gången är det svårare för igår var det 50-50."" Samtidigt, England -- försöker bli den första nationen att försvara VM-titeln -- avslöjade att stjärna kicker Jonny Wilkinson återigen hade problem med matchbollarna under semifinalen. Flughalvan, som uttryckte sin oro efter att ha kämpat med stöveln mot Australien, avvisade en boll innan han sparkade en vital trepoängare mot Frankrike. ""Vi sa det inte förra veckan men en icke-match bollen kom ut på fältet i Marseille som Jonny sparkade,"" chef för rugby Rob Andrew sade. ""Han tänkte inte på det när han sparkade det. Matchbollarna är märkta, numrerade ett till sex. Igår kväll hade de ""World Cup semifinal England vs Frankrike"" skrivet på dem. På matchkvällen var Jonny vaksam när han sparkade för mål att de faktiskt var matchbollar han sparkade. ""Träningsbollarna förlorar tryck och form. Hela frågan förra veckan, arrangörerna accepterade alla sex matchbollar bör användas av båda sidor på torsdagen före matchen. "" E-post till en vän.
""""""
print(summarizer(ARTICLE, max_length=130, min_length=30, num_beams=10 ,do_sample=False))
>>> [{'summary_text': """""" Frankrike lås Sebastien Chabal har nämnts för en farlig tackling på Englands Simon Shaw under VM semifinal i Paris. Sale Sharks framåt kommer att ställas inför en disciplinär utfrågning på måndag efter hans tackling på motsatt andra - rower Shaw noterades genom att citera kommissionär Dennis Wheelahan. Om Chabal blir avstängd missar Chabal fredagens tredje och fjärde match på Parc des Princes.""""""}]
```

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 2*2 = 4
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |
|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|
| 2.2349        | 1.0   | 17944 | 2.0643          | 21.9564 | 10.2133 | 17.9958 | 20.6502   | 19.9992 |
| 2.0726        | 2.0   | 35888 | 2.0253          | 22.0568 | 10.3302 | 18.0648 | 20.7482   | 19.9996 |
| 1.8658        | 3.0   | 53832 | 2.0333          | 22.0871 | 10.2902 | 18.0577 | 20.7082   | 19.998  |
| 1.8121        | 4.0   | 71776 | 1.9759          | 22.2046 | 10.4332 | 18.1753 | 20.846    | 19.9971 |


### Framework versions

- Transformers 4.22.1
- Pytorch 1.12.1+cu113
- Datasets 2.4.0
- Tokenizers 0.12.1
",,,1,[],[],NLP,2022-08,16698295359.281437,,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,1,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
65062,doe2vec-d2-m8-ls24-VAE-kl0.001,['BasStein/250000-randomfunctions-2d'],,0.0363,Code Carbon,pre-training,"Leiden, The Netherlands",1 Tesla T4,,,,,,,False,3,0,['keras'],2022-09-02 10:37:48+00:00,2022-08-26 14:56:33+00:00,"
## Model description

DoE2Vec model that can transform any design of experiments (function landscape) to a feature vector.  
For different input dimensions or sample size you require a different model.  
Each model name is build up like doe2vec-d{dimension\}-m{sample size}-ls{latent size}-{AE or VAE}-kl{Kl loss weight}

Example code of loading this huggingface model using the doe2vec package.

First install the package

```zsh
pip install doe2vec
```

Then import and load the model.

```python
from doe2vec import doe_model

obj = doe_model(
    2,
    8,
    latent_dim=24,
    kl_weight=0.001,
    model_type=""VAE""
)
obj.load_from_huggingface()
#test the model
obj.plot_label_clusters_bbob()
```

## Intended uses & limitations

The model is intended to be used to generate feature representations for optimization function landscapes.
The representations can then be used for downstream tasks such as automatic optimization pipelines and meta-learning.


## Training procedure

The model is trained using a weighed KL loss and mean squared error reconstruction loss.
The model is trained using 250.000 randomly generated functions (see the dataset) over 100 epochs.

- **Hardware:** 1x Tesla T4 GPU
- **Optimizer:** Adam

",,,1,[],[],Not Specified,2022-08,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,1.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
65477,demo_knots_1_2,['dav3794/autotrain-data-demo-knots-1-2_bis'],,0.0401933452212558,AutoTrain,Not Specified,Not Specified,Not Specified,0.857,0.381,0.901,,,1680217005.0,True,7,0,"['transformers', 'pytorch']",2022-08-29 08:26:35+00:00,2022-08-29 08:21:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1328150718
- CO2 Emissions (in grams): 0.0402

## Validation Metrics

- Loss: 0.381
- Accuracy: 0.857
- Precision: 0.842
- Recall: 0.970
- AUC: 0.889
- F1: 0.901

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dav3794/autotrain-demo-knots-1-2_bis-1328150718
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dav3794/autotrain-demo-knots-1-2_bis-1328150718"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dav3794/autotrain-demo-knots-1-2_bis-1328150718"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,41803363112.743256,0.8784493742889647,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
65711,autotrain-enhanced-tosdr-summariser-1339851270,['Laksitha/autotrain-data-enhanced-tosdr-summariser'],,0.007732320614947,AutoTrain,Not Specified,Not Specified,Not Specified,,2.532,,0.3506499999999999,0.20884,920019705.0,True,8,0,"['transformers', 'pytorch']",2022-08-30 16:39:36+00:00,2022-08-30 16:37:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1339851270
- CO2 Emissions (in grams): 0.0077

## Validation Metrics

- Loss: 2.532
- Rouge1: 35.065
- Rouge2: 14.118
- RougeL: 20.884
- RougeLsum: 31.861
- Gen Len: 90.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Laksitha/autotrain-enhanced-tosdr-summariser-1339851270
```",,,1,[],[],NLP,2022-08,118983646800.87778,0.2617732077427657,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
65712,autotrain-enhanced-tosdr-summariser-1339851272,['Laksitha/autotrain-data-enhanced-tosdr-summariser'],,0.0119601182774247,AutoTrain,Not Specified,Not Specified,Not Specified,,2.416,,0.34945,0.19876,920019705.0,True,2,0,"['transformers', 'pytorch']",2022-08-30 16:40:01+00:00,2022-08-30 16:38:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1339851272
- CO2 Emissions (in grams): 0.0120

## Validation Metrics

- Loss: 2.416
- Rouge1: 34.945
- Rouge2: 12.533
- RougeL: 19.876
- RougeLsum: 31.821
- Gen Len: 92.917

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Laksitha/autotrain-enhanced-tosdr-summariser-1339851272
```",,,1,[],[],NLP,2022-08,76923963765.18964,0.253394436438591,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
65909,attribute-classification,['AaronCU/autotrain-data-attribute-classification'],,0.0028470089436147,AutoTrain,Not Specified,Not Specified,Not Specified,0.949,0.163,0.947,,,328518765.0,True,11,0,"['transformers', 'pytorch']",2022-08-31 19:25:23+00:00,2022-08-31 19:24:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1343651539
- CO2 Emissions (in grams): 0.0028

## Validation Metrics

- Loss: 0.163
- Accuracy: 0.949
- Macro F1: 0.947
- Micro F1: 0.949
- Weighted F1: 0.949
- Macro Precision: 0.943
- Micro Precision: 0.949
- Weighted Precision: 0.951
- Macro Recall: 0.952
- Micro Recall: 0.949
- Weighted Recall: 0.949


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/AaronCU/autotrain-attribute-classification-1343651539
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""AaronCU/autotrain-attribute-classification-1343651539"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""AaronCU/autotrain-attribute-classification-1343651539"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-08,115390844042.4134,0.9479989451476792,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
65939,comments-text-classification-model,['EricPeter/autotrain-data-comments'],,0.0067037448010476,AutoTrain,Not Specified,Not Specified,Not Specified,0.619,1.08,0.36,,,1334473517.0,True,15,1,"['transformers', 'pytorch']",2022-09-01 00:17:31+00:00,2022-09-01 00:16:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1344451577
- CO2 Emissions (in grams): 0.0067

## Validation Metrics

- Loss: 1.080
- Accuracy: 0.619
- Macro F1: 0.360
- Micro F1: 0.619
- Weighted F1: 0.564
- Macro Precision: 0.476
- Micro Precision: 0.619
- Weighted Precision: 0.590
- Macro Recall: 0.344
- Micro Recall: 0.619
- Weighted Recall: 0.619


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/EricPeter/autotrain-comments-1344451577
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""EricPeter/autotrain-comments-1344451577"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""EricPeter/autotrain-comments-1344451577"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,199063889900.97903,0.4552400408580184,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
65990,LRO_v1.0.2a,['PhucLe/autotrain-data-LRO_v1.0.2'],,1.2585708613878817,AutoTrain,Not Specified,Not Specified,Not Specified,0.818,0.523,0.817,,,498663405.0,True,14,0,"['transformers', 'pytorch']",2022-09-01 09:56:58+00:00,2022-09-01 09:55:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1345851607
- CO2 Emissions (in grams): 1.2586

## Validation Metrics

- Loss: 0.523
- Accuracy: 0.818
- Macro F1: 0.817
- Micro F1: 0.818
- Weighted F1: 0.817
- Macro Precision: 0.824
- Micro Precision: 0.818
- Weighted Precision: 0.824
- Macro Recall: 0.818
- Micro Recall: 0.818
- Weighted Recall: 0.818


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/PhucLe/autotrain-LRO_v1.0.2-1345851607
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""PhucLe/autotrain-LRO_v1.0.2-1345851607"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""PhucLe/autotrain-LRO_v1.0.2-1345851607"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,396214007.72786194,0.8174996941896024,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66137,doe2vec-d2-m8-ls32-VAE-kl0.001,['BasStein/1595-randomfunctions-2d'],,0.0363,Code Carbon,pre-training,"Leiden, The Netherlands",1 Tesla T4,,,,,,,False,0,0,['keras'],2022-09-02 10:38:09+00:00,2022-09-02 10:37:51+00:00,"
## Model description

DoE2Vec model that can transform any design of experiments (function landscape) to a feature vector.  
For different input dimensions or sample size you require a different model.  
Each model name is build up like doe2vec-d{dimension\}-m{sample size}-ls{latent size}-{AE or VAE}-kl{Kl loss weight}

Example code of loading this huggingface model using the doe2vec package.

First install the package

```zsh
pip install doe2vec
```

Then import and load the model.

```python
from doe2vec import doe_model

obj = doe_model(
    2,
    8,
    latent_dim=32,
    kl_weight=0.001,
    model_type=""VAE""
)
obj.load_from_huggingface()
#test the model
obj.plot_label_clusters_bbob()
```

## Intended uses & limitations

The model is intended to be used to generate feature representations for optimization function landscapes.
The representations can then be used for downstream tasks such as automatic optimization pipelines and meta-learning.


## Training procedure

The model is trained using a weighed KL loss and mean squared error reconstruction loss.
The model is trained using 250.000 randomly generated functions (see the dataset) over 100 epochs.

- **Hardware:** 1x Tesla T4 GPU
- **Optimizer:** Adam

",,,1,[],[],Not Specified,2022-09,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,1.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66140,doe2vec-d5-m8-ls24-VAE-kl0.001,['BasStein/250000-randomfunctions-5d'],,0.0363,Code Carbon,pre-training,"Leiden, The Netherlands",1 Tesla T4,,,,,,,False,0,0,['keras'],2022-09-02 10:48:04+00:00,2022-09-02 10:47:43+00:00,"
## Model description

DoE2Vec model that can transform any design of experiments (function landscape) to a feature vector.  
For different input dimensions or sample size you require a different model.  
Each model name is build up like doe2vec-d{dimension\}-m{sample size}-ls{latent size}-{AE or VAE}-kl{Kl loss weight}

Example code of loading this huggingface model using the doe2vec package.

First install the package

```zsh
pip install doe2vec
```

Then import and load the model.

```python
from doe2vec import doe_model

obj = doe_model(
    5,
    8,
    latent_dim=24,
    kl_weight=0.001,
    model_type=""VAE""
)
obj.load_from_huggingface()
#test the model
obj.plot_label_clusters_bbob()
```

## Intended uses & limitations

The model is intended to be used to generate feature representations for optimization function landscapes.
The representations can then be used for downstream tasks such as automatic optimization pipelines and meta-learning.


## Training procedure

The model is trained using a weighed KL loss and mean squared error reconstruction loss.
The model is trained using 250.000 randomly generated functions (see the dataset) over 100 epochs.

- **Hardware:** 1x Tesla T4 GPU
- **Optimizer:** Adam

",,,1,[],[],Not Specified,2022-09,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,1.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66141,doe2vec-d5-m8-ls32-VAE-kl0.001,['BasStein/250000-randomfunctions-5d'],,0.0363,Code Carbon,pre-training,"Leiden, The Netherlands",1 Tesla T4,,,,,,,False,0,0,['keras'],2022-09-02 10:48:26+00:00,2022-09-02 10:48:06+00:00,"
## Model description

DoE2Vec model that can transform any design of experiments (function landscape) to a feature vector.  
For different input dimensions or sample size you require a different model.  
Each model name is build up like doe2vec-d{dimension\}-m{sample size}-ls{latent size}-{AE or VAE}-kl{Kl loss weight}

Example code of loading this huggingface model using the doe2vec package.

First install the package

```zsh
pip install doe2vec
```

Then import and load the model.

```python
from doe2vec import doe_model

obj = doe_model(
    5,
    8,
    latent_dim=32,
    kl_weight=0.001,
    model_type=""VAE""
)
obj.load_from_huggingface()
#test the model
obj.plot_label_clusters_bbob()
```

## Intended uses & limitations

The model is intended to be used to generate feature representations for optimization function landscapes.
The representations can then be used for downstream tasks such as automatic optimization pipelines and meta-learning.


## Training procedure

The model is trained using a weighed KL loss and mean squared error reconstruction loss.
The model is trained using 250.000 randomly generated functions (see the dataset) over 100 epochs.

- **Hardware:** 1x Tesla T4 GPU
- **Optimizer:** Adam

",,,1,[],[],Not Specified,2022-09,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,1.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66142,doe2vec-d10-m8-ls24-VAE-kl0.001,['BasStein/250000-randomfunctions-10d'],,0.0363,Code Carbon,pre-training,"Leiden, The Netherlands",1 Tesla T4,,,,,,,False,0,0,['keras'],2022-09-02 10:51:21+00:00,2022-09-02 10:50:52+00:00,"
## Model description

DoE2Vec model that can transform any design of experiments (function landscape) to a feature vector.  
For different input dimensions or sample size you require a different model.  
Each model name is build up like doe2vec-d{dimension\}-m{sample size}-ls{latent size}-{AE or VAE}-kl{Kl loss weight}

Example code of loading this huggingface model using the doe2vec package.

First install the package

```zsh
pip install doe2vec
```

Then import and load the model.

```python
from doe2vec import doe_model

obj = doe_model(
    10,
    8,
    latent_dim=24,
    kl_weight=0.001,
    model_type=""VAE""
)
obj.load_from_huggingface()
#test the model
obj.plot_label_clusters_bbob()
```

## Intended uses & limitations

The model is intended to be used to generate feature representations for optimization function landscapes.
The representations can then be used for downstream tasks such as automatic optimization pipelines and meta-learning.


## Training procedure

The model is trained using a weighed KL loss and mean squared error reconstruction loss.
The model is trained using 250.000 randomly generated functions (see the dataset) over 100 epochs.

- **Hardware:** 1x Tesla T4 GPU
- **Optimizer:** Adam

",,,1,[],[],Not Specified,2022-09,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,1.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66143,doe2vec-d10-m8-ls32-VAE-kl0.001,['BasStein/250000-randomfunctions-10d'],,0.0363,Code Carbon,pre-training,"Leiden, The Netherlands",1 Tesla T4,,,,,,,False,0,0,['keras'],2022-09-02 10:51:54+00:00,2022-09-02 10:51:24+00:00,"
## Model description

DoE2Vec model that can transform any design of experiments (function landscape) to a feature vector.  
For different input dimensions or sample size you require a different model.  
Each model name is build up like doe2vec-d{dimension\}-m{sample size}-ls{latent size}-{AE or VAE}-kl{Kl loss weight}

Example code of loading this huggingface model using the doe2vec package.

First install the package

```zsh
pip install doe2vec
```

Then import and load the model.

```python
from doe2vec import doe_model

obj = doe_model(
    10,
    8,
    latent_dim=32,
    kl_weight=0.001,
    model_type=""VAE""
)
obj.load_from_huggingface()
#test the model
obj.plot_label_clusters_bbob()
```

## Intended uses & limitations

The model is intended to be used to generate feature representations for optimization function landscapes.
The representations can then be used for downstream tasks such as automatic optimization pipelines and meta-learning.


## Training procedure

The model is trained using a weighed KL loss and mean squared error reconstruction loss.
The model is trained using 250.000 randomly generated functions (see the dataset) over 100 epochs.

- **Hardware:** 1x Tesla T4 GPU
- **Optimizer:** Adam

",,,1,[],[],Not Specified,2022-09,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,1.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66164,LRO_v1.0.2b,['PhucLe/autotrain-data-LRO_v1.0.2b'],,5.027600666336915,AutoTrain,Not Specified,Not Specified,Not Specified,0.833,0.685,0.833,,,1340715821.0,True,16,0,"['transformers', 'pytorch']",2022-09-02 14:51:12+00:00,2022-09-02 14:47:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1350751935
- CO2 Emissions (in grams): 5.0276

## Validation Metrics

- Loss: 0.685
- Accuracy: 0.833
- Macro F1: 0.833
- Micro F1: 0.833
- Weighted F1: 0.833
- Macro Precision: 0.838
- Micro Precision: 0.833
- Weighted Precision: 0.838
- Macro Recall: 0.833
- Micro Recall: 0.833
- Weighted Recall: 0.833


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/PhucLe/autotrain-LRO_v1.0.2b-1350751935
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""PhucLe/autotrain-LRO_v1.0.2b-1350751935"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""PhucLe/autotrain-LRO_v1.0.2b-1350751935"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,266671104.16644108,0.8329999999999999,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66350,PromptGeneration-base,['SamAct/autotrain-data-t3'],,2.4412207269598545,AutoTrain,Not Specified,Not Specified,Not Specified,,0.01,,0.99696,0.99689,2279605745.0,True,8,1,"['transformers', 'pytorch']",2022-10-28 19:51:16+00:00,2022-09-03 15:05:59+00:00,"## If you like this model you can by me a coffee here: https://www.buymeacoffee.com/SamAct
## Usecase
1. Prompt Generation.
2. Title Generation.

## Features
Excellent accuracy for one line prompts. Prompts can be used for image generation, title or meta descriptions.

# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1353852105
- CO2 Emissions (in grams): 2.4412

## Validation Metrics

- Loss: 0.010
- Rouge1: 99.696
- Rouge2: 99.467
- RougeL: 99.689
- RougeLsum: 99.687
- Gen Len: 19.770

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""The authors present a new class of drugs that have the potential to treat kidney disease. In this study, they investigate the molecular and biological mechanisms behind the adverse effects of heavy metal poisoning caused by excessive use of end-resteroids. They examine several different pathways involved in the pathway of transcription and proteomics in order to tease out the etiology of the phenomenon of kidney toxicity from the pathophysiology. The authors suggest that an excess of lipoproteins leads to cataractosis, a chronic inflammation of the kidney, fibrosis, and kidney degeneration. They also show that certain enzymes, such as those involved in disrupterase and creatine kinase, are overexpressed in alenderrate-treated mouse models. Although the authors do not yet have specific treatments for either cataracts or kidney disease, they note that suppressing these pathways may be therapeutics in the treatment of patients with both types of disease. Lastly, the authors discuss the role of spirometry in this review of the literature. It was previously reported that spirometry had a significant effect on renal function in treating acute kidney disease. The authors argue that if spirometry is neglected, then all other aspects of kidney function--irregular growth, tissue growth, and apoptosis--will suffer as a result.""}' https://api-inference.huggingface.co/SamAct/autotrain-t3-1353852105
```",,,1,[],[],NLP,2022-09,933797472.6434836,0.9969249987712216,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66387,summarization_trial_model,['rosettastone/autotrain-data-summarization-trial'],,1116.1106035336509,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2022-09-04 03:10:17+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66456,autotrain-Lucy-Alicorp-1356152290,['luch0247/autotrain-data-Lucy-Alicorp'],,0.6615928015918582,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,1.0,,,437130033.0,True,15,0,"['transformers', 'pytorch']",2022-09-04 03:52:28+00:00,2022-09-04 03:51:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1356152290
- CO2 Emissions (in grams): 0.6616

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/luch0247/autotrain-Lucy-Alicorp-1356152290
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""luch0247/autotrain-Lucy-Alicorp-1356152290"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""luch0247/autotrain-Lucy-Alicorp-1356152290"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,660723683.734499,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66881,autotrain-ecomm1.8-1360552485,['neuralspace/autotrain-data-ecomm1.8'],,0.0347977376041225,AutoTrain,Not Specified,Not Specified,Not Specified,0.914,0.539,0.903,,,1341474285.0,True,15,0,"['transformers', 'pytorch']",2022-09-06 04:02:09+00:00,2022-09-06 03:56:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1360552485
- CO2 Emissions (in grams): 0.0348

## Validation Metrics

- Loss: 0.539
- Accuracy: 0.914
- Macro F1: 0.903
- Micro F1: 0.914
- Weighted F1: 0.907
- Macro Precision: 0.927
- Micro Precision: 0.914
- Weighted Precision: 0.928
- Macro Recall: 0.907
- Micro Recall: 0.914
- Weighted Recall: 0.914


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/neuralspace/autotrain-ecomm1.8-1360552485
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""neuralspace/autotrain-ecomm1.8-1360552485"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""neuralspace/autotrain-ecomm1.8-1360552485"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,38550617866.63611,0.9084667033571822,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66884,autotrain-cuisine_classification-1361652530,['aaazzzz/autotrain-data-cuisine_classification'],,181.0388682785841,AutoTrain,Not Specified,Not Specified,Not Specified,0.731,0.92,0.669,,,1340998701.0,True,15,0,"['transformers', 'pytorch']",2022-09-06 05:55:22+00:00,2022-09-06 04:30:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1361652530
- CO2 Emissions (in grams): 181.0389

## Validation Metrics

- Loss: 0.920
- Accuracy: 0.731
- Macro F1: 0.669
- Micro F1: 0.731
- Weighted F1: 0.726
- Macro Precision: 0.774
- Micro Precision: 0.731
- Weighted Precision: 0.738
- Macro Recall: 0.623
- Micro Recall: 0.731
- Weighted Recall: 0.731


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/aaazzzz/autotrain-cuisine_classification-1361652530
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""aaazzzz/autotrain-cuisine_classification-1361652530"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""aaazzzz/autotrain-cuisine_classification-1361652530"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,7407241.957215841,0.6986271428571429,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66951,autotrain-emotion-detection-1366352626,['rahulmallah/autotrain-data-emotion-detection'],,0.0371606670722015,AutoTrain,Not Specified,Not Specified,Not Specified,0.394,1.772,0.197,,,438039917.0,True,42,1,"['transformers', 'pytorch']",2022-09-06 13:18:36+00:00,2022-09-06 13:14:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1366352626
- CO2 Emissions (in grams): 0.0372

## Validation Metrics

- Loss: 1.772
- Accuracy: 0.394
- Macro F1: 0.197
- Micro F1: 0.394
- Weighted F1: 0.351
- Macro Precision: 0.217
- Micro Precision: 0.394
- Weighted Precision: 0.345
- Macro Recall: 0.213
- Micro Recall: 0.394
- Weighted Recall: 0.394


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/rahulmallah/autotrain-emotion-detection-1366352626
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""rahulmallah/autotrain-emotion-detection-1366352626"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""rahulmallah/autotrain-emotion-detection-1366352626"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,11787730186.568184,0.2626666666666666,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
66977,autotrain-arabic_cuisine-1367052683,['Azizjah/autotrain-data-arabic_cuisine'],,0.0243096886515892,AutoTrain,Not Specified,Not Specified,Not Specified,0.439,2.302,0.133,,,442743405.0,True,14,0,"['transformers', 'pytorch']",2022-09-06 15:18:01+00:00,2022-09-06 15:14:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1367052683
- CO2 Emissions (in grams): 0.0243

## Validation Metrics

- Loss: 2.302
- Accuracy: 0.439
- Macro F1: 0.133
- Micro F1: 0.439
- Weighted F1: 0.391
- Macro Precision: 0.167
- Micro Precision: 0.439
- Weighted Precision: 0.378
- Macro Recall: 0.140
- Micro Recall: 0.439
- Weighted Recall: 0.439


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Azizjah/autotrain-arabic_cuisine-1367052683
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Azizjah/autotrain-arabic_cuisine-1367052683"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Azizjah/autotrain-arabic_cuisine-1367052683"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,18212631652.56773,0.2041503496503496,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67028,dark_IntentCLF,['SalmanFaroz/dark_IntentCLF'],,0.0602773344712369,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,438184493.0,True,0,0,"['transformers', 'pytorch']",2023-04-05 14:45:10+00:00,2022-09-06 22:23:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1368152708
- CO2 Emissions (in grams): 0.0603


## Usage

Python API:

```
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F

tokenizer = AutoTokenizer.from_pretrained(""SalmanFaroz/dark_IntentCLF"")
model = AutoModelForSequenceClassification.from_pretrained(""SalmanFaroz/dark_IntentCLF"")

# Define your input sequence
input_text = ""I love AutoTrain""

# Tokenize your input sequence
inputs = tokenizer(input_text, return_tensors=""pt"")

# Pass the inputs to the model's forward method to get the logits
outputs = model(**inputs)
logits = outputs.logits

# Apply a softmax function to the logits to get the output probabilities
probs = F.softmax(logits, dim=1)

# Convert the tensor of output probabilities to a dictionary
class_probs = {model.config.id2label[i]: prob.item() for i, prob in enumerate(probs[0])}

print(class_probs)

```",,,1,[],[],NLP,2022-09,7269473622.943506,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67068,autotrain-citizen_nlu_bn-1370652766,['neuralspace/autotrain-data-citizen_nlu_bn'],,0.0843150353265822,AutoTrain,Not Specified,Not Specified,Not Specified,0.971,0.117,0.971,,,334011693.0,True,4,0,"['transformers', 'pytorch']",2022-09-07 05:42:31+00:00,2022-09-07 05:33:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1370652766
- CO2 Emissions (in grams): 0.0843

## Validation Metrics

- Loss: 0.117
- Accuracy: 0.971
- Macro F1: 0.971
- Micro F1: 0.971
- Weighted F1: 0.971
- Macro Precision: 0.973
- Micro Precision: 0.971
- Weighted Precision: 0.972
- Macro Recall: 0.970
- Micro Recall: 0.971
- Weighted Recall: 0.971


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/neuralspace/autotrain-citizen_nlu_bn-1370652766
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""neuralspace/autotrain-citizen_nlu_bn-1370652766"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""neuralspace/autotrain-citizen_nlu_bn-1370652766"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,3961472490.715963,0.971,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67070,autotrain-citizen_nlu_hindi-1370952776,['neuralspace/autotrain-data-citizen_nlu_hindi'],,0.0628354508876492,AutoTrain,Not Specified,Not Specified,Not Specified,0.974,0.101,0.974,,,334011693.0,True,4,0,"['transformers', 'pytorch']",2022-09-07 05:48:02+00:00,2022-09-07 05:39:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1370952776
- CO2 Emissions (in grams): 0.0628

## Validation Metrics

- Loss: 0.101
- Accuracy: 0.974
- Macro F1: 0.974
- Micro F1: 0.974
- Weighted F1: 0.974
- Macro Precision: 0.975
- Micro Precision: 0.974
- Weighted Precision: 0.975
- Macro Recall: 0.973
- Micro Recall: 0.974
- Weighted Recall: 0.974


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/neuralspace/autotrain-citizen_nlu_hindi-1370952776
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""neuralspace/autotrain-citizen_nlu_hindi-1370952776"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""neuralspace/autotrain-citizen_nlu_hindi-1370952776"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,5315656819.224839,0.974,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67568,autotrain-ad_detection_ver_1-1395053127,['dhruv0808/autotrain-data-ad_detection_ver_1'],,0.0096526980679869,AutoTrain,Not Specified,Not Specified,Not Specified,0.941,0.178,,,,343266993.0,True,2,1,['pytorch'],2022-09-09 12:35:54+00:00,2022-09-09 12:33:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1395053127
- CO2 Emissions (in grams): 0.0097

## Validation Metrics

- Loss: 0.178
- Accuracy: 0.941
- Precision: 0.947
- Recall: 0.947
- AUC: 0.974
- F1: 0.947",,,1,[],[],Computer Vision,2022-09,35561766314.68899,0.9409999999999998,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67601,autotrain-person-classifier-1401653210,['DominikB/autotrain-data-person-classifier'],,0.0143182831771501,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,,,,347596479.0,True,2,0,['pytorch'],2022-09-09 15:34:30+00:00,2022-09-09 14:15:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1401653210
- CO2 Emissions (in grams): 0.0143

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-09,24276407632.076557,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67643,autotrain-hurricane3-1415853436,['monkseal555/autotrain-data-hurricane3'],,1.2190828686910384,AutoTrain,Not Specified,Not Specified,Not Specified,0.224,2.42,0.067,,,110445167.0,True,3,0,['pytorch'],2022-09-09 19:29:48+00:00,2022-09-09 19:28:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1415853436
- CO2 Emissions (in grams): 1.2191

## Validation Metrics

- Loss: 2.420
- Accuracy: 0.224
- Macro F1: 0.067
- Micro F1: 0.224
- Weighted F1: 0.147
- Macro Precision: 0.063
- Micro Precision: 0.224
- Weighted Precision: 0.148
- Macro Recall: 0.104
- Micro Recall: 0.224
- Weighted Recall: 0.224",,,1,[],[],Computer Vision,2022-09,90596931.37890446,0.103147766323024,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67655,autotrain-donut-vs-croissant-1417653460,['victor/autotrain-data-donut-vs-croissant'],,2.2028215716577684,AutoTrain,Not Specified,Not Specified,Not Specified,0.994,0.023,,,,347596479.0,True,2,2,['pytorch'],2022-09-10 12:34:30+00:00,2022-09-09 20:34:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1417653460
- CO2 Emissions (in grams): 2.2028

## Validation Metrics

- Loss: 0.023
- Accuracy: 0.994
- Precision: 0.995
- Recall: 0.995
- AUC: 1.000
- F1: 0.995",,,1,[],[],Computer Vision,2022-09,157796021.00882405,0.994,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67784,autotrain-encyclopedia_britannica-1423853554,['davanstrien/autotrain-data-encyclopedia_britannica'],,3.1471897890349294,AutoTrain,Not Specified,Not Specified,Not Specified,0.993,0.033,,,,347596479.0,True,3,0,['pytorch'],2022-09-10 17:46:09+00:00,2022-09-10 17:42:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1423853554
- CO2 Emissions (in grams): 3.1472

## Validation Metrics

- Loss: 0.033
- Accuracy: 0.993
- Precision: 0.993
- Recall: 1.000
- AUC: 0.996
- F1: 0.996",,,1,[],[],Computer Vision,2022-09,110446621.36711772,0.993,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67965,autotrain-human_art_or_not-1432453604,['nuts/autotrain-data-human_art_or_not'],,1.7172622019575956,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,,,,347596479.0,True,3,1,['pytorch'],2022-09-11 15:40:36+00:00,2022-09-11 15:39:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1432453604
- CO2 Emissions (in grams): 1.7173

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-09,202413165.9124372,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
67985,hedhikaa-classifier,['shahukareem/autotrain-data-hedhikaa-classification'],,0.0059374716509795,AutoTrain,Not Specified,Not Specified,Not Specified,0.976,0.177,0.961,,,347604671.0,True,3,0,['pytorch'],2022-09-11 17:21:05+00:00,2022-09-11 17:20:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1433153614
- CO2 Emissions (in grams): 0.0059

## Validation Metrics

- Loss: 0.177
- Accuracy: 0.976
- Macro F1: 0.961
- Micro F1: 0.976
- Weighted F1: 0.976
- Macro Precision: 0.969
- Micro Precision: 0.976
- Weighted Precision: 0.979
- Macro Recall: 0.958
- Micro Recall: 0.976
- Weighted Recall: 0.976",,,1,[],[],Computer Vision,2022-09,58544224113.0795,0.9684419204956116,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68039,butterflies,['NimaBoscarino/autotrain-data-temp-nima'],,0.0116594714817232,AutoTrain,Not Specified,Not Specified,Not Specified,0.88,0.606,0.844,,,347895743.0,True,4,0,['pytorch'],2022-09-11 22:52:50+00:00,2022-09-11 22:50:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1433953639
- CO2 Emissions (in grams): 0.0117

## Validation Metrics

- Loss: 0.606
- Accuracy: 0.880
- Macro F1: 0.844
- Micro F1: 0.880
- Weighted F1: 0.844
- Macro Precision: 0.827
- Micro Precision: 0.880
- Weighted Precision: 0.827
- Macro Recall: 0.880
- Micro Recall: 0.880
- Weighted Recall: 0.880",,,1,[],[],Computer Vision,2022-09,29838037131.04353,0.8616241299303943,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68046,autotrain-tosdr_tldr_legal_summarisation_v1-1434353657,['Laksitha/autotrain-data-tosdr_tldr_legal_summarisation_v1'],,2.9024601099439225,AutoTrain,Not Specified,Not Specified,Not Specified,,2.821,,0.3296099999999999,0.2055099999999999,920019705.0,True,3,0,['pytorch'],2022-09-11 23:58:37+00:00,2022-09-11 23:56:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1434353657
- CO2 Emissions (in grams): 2.9025

## Validation Metrics

- Loss: 2.821
- Rouge1: 32.961
- Rouge2: 10.761
- RougeL: 20.551
- RougeLsum: 30.094
- Gen Len: 92.222

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Laksitha/autotrain-tosdr_tldr_legal_summarisation_v1-1434353657
```",,,1,[],[],NLP,2022-09,316979276.25188804,0.2531699473015398,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68087,autotrain-climate-text-classification-1437253674,['prathap-reddy/autotrain-data-climate-text-classification'],,2.621274122165296,AutoTrain,Not Specified,Not Specified,Not Specified,0.884,0.3,0.699,,,1334461229.0,True,5,0,['pytorch'],2022-09-12 06:11:45+00:00,2022-09-12 06:10:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1437253674
- CO2 Emissions (in grams): 2.6213

## Validation Metrics

- Loss: 0.300
- Accuracy: 0.884
- Precision: 0.844
- Recall: 0.596
- AUC: 0.885
- F1: 0.699

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/prathap-reddy/autotrain-climate-text-classification-1437253674
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""prathap-reddy/autotrain-climate-text-classification-1437253674"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""prathap-reddy/autotrain-climate-text-classification-1437253674"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,509088773.93474287,0.7806898294377763,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68092,autotrain-furrygendataset-1436353679,['MommySernox/autotrain-data-furrygendataset'],,2.482027438387914,AutoTrain,Not Specified,Not Specified,Not Specified,0.896,0.302,0.895,,,347616959.0,True,3,0,['pytorch'],2022-09-12 07:02:05+00:00,2022-09-12 06:46:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1436353679
- CO2 Emissions (in grams): 2.4820
- Currently,The model has been trained to recognize o ly the following species:
-Sergal
-Protogen
-Wolf
-Fox
-Synth
-Shark
-Dragon

Works best if submitted image's background isn't plain black/transparent

## Validation Metrics

- Loss: 0.302
- Accuracy: 0.896
- Macro F1: 0.895
- Micro F1: 0.896
- Weighted F1: 0.897
- Macro Precision: 0.900
- Micro Precision: 0.896
- Weighted Precision: 0.908
- Macro Recall: 0.900
- Micro Recall: 0.896
- Weighted Recall: 0.896",,,1,[],[],Computer Vision,2022-09,140053632.61647844,0.895499720826354,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68239,autotrain-trial-run-1444253725,['YilinWang42/autotrain-data-trial-run'],,0.0097739269807768,AutoTrain,Not Specified,Not Specified,Not Specified,0.98,0.082,0.76,,,496331185.0,True,4,0,['pytorch'],2022-09-12 23:54:52+00:00,2022-09-12 23:53:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1444253725
- CO2 Emissions (in grams): 0.0098

## Validation Metrics

- Loss: 0.082
- Accuracy: 0.980
- Precision: 0.743
- Recall: 0.778
- F1: 0.760

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/YilinWang42/autotrain-trial-run-1444253725
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""YilinWang42/autotrain-trial-run-1444253725"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""YilinWang42/autotrain-trial-run-1444253725"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,50781143134.80917,0.8560919540229885,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68881,autotrain-pruebaa-1470254048,['lehomme/autotrain-data-pruebaa'],,0.0177064308388526,AutoTrain,Not Specified,Not Specified,Not Specified,0.738,0.637,0.739,,,1337733933.0,True,5,0,['pytorch'],2022-09-15 10:26:22+00:00,2022-09-15 10:24:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1470254048
- CO2 Emissions (in grams): 0.0177

## Validation Metrics

- Loss: 0.637
- Accuracy: 0.738
- Macro F1: 0.739
- Micro F1: 0.738
- Weighted F1: 0.739
- Macro Precision: 0.744
- Micro Precision: 0.738
- Weighted Precision: 0.744
- Macro Recall: 0.738
- Micro Recall: 0.738
- Weighted Recall: 0.738


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lehomme/autotrain-pruebaa-1470254048
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lehomme/autotrain-pruebaa-1470254048"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lehomme/autotrain-pruebaa-1470254048"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,75550738891.12973,0.7384996614759648,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68903,autotrain-food101-1471154050,['juliensimon/autotrain-data-food101'],,135.18748471833436,AutoTrain,Not Specified,Not Specified,Not Specified,0.89,0.391,0.89,,,343571505.0,True,22,0,['pytorch'],2022-09-15 14:30:10+00:00,2022-09-15 12:42:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1471154050
- CO2 Emissions (in grams): 135.1875

## Validation Metrics

- Loss: 0.391
- Accuracy: 0.890
- Macro F1: 0.890
- Micro F1: 0.890
- Weighted F1: 0.890
- Macro Precision: 0.892
- Micro Precision: 0.890
- Weighted Precision: 0.892
- Macro Recall: 0.890
- Micro Recall: 0.890
- Weighted Recall: 0.890",,,1,[],[],Computer Vision,2022-09,2541444.6146093896,0.8899999999999999,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68904,autotrain-food101-1471154053,['juliensimon/autotrain-data-food101'],,179.11544810549532,AutoTrain,Not Specified,Not Specified,Not Specified,0.915,0.301,0.915,,,348002367.0,True,2,0,['pytorch'],2022-09-23 11:23:42+00:00,2022-09-15 12:42:49+00:00,"
# Usage

```
from transformers import pipeline
p = pipeline(""image-classification"", model=""juliensimon/autotrain-food101-1471154053"")
result = p(""my_image.jpg"")
```

# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1471154053
- CO2 Emissions (in grams): 179.1154

## Validation Metrics

- Loss: 0.301
- Accuracy: 0.915
- Macro F1: 0.915
- Micro F1: 0.915
- Weighted F1: 0.915
- Macro Precision: 0.917
- Micro Precision: 0.915
- Weighted Precision: 0.917
- Macro Recall: 0.915
- Micro Recall: 0.915
- Weighted Recall: 0.915",,,1,[],[],Computer Vision,2022-09,1942894.2097447328,0.915,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68954,autotrain-ratnakar_1000_sample_curated-1474454086,['hemangjoshi37a/autotrain-data-ratnakar_1000_sample_curated'],,2.180256368490792,AutoTrain,Not Specified,Not Specified,Not Specified,0.957,0.177,0.863,,,1330303153.0,True,7,0,['pytorch'],2023-02-16 12:45:12+00:00,2022-09-15 17:37:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1474454086
- CO2 Emissions (in grams): 2.1803

## Validation Metrics

- Loss: 0.177
- Accuracy: 0.957
- Precision: 0.839
- Recall: 0.888
- F1: 0.863

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/hemangjoshi37a/autotrain-ratnakar_1000_sample_curated-1474454086
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""hemangjoshi37a/autotrain-ratnakar_1000_sample_curated-1474454086"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hemangjoshi37a/autotrain-ratnakar_1000_sample_curated-1474454086"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```


# GitHub Link to this project : [Telegram Trade Msg Backtest ML](https://github.com/hemangjoshi37a/TelegramTradeMsgBacktestML)

# Need custom model for your application? : Place a order on hjLabs.in : [Custom Token Classification or Named Entity Recognition (NER) model as in Natural Language Processing (NLP) Machine Learning](https://hjlabs.in/product/custom-token-classification-or-named-entity-recognition-ner-model-as-in-natural-language-processing-nlp-machine-learning/)

## What this repository contains? :

1. Label data using LabelStudio NER(Named Entity Recognition or Token Classification) tool.
 ![Screenshot from 2022-09-30 12-28-50](https://user-images.githubusercontent.com/12392345/193394190-3ad215d1-3205-4af3-949e-6d95cf866c6c.png) convert to  ![Screenshot from 2022-09-30 18-59-14](https://user-images.githubusercontent.com/12392345/193394213-9bb936e7-34ea-4cbc-9132-80c7e5a006d7.png)

2. Convert LabelStudio CSV or JSON to HuggingFace-autoTrain dataset conversion script
![Screenshot from 2022-10-01 10-36-03](https://user-images.githubusercontent.com/12392345/193394227-32e293d4-6736-4e71-b687-b0c2fcad732c.png)

3. Train NER model on Hugginface-autoTrain.
 ![Screenshot from 2022-10-01 10-38-24](https://user-images.githubusercontent.com/12392345/193394247-bf51da86-45bb-41b4-b4da-3de86014e6a5.png)

4. Use Hugginface-autoTrain model to predict labels on new data in LabelStudio using LabelStudio-ML-Backend.
 ![Screenshot from 2022-10-01 10-41-07](https://user-images.githubusercontent.com/12392345/193394251-bfba07d4-c56b-4fe8-ba7f-08a1c69f0e2c.png)
 ![Screenshot from 2022-10-01 10-42-36](https://user-images.githubusercontent.com/12392345/193394261-df4bc8f8-9ffd-4819-ba26-04fddbba8e7b.png)
 ![Screenshot from 2022-10-01 10-44-56](https://user-images.githubusercontent.com/12392345/193394267-c5a111c3-8d00-4d6f-b3c6-0ea82e4ac474.png)

5. Define python function to predict labels using Hugginface-autoTrain model.
 ![Screenshot from 2022-10-01 10-47-08](https://user-images.githubusercontent.com/12392345/193394278-81389606-f690-454a-bb2b-ef3f1db39571.png)
![Screenshot from 2022-10-01 10-47-25](https://user-images.githubusercontent.com/12392345/193394288-27a0c250-41af-48b1-9c57-c146dc51da1d.png)

6. Only label new data from newly predicted-labels-dataset that has falsified labels.
 ![Screenshot from 2022-09-30 22-47-23](https://user-images.githubusercontent.com/12392345/193394294-fdfaf40a-c9cd-4c2d-836e-1878b503a668.png)

7. Backtest Truely labelled dataset against real historical data of the stock using zerodha kiteconnect and jugaad_trader.
 ![Screenshot from 2022-10-01 00-05-55](https://user-images.githubusercontent.com/12392345/193394303-137c2a2a-3341-4be3-8ece-5191669ec53a.png)

8. Evaluate total gained percentage since inception summation-wise and compounded and plot.
 ![Screenshot from 2022-10-01 00-06-59](https://user-images.githubusercontent.com/12392345/193394308-446eddd9-c5d1-47e3-a231-9edc620284bb.png)

9. Listen to telegram channel for new LIVE messages using telegram API for algotrading.
 ![Screenshot from 2022-10-01 00-09-29](https://user-images.githubusercontent.com/12392345/193394319-8cc915b7-216e-4e05-a7bf-28360b17de99.png)

10. Serve the app as flask web API for web request and respond to it as labelled tokens.
 ![Screenshot from 2022-10-01 00-12-12](https://user-images.githubusercontent.com/12392345/193394323-822c2a59-ca72-45b1-abca-a6e5df3364b0.png)

11. Outperforming or underperforming results of the telegram channel tips against exchange index by percentage.
 ![Screenshot from 2022-10-01 11-16-27](https://user-images.githubusercontent.com/12392345/193394685-53235198-04f8-4d3c-a341-535dd9093252.png)



Place a custom order on hjLabs.in : [https://hjLabs.in](https://hjlabs.in/?product=custom-algotrading-software-for-zerodha-and-angel-w-source-code)


----------------------------------------------------------------------

### Social Media :
* [WhatsApp/917016525813](https://wa.me/917016525813)
* [telegram/hjlabs](https://t.me/hjlabs) 
* [Gmail/hemangjoshi37a@gmail.com](mailto:hemangjoshi37a@gmail.com)
* [Facebook/hemangjoshi37](https://www.facebook.com/hemangjoshi37/)
* [Twitter/HemangJ81509525](https://twitter.com/HemangJ81509525)
* [LinkedIn/hemang-joshi-046746aa](https://www.linkedin.com/in/hemang-joshi-046746aa/)
* [Tumblr/hemangjoshi37a-blog](https://www.tumblr.com/blog/hemangjoshi37a-blog)
* [Pinterest/hemangjoshi37a](https://in.pinterest.com/hemangjoshi37a/)
* [Blogger/hemangjoshi](http://hemangjoshi.blogspot.com/)
* [Instagram/hemangjoshi37](https://www.instagram.com/hemangjoshi37/)
  
### Checkout Our Other Repositories

- [pyPortMan](https://github.com/hemangjoshi37a/pyPortMan)
- [transformers_stock_prediction](https://github.com/hemangjoshi37a/transformers_stock_prediction)
- [TrendMaster](https://github.com/hemangjoshi37a/TrendMaster)
- [hjAlgos_notebooks](https://github.com/hemangjoshi37a/hjAlgos_notebooks)
- [AutoCut](https://github.com/hemangjoshi37a/AutoCut)
- [My_Projects](https://github.com/hemangjoshi37a/My_Projects)
- [Cool Arduino and ESP8266 or NodeMCU Projects](https://github.com/hemangjoshi37a/my_Arduino)
- [Telegram Trade Msg Backtest ML](https://github.com/hemangjoshi37a/TelegramTradeMsgBacktestML)

### Checkout Our Other Products

- [WiFi IoT LED Matrix Display](https://hjlabs.in/product/wifi-iot-led-display)
- [SWiBoard WiFi Switch Board IoT Device](https://hjlabs.in/product/swiboard-wifi-switch-board-iot-device)
- [Electric Bicycle](https://hjlabs.in/product/electric-bicycle)
- [Product 3D Design Service with Solidworks](https://hjlabs.in/product/product-3d-design-with-solidworks/)
- [AutoCut : Automatic Wire Cutter Machine](https://hjlabs.in/product/automatic-wire-cutter-machine/)
- [Custom AlgoTrading Software Coding Services](https://hjlabs.in/product/custom-algotrading-software-for-zerodha-and-angel-w-source-code//)
- [SWiBoard :Tasmota MQTT Control App](https://play.google.com/store/apps/details?id=in.hjlabs.swiboard)
- [Custom Token Classification or Named Entity Recognition (NER) model as in Natural Language Processing (NLP) Machine Learning](https://hjlabs.in/product/custom-token-classification-or-named-entity-recognition-ner-model-as-in-natural-language-processing-nlp-machine-learning/)

## Some Cool Arduino and ESP8266 (or NodeMCU) IoT projects:
- [IoT_LED_over_ESP8266_NodeMCU : Turn LED on and off using web server hosted on a nodemcu or esp8266](https://github.com/hemangjoshi37a/my_Arduino/tree/master/IoT_LED_over_ESP8266_NodeMCU)
- [ESP8266_NodeMCU_BasicOTA : Simple OTA (Over The Air) upload code from Arduino IDE using WiFi to NodeMCU or ESP8266](https://github.com/hemangjoshi37a/my_Arduino/tree/master/ESP8266_NodeMCU_BasicOTA)  
- [IoT_CSV_SD : Read analog value of Voltage and Current and write it to SD Card in CSV format for Arduino, ESP8266, NodeMCU etc](https://github.com/hemangjoshi37a/my_Arduino/tree/master/IoT_CSV_SD)  
- [Honeywell_I2C_Datalogger : Log data in A SD Card from a Honeywell I2C HIH8000 or HIH6000 series sensor having external I2C RTC clock](https://github.com/hemangjoshi37a/my_Arduino/tree/master/Honeywell_I2C_Datalogger)
- [IoT_Load_Cell_using_ESP8266_NodeMC : Read ADC value from High Precision 12bit ADS1015 ADC Sensor and Display on SSD1306 SPI Display as progress bar for Arduino or ESP8266 or NodeMCU](https://github.com/hemangjoshi37a/my_Arduino/tree/master/IoT_Load_Cell_using_ESP8266_NodeMC)
- [IoT_SSD1306_ESP8266_NodeMCU : Read from High Precision 12bit ADC seonsor ADS1015 and display to SSD1306 SPI as progress bar in ESP8266 or NodeMCU or Arduino](https://github.com/hemangjoshi37a/my_Arduino/tree/master/IoT_SSD1306_ESP8266_NodeMCU)  


## Checkout Our Awesome 3D GrabCAD Models:
- [AutoCut : Automatic Wire Cutter Machine](https://grabcad.com/library/automatic-wire-cutter-machine-1)
- [ESP Matrix Display 5mm Acrylic Box](https://grabcad.com/library/esp-matrix-display-5mm-acrylic-box-1)
- [Arcylic Bending Machine w/ Hot Air Gun](https://grabcad.com/library/arcylic-bending-machine-w-hot-air-gun-1)
- [Automatic Wire Cutter/Stripper](https://grabcad.com/library/automatic-wire-cutter-stripper-1)

## Our HuggingFace Models :
- [hemangjoshi37a/autotrain-ratnakar_1000_sample_curated-1474454086 : Stock tip message NER(Named Entity Recognition or Token Classification) using HUggingFace-AutoTrain and LabelStudio and Ratnakar Securities Pvt. Ltd.](https://huggingface.co/hemangjoshi37a/autotrain-ratnakar_1000_sample_curated-1474454086)

## Our HuggingFace Datasets :
- [hemangjoshi37a/autotrain-data-ratnakar_1000_sample_curated : Stock tip message NER(Named Entity Recognition or Token Classification) using HUggingFace-AutoTrain and LabelStudio and Ratnakar Securities Pvt. Ltd.](https://huggingface.co/datasets/hemangjoshi37a/autotrain-data-ratnakar_1000_sample_curated)

## We sell Gigs on Fiverr : 
- [code android and ios app for you using flutter firebase software stack](https://business.fiverr.com/share/3v14pr)
- [code custom algotrading software for zerodha or angel broking](https://business.fiverr.com/share/kzkvEy)

## Awesome Fiverr. Gigs:
- [develop machine learning ner model as in nlp using python](https://www.fiverr.com/share/9YNabx)
- [train custom chatgpt question answering model](https://www.fiverr.com/share/rwx6r7)
- [build algotrading, backtesting and stock monitoring tools using python](https://www.fiverr.com/share/A7Y14q)
- [tutor you in your science problems](https://www.fiverr.com/share/zPzmlz)
- [make apps for you crossplatform	](https://www.fiverr.com/share/BGw12l)
",,,1,[],[],NLP,2022-09,610159049.2868767,0.9075725274725276,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68955,autotrain-graphwerk-1472254089,['edub0420/autotrain-data-graphwerk'],,0.0037659513202956,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.005,,,,110392879.0,True,2,0,['pytorch'],2022-09-15 18:00:49+00:00,2022-09-15 17:59:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1472254089
- CO2 Emissions (in grams): 0.0038

## Validation Metrics

- Loss: 0.005
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-09,29313411037.75472,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
68956,autotrain-graphwerk-1472254090,['edub0420/autotrain-data-graphwerk'],,0.8959954972786571,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.004,,,,343266993.0,True,2,0,['pytorch'],2022-09-15 18:00:54+00:00,2022-09-15 17:59:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1472254090
- CO2 Emissions (in grams): 0.8960

## Validation Metrics

- Loss: 0.004
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-09,383112408.5361815,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
69082,autotrain-gahhaha-1478754178,['Tritkoman/autotrain-data-gahhaha'],,39.86630127427062,AutoTrain,Not Specified,Not Specified,Not Specified,,1.716,,,,4918417081.0,True,2,0,['pytorch'],2022-09-16 06:11:41+00:00,2022-09-16 05:42:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1478754178
- CO2 Emissions (in grams): 39.8663

## Validation Metrics

- Loss: 1.716
- SacreBLEU: 9.095
- Gen len: 11.146",,,1,[],[],NLP,2022-09,123372796.6676032,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
69083,autotrain-firsttransformersproject-1478954182,['shamr9/autotrain-data-firsttransformersproject'],,5.113476145275885,AutoTrain,Not Specified,Not Specified,Not Specified,,0.534,,0.04247,0.0426,4918515385.0,True,2,0,['pytorch'],2022-09-16 15:46:18+00:00,2022-09-16 05:53:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1478954182
- CO2 Emissions (in grams): 5.1135

## Validation Metrics

- Loss: 0.534
- Rouge1: 4.247
- Rouge2: 0.522
- RougeL: 4.260
- RougeLsum: 4.241
- Gen Len: 18.928

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/shamr9/autotrain-firsttransformersproject-1478954182
```",,,1,[],[],NLP,2022-09,961873145.637728,0.0425349006700364,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
69132,autotrain-anli-1480954206,['yiwuanwow/autotrain-data-anli'],,2.44759580195597,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-09-16 09:39:43+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
69328,autotrain-qjnwjkwnw-1490354394,['Tritkoman/autotrain-data-qjnwjkwnw'],,148.66763338560511,AutoTrain,Not Specified,Not Specified,Not Specified,,2.112,,,,4918417081.0,True,2,0,['pytorch'],2022-09-17 15:05:08+00:00,2022-09-17 12:56:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1490354394
- CO2 Emissions (in grams): 148.6676

## Validation Metrics

- Loss: 2.112
- SacreBLEU: 8.676
- Gen len: 13.161",,,1,[],[],NLP,2022-09,33083307.83905672,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
69343,autotrain-akakka-1492154441,['Tritkoman/autotrain-data-akakka'],,4.471184695619804,AutoTrain,Not Specified,Not Specified,Not Specified,,0.899,,,,310020485.0,True,2,0,['pytorch'],2022-09-17 15:05:32+00:00,2022-09-17 15:00:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1492154441
- CO2 Emissions (in grams): 4.4712

## Validation Metrics

- Loss: 0.899
- SacreBLEU: 59.218
- Gen len: 9.889",,,1,[],[],NLP,2022-09,69337436.51961227,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
69344,Interlinguetranslator,['Tritkoman/autotrain-data-akakka'],,0.2617035619368602,AutoTrain,Not Specified,Not Specified,Not Specified,,0.77,,,,4918417081.0,True,3,0,['pytorch'],2022-09-17 15:45:24+00:00,2022-09-17 15:07:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1492154444
- CO2 Emissions (in grams): 0.2617

## Validation Metrics

- Loss: 0.770
- SacreBLEU: 62.097
- Gen len: 8.635",,,1,[],[],NLP,2022-09,18793848446.68885,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
69377,Kvenfinnishtranslator,['Tritkoman/autotrain-data-wnkeknrr'],,0.007023045912239,AutoTrain,Not Specified,Not Specified,Not Specified,,2.873,,,,310020485.0,True,2,0,['pytorch'],2022-09-17 18:38:22+00:00,2022-09-17 18:36:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1495654541
- CO2 Emissions (in grams): 0.0070

## Validation Metrics

- Loss: 2.873
- SacreBLEU: 22.653
- Gen len: 7.114",,,1,[],[],NLP,2022-09,44143308882.507805,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
69392,earnings-transcript-summary,['joshuaperk/autotrain-data-earnings-transcript-summary'],,1.3579793641309694,AutoTrain,Not Specified,Not Specified,Not Specified,,2.246,,0.3210499999999999,0.26838,1625537793.0,True,2,0,['pytorch'],2022-10-08 02:06:34+00:00,2022-09-17 20:15:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1497554606
- CO2 Emissions (in grams): 1.3580

## Validation Metrics

- Loss: 2.246
- Rouge1: 32.105
- Rouge2: 19.375
- RougeL: 26.838
- RougeLsum: 27.080
- Gen Len: 19.417

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/joshuaperk/autotrain-earnings-transcript-summary-1497554606
```",,,1,[],[],NLP,2022-09,1197026873.8511007,0.2923617698454439,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
69460,autotrain-stripai_test-1499654675,['roupenminassian/autotrain-data-stripai_test'],,5.167876375083602,AutoTrain,Not Specified,Not Specified,Not Specified,0.776,0.531,,,,343266993.0,True,3,0,['pytorch'],2022-09-18 06:48:22+00:00,2022-09-18 06:44:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1499654675
- CO2 Emissions (in grams): 5.1679

## Validation Metrics

- Loss: 0.531
- Accuracy: 0.776
- Precision: 0.785
- Recall: 0.720
- AUC: 0.836
- F1: 0.751",,,1,[],[],Computer Vision,2022-09,66423220.69758236,0.7760000000000001,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
70329,autotrain-line_clip_no_nut_boltline_clip_no_nut_bolt-1523955096,['tianchez/autotrain-data-line_clip_no_nut_boltline_clip_no_nut_bolt'],,10.423410288264847,AutoTrain,Not Specified,Not Specified,Not Specified,0.798,0.58,0.542,,,110420527.0,True,3,0,['pytorch'],2022-09-21 15:49:25+00:00,2022-09-21 15:42:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1523955096
- CO2 Emissions (in grams): 10.4234

## Validation Metrics

- Loss: 0.580
- Accuracy: 0.798
- Macro F1: 0.542
- Micro F1: 0.798
- Weighted F1: 0.796
- Macro Precision: 0.548
- Micro Precision: 0.798
- Weighted Precision: 0.796
- Macro Recall: 0.537
- Micro Recall: 0.798
- Weighted Recall: 0.798",,,1,[],[],Computer Vision,2022-09,10593512.482600488,0.6455462686567165,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
70428,autotrain-dogs-and-cats-1527055142,['omarques/autotrain-data-dogs-and-cats'],,0.8187420113922029,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.068,,,,343266993.0,True,2,1,['pytorch'],2022-09-21 21:38:24+00:00,2022-09-21 21:37:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1527055142
- CO2 Emissions (in grams): 0.8187

## Validation Metrics

- Loss: 0.068
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-09,419261486.797658,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
70432,autotrain-test-dogs-cats-1527155150,['omarques/autotrain-data-test-dogs-cats'],,0.7873922658787444,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.043,,,,347596479.0,True,2,2,['pytorch'],2022-09-21 21:46:49+00:00,2022-09-21 21:46:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1527155150
- CO2 Emissions (in grams): 0.7874

## Validation Metrics

- Loss: 0.043
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-09,441452747.3318218,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
70576,imdb,['Alexei1/autotrain-data-imdb-sentiment-analysis'],,0.0185647651897548,AutoTrain,Not Specified,Not Specified,Not Specified,0.487,0.694,0.218,,,,True,3,0,['joblib'],2022-09-22 09:10:57+00:00,2022-09-22 08:59:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1530155186
- CO2 Emissions (in grams): 0.0186

## Validation Metrics

- Loss: 0.694
- Accuracy: 0.487
- Macro F1: 0.218
- Micro F1: 0.487
- Weighted F1: 0.319
- Macro Precision: 0.162
- Micro Precision: 0.487
- Weighted Precision: 0.237
- Macro Recall: 0.333
- Micro Recall: 0.487
- Weighted Recall: 0.487

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-09,,0.3011801418439716,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
70657,test-fake_news_classification,['Phoenyx83/autotrain-data-fake-news-classification'],,34.12749821455322,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,1.0,,,263166449.0,True,87,1,['pytorch'],2022-09-22 15:25:34+00:00,2022-09-22 15:09:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1532855215
- CO2 Emissions (in grams): 34.1275

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Phoenyx83/autotrain-fake-news-classification-1532855215
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Phoenyx83/autotrain-fake-news-classification-1532855215"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Phoenyx83/autotrain-fake-news-classification-1532855215"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,7711272.808382307,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
70725,autotrain-lucy-song-request-1537055286,['ankleBowl/autotrain-data-lucy-song-request'],,1.0504382303760451,AutoTrain,Not Specified,Not Specified,Not Specified,0.997,0.014,0.997,,,265497461.0,True,160,0,['pytorch'],2022-09-23 00:55:24+00:00,2022-09-23 00:54:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1537055286
- CO2 Emissions (in grams): 1.0504

## Validation Metrics

- Loss: 0.014
- Accuracy: 0.997
- Precision: 0.997
- Recall: 0.997
- F1: 0.997

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ankleBowl/autotrain-lucy-song-request-1537055286
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ankleBowl/autotrain-lucy-song-request-1537055286"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ankleBowl/autotrain-lucy-song-request-1537055286"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,252749236.7685008,0.997,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
70775,yoda-fits,['gazquez/autotrain-data-yoda-fits'],,18.026996827523217,AutoTrain,Not Specified,Not Specified,Not Specified,,1.179,,0.62134,0.60269,4918515385.0,True,174,0,['pytorch'],2022-10-10 08:01:08+00:00,2022-09-23 07:38:48+00:00,"
# What is YODA

YODA is a series of models for Google Feed product optimization. We aim to increase the market reach for ecommerce by augmenting and improving certain metadata like short titles, colors, measures and more. YODA is being used in production by +300 companies with +3.5M products.

## FITS - First Intergalactic Title Shortener

Trained with more than 2M lines of long and short titles from real products, the FITS model is capable of extracting the key features of an already short text. It generates a short title for better SEM (Search Engine Marketing) and product position in google indexes.

### The problem with product titles

Product titles are not long text with a common sense or clear context. Words in product titles may be disorganized or may not make sense at all. 
Detecting context or meaning in short sentences raises a problem and with abstractive summarization, we may see certain decorations or errors on the model output.

To palliate that problem we run some post-processing on our model later on our custom API. You may contact [Iván R. Gázquez](mailto:ivan@gazquez.com) (lead ML developer) for any problem or inquiry.

## Validation Metrics

- Loss: 1.179
- Rouge1: 62.134
- Rouge2: 40.392
- RougeL: 60.269
- RougeLsum: 60.293
- Gen Len: 9.153

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/gazquez/autotrain-yoda-fits-1539355334
```",,,1,[],[],NLP,2022-09,272841640.3496849,0.6118729191277991,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
70782,bart-base-cnn-xsum-swe,['Gabriel/xsum_swe'],,0.0334,Not Specified,fine-tuning,"Fredericia, Denmark",Tesla P100-PCIE-16GB,,,,,,557723065.0,False,9,0,"['tensorboard', 'transformers', 'pytorch']",2022-12-20 09:37:04+00:00,2022-09-23 08:27:02+00:00,"
<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# bart-base-cnn-xsum-swe

This model is a fine-tuned version of [Gabriel/bart-base-cnn-swe](https://huggingface.co/Gabriel/bart-base-cnn-swe) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 2.1027
- Rouge1: 30.9467
- Rouge2: 12.2589
- Rougel: 25.4487
- Rougelsum: 25.4792
- Gen Len: 19.7379

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 4e-05
- train_batch_size: 16
- eval_batch_size: 16
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 500
- num_epochs: 4
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |
|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|
| 2.3076        | 1.0   | 6375  | 2.1986          | 29.7041 | 10.9883 | 24.2149 | 24.2406   | 19.7193 |
| 2.0733        | 2.0   | 12750 | 2.1246          | 30.4521 | 11.8107 | 24.9519 | 24.9745   | 19.6592 |
| 1.8933        | 3.0   | 19125 | 2.0989          | 30.9407 | 12.2682 | 25.4135 | 25.4378   | 19.7195 |
| 1.777         | 4.0   | 25500 | 2.1027          | 30.9467 | 12.2589 | 25.4487 | 25.4792   | 19.7379 |


### Framework versions

- Transformers 4.22.2
- Pytorch 1.12.1+cu113
- Datasets 2.5.1
- Tokenizers 0.12.1
",,,1,[],[],NLP,2022-09,16698295359.281437,,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,1,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
71563,autotrain-byt5-summary-1562255681,['mklehr/autotrain-data-byt5-summary'],,2.2525628167913614,AutoTrain,Not Specified,Not Specified,Not Specified,,0.918,,0.12572,0.11701,2326697929.0,True,2,0,['pytorch'],2022-09-26 16:29:17+00:00,2022-09-26 16:27:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1562255681
- CO2 Emissions (in grams): 2.2526

## Validation Metrics

- Loss: 0.918
- Rouge1: 12.572
- Rouge2: 2.448
- RougeL: 11.701
- RougeLsum: 11.785
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/mklehr/autotrain-byt5-summary-1562255681
```",,,1,[],[],NLP,2022-09,1032911451.639,0.1212087273925761,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
71729,autotrain-sphere-banking77-1565555714,['lewtun/autotrain-data-sphere-banking77'],,0.0403225925465886,AutoTrain,Not Specified,Not Specified,Not Specified,0.919,0.317,0.92,,,268084977.0,True,19,0,['pytorch'],2022-09-27 08:51:27+00:00,2022-09-27 08:46:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1565555714
- CO2 Emissions (in grams): 0.0403

## Validation Metrics

- Loss: 0.317
- Accuracy: 0.919
- Macro F1: 0.920
- Micro F1: 0.919
- Weighted F1: 0.920
- Macro Precision: 0.925
- Micro Precision: 0.919
- Weighted Precision: 0.923
- Macro Recall: 0.919
- Micro Recall: 0.919
- Weighted Recall: 0.919


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lewtun/autotrain-sphere-banking77-1565555714
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lewtun/autotrain-sphere-banking77-1565555714"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lewtun/autotrain-sphere-banking77-1565555714"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,6648505467.257728,0.9194997281131048,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
71738,autotrain-sphere-emotion-1565855719,['lewtun/autotrain-data-sphere-emotion'],,0.0242924820006723,AutoTrain,Not Specified,Not Specified,Not Specified,0.943,0.134,0.915,,,267866609.0,True,4,0,['pytorch'],2022-09-27 09:35:07+00:00,2022-09-27 09:32:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1565855719
- CO2 Emissions (in grams): 0.0243

## Validation Metrics

- Loss: 0.134
- Accuracy: 0.943
- Macro F1: 0.915
- Micro F1: 0.943
- Weighted F1: 0.943
- Macro Precision: 0.911
- Micro Precision: 0.943
- Weighted Precision: 0.943
- Macro Recall: 0.920
- Micro Recall: 0.943
- Weighted Recall: 0.943


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lewtun/autotrain-sphere-emotion-1565855719
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lewtun/autotrain-sphere-emotion-1565855719"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lewtun/autotrain-sphere-emotion-1565855719"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,11026728721.774364,0.9287890204520992,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72153,autotrain-sphere-intent-classification-1584456046,['bggmyfuture-ai/autotrain-data-sphere-intent-classification'],,1.893124351907886,AutoTrain,Not Specified,Not Specified,Not Specified,0.744,0.69,0.678,,,267860465.0,True,6,0,['pytorch'],2022-09-28 15:35:06+00:00,2022-09-28 15:34:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1584456046
- CO2 Emissions (in grams): 1.8931

## Validation Metrics

- Loss: 0.690
- Accuracy: 0.744
- Macro F1: 0.678
- Micro F1: 0.744
- Weighted F1: 0.739
- Macro Precision: 0.697
- Micro Precision: 0.744
- Weighted Precision: 0.738
- Macro Recall: 0.669
- Micro Recall: 0.744
- Weighted Recall: 0.744


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/bggmyfuture-ai/autotrain-sphere-intent-classification-1584456046
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""bggmyfuture-ai/autotrain-sphere-intent-classification-1584456046"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""bggmyfuture-ai/autotrain-sphere-intent-classification-1584456046"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,141491215.1597706,0.7094683544303798,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72156,autotrain-auto-train-intent-classification-20220928-1584756071,['sairahul5223/autotrain-data-auto-train-intent-classification-20220928'],,1.0197546564964108,AutoTrain,Not Specified,Not Specified,Not Specified,0.666,0.764,0.618,,,267857393.0,True,5,0,['pytorch'],2022-09-28 15:41:12+00:00,2022-09-28 15:40:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1584756071
- CO2 Emissions (in grams): 1.0198

## Validation Metrics

- Loss: 0.764
- Accuracy: 0.666
- Macro F1: 0.618
- Micro F1: 0.666
- Weighted F1: 0.658
- Macro Precision: 0.659
- Micro Precision: 0.666
- Weighted Precision: 0.664
- Macro Recall: 0.604
- Micro Recall: 0.666
- Weighted Recall: 0.666


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sairahul5223/autotrain-auto-train-intent-classification-20220928-1584756071
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sairahul5223/autotrain-auto-train-intent-classification-20220928-1584756071"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sairahul5223/autotrain-auto-train-intent-classification-20220928-1584756071"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,262668467.6491524,0.6411028037383177,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72163,autotrain-emotion-detection-1587956110,['hoshingakag/autotrain-data-emotion-detection'],,2.3491292126039087,AutoTrain,Not Specified,Not Specified,Not Specified,0.888,0.448,0.823,,,438018413.0,True,4,0,['pytorch'],2022-09-28 15:53:01+00:00,2022-09-28 15:51:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1587956110
- CO2 Emissions (in grams): 2.3491

## Validation Metrics

- Loss: 0.448
- Accuracy: 0.888
- Macro F1: 0.823
- Micro F1: 0.888
- Weighted F1: 0.884
- Macro Precision: 0.885
- Micro Precision: 0.888
- Weighted Precision: 0.890
- Macro Recall: 0.800
- Micro Recall: 0.888
- Weighted Recall: 0.888


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/hoshingakag/autotrain-emotion-detection-1587956110
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""hoshingakag/autotrain-emotion-detection-1587956110"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hoshingakag/autotrain-emotion-detection-1587956110"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,186459906.3558856,0.8542653419053184,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72181,magpie-idioms-xlmroberta,['imranraad/autotrain-data-magpie-metaphor-xlmr'],,9.232131148683266,AutoTrain,Not Specified,Not Specified,Not Specified,0.985,0.137,0.0,,,1109889457.0,True,3,0,['pytorch'],2022-09-29 21:54:22+00:00,2022-09-28 17:21:19+00:00,"
# Fine-tune datasets
 - MAGPIE corpus: https://aclanthology.org/2020.lrec-1.35/

# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1590556166
- CO2 Emissions (in grams): 9.2321

## Validation Metrics

- Loss: 0.137
- Accuracy: 0.985
- Precision: 0.000
- Recall: 0.000
- F1: 0.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/imranraad/autotrain-magpie-metaphor-xlmr-1590556166
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""imranraad/autotrain-magpie-metaphor-xlmr-1590556166"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""imranraad/autotrain-magpie-metaphor-xlmr-1590556166"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,120220287.07405204,0.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72267,autotrain-distilbert-risk-ranker-1593356256,['mrosinski/autotrain-data-distilbert-risk-ranker'],,0.0201678425471772,AutoTrain,Not Specified,Not Specified,Not Specified,0.511,0.995,0.506,,,267857393.0,True,7,0,['pytorch'],2022-09-29 02:48:55+00:00,2022-09-29 02:46:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1593356256
- CO2 Emissions (in grams): 0.0202

## Validation Metrics

- Loss: 0.995
- Accuracy: 0.511
- Macro F1: 0.506
- Micro F1: 0.511
- Weighted F1: 0.506
- Macro Precision: 0.505
- Micro Precision: 0.511
- Weighted Precision: 0.505
- Macro Recall: 0.511
- Micro Recall: 0.511
- Weighted Recall: 0.511


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/mrosinski/autotrain-distilbert-risk-ranker-1593356256
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mrosinski/autotrain-distilbert-risk-ranker-1593356256"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mrosinski/autotrain-distilbert-risk-ranker-1593356256"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-09,13281410362.73068,0.508487708947886,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72343,idiom-xlm-roberta,[''],,0.0421576133189314,AutoTrain,Not Specified,Not Specified,Not Specified,0.996,0.012,0.0,,,1109889457.0,True,2,2,"['transformers', 'pytorch']",2023-01-27 19:49:00+00:00,2022-09-29 09:32:38+00:00,"
# Fine-tune datasets
 - MAGPIE corpus: https://aclanthology.org/2020.lrec-1.35/
 - EPIE corpus: https://link.springer.com/content/pdf/10.1007/978-3-030-58323-1.pdf


# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1595156286
- CO2 Emissions (in grams): 0.0422

## Validation Metrics

- Loss: 0.012
- Accuracy: 0.996
- Precision: 0.000
- Recall: 0.000
- F1: 0.000

## Usage

### You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/imranraad/autotrain-magpie-epie-combine-xlmr-metaphor-1595156286
```

### Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""imranraad/autotrain-magpie-epie-combine-xlmr-metaphor-1595156286"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""imranraad/autotrain-magpie-epie-combine-xlmr-metaphor-1595156286"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```

### How to get the idioms:

```
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

model = AutoModelForTokenClassification.from_pretrained(""imranraad/idiom-xlm-roberta"")

tokenizer = AutoTokenizer.from_pretrained(""imranraad/idiom-xlm-roberta"")

pipeline_idioms = pipeline(""ner"", model=model, tokenizer=tokenizer, aggregation_strategy=""simple"")

text = ""Why are you so bent out of shape? - Why are you so upset?""

idioms = pipeline_idioms(text)
for idiom in idioms:
    if idiom['entity_group'] == '1':
        print(idiom['word'])
```",,,1,[],[],NLP,2022-09,26327141638.76992,0.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72359,test1,[''],,0.1069,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,,False,3,0,"['transformers', 'tf']",2022-09-30 08:00:38+00:00,2022-09-29 11:00:56+00:00,"
## About the Model
An Environmental due diligence classification model, trained on customized environmental Dataset to detect contamination and remediation activities (both prevailing as well as planned) as a part of site assessment process.  This model can identify the source of contamination, the extent of contamination, the types of contaminants present at the site, the flow of contaminants and their interaction with ground water, surface water and other surrounding water bodies .

This model was built on top of distilbert-base-uncased model and trained for 10 epochs with a batch size of 16, a learning rate of 5e-5, and a maximum sequence length of 512.

- Dataset : Open Source News data + Custom data
- Carbon emission 0.1069 Kg

## Usage
The easiest way is to load through the pipeline object offered by transformers library.
```python
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from transformers import pipeline
tokenizer = AutoTokenizer.from_pretrained(""d4data/environmental-due-diligence-model"")
model = TFAutoModelForSequenceClassification.from_pretrained(""d4data/environmental-due-diligence-model"")

classifier = pipeline('text-classification', model=model, tokenizer=tokenizer) # cuda = 0,1 based on gpu availability
classifier(""At the every month post-injection monitoring event, TCE, carbon tetrachloride, and chloroform concentrations were above CBSGs in three of the wells"")
```

## Author
This model is part of the Research topic ""Environmental Due Diligence"" conducted by Deepak John Reji, Afreen Aman, Shaina Raza. If you use this work (code, model or dataset), please cite as:
> Environmental Due Diligence, (2020), GitHub repository https://github.com/dreji18/environmental-due-diligence

",,,1,[],[],NLP,2022-09,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,2,0,0.0,0,1,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72455,GermantoNorthFrisian,['Tritkoman/autotrain-data-ttreddsd'],,21.087082943674982,AutoTrain,Not Specified,Not Specified,Not Specified,,1.347,,,,295861701.0,True,3,1,['pytorch'],2022-09-29 17:33:29+00:00,2022-09-29 17:21:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1599456406
- CO2 Emissions (in grams): 21.0871

## Validation Metrics

- Loss: 1.347
- SacreBLEU: 40.859
- Gen len: 13.513",,,1,[],[],NLP,2022-09,14030470.77636421,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72572,GermantoInterlingua,['Tritkoman/autotrain-data-sklskskak'],,74.90468999750897,AutoTrain,Not Specified,Not Specified,Not Specified,,1.437,,,,4918417081.0,True,2,0,['pytorch'],2022-09-30 06:18:54+00:00,2022-09-30 05:19:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1610956845
- CO2 Emissions (in grams): 74.9047

## Validation Metrics

- Loss: 1.437
- SacreBLEU: 25.270
- Gen len: 10.917",,,1,[],[],NLP,2022-09,65662338.1147905,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72674,GermantoLinguaFrancaNova,['Tritkoman/autotrain-data-hhbgvffddf'],,0.262567988153626,AutoTrain,Not Specified,Not Specified,Not Specified,,1.995,,,,4918417081.0,True,2,0,['pytorch'],2022-09-30 13:03:11+00:00,2022-09-30 12:15:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1616457033
- CO2 Emissions (in grams): 0.2626

## Validation Metrics

- Loss: 1.995
- SacreBLEU: 16.326
- Gen len: 10.246",,,1,[],[],NLP,2022-09,18731975346.980537,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72692,GermantoSwabian,['Tritkoman/autotrain-data-gurbswab2'],,0.2690788847590159,AutoTrain,Not Specified,Not Specified,Not Specified,,1.983,,,,4918417081.0,True,3,1,['pytorch'],2022-09-30 14:14:23+00:00,2022-09-30 13:34:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1619457138
- CO2 Emissions (in grams): 0.2691

## Validation Metrics

- Loss: 1.983
- SacreBLEU: 15.543
- Gen len: 15.608",,,1,[],[],NLP,2022-09,18278718099.359158,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
72866,newSentiment_1Oct22,['ashwinperti/autotrain-data-ashwin_sentiment140dataset'],,1.3744604633696438,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-10-01 08:42:33+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73001,autotrain-text-classification-kunuz-1630257501,['Akhror/autotrain-data-text-classification-kunuz'],,22.431314788743983,AutoTrain,Not Specified,Not Specified,Not Specified,0.647,0.975,0.542,,,556862191.0,True,4,1,['pytorch'],2022-10-02 02:10:16+00:00,2022-10-02 02:00:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1630257501
- CO2 Emissions (in grams): 22.4313

## Validation Metrics

- Loss: 0.975
- Accuracy: 0.647
- Macro F1: 0.542
- Micro F1: 0.647
- Weighted F1: 0.629
- Macro Precision: 0.534
- Micro Precision: 0.647
- Weighted Precision: 0.616
- Macro Recall: 0.558
- Micro Recall: 0.647
- Weighted Recall: 0.647


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Akhror/autotrain-text-classification-kunuz-1630257501
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Akhror/autotrain-text-classification-kunuz-1630257501"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Akhror/autotrain-text-classification-kunuz-1630257501"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,24825214.047614947,0.5898637510513036,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73360,dog_food,['lewtun/dog_food'],,6.799888815236616,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,1.0,,,347600575.0,True,4,0,"['transformers', 'pytorch']",2022-10-04 19:07:06+00:00,2022-10-03 19:12:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1647758504
- CO2 Emissions (in grams): 6.7999

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2022-10,51118567.44203317,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73387,autotrain-bart-meeting-summarization-1648858537,['abrizk/autotrain-data-bart-meeting-summarization'],,0.3760970336042794,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-10-03 22:33:56+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73391,autotrain-fake-news-1649058538,['suresh-subramanian/autotrain-data-fake-news'],,0.0409785418562958,AutoTrain,Not Specified,Not Specified,Not Specified,0.815,0.387,0.745,,,556846831.0,True,4,0,['pytorch'],2022-10-03 22:11:11+00:00,2022-10-03 22:07:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1649058538
- CO2 Emissions (in grams): 0.0410

## Validation Metrics

- Loss: 0.387
- Accuracy: 0.815
- Precision: 0.760
- Recall: 0.730
- AUC: 0.902
- F1: 0.745

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/suresh-subramanian/autotrain-fake-news-1649058538
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058538"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058538"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,13588741955.552233,0.7784294871794872,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73392,autotrain-fake-news-1649058539,['suresh-subramanian/autotrain-data-fake-news'],,0.0402978723064698,AutoTrain,Not Specified,Not Specified,Not Specified,0.779,0.478,0.635,,,737766955.0,True,4,0,['pytorch'],2022-10-03 22:12:00+00:00,2022-10-03 22:07:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1649058539
- CO2 Emissions (in grams): 0.0403

## Validation Metrics

- Loss: 0.478
- Accuracy: 0.779
- Precision: 0.814
- Recall: 0.520
- AUC: 0.881
- F1: 0.635

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/suresh-subramanian/autotrain-fake-news-1649058539
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058539"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058539"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,18307838919.86158,0.6996676096181047,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73393,autotrain-fake-news-1649058540,['suresh-subramanian/autotrain-data-fake-news'],,4.630852478388675,AutoTrain,Not Specified,Not Specified,Not Specified,0.725,0.527,0.523,,,438006125.0,True,4,0,['pytorch'],2022-10-03 22:10:07+00:00,2022-10-03 22:07:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1649058540
- CO2 Emissions (in grams): 4.6309

## Validation Metrics

- Loss: 0.527
- Accuracy: 0.725
- Precision: 0.729
- Recall: 0.408
- AUC: 0.825
- F1: 0.523

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/suresh-subramanian/autotrain-fake-news-1649058540
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058540"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058540"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,94584339.93397392,0.6076522435897436,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73394,autotrain-fake-news-1649058541,['suresh-subramanian/autotrain-data-fake-news'],,4.695596043893512,AutoTrain,Not Specified,Not Specified,Not Specified,0.779,0.459,0.646,,,433318253.0,True,4,0,['pytorch'],2022-10-03 22:10:11+00:00,2022-10-03 22:07:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1649058541
- CO2 Emissions (in grams): 4.6956

## Validation Metrics

- Loss: 0.459
- Accuracy: 0.779
- Precision: 0.790
- Recall: 0.546
- AUC: 0.881
- F1: 0.646

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/suresh-subramanian/autotrain-fake-news-1649058541
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058541"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058541"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,92281842.16645253,0.7062933333333333,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73395,autotrain-fake-news-1649058542,['suresh-subramanian/autotrain-data-fake-news'],,12.699762619910535,AutoTrain,Not Specified,Not Specified,Not Specified,0.637,0.624,0.039,,,1334461229.0,True,4,0,['pytorch'],2022-10-03 22:13:59+00:00,2022-10-03 22:08:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1649058542
- CO2 Emissions (in grams): 12.6998

## Validation Metrics

- Loss: 0.624
- Accuracy: 0.637
- Precision: 1.000
- Recall: 0.020
- AUC: 0.652
- F1: 0.039

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/suresh-subramanian/autotrain-fake-news-1649058542
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058542"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""suresh-subramanian/autotrain-fake-news-1649058542"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,105077651.365534,0.0735,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73492,autotrain-damages-1652858619,['mouss/autotrain-data-damages'],,0.0073164334313121,AutoTrain,Not Specified,Not Specified,Not Specified,0.989,0.082,,,,343266993.0,True,3,0,['pytorch'],2022-10-04 09:41:15+00:00,2022-10-04 09:39:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1652858619
- CO2 Emissions (in grams): 0.0073

## Validation Metrics

- Loss: 0.082
- Accuracy: 0.989
- Precision: 1.000
- Recall: 0.978
- AUC: 0.995
- F1: 0.989",,,1,[],[],Computer Vision,2022-10,46917257735.2952,0.989,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73529,autotrain-person-name-validity1-1655358687,['Akshata/autotrain-data-person-name-validity1'],,0.0150120248218022,AutoTrain,Not Specified,Not Specified,Not Specified,0.991,0.038,0.0,,,1336512689.0,True,3,0,['pytorch'],2022-10-04 13:17:17+00:00,2022-10-04 13:15:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1655358687
- CO2 Emissions (in grams): 0.0150

## Validation Metrics

- Loss: 0.038
- Accuracy: 0.991
- Precision: 0.000
- Recall: 0.000
- F1: 0.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Akshata/autotrain-person-name-validity1-1655358687
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Akshata/autotrain-person-name-validity1-1655358687"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Akshata/autotrain-person-name-validity1-1655358687"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,89029475028.50925,0.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73547,autotrain-khilhlkhlk-1656058698,['flori1995/autotrain-data-khilhlkhlk'],,0.0295948437133704,AutoTrain,Not Specified,Not Specified,Not Specified,,1.334,,0.64877,0.64527,557969145.0,True,2,0,['pytorch'],2022-10-04 13:44:12+00:00,2022-10-04 13:40:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1656058698
- CO2 Emissions (in grams): 0.0296

## Validation Metrics

- Loss: 1.334
- Rouge1: 64.877
- Rouge2: 50.407
- RougeL: 64.527
- RougeLsum: 64.661
- Gen Len: 9.992

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/flori1995/autotrain-khilhlkhlk-1656058698
```",,,1,[],[],NLP,2022-10,18853593227.38778,0.6470152667614603,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73616,AI-image-detector,['Colby/autotrain-data-ai-image-detector'],,7.940487247386902,AutoTrain,Not Specified,Not Specified,Not Specified,0.942,0.163,0.958,,,347596479.0,True,1505,10,['pytorch'],2022-10-17 12:51:41+00:00,2022-10-04 17:12:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1519658722
- CO2 Emissions (in grams): 7.9405

## Validation Metrics

- Loss: 0.163
- Accuracy: 0.942
- Precision: 0.938
- Recall: 0.978
- AUC: 0.980
- F1: 0.958

# License Notice

This work is licensed under a [Creative Commons Attribution-NoDerivatives 4.0 International License](https://creativecommons.org/licenses/by-nd/4.0/).

You may distribute and make this model available to others as part of your own web page, app, or service so long as you provide attribution. However, use of this model within text-to-image systems to evade AI image detection would be considered a ""derivative work"" and as such prohibited by the license terms.",,,1,[],[],Computer Vision,2022-10,43775207.7637791,0.9499326315789473,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73626,bart-base-cnn-xsum-cite-swe,['Gabriel/citesum_swe'],,0.0334,Not Specified,fine-tuning,"Fredericia, Denmark",Tesla P100-PCIE-16GB,,,,,,557723065.0,False,5,1,"['tensorboard', 'transformers', 'pytorch']",2022-10-04 19:58:22+00:00,2022-10-04 18:37:47+00:00,"
<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# bart-base-cnn-xsum-cite-swe

This model is a fine-tuned version of [Gabriel/bart-base-cnn-xsum-swe](https://huggingface.co/Gabriel/bart-base-cnn-xsum-swe) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 2.4203
- Rouge1: 29.6279
- Rouge2: 11.5697
- Rougel: 24.2429
- Rougelsum: 24.4557
- Gen Len: 19.9371

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 16
- eval_batch_size: 16
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 500
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rouge1  | Rouge2  | Rougel  | Rougelsum | Gen Len |
|:-------------:|:-----:|:----:|:---------------:|:-------:|:-------:|:-------:|:---------:|:-------:|
| 2.4833        | 1.0   | 2558 | 2.4203          | 29.6279 | 11.5697 | 24.2429 | 24.4557   | 19.9371 |


### Framework versions

- Transformers 4.22.2
- Pytorch 1.12.1+cu113
- Datasets 2.5.1
- Tokenizers 0.12.1
",,,1,[],[],NLP,2022-10,16698295359.281437,,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,1,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73656,autotrain-in-class-test-demo-1659958764,['omarques/autotrain-data-in-class-test-demo'],,3.2447037790637503,AutoTrain,Not Specified,Not Specified,Not Specified,0.991,0.044,0.988,,,,True,4,0,['joblib'],2022-10-04 21:31:32+00:00,2022-10-04 21:27:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1659958764
- CO2 Emissions (in grams): 3.2447

## Validation Metrics

- Loss: 0.044
- Accuracy: 0.991
- Precision: 1.000
- Recall: 0.977
- AUC: 0.999
- F1: 0.988

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,0.9894977261243052,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73657,autotrain-in-class-test-demo-1659958767,['omarques/autotrain-data-in-class-test-demo'],,0.1503169877612804,AutoTrain,Not Specified,Not Specified,Not Specified,0.983,0.076,0.976,,,,True,4,6,['joblib'],2022-10-04 21:28:14+00:00,2022-10-04 21:27:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1659958767
- CO2 Emissions (in grams): 0.1503

## Validation Metrics

- Loss: 0.076
- Accuracy: 0.983
- Precision: 1.000
- Recall: 0.953
- AUC: 0.999
- F1: 0.976

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,0.9794874936191936,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73823,autotrain-sphere-lecture-demo-1671659193,['bvint/autotrain-data-sphere-lecture-demo'],,0.0047353241110689,AutoTrain,Not Specified,Not Specified,Not Specified,0.658,0.803,0.478,,,737770027.0,True,4,0,['pytorch'],2022-10-05 15:44:26+00:00,2022-10-05 15:43:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1671659193
- CO2 Emissions (in grams): 0.0047

## Validation Metrics

- Loss: 0.803
- Accuracy: 0.658
- Macro F1: 0.478
- Micro F1: 0.658
- Weighted F1: 0.580
- Macro Precision: 0.424
- Micro Precision: 0.658
- Weighted Precision: 0.520
- Macro Recall: 0.549
- Micro Recall: 0.658
- Weighted Recall: 0.658


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/bvint/autotrain-sphere-lecture-demo-1671659193
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""bvint/autotrain-sphere-lecture-demo-1671659193"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""bvint/autotrain-sphere-lecture-demo-1671659193"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,155801379102.10837,0.5537394366197184,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73952,GermantoHunsrik,['Tritkoman/autotrain-data-kskakqka'],,2.3711122483132376,AutoTrain,Not Specified,Not Specified,Not Specified,,1.5,,,,295861701.0,True,2,1,['pytorch'],2022-10-06 05:41:38+00:00,2022-10-06 05:39:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1676659310
- CO2 Emissions (in grams): 2.3711

## Validation Metrics

- Loss: 1.500
- SacreBLEU: 30.765
- Gen len: 10.639",,,1,[],[],NLP,2022-10,124777602.24572672,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73953,autotrain-stocks-ner-2000-sample-test-1676759313,['hemangjoshi37a/autotrain-data-stocks-ner-2000-sample-test'],,0.0110294087066048,AutoTrain,Not Specified,Not Specified,Not Specified,0.973,0.097,0.912,,,1330303153.0,True,163,0,['pytorch'],2023-02-16 12:46:04+00:00,2022-10-06 05:43:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1676759313
- CO2 Emissions (in grams): 0.0110

## Validation Metrics

- Loss: 0.097
- Accuracy: 0.973
- Precision: 0.903
- Recall: 0.921
- F1: 0.912

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/hemangjoshi37a/autotrain-stocks-ner-2000-sample-test-1676759313
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""hemangjoshi37a/autotrain-stocks-ner-2000-sample-test-1676759313"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hemangjoshi37a/autotrain-stocks-ner-2000-sample-test-1676759313"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```



# GitHub Link to this project : [Telegram Trade Msg Backtest ML](https://github.com/hemangjoshi37a/TelegramTradeMsgBacktestML)

# Need custom model for your application? : Place a order on hjLabs.in : [Custom Token Classification or Named Entity Recognition (NER) model as in Natural Language Processing (NLP) Machine Learning](https://hjlabs.in/product/custom-token-classification-or-named-entity-recognition-ner-model-as-in-natural-language-processing-nlp-machine-learning/)

## What this repository contains? :

1. Label data using LabelStudio NER(Named Entity Recognition or Token Classification) tool.
 ![Screenshot from 2022-09-30 12-28-50](https://user-images.githubusercontent.com/12392345/193394190-3ad215d1-3205-4af3-949e-6d95cf866c6c.png) convert to  ![Screenshot from 2022-09-30 18-59-14](https://user-images.githubusercontent.com/12392345/193394213-9bb936e7-34ea-4cbc-9132-80c7e5a006d7.png)

2. Convert LabelStudio CSV or JSON to HuggingFace-autoTrain dataset conversion script
![Screenshot from 2022-10-01 10-36-03](https://user-images.githubusercontent.com/12392345/193394227-32e293d4-6736-4e71-b687-b0c2fcad732c.png)

3. Train NER model on Hugginface-autoTrain.
 ![Screenshot from 2022-10-01 10-38-24](https://user-images.githubusercontent.com/12392345/193394247-bf51da86-45bb-41b4-b4da-3de86014e6a5.png)

4. Use Hugginface-autoTrain model to predict labels on new data in LabelStudio using LabelStudio-ML-Backend.
 ![Screenshot from 2022-10-01 10-41-07](https://user-images.githubusercontent.com/12392345/193394251-bfba07d4-c56b-4fe8-ba7f-08a1c69f0e2c.png)
 ![Screenshot from 2022-10-01 10-42-36](https://user-images.githubusercontent.com/12392345/193394261-df4bc8f8-9ffd-4819-ba26-04fddbba8e7b.png)
 ![Screenshot from 2022-10-01 10-44-56](https://user-images.githubusercontent.com/12392345/193394267-c5a111c3-8d00-4d6f-b3c6-0ea82e4ac474.png)

5. Define python function to predict labels using Hugginface-autoTrain model.
 ![Screenshot from 2022-10-01 10-47-08](https://user-images.githubusercontent.com/12392345/193394278-81389606-f690-454a-bb2b-ef3f1db39571.png)
![Screenshot from 2022-10-01 10-47-25](https://user-images.githubusercontent.com/12392345/193394288-27a0c250-41af-48b1-9c57-c146dc51da1d.png)

6. Only label new data from newly predicted-labels-dataset that has falsified labels.
 ![Screenshot from 2022-09-30 22-47-23](https://user-images.githubusercontent.com/12392345/193394294-fdfaf40a-c9cd-4c2d-836e-1878b503a668.png)

7. Backtest Truely labelled dataset against real historical data of the stock using zerodha kiteconnect and jugaad_trader.
 ![Screenshot from 2022-10-01 00-05-55](https://user-images.githubusercontent.com/12392345/193394303-137c2a2a-3341-4be3-8ece-5191669ec53a.png)

8. Evaluate total gained percentage since inception summation-wise and compounded and plot.
 ![Screenshot from 2022-10-01 00-06-59](https://user-images.githubusercontent.com/12392345/193394308-446eddd9-c5d1-47e3-a231-9edc620284bb.png)

9. Listen to telegram channel for new LIVE messages using telegram API for algotrading.
 ![Screenshot from 2022-10-01 00-09-29](https://user-images.githubusercontent.com/12392345/193394319-8cc915b7-216e-4e05-a7bf-28360b17de99.png)

10. Serve the app as flask web API for web request and respond to it as labelled tokens.
 ![Screenshot from 2022-10-01 00-12-12](https://user-images.githubusercontent.com/12392345/193394323-822c2a59-ca72-45b1-abca-a6e5df3364b0.png)

11. Outperforming or underperforming results of the telegram channel tips against exchange index by percentage.
 ![Screenshot from 2022-10-01 11-16-27](https://user-images.githubusercontent.com/12392345/193394685-53235198-04f8-4d3c-a341-535dd9093252.png)



Place a custom order on hjLabs.in : [https://hjLabs.in](https://hjlabs.in/?product=custom-algotrading-software-for-zerodha-and-angel-w-source-code)


----------------------------------------------------------------------

### Social Media :
* [WhatsApp/917016525813](https://wa.me/917016525813)
* [telegram/hjlabs](https://t.me/hjlabs) 
* [Gmail/hemangjoshi37a@gmail.com](mailto:hemangjoshi37a@gmail.com)
* [Facebook/hemangjoshi37](https://www.facebook.com/hemangjoshi37/)
* [Twitter/HemangJ81509525](https://twitter.com/HemangJ81509525)
* [LinkedIn/hemang-joshi-046746aa](https://www.linkedin.com/in/hemang-joshi-046746aa/)
* [Tumblr/hemangjoshi37a-blog](https://www.tumblr.com/blog/hemangjoshi37a-blog)
* [Pinterest/hemangjoshi37a](https://in.pinterest.com/hemangjoshi37a/)
* [Blogger/hemangjoshi](http://hemangjoshi.blogspot.com/)
* [Instagram/hemangjoshi37](https://www.instagram.com/hemangjoshi37/)
  
### Checkout Our Other Repositories

- [pyPortMan](https://github.com/hemangjoshi37a/pyPortMan)
- [transformers_stock_prediction](https://github.com/hemangjoshi37a/transformers_stock_prediction)
- [TrendMaster](https://github.com/hemangjoshi37a/TrendMaster)
- [hjAlgos_notebooks](https://github.com/hemangjoshi37a/hjAlgos_notebooks)
- [AutoCut](https://github.com/hemangjoshi37a/AutoCut)
- [My_Projects](https://github.com/hemangjoshi37a/My_Projects)
- [Cool Arduino and ESP8266 or NodeMCU Projects](https://github.com/hemangjoshi37a/my_Arduino)
- [Telegram Trade Msg Backtest ML](https://github.com/hemangjoshi37a/TelegramTradeMsgBacktestML)

### Checkout Our Other Products

- [WiFi IoT LED Matrix Display](https://hjlabs.in/product/wifi-iot-led-display)
- [SWiBoard WiFi Switch Board IoT Device](https://hjlabs.in/product/swiboard-wifi-switch-board-iot-device)
- [Electric Bicycle](https://hjlabs.in/product/electric-bicycle)
- [Product 3D Design Service with Solidworks](https://hjlabs.in/product/product-3d-design-with-solidworks/)
- [AutoCut : Automatic Wire Cutter Machine](https://hjlabs.in/product/automatic-wire-cutter-machine/)
- [Custom AlgoTrading Software Coding Services](https://hjlabs.in/product/custom-algotrading-software-for-zerodha-and-angel-w-source-code//)
- [SWiBoard :Tasmota MQTT Control App](https://play.google.com/store/apps/details?id=in.hjlabs.swiboard)
- [Custom Token Classification or Named Entity Recognition (NER) model as in Natural Language Processing (NLP) Machine Learning](https://hjlabs.in/product/custom-token-classification-or-named-entity-recognition-ner-model-as-in-natural-language-processing-nlp-machine-learning/)

## Some Cool Arduino and ESP8266 (or NodeMCU) IoT projects:
- [IoT_LED_over_ESP8266_NodeMCU : Turn LED on and off using web server hosted on a nodemcu or esp8266](https://github.com/hemangjoshi37a/my_Arduino/tree/master/IoT_LED_over_ESP8266_NodeMCU)
- [ESP8266_NodeMCU_BasicOTA : Simple OTA (Over The Air) upload code from Arduino IDE using WiFi to NodeMCU or ESP8266](https://github.com/hemangjoshi37a/my_Arduino/tree/master/ESP8266_NodeMCU_BasicOTA)  
- [IoT_CSV_SD : Read analog value of Voltage and Current and write it to SD Card in CSV format for Arduino, ESP8266, NodeMCU etc](https://github.com/hemangjoshi37a/my_Arduino/tree/master/IoT_CSV_SD)  
- [Honeywell_I2C_Datalogger : Log data in A SD Card from a Honeywell I2C HIH8000 or HIH6000 series sensor having external I2C RTC clock](https://github.com/hemangjoshi37a/my_Arduino/tree/master/Honeywell_I2C_Datalogger)
- [IoT_Load_Cell_using_ESP8266_NodeMC : Read ADC value from High Precision 12bit ADS1015 ADC Sensor and Display on SSD1306 SPI Display as progress bar for Arduino or ESP8266 or NodeMCU](https://github.com/hemangjoshi37a/my_Arduino/tree/master/IoT_Load_Cell_using_ESP8266_NodeMC)
- [IoT_SSD1306_ESP8266_NodeMCU : Read from High Precision 12bit ADC seonsor ADS1015 and display to SSD1306 SPI as progress bar in ESP8266 or NodeMCU or Arduino](https://github.com/hemangjoshi37a/my_Arduino/tree/master/IoT_SSD1306_ESP8266_NodeMCU)  


## Checkout Our Awesome 3D GrabCAD Models:
- [AutoCut : Automatic Wire Cutter Machine](https://grabcad.com/library/automatic-wire-cutter-machine-1)
- [ESP Matrix Display 5mm Acrylic Box](https://grabcad.com/library/esp-matrix-display-5mm-acrylic-box-1)
- [Arcylic Bending Machine w/ Hot Air Gun](https://grabcad.com/library/arcylic-bending-machine-w-hot-air-gun-1)
- [Automatic Wire Cutter/Stripper](https://grabcad.com/library/automatic-wire-cutter-stripper-1)

## Our HuggingFace Models :
- [hemangjoshi37a/autotrain-ratnakar_1000_sample_curated-1474454086 : Stock tip message NER(Named Entity Recognition or Token Classification) using HUggingFace-AutoTrain and LabelStudio and Ratnakar Securities Pvt. Ltd.](https://huggingface.co/hemangjoshi37a/autotrain-ratnakar_1000_sample_curated-1474454086)

## Our HuggingFace Datasets :
- [hemangjoshi37a/autotrain-data-ratnakar_1000_sample_curated : Stock tip message NER(Named Entity Recognition or Token Classification) using HUggingFace-AutoTrain and LabelStudio and Ratnakar Securities Pvt. Ltd.](https://huggingface.co/datasets/hemangjoshi37a/autotrain-data-ratnakar_1000_sample_curated)

## We sell Gigs on Fiverr : 
- [code android and ios app for you using flutter firebase software stack](https://business.fiverr.com/share/3v14pr)
- [code custom algotrading software for zerodha or angel broking](https://business.fiverr.com/share/kzkvEy)

## Awesome Fiverr. Gigs:
- [develop machine learning ner model as in nlp using python](https://www.fiverr.com/share/9YNabx)
- [train custom chatgpt question answering model](https://www.fiverr.com/share/rwx6r7)
- [build algotrading, backtesting and stock monitoring tools using python](https://www.fiverr.com/share/A7Y14q)
- [tutor you in your science problems](https://www.fiverr.com/share/zPzmlz)
- [make apps for you crossplatform	](https://www.fiverr.com/share/BGw12l)
",,,1,[],[],NLP,2022-10,120614185981.10046,0.9415129973474802,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
73983,autotrain-chest-xray-demo-1677859324,['juliensimon/autotrain-data-chest-xray-demo'],,13.219748263433518,AutoTrain,Not Specified,Not Specified,Not Specified,0.934,0.209,,,,347596479.0,True,9,0,['pytorch'],2022-10-17 09:37:49+00:00,2022-10-06 09:13:05+00:00,"
Original dataset: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia

# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1677859324
- CO2 Emissions (in grams): 13.2197

## Validation Metrics

- Loss: 0.209
- Accuracy: 0.934
- Precision: 0.933
- Recall: 0.964
- AUC: 0.976
- F1: 0.948",,,1,[],[],Computer Vision,2022-10,26293729.053938884,0.9340000000000002,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74086,autotrain-isuzu-f-left-1681159381,['tursunali/autotrain-data-isuzu-f-left'],,0.8519213945001354,AutoTrain,Not Specified,Not Specified,Not Specified,0.99,0.021,,,,343266993.0,True,3,0,['pytorch'],2022-10-06 17:56:05+00:00,2022-10-06 17:55:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1681159381
- CO2 Emissions (in grams): 0.8519

## Validation Metrics

- Loss: 0.021
- Accuracy: 0.990
- Precision: 1.000
- Recall: 0.974
- AUC: 1.000
- F1: 0.987",,,1,[],[],Computer Vision,2022-10,402932706.25209713,0.99,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74173,EnglishtoAncientGreek,['Tritkoman/autotrain-data-kskskkw'],,45.2679908890355,AutoTrain,Not Specified,Not Specified,Not Specified,,2.056,,,,4918417081.0,True,2,0,['pytorch'],2022-12-19 09:29:28+00:00,2022-10-07 04:44:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1684859425
- CO2 Emissions (in grams): 45.2680

## Validation Metrics

- Loss: 2.056
- SacreBLEU: 6.077
- Gen len: 15.482",,,1,[],[],NLP,2022-10,108651101.68145998,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74176,autotrain-name_classification-1685059436,['deepaksiloka/autotrain-data-name_classification'],,0.3857777634696358,AutoTrain,Not Specified,Not Specified,Not Specified,0.988,0.063,0.989,,,265491317.0,True,3,0,['pytorch'],2022-10-07 05:04:53+00:00,2022-10-07 05:04:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1685059436
- CO2 Emissions (in grams): 0.3858

## Validation Metrics

- Loss: 0.063
- Accuracy: 0.988
- Precision: 0.989
- Recall: 0.989
- F1: 0.989

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepaksiloka/autotrain-name_classification-1685059436
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""deepaksiloka/autotrain-name_classification-1685059436"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepaksiloka/autotrain-name_classification-1685059436"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,688197563.8310645,0.9884997470915527,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74198,autotrain-name_classification_3-1686659457,['deepaksiloka/autotrain-data-name_classification_3'],,1.0819688360882111,AutoTrain,Not Specified,Not Specified,Not Specified,0.978,0.087,0.983,,,435643185.0,True,3,0,['pytorch'],2022-10-07 07:26:40+00:00,2022-10-07 07:25:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1686659457
- CO2 Emissions (in grams): 1.0820

## Validation Metrics

- Loss: 0.087
- Accuracy: 0.978
- Precision: 0.979
- Recall: 0.987
- F1: 0.983

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/deepaksiloka/autotrain-name_classification_3-1686659457
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""deepaksiloka/autotrain-name_classification_3-1686659457"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""deepaksiloka/autotrain-name_classification_3-1686659457"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,402639309.44170254,0.9804936257011728,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74317,html-bart-78.8,[''],,4721.333535315218,AutoTrain,Not Specified,Not Specified,Not Specified,,0.201,,0.7885,0.78683,557969145.0,True,57,1,"['transformers', 'pytorch']",2022-12-08 09:13:49+00:00,2022-10-07 15:31:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1670059180
- CO2 Emissions (in grams): 4721.3335

## Validation Metrics

- Loss: 0.201
- Rouge1: 78.850
- Rouge2: 73.326
- RougeL: 78.683
- RougeLsum: 78.683
- Gen Len: 17.217

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' 
```",,,1,[],[],NLP,2022-10,118180.41255218106,0.7876641148203868,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74357,autotrain-vision_6_categories_70_images_each-1691759542,['renee127/autotrain-data-vision_6_categories_70_images_each'],,1.8709463080714803,AutoTrain,Not Specified,Not Specified,Not Specified,0.988,0.056,0.988,,,347612863.0,True,2,0,['pytorch'],2022-10-07 19:35:23+00:00,2022-10-07 19:33:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1691759542
- CO2 Emissions (in grams): 1.8709

## Validation Metrics

- Loss: 0.056
- Accuracy: 0.988
- Macro F1: 0.988
- Micro F1: 0.988
- Weighted F1: 0.988
- Macro Precision: 0.989
- Micro Precision: 0.988
- Weighted Precision: 0.989
- Macro Recall: 0.988
- Micro Recall: 0.988
- Weighted Recall: 0.988",,,1,[],[],Computer Vision,2022-10,185795210.42392164,0.988,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74767,EnglishtoLinguaFrancaNova,['Tritkoman/autotrain-data-wwwwdsxzaa'],,13.980549591928089,AutoTrain,Not Specified,Not Specified,Not Specified,,1.741,,,,295861701.0,True,2,0,['pytorch'],2022-10-09 12:35:27+00:00,2022-10-09 12:26:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1702359709
- CO2 Emissions (in grams): 13.9805

## Validation Metrics

- Loss: 1.741
- SacreBLEU: 27.270
- Gen len: 13.747",,,1,[],[],NLP,2022-10,21162379.85170632,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74773,autotrain-amx2-1702259725,['tejas23/autotrain-data-amx2'],,7.704828730137597,AutoTrain,Not Specified,Not Specified,Not Specified,0.827,0.421,0.53,,,,True,4,0,['joblib'],2022-10-09 13:10:48+00:00,2022-10-09 13:03:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1702259725
- CO2 Emissions (in grams): 7.7048

## Validation Metrics

- Loss: 0.421
- Accuracy: 0.827
- Macro F1: 0.530
- Micro F1: 0.827
- Weighted F1: 0.805
- Macro Precision: 0.579
- Micro Precision: 0.827
- Weighted Precision: 0.795
- Macro Recall: 0.513
- Micro Recall: 0.827
- Weighted Recall: 0.827

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,0.6459985261606485,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74774,autotrain-amx2-1702259728,['tejas23/autotrain-data-amx2'],,0.0082468973760525,AutoTrain,Not Specified,Not Specified,Not Specified,0.831,0.434,0.521,,,,True,3,0,['joblib'],2022-10-09 13:08:34+00:00,2022-10-09 13:03:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1702259728
- CO2 Emissions (in grams): 0.0082

## Validation Metrics

- Loss: 0.434
- Accuracy: 0.831
- Macro F1: 0.521
- Micro F1: 0.831
- Weighted F1: 0.803
- Macro Precision: 0.590
- Micro Precision: 0.831
- Weighted Precision: 0.794
- Macro Recall: 0.507
- Micro Recall: 0.831
- Weighted Recall: 0.831

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,0.6404600591715977,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
74776,autotrain-amx2-1702259729,['tejas23/autotrain-data-amx2'],,0.0027665450339142,AutoTrain,Not Specified,Not Specified,Not Specified,0.824,6.095,0.543,,,,True,3,0,['joblib'],2022-10-09 13:05:26+00:00,2022-10-09 13:03:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1702259729
- CO2 Emissions (in grams): 0.0028

## Validation Metrics

- Loss: 6.095
- Accuracy: 0.824
- Macro F1: 0.543
- Micro F1: 0.824
- Weighted F1: 0.808
- Macro Precision: 0.572
- Micro Precision: 0.824
- Weighted Precision: 0.801
- Macro Recall: 0.543
- Micro Recall: 0.824
- Weighted Recall: 0.824

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,0.6546188734455011,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
75115,EnglishtoAncientGreekV2,['Tritkoman/autotrain-data-llslslakak'],,47.55255625240336,AutoTrain,Not Specified,Not Specified,Not Specified,,2.042,,,,4918417081.0,True,2,0,['pytorch'],2022-10-10 18:01:03+00:00,2022-10-10 17:23:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1715360000
- CO2 Emissions (in grams): 47.5526

## Validation Metrics

- Loss: 2.042
- SacreBLEU: 6.381
- Gen len: 15.893",,,1,[],[],NLP,2022-10,103431181.59397408,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
75143,EnglishtoAncientGreekV3,['Tritkoman/autotrain-data-wdwssqddwd'],,0.642110734276787,AutoTrain,Not Specified,Not Specified,Not Specified,,0.741,,,,4918417081.0,True,2,0,['pytorch'],2022-10-10 20:50:23+00:00,2022-10-10 19:02:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1716860020
- CO2 Emissions (in grams): 0.6421

## Validation Metrics

- Loss: 0.741
- SacreBLEU: 31.314
- Gen len: 14.605",,,1,[],[],NLP,2022-10,7659764614.493855,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,1.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
75292,EnglishtoOldEastSlavic,['Tritkoman/autotrain-data-saxxswddwdww'],,101.24633341477742,AutoTrain,Not Specified,Not Specified,Not Specified,,0.197,,,,4918417081.0,True,2,0,['pytorch'],2022-10-11 12:54:43+00:00,2022-10-11 11:35:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1721860180
- CO2 Emissions (in grams): 101.2463

## Validation Metrics

- Loss: 0.197
- SacreBLEU: 53.871
- Gen len: 11.637",,,1,[],[],NLP,2022-10,48578718.01490968,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,1.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
75303,autotrain-intelligize-edgar-analysis-2-1722460190,['asadcr/autotrain-data-intelligize-edgar-analysis-2'],,0.9669951284881568,AutoTrain,Not Specified,Not Specified,Not Specified,,1.652,,0.50229,0.50229,557969145.0,True,2,0,['pytorch'],2022-10-11 12:43:56+00:00,2022-10-11 12:42:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1722460190
- CO2 Emissions (in grams): 0.9670

## Validation Metrics

- Loss: 1.652
- Rouge1: 50.229
- Rouge2: 41.591
- RougeL: 50.229
- RougeLsum: 53.205
- Gen Len: 10.250

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/asadcr/autotrain-intelligize-edgar-analysis-2-1722460190
```",,,1,[],[],NLP,2022-10,577013398.0637046,0.50229,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
75309,EnglishtoSaterlandFrisian,['Tritkoman/autotrain-data-wdxsxsxswddwwd'],,2.82976037007073,AutoTrain,Not Specified,Not Specified,Not Specified,,0.021,,,,314179909.0,True,3,0,['pytorch'],2022-10-11 13:01:51+00:00,2022-10-11 12:59:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1722960201
- CO2 Emissions (in grams): 2.8298

## Validation Metrics

- Loss: 0.021
- SacreBLEU: 92.565
- Gen len: 10.877",,,1,[],[],NLP,2022-10,111027036.89081174,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,1.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
75398,RussiantoChukchi,['Tritkoman/autotrain-data-kkakkakqa'],,96.54051975402358,AutoTrain,Not Specified,Not Specified,Not Specified,,0.151,,,,4918417081.0,True,2,0,['pytorch'],2022-10-11 20:05:54+00:00,2022-10-11 19:02:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1726160287
- CO2 Emissions (in grams): 96.5405

## Validation Metrics

- Loss: 0.151
- SacreBLEU: 51.859
- Gen len: 14.625",,,1,[],[],NLP,2022-10,50946660.464763165,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,1.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
75647,autotrain-movie-rationales-1734060527,['rgoldstein/autotrain-data-movie-rationales'],,5.912842155368309,AutoTrain,Not Specified,Not Specified,Not Specified,0.934,0.198,0.934,,,737766955.0,True,4,0,['pytorch'],2022-10-12 14:34:03+00:00,2022-10-12 14:30:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1734060527
- CO2 Emissions (in grams): 5.9128

## Validation Metrics

- Loss: 0.198
- Accuracy: 0.934
- Precision: 0.937
- Recall: 0.931
- AUC: 0.983
- F1: 0.934

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/rgoldstein/autotrain-movie-rationales-1734060527
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""rgoldstein/autotrain-movie-rationales-1734060527"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""rgoldstein/autotrain-movie-rationales-1734060527"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,124773659.70782366,0.9340000000000002,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
75918,autotrain-t5-clickbait-binary-1750661108,['jlh/autotrain-data-t5-clickbait-binary'],,8.44003619525122,AutoTrain,Not Specified,Not Specified,Not Specified,0.998,0.006,0.999,,,498660333.0,True,4,0,['pytorch'],2022-10-13 18:29:18+00:00,2022-10-13 18:25:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1750661108
- CO2 Emissions (in grams): 8.4400

## Validation Metrics

- Loss: 0.006
- Accuracy: 0.998
- Precision: 0.998
- Recall: 0.999
- AUC: 1.000
- F1: 0.999

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jlh/autotrain-t5-clickbait-binary-1750661108
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jlh/autotrain-t5-clickbait-binary-1750661108"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jlh/autotrain-t5-clickbait-binary-1750661108"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,59082724.46515938,0.9984997496244366,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76108,autotrain-fbert-singlish-1755361190,['DingYao/autotrain-data-fbert-singlish'],,1.3946229895659434,AutoTrain,Not Specified,Not Specified,Not Specified,0.843,0.399,0.832,,,438009197.0,True,5,0,['pytorch'],2022-10-14 06:27:58+00:00,2022-10-14 06:26:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1755361190
- CO2 Emissions (in grams): 1.3946

## Validation Metrics

- Loss: 0.399
- Accuracy: 0.843
- Macro F1: 0.832
- Micro F1: 0.843
- Weighted F1: 0.844
- Macro Precision: 0.818
- Micro Precision: 0.843
- Weighted Precision: 0.848
- Macro Recall: 0.849
- Micro Recall: 0.843
- Weighted Recall: 0.843


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/DingYao/autotrain-fbert-singlish-1755361190
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""DingYao/autotrain-fbert-singlish-1755361190"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""DingYao/autotrain-fbert-singlish-1755361190"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,314069967.4944582,0.8374638805970149,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76454,autotrain-jwan-autotrain1-1768961489,['jwan2021/autotrain-data-jwan-autotrain1'],,2.9876405883375106,AutoTrain,Not Specified,Not Specified,Not Specified,0.983,0.042,0.976,,,,True,4,0,['joblib'],2022-10-15 15:03:40+00:00,2022-10-15 15:00:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1768961489
- CO2 Emissions (in grams): 2.9876

## Validation Metrics

- Loss: 0.042
- Accuracy: 0.983
- Precision: 1.000
- Recall: 0.953
- AUC: 1.000
- F1: 0.976

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,0.9794874936191936,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76477,autotrain-poem-sentiment-analysis-1770161500,['jwan2021/autotrain-data-poem-sentiment-analysis'],,1.2662388515647711,AutoTrain,Not Specified,Not Specified,Not Specified,0.81,0.572,0.59,,,556852975.0,True,4,0,['pytorch'],2022-10-15 18:02:48+00:00,2022-10-15 18:01:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1770161500
- CO2 Emissions (in grams): 1.2662

## Validation Metrics

- Loss: 0.572
- Accuracy: 0.810
- Macro F1: 0.590
- Micro F1: 0.810
- Weighted F1: 0.787
- Macro Precision: 0.570
- Micro Precision: 0.810
- Weighted Precision: 0.766
- Macro Recall: 0.611
- Micro Recall: 0.810
- Weighted Recall: 0.810


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jwan2021/autotrain-poem-sentiment-analysis-1770161500
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161500"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161500"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,439769301.2750807,0.6827142857142857,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76478,autotrain-poem-sentiment-analysis-1770161501,['jwan2021/autotrain-data-poem-sentiment-analysis'],,1.6081724212150736,AutoTrain,Not Specified,Not Specified,Not Specified,0.821,0.599,0.677,,,737773099.0,True,4,0,['pytorch'],2022-10-25 23:46:19+00:00,2022-10-15 18:01:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
  - Text Sentiment Analysis: Positive, Neutral, Negative
- Model ID: 1770161501
- CO2 Emissions (in grams): 1.6082

## Validation Metrics

- Loss: 0.599
- Accuracy: 0.821
- Macro F1: 0.677
- Micro F1: 0.821
- Weighted F1: 0.814
- Macro Precision: 0.741
- Micro Precision: 0.821
- Weighted Precision: 0.825
- Macro Recall: 0.683
- Micro Recall: 0.821
- Weighted Recall: 0.821


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jwan2021/autotrain-poem-sentiment-analysis-1770161501
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161501"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161501"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,458764924.2502037,0.7420787716955941,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76479,autotrain-poem-sentiment-analysis-1770161502,['jwan2021/autotrain-data-poem-sentiment-analysis'],,0.9444638089570118,AutoTrain,Not Specified,Not Specified,Not Specified,0.799,0.589,0.58,,,438012269.0,True,4,0,['pytorch'],2022-10-15 18:02:44+00:00,2022-10-15 18:01:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1770161502
- CO2 Emissions (in grams): 0.9445

## Validation Metrics

- Loss: 0.589
- Accuracy: 0.799
- Macro F1: 0.580
- Micro F1: 0.799
- Weighted F1: 0.778
- Macro Precision: 0.554
- Micro Precision: 0.799
- Weighted Precision: 0.760
- Macro Recall: 0.609
- Micro Recall: 0.799
- Weighted Recall: 0.799


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jwan2021/autotrain-poem-sentiment-analysis-1770161502
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161502"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161502"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,463768187.6701075,0.6721102248005801,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76480,autotrain-poem-sentiment-analysis-1770161503,['jwan2021/autotrain-data-poem-sentiment-analysis'],,1.118838634517984,AutoTrain,Not Specified,Not Specified,Not Specified,0.804,0.562,0.583,,,433324397.0,True,4,0,['pytorch'],2022-10-15 18:02:49+00:00,2022-10-15 18:02:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1770161503
- CO2 Emissions (in grams): 1.1188

## Validation Metrics

- Loss: 0.562
- Accuracy: 0.804
- Macro F1: 0.583
- Micro F1: 0.804
- Weighted F1: 0.783
- Macro Precision: 0.559
- Micro Precision: 0.804
- Weighted Precision: 0.764
- Macro Recall: 0.610
- Micro Recall: 0.804
- Weighted Recall: 0.804


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jwan2021/autotrain-poem-sentiment-analysis-1770161503
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161503"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161503"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,387298385.6932006,0.6758932948810382,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76481,autotrain-poem-sentiment-analysis-1770161504,['jwan2021/autotrain-data-poem-sentiment-analysis'],,1.463756126402436,AutoTrain,Not Specified,Not Specified,Not Specified,0.732,0.804,0.507,,,1334469421.0,True,5,0,['pytorch'],2022-10-15 18:03:58+00:00,2022-10-15 18:02:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1770161504
- CO2 Emissions (in grams): 1.4638

## Validation Metrics

- Loss: 0.804
- Accuracy: 0.732
- Macro F1: 0.507
- Micro F1: 0.732
- Weighted F1: 0.715
- Macro Precision: 0.497
- Micro Precision: 0.732
- Weighted Precision: 0.702
- Macro Recall: 0.523
- Micro Recall: 0.732
- Weighted Recall: 0.732


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jwan2021/autotrain-poem-sentiment-analysis-1770161504
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161504"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jwan2021/autotrain-poem-sentiment-analysis-1770161504"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,911674695.6201018,0.5990702179176756,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76513,autotrain-us-housing-prices-1771761510,['jwan2021/autotrain-data-us-housing-prices'],,4.466856397835458,AutoTrain,Not Specified,Not Specified,Not Specified,,102613.797,,,,,True,3,1,['joblib'],2022-10-15 20:57:20+00:00,2022-10-15 20:51:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1771761510
- CO2 Emissions (in grams): 4.4669

## Validation Metrics

- Loss: 102613.797
- R2: 0.919
- MSE: 10529591296.000
- MAE: 82375.211
- RMSLE: 0.100

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76514,autotrain-us-housing-prices-1771761511,['jwan2021/autotrain-data-us-housing-prices'],,32.513983893680546,AutoTrain,Not Specified,Not Specified,Not Specified,,134406.507,,,,,True,4,0,['joblib'],2022-10-15 22:17:33+00:00,2022-10-15 20:51:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1771761511
- CO2 Emissions (in grams): 32.5140

## Validation Metrics

- Loss: 134406.507
- R2: 0.861
- MSE: 18065109105.270
- MAE: 103271.843
- RMSLE: 0.139

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76515,autotrain-us-housing-prices-1771761512,['jwan2021/autotrain-data-us-housing-prices'],,50.53686341531619,AutoTrain,Not Specified,Not Specified,Not Specified,,122809.223,,,,,True,4,0,['joblib'],2022-10-15 23:05:11+00:00,2022-10-15 20:51:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1771761512
- CO2 Emissions (in grams): 50.5369

## Validation Metrics

- Loss: 122809.223
- R2: 0.884
- MSE: 15082105200.447
- MAE: 95586.887
- RMSLE: 0.130

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76516,autotrain-us-housing-prices-1771761513,['jwan2021/autotrain-data-us-housing-prices'],,0.1297870838472963,AutoTrain,Not Specified,Not Specified,Not Specified,,100581.032,,,,,True,7,0,['joblib'],2022-10-15 20:52:10+00:00,2022-10-15 20:51:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1771761513
- CO2 Emissions (in grams): 0.1298

## Validation Metrics

- Loss: 100581.032
- R2: 0.922
- MSE: 10116543945.030
- MAE: 81586.656
- RMSLE: 0.101

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76517,autotrain-us-housing-prices-1771761514,['jwan2021/autotrain-data-us-housing-prices'],,0.1288210176412382,AutoTrain,Not Specified,Not Specified,Not Specified,,100595.98,,,,,True,5,0,['joblib'],2022-10-15 20:52:15+00:00,2022-10-15 20:51:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1771761514
- CO2 Emissions (in grams): 0.1288

## Validation Metrics

- Loss: 100595.980
- R2: 0.922
- MSE: 10119551129.473
- MAE: 81601.198
- RMSLE: 0.101

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76790,autotrain-in-class-test-1780161764,['pachi107/autotrain-data-in-class-test'],,3.162191628403084,AutoTrain,Not Specified,Not Specified,Not Specified,0.974,0.044,0.964,,,,True,3,1,['joblib'],2022-10-17 02:16:39+00:00,2022-10-17 02:12:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1780161764
- CO2 Emissions (in grams): 3.1622

## Validation Metrics

- Loss: 0.044
- Accuracy: 0.974
- Precision: 1.000
- Recall: 0.930
- AUC: 1.000
- F1: 0.964

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,0.9689742002063982,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76808,autotrain-17102022-cert-1781461794,['teacookies/autotrain-data-17102022-cert'],,16.43804270120875,AutoTrain,Not Specified,Not Specified,Not Specified,0.994,0.023,0.847,,,667179825.0,True,3,0,['pytorch'],2022-10-17 04:52:52+00:00,2022-10-17 04:43:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1781461794
- CO2 Emissions (in grams): 16.4380

## Validation Metrics

- Loss: 0.023
- Accuracy: 0.994
- Precision: 0.821
- Recall: 0.876
- F1: 0.847

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-17102022-cert-1781461794
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-17102022-cert-1781461794"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-17102022-cert-1781461794"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,40587546.6518249,0.9146311787072244,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76821,autotrain-17102022133-cert-1781761805,['teacookies/autotrain-data-17102022133-cert'],,20.262915394129607,AutoTrain,Not Specified,Not Specified,Not Specified,0.995,0.017,0.844,,,667179825.0,True,3,0,['pytorch'],2022-10-17 06:07:46+00:00,2022-10-17 05:54:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1781761805
- CO2 Emissions (in grams): 20.2629

## Validation Metrics

- Loss: 0.017
- Accuracy: 0.995
- Precision: 0.824
- Recall: 0.865
- F1: 0.844

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-17102022133-cert-1781761805
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-17102022133-cert-1781761805"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-17102022133-cert-1781761805"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,32926151.64317813,0.9133007069059272,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76827,autotrain-17101457-1200cut_rich_neg-1782461850,['teacookies/autotrain-data-17101457-1200cut_rich_neg'],,15.90515729014607,AutoTrain,Not Specified,Not Specified,Not Specified,0.994,0.022,0.769,,,667179825.0,True,3,0,['pytorch'],2022-10-17 07:16:47+00:00,2022-10-17 07:06:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1782461850
- CO2 Emissions (in grams): 15.9052

## Validation Metrics

- Loss: 0.022
- Accuracy: 0.994
- Precision: 0.736
- Recall: 0.804
- F1: 0.769

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-17101457-1200cut_rich_neg-1782461850
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-17101457-1200cut_rich_neg-1782461850"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-17101457-1200cut_rich_neg-1782461850"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,41947389.31713342,0.8671423709585933,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76838,autotrain-17102022_only_sceond_label_no_split-1783361880,['teacookies/autotrain-data-17102022_only_sceond_label_no_split'],,18.179473658039548,AutoTrain,Not Specified,Not Specified,Not Specified,0.997,0.013,0.854,,,667179825.0,True,3,0,['pytorch'],2022-10-17 08:20:42+00:00,2022-10-17 08:09:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1783361880
- CO2 Emissions (in grams): 18.1795

## Validation Metrics

- Loss: 0.013
- Accuracy: 0.997
- Precision: 0.834
- Recall: 0.875
- F1: 0.854

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-17102022_only_sceond_label_no_split-1783361880
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-17102022_only_sceond_label_no_split-1783361880"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-17102022_only_sceond_label_no_split-1783361880"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,36699622.74760093,0.91997622906537,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76848,autotrain-17102022_change_modlel-1783861900,['teacookies/autotrain-data-17102022_change_modlel'],,22.12649933027385,AutoTrain,Not Specified,Not Specified,Not Specified,0.994,0.025,0.871,,,1109929393.0,True,3,0,['pytorch'],2022-10-17 08:48:09+00:00,2022-10-17 08:34:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1783861900
- CO2 Emissions (in grams): 22.1265

## Validation Metrics

- Loss: 0.025
- Accuracy: 0.994
- Precision: 0.859
- Recall: 0.883
- F1: 0.871

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-17102022_change_modlel-1783861900
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-17102022_change_modlel-1783861900"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-17102022_change_modlel-1783861900"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,50162900.89238725,0.9284439678284184,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76849,autotrain-17102022_modifty_split_func_cert-1783761910,['teacookies/autotrain-data-17102022_modifty_split_func_cert'],,0.0796750250015584,AutoTrain,Not Specified,Not Specified,Not Specified,0.995,0.017,0.867,,,667179825.0,True,3,0,['pytorch'],2022-10-17 08:46:32+00:00,2022-10-17 08:35:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1783761910
- CO2 Emissions (in grams): 0.0797

## Validation Metrics

- Loss: 0.017
- Accuracy: 0.995
- Precision: 0.850
- Recall: 0.884
- F1: 0.867

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-17102022_modifty_split_func_cert-1783761910
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-17102022_modifty_split_func_cert-1783761910"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-17102022_modifty_split_func_cert-1783761910"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,8373763610.202197,0.9266004296455423,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76907,autotrain-17102022_relabel-1786061945,['teacookies/autotrain-data-17102022_relabel'],,16.970831166674337,AutoTrain,Not Specified,Not Specified,Not Specified,0.994,0.022,0.868,,,667179825.0,True,3,0,['pytorch'],2022-10-17 11:03:23+00:00,2022-10-17 10:52:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1786061945
- CO2 Emissions (in grams): 16.9708

## Validation Metrics

- Loss: 0.022
- Accuracy: 0.994
- Precision: 0.851
- Recall: 0.885
- F1: 0.868

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-17102022_relabel-1786061945
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-17102022_relabel-1786061945"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-17102022_relabel-1786061945"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,39313326.403844185,0.9267368421052632,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76932,autotrain-17102022-cert_update_date-1786462003,['teacookies/autotrain-data-17102022-cert_update_date'],,18.37074974959855,AutoTrain,Not Specified,Not Specified,Not Specified,0.995,0.019,0.851,,,667179825.0,True,3,0,['pytorch'],2022-10-17 12:34:15+00:00,2022-10-17 12:23:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1786462003
- CO2 Emissions (in grams): 18.3707

## Validation Metrics

- Loss: 0.019
- Accuracy: 0.995
- Precision: 0.835
- Recall: 0.867
- F1: 0.851

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-17102022-cert_update_date-1786462003
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-17102022-cert_update_date-1786462003"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-17102022-cert_update_date-1786462003"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,36317506.5848676,0.9173835319609968,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76950,autotrain-171022-update_label2-1788462049,['teacookies/autotrain-data-171022-update_label2'],,19.661735872263936,AutoTrain,Not Specified,Not Specified,Not Specified,0.991,0.031,0.783,,,667179825.0,True,3,0,['pytorch'],2022-10-17 13:47:28+00:00,2022-10-17 13:36:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1788462049
- CO2 Emissions (in grams): 19.6617

## Validation Metrics

- Loss: 0.031
- Accuracy: 0.991
- Precision: 0.755
- Recall: 0.812
- F1: 0.783

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-171022-update_label2-1788462049
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-171022-update_label2-1788462049"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-171022-update_label2-1788462049"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,33932905.48375056,0.8748060879368659,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76992,autotrain-ethos-sentiments-1790262079,['pachi107/autotrain-data-ethos-sentiments'],,0.8438685047317921,AutoTrain,Not Specified,Not Specified,Not Specified,0.755,0.513,0.751,,,556846831.0,True,4,0,['pytorch'],2022-10-17 16:30:34+00:00,2022-10-17 16:29:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1790262079
- CO2 Emissions (in grams): 0.8439

## Validation Metrics

- Loss: 0.513
- Accuracy: 0.755
- Precision: 0.881
- Recall: 0.655
- AUC: 0.857
- F1: 0.751

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pachi107/autotrain-ethos-sentiments-1790262079
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262079"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262079"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,659873935.1896815,0.7529946879150066,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76993,autotrain-ethos-sentiments-1790262080,['pachi107/autotrain-data-ethos-sentiments'],,1.1703390276575862,AutoTrain,Not Specified,Not Specified,Not Specified,0.83,0.469,0.848,,,737766955.0,True,4,0,['pytorch'],2022-10-17 16:30:55+00:00,2022-10-17 16:29:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1790262080
- CO2 Emissions (in grams): 1.1703

## Validation Metrics

- Loss: 0.469
- Accuracy: 0.830
- Precision: 0.856
- Recall: 0.841
- AUC: 0.898
- F1: 0.848

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pachi107/autotrain-ethos-sentiments-1790262080
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262080"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262080"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,630387381.4040263,0.8389034564958283,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76994,autotrain-ethos-sentiments-1790262081,['pachi107/autotrain-data-ethos-sentiments'],,1.14595289523453,AutoTrain,Not Specified,Not Specified,Not Specified,0.795,0.498,0.83,,,438006125.0,True,4,0,['pytorch'],2022-10-17 16:30:36+00:00,2022-10-17 16:29:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1790262081
- CO2 Emissions (in grams): 1.1460

## Validation Metrics

- Loss: 0.498
- Accuracy: 0.795
- Precision: 0.781
- Recall: 0.885
- AUC: 0.857
- F1: 0.830

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pachi107/autotrain-ethos-sentiments-1790262081
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262081"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262081"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,382220008.18834525,0.812123076923077,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76995,autotrain-ethos-sentiments-1790262082,['pachi107/autotrain-data-ethos-sentiments'],,0.8181506582658064,AutoTrain,Not Specified,Not Specified,Not Specified,0.775,0.565,0.807,,,433318253.0,True,4,0,['pytorch'],2022-10-17 16:30:43+00:00,2022-10-17 16:29:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1790262082
- CO2 Emissions (in grams): 0.8182

## Validation Metrics

- Loss: 0.565
- Accuracy: 0.775
- Precision: 0.783
- Recall: 0.832
- AUC: 0.823
- F1: 0.807

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pachi107/autotrain-ethos-sentiments-1790262082
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262082"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262082"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,529631368.77318335,0.790676359039191,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
76996,autotrain-ethos-sentiments-1790262083,['pachi107/autotrain-data-ethos-sentiments'],,2.331522912581982,AutoTrain,Not Specified,Not Specified,Not Specified,0.72,0.578,0.778,,,1334461229.0,True,4,0,['pytorch'],2022-10-17 16:31:41+00:00,2022-10-17 16:30:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1790262083
- CO2 Emissions (in grams): 2.3315

## Validation Metrics

- Loss: 0.578
- Accuracy: 0.720
- Precision: 0.705
- Recall: 0.867
- AUC: 0.769
- F1: 0.778

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pachi107/autotrain-ethos-sentiments-1790262083
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262083"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pachi107/autotrain-ethos-sentiments-1790262083"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,572356043.2533717,0.7478771695594125,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77099,autotrain-17102022-update_scope_and_date-1789062099,['teacookies/autotrain-data-17102022-update_scope_and_date'],,19.692537664708304,AutoTrain,Not Specified,Not Specified,Not Specified,0.992,0.029,0.801,,,667179825.0,True,3,0,['pytorch'],2022-10-18 01:53:54+00:00,2022-10-18 01:42:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1789062099
- CO2 Emissions (in grams): 19.6925

## Validation Metrics

- Loss: 0.029
- Accuracy: 0.992
- Precision: 0.777
- Recall: 0.826
- F1: 0.801

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-17102022-update_scope_and_date-1789062099
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-17102022-update_scope_and_date-1789062099"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-17102022-update_scope_and_date-1789062099"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,33879829.82993993,0.8863268265476854,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77158,autotrain-181022022-cert-1796662109,['teacookies/autotrain-data-181022022-cert'],,18.56487105177345,AutoTrain,Not Specified,Not Specified,Not Specified,0.991,0.029,0.79,,,667179825.0,True,3,0,['pytorch'],2022-10-18 06:27:08+00:00,2022-10-18 06:15:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1796662109
- CO2 Emissions (in grams): 18.5649

## Validation Metrics

- Loss: 0.029
- Accuracy: 0.991
- Precision: 0.767
- Recall: 0.813
- F1: 0.790

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-181022022-cert-1796662109
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-181022022-cert-1796662109"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-181022022-cert-1796662109"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,35937757.02181708,0.8791577765300393,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77164,autotrain-animals-1797562141,['micole66/autotrain-data-animals'],,0.6998538355363139,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.096,,,,347596479.0,True,2,0,['pytorch'],2022-10-18 06:38:49+00:00,2022-10-18 06:38:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1797562141
- CO2 Emissions (in grams): 0.6999

## Validation Metrics

- Loss: 0.096
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-10,496670106.4567703,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77170,autotrain-strano-o-normale-1798362191,['micole66/autotrain-data-strano-o-normale'],,0.6330824015396253,AutoTrain,Not Specified,Not Specified,Not Specified,0.75,0.645,0.667,,,439787885.0,True,4,0,['pytorch'],2022-10-18 07:08:01+00:00,2022-10-18 07:07:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1798362191
- CO2 Emissions (in grams): 0.6331

## Validation Metrics

- Loss: 0.645
- Accuracy: 0.750
- Precision: 1.000
- Recall: 0.500
- AUC: 0.625
- F1: 0.667

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/micole66/autotrain-strano-o-normale-1798362191
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""micole66/autotrain-strano-o-normale-1798362191"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""micole66/autotrain-strano-o-normale-1798362191"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,694677160.3988004,0.7060691601976006,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77176,autotrain-18102022_retoken-1799162225,['teacookies/autotrain-data-18102022_retoken'],,20.17997164723111,AutoTrain,Not Specified,Not Specified,Not Specified,0.993,0.024,0.86,,,667179825.0,True,3,0,['pytorch'],2022-10-18 08:01:54+00:00,2022-10-18 07:50:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1799162225
- CO2 Emissions (in grams): 20.1800

## Validation Metrics

- Loss: 0.024
- Accuracy: 0.993
- Precision: 0.829
- Recall: 0.893
- F1: 0.860

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-18102022_retoken-1799162225
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-18102022_retoken-1799162225"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-18102022_retoken-1799162225"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,33061484.75642401,0.9217269293038316,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77185,autotrain-pachyderm-1799762243,['micole66/autotrain-data-pachyderm'],,1.2406150246482144,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.463,1.0,,,1330262193.0,True,3,0,['pytorch'],2022-10-18 08:35:41+00:00,2022-10-18 08:34:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1799762243
- CO2 Emissions (in grams): 1.2406

## Validation Metrics

- Loss: 0.463
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/micole66/autotrain-pachyderm-1799762243
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""micole66/autotrain-pachyderm-1799762243"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""micole66/autotrain-pachyderm-1799762243"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,1072260263.3135172,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77240,autotrain-yash-1801862270,['Yaswantthhh/autotrain-data-yash'],,0.9492814628505252,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-10-18 12:13:55+00:00,,,,,1,[],[],Computer Vision,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77241,autotrain-yash-1801862271,['Yaswantthhh/autotrain-data-yash'],,1.51853168148008,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-10-18 12:14:43+00:00,,,,,1,[],[],Computer Vision,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77276,autotrain-sexy-or-ugly-1802962297,['micole66/autotrain-data-sexy-or-ugly'],,0.316594943692132,AutoTrain,Not Specified,Not Specified,Not Specified,0.8,0.616,0.5,,,265491317.0,True,3,0,['pytorch'],2022-10-18 15:59:45+00:00,2022-10-18 15:59:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1802962297
- CO2 Emissions (in grams): 0.3166

## Validation Metrics

- Loss: 0.616
- Accuracy: 0.800
- Precision: 0.429
- Recall: 0.600
- F1: 0.500

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/micole66/autotrain-sexy-or-ugly-1802962297
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""micole66/autotrain-sexy-or-ugly-1802962297"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""micole66/autotrain-sexy-or-ugly-1802962297"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,838583566.4456254,0.6153846153846154,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77282,autotrain-mercuryorsodium-1804662320,['micole66/autotrain-data-mercuryorsodium'],,0.3397575484174952,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.186,,,,110392879.0,True,2,0,['pytorch'],2022-10-18 16:32:30+00:00,2022-10-18 16:32:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1804662320
- CO2 Emissions (in grams): 0.3398

## Validation Metrics

- Loss: 0.186
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-10,324916633.97673464,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77462,autotrain-only-rssi-1813762559,['pcoloc/autotrain-data-only-rssi'],,1.3554114117578944,AutoTrain,Not Specified,Not Specified,Not Specified,,83.432,,,,,True,5,0,['joblib'],2022-10-19 08:57:26+00:00,2022-10-19 08:55:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1813762559
- CO2 Emissions (in grams): 1.3554

## Validation Metrics

- Loss: 83.432
- R2: 0.312
- MSE: 6960.888
- MAE: 60.449
- RMSLE: 0.532

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77495,autotrain-code_classification-1815762639,['kjhanjee/autotrain-data-code_classification'],,11.438220107218369,AutoTrain,Not Specified,Not Specified,Not Specified,0.794,0.849,0.788,,,556982127.0,True,4,0,['pytorch'],2022-10-19 11:01:40+00:00,2022-10-19 10:56:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1815762639
- CO2 Emissions (in grams): 11.4382

## Validation Metrics

- Loss: 0.849
- Accuracy: 0.794
- Macro F1: 0.788
- Micro F1: 0.794
- Weighted F1: 0.788
- Macro Precision: 0.797
- Micro Precision: 0.794
- Weighted Precision: 0.797
- Macro Recall: 0.794
- Micro Recall: 0.794
- Weighted Recall: 0.794


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/kjhanjee/autotrain-code_classification-1815762639
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kjhanjee/autotrain-code_classification-1815762639"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kjhanjee/autotrain-code_classification-1815762639"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,48694825.049616136,0.7909886219974716,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
77532,swin-muppet-faces,['Norod78/MuppetFaces'],,0.0115298552909659,AutoTrain,Not Specified,Not Specified,Not Specified,0.963,0.208,0.935,,,347686655.0,True,6,0,"['transformers', 'pytorch']",2022-10-19 15:05:33+00:00,2022-10-19 14:46:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1816962673
- CO2 Emissions (in grams): 0.0115

## Validation Metrics

- Loss: 0.208
- Accuracy: 0.963
- Macro F1: 0.935
- Micro F1: 0.963
- Weighted F1: 0.962
- Macro Precision: 0.945
- Micro Precision: 0.963
- Weighted Precision: 0.965
- Macro Recall: 0.933
- Micro Recall: 0.963
- Weighted Recall: 0.963",,,1,[],[],Computer Vision,2022-10,30155335537.68245,0.9487934668071656,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78082,autotrain-21102022-cert-1827562840,['teacookies/autotrain-data-21102022-cert'],,19.94429730071814,AutoTrain,Not Specified,Not Specified,Not Specified,0.992,0.028,0.851,,,667179825.0,True,3,0,['pytorch'],2022-10-21 07:41:52+00:00,2022-10-21 07:29:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1827562840
- CO2 Emissions (in grams): 19.9443

## Validation Metrics

- Loss: 0.028
- Accuracy: 0.992
- Precision: 0.820
- Recall: 0.885
- F1: 0.851

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-21102022-cert-1827562840
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-21102022-cert-1827562840"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-21102022-cert-1827562840"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,33452160.03052545,0.9161063483450894,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78104,autotrain-21102022_cert_check_date-1828162855,['teacookies/autotrain-data-21102022_cert_check_date'],,22.87049697186888,AutoTrain,Not Specified,Not Specified,Not Specified,0.994,0.021,0.89,,,667179825.0,True,3,0,['pytorch'],2022-10-21 08:43:12+00:00,2022-10-21 08:30:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1828162855
- CO2 Emissions (in grams): 22.8705

## Validation Metrics

- Loss: 0.021
- Accuracy: 0.994
- Precision: 0.867
- Recall: 0.914
- F1: 0.890

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-21102022_cert_check_date-1828162855
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-21102022_cert_check_date-1828162855"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-21102022_cert_check_date-1828162855"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,29172073.777873877,0.9391295116772824,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78416,autotrain-600-dragino-1839063122,['pcoloc/autotrain-data-600-dragino'],,0.127629627488599,AutoTrain,Not Specified,Not Specified,Not Specified,,93.595,,,,,True,3,0,['joblib'],2022-10-22 11:43:07+00:00,2022-10-22 11:42:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1839063122
- CO2 Emissions (in grams): 0.1276

## Validation Metrics

- Loss: 93.595
- R2: 0.502
- MSE: 8760.052
- MAE: 77.527
- RMSLE: 0.445

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78445,autotrain-person-intruder-classification-1840363138,['SergioVillanueva/autotrain-data-person-intruder-classification'],,0.5267790340228428,AutoTrain,Not Specified,Not Specified,Not Specified,0.818,0.464,,,,343266993.0,True,2,0,['pytorch'],2022-10-22 15:13:21+00:00,2022-10-22 15:12:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1840363138
- CO2 Emissions (in grams): 0.5268

## Validation Metrics

- Loss: 0.464
- Accuracy: 0.818
- Precision: 0.778
- Recall: 1.000
- AUC: 1.000
- F1: 0.875",,,1,[],[],Computer Vision,2022-10,651633741.7200907,0.818,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78521,autotrain-model1-binary-class-1843363194,['kem000123/autotrain-data-model1-binary-class'],,4.092983833698762,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.036,1.0,,,,True,4,0,['joblib'],2022-10-23 03:54:45+00:00,2022-10-23 03:49:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1843363194
- CO2 Emissions (in grams): 4.0930

## Validation Metrics

- Loss: 0.036
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78524,autotrain-model2-text-class-1843563203,['kem000123/autotrain-data-model2-text-class'],,3.652284357860415,AutoTrain,Not Specified,Not Specified,Not Specified,0.921,0.202,0.832,,,1334461229.0,True,4,1,['pytorch'],2022-10-23 04:16:06+00:00,2022-10-23 04:14:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1843563203
- CO2 Emissions (in grams): 3.6523

## Validation Metrics

- Loss: 0.202
- Accuracy: 0.921
- Precision: 0.803
- Recall: 0.862
- AUC: 0.966
- F1: 0.832

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/kem000123/autotrain-model2-text-class-1843563203
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kem000123/autotrain-model2-text-class-1843563203"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kem000123/autotrain-model2-text-class-1843563203"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,365377144.34200174,0.8742407301768397,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78550,m2m100_418M_br_fr,[''],,2100.0,MLCO2,fine-tuning,"Paris, France",2 NVidia GeForce RTX 3090 GPUs,,,,,,1944194627.0,False,20,0,"['transformers', 'pytorch']",2023-03-22 17:43:18+00:00,2022-10-23 09:41:21+00:00,"
Breton-French translator `m2m100_418M_br_fr`
============================================

This model is a fine-tuned version of
[facebook/m2m100_418M](https://huggingface.co/facebook/m2m100_418M) (Fan et al., 2021) on a
Breton-French parallel corpus. In order to obtain the best possible results, we use all our parallel
data on training and consequently report no quantitative evaluation at this time. Empirical
qualitative evidence suggests that the translations are generally adequate for short and simple
examples, the behaviour of the model on long and/or complex inputs is currently unknown.

Try this model online in [Troer](https://huggingface.co/spaces/lgrobol/troer), feedback and
suggestions are welcome!

## Model description

See the description of the [base model](https://huggingface.co/facebook/m2m100_418M).

## Intended uses & limitations

This is intended as a **demonstration** of the improvements brought by fine-tuning a large-scale
many-to-many translation system on a medium-sized dataset of high-quality data. As it is, and as far
as I can tell it usually provides translations that are least as good as those of other available
Breton-French translators, but it has not been evaluated quantitatively at a large scale.

## Training and evaluation data

The training dataset consists of:

- The [OfisPublik corpus v1](https://opus.nlpl.eu/OfisPublik-v1.php) (Tyers, 2009)
- The [Tatoeba corpus v2022-03-03](https://opus.nlpl.eu/Tatoeba-v2022-03-03.php)
- Part of the [OpenSubtitles corpus v2018](https://opus.nlpl.eu/OpenSubtitles-v2018.php)

These are obtained from the [OPUS](https://opus.nlpl.eu/) base (Tiedemann, 2012) and filtered using
[OpusFilter](https://helsinki-nlp.github.io/OpusFilter) (Aulamo et al., 2020), see
[`dl_opus.yaml`](dl_opus.yaml) for the details. The filtering is slightly non-deterministic due to
the retraining of a statistical alignment model, but in my experience, different runs tend to give
extremely similar results. Do not hesitate to reach out if you experience difficulties in using this
to collect data.

In addition to these, the training dataset also includes parallel br/fr sentences, provided as
glosses in the [Arbres](https://arbres.iker.cnrs.fr) wiki (Jouitteau, 2022), obtained from their
[ongoing port](https://github.com/Autogramm/Breton/commit/45ac2c444a979b7ee41e5f24a3bfd1ec39f09d7d)
to Universal Dependencies in the Autogramm project.

## Training procedure

The training hyperparameters are those suggested by Adelani et al. (2022) in their [code
release](https://github.com/masakhane-io/lafand-mt), which gave their best results for machine
translation of several African languages.

More specifically, we train this model with [zeldarose](https://github.com/LoicGrobol/zeldarose) with the following parameters

```bash
zeldarose transformer \
   --config train_config.toml \
   --tokenizer ""facebook/m2m100_418M"" --pretrained-model ""facebook/m2m100_418M"" \
   --out-dir m2m100_418M+br-fr --model-name m2m100_418M+br-fr \
   --strategy ddp --accelerator gpu --num-devices 4 --device-batch-size 2 --num-workers 8\
   --max-epochs 16 --precision 16 --tf32-mode medium \
   --val-data {val_path}.jsonl \
   {train_path}.jsonl

```

### Training hyperparameters

The following hyperparameters were used during training:

```toml
[task]
change_ratio = 0.3
denoise_langs = []
poisson_lambda = 3.0
source_langs = [""br""]
target_langs = [""fr""]

[tuning]
batch_size = 16
betas = [0.9, 0.999]
epsilon = 1e-8
learning_rate = 5e-5
gradient_clipping = 1.0
lr_decay_steps = -1
warmup_steps = 1024
```

### Framework versions

- Transformers 4.26.1
- Pytorch 1.12.1
- Datasets 2.10.0
- Tokenizers 0.13.2
- Pytorch-lightning 1.9.3
- Zeldarose [c6456ead](https://github.com/LoicGrobol/spertiniite/commit/c6456ead3649c4e6ddfb4a5a74b40f344eded09f)

### Carbon emissions

At this time, we estimate emissions of a rough 300 gCO<sub>2</sub> per fine-tuning run. So far, we
account for

- Fine-tuning the 3 released versions
- 8 development runs

So far, the equivalent carbon emissions for this model are approximately 3300 gCO<sub>2</sub>.

## References

- Adelani, David, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,
  et al. 2022. “A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African
  News Translation”. In Proceedings of the 2022 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies, 3053‑70. Seattle, United
  States: Association for Computational Linguistics.
  <https://doi.org/10.18653/v1/2022.naacl-main.223>.
- Mikko Aulamo, Sami Virpioja, and Jörg Tiedemann. 2020. OpusFilter: A Configurable Parallel Corpus
  Filtering Toolbox. In Proceedings of the 58th Annual Meeting of the Association for Computational
  Linguistics: System Demonstrations, pages 150–156, Online. Association for Computational
  Linguistics.
- Fan, Angela, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep
  Baines, et al. 2021. “Beyond english-centric multilingual machine translation”. The Journal of
  Machine Learning Research 22 (1): 107:4839-107:4886.
- Tiedemann, Jorg 2012, “Parallel Data, Tools and Interfaces in OPUS”. In Proceedings of the 8th
  International Conference on Language Resources and Evaluation (LREC 2012)
- Jouitteau, Mélanie. (éd.). 2009-2022. ARBRES, wikigrammaire des dialectes du breton et centre de
  ressources pour son étude linguistique formelle, IKER, CNRS, <http://arbres.iker.cnrs.fr>.
- Tyers, Francis M. 2009 “Rule-based augmentation of training data in Breton-French statistical
  machine translation”. In Proceedings of the 13th Annual Conference of the European Association of
  Machine Translation, EAMT09. Barcelona, España. 213--218
",,,1,[],[],NLP,2022-10,925806.9652380951,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78554,autotrain-231022022-cert4-1847463269,['teacookies/autotrain-data-231022022-cert4'],,17.781243387408683,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.004,0.962,,,667179825.0,True,2,0,['pytorch'],2022-10-23 10:35:22+00:00,2022-10-23 10:24:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1847463269
- CO2 Emissions (in grams): 17.7812

## Validation Metrics

- Loss: 0.004
- Accuracy: 0.999
- Precision: 0.955
- Recall: 0.969
- F1: 0.962

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-231022022-cert4-1847463269
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-231022022-cert4-1847463269"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-231022022-cert4-1847463269"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,37521550.68483263,0.9801509433962262,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78708,autotrain-24102022_cert-1855763468,['teacookies/autotrain-data-24102022_cert'],,15.043841231531312,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.005,0.947,,,667179825.0,True,2,0,['pytorch'],2022-10-24 03:34:08+00:00,2022-10-24 03:24:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1855763468
- CO2 Emissions (in grams): 15.0438

## Validation Metrics

- Loss: 0.005
- Accuracy: 0.999
- Precision: 0.942
- Recall: 0.952
- F1: 0.947

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-24102022_cert-1855763468
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-24102022_cert-1855763468"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-24102022_cert-1855763468"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,44349033.91572737,0.9723052415210688,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78712,autotrain-24102022-cert2-1856563478,['teacookies/autotrain-data-24102022-cert2'],,16.894326665784842,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.004,0.968,,,667179825.0,True,2,0,['pytorch'],2022-10-24 04:33:47+00:00,2022-10-24 04:22:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1856563478
- CO2 Emissions (in grams): 16.8943

## Validation Metrics

- Loss: 0.004
- Accuracy: 0.999
- Precision: 0.961
- Recall: 0.974
- F1: 0.968

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-24102022-cert2-1856563478
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-24102022-cert2-1856563478"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-24102022-cert2-1856563478"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,39491353.41103608,0.9832557193695982,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78722,autotrain-cat_vs_dogs-1858163503,['kem000123/autotrain-data-cat_vs_dogs'],,0.7950743476524714,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.007,,,,110392879.0,True,3,1,['pytorch'],2022-10-24 05:44:23+00:00,2022-10-24 05:43:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1858163503
- CO2 Emissions (in grams): 0.7951

## Validation Metrics

- Loss: 0.007
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-10,138845982.54986456,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78723,autotrain-24102022-cert4-1858363508,['teacookies/autotrain-data-24102022-cert4'],,19.82493725454133,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.003,0.967,,,667179825.0,True,2,0,['pytorch'],2022-10-24 06:11:13+00:00,2022-10-24 05:59:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1858363508
- CO2 Emissions (in grams): 19.8249

## Validation Metrics

- Loss: 0.003
- Accuracy: 0.999
- Precision: 0.963
- Recall: 0.971
- F1: 0.967

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-24102022-cert4-1858363508
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-24102022-cert4-1858363508"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-24102022-cert4-1858363508"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,33653565.528797224,0.9827395727365208,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78738,autotrain-24102022-cert5-1858763528,['teacookies/autotrain-data-24102022-cert5'],,15.97111881210848,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.003,0.966,,,667179825.0,True,2,0,['pytorch'],2022-10-24 08:02:36+00:00,2022-10-24 07:53:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1858763528
- CO2 Emissions (in grams): 15.9711

## Validation Metrics

- Loss: 0.003
- Accuracy: 0.999
- Precision: 0.961
- Recall: 0.970
- F1: 0.966

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-24102022-cert5-1858763528
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-24102022-cert5-1858763528"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-24102022-cert5-1858763528"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,41774144.49476005,0.9822229007633588,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78744,autotrain-abunawaf-cognition-1859363548,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition'],,0.9315924025671088,AutoTrain,Not Specified,Not Specified,Not Specified,0.837,0.392,0.81,,,438006125.0,True,4,0,['pytorch'],2022-10-24 08:45:39+00:00,2022-10-24 08:44:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859363548
- CO2 Emissions (in grams): 0.9316

## Validation Metrics

- Loss: 0.392
- Accuracy: 0.837
- Precision: 0.787
- Recall: 0.833
- AUC: 0.900
- F1: 0.810

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363548
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363548"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363548"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,470169275.5254597,0.8232786885245902,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78745,autotrain-abunawaf-cognition-1859363549,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition'],,1.0566666951225436,AutoTrain,Not Specified,Not Specified,Not Specified,0.854,0.385,0.832,,,438006125.0,True,5,0,['pytorch'],2022-10-24 08:45:44+00:00,2022-10-24 08:44:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859363549
- CO2 Emissions (in grams): 1.0567

## Validation Metrics

- Loss: 0.385
- Accuracy: 0.854
- Precision: 0.795
- Recall: 0.873
- AUC: 0.900
- F1: 0.832

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363549
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363549"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363549"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,414516826.3765554,0.8428564650059311,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78746,autotrain-abunawaf-cognition-1859363550,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition'],,1.173820365058826,AutoTrain,Not Specified,Not Specified,Not Specified,0.846,0.369,0.817,,,438006125.0,True,5,0,['pytorch'],2022-10-24 08:45:55+00:00,2022-10-24 08:45:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859363550
- CO2 Emissions (in grams): 1.1738

## Validation Metrics

- Loss: 0.369
- Accuracy: 0.846
- Precision: 0.802
- Recall: 0.833
- AUC: 0.901
- F1: 0.817

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363550
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363550"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363550"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,373145787.9230519,0.8312471437161757,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78747,autotrain-abunawaf-cognition-1859363551,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition'],,1.7828199447393138,AutoTrain,Not Specified,Not Specified,Not Specified,0.858,0.372,0.837,,,438006125.0,True,5,0,['pytorch'],2022-10-24 08:46:21+00:00,2022-10-24 08:45:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859363551
- CO2 Emissions (in grams): 1.7828

## Validation Metrics

- Loss: 0.372
- Accuracy: 0.858
- Precision: 0.796
- Recall: 0.882
- AUC: 0.919
- F1: 0.837

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363551
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363551"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363551"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,245681638.4023827,0.8473699115044246,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78748,autotrain-abunawaf-cognition-1859363552,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition'],,1.1831906042914635,AutoTrain,Not Specified,Not Specified,Not Specified,0.854,0.369,0.827,,,438006125.0,True,4,0,['pytorch'],2022-10-24 08:46:12+00:00,2022-10-24 08:45:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859363552
- CO2 Emissions (in grams): 1.1832

## Validation Metrics

- Loss: 0.369
- Accuracy: 0.854
- Precision: 0.811
- Recall: 0.843
- AUC: 0.912
- F1: 0.827

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363552
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363552"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-1859363552"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,370190672.0788183,0.8402831647828674,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78751,autotrain-abunawaf-cognition-auto-1859563553,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition-auto'],,1.7868012751172693,AutoTrain,Not Specified,Not Specified,Not Specified,0.854,0.382,0.827,,,556846831.0,True,4,0,['pytorch'],2022-10-24 08:55:46+00:00,2022-10-24 08:54:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859563553
- CO2 Emissions (in grams): 1.7868

## Validation Metrics

- Loss: 0.382
- Accuracy: 0.854
- Precision: 0.811
- Recall: 0.843
- AUC: 0.915
- F1: 0.827

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563553
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563553"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563553"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,311644522.9553878,0.8402831647828674,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78752,autotrain-abunawaf-cognition-auto-1859563554,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition-auto'],,1.1747519267416993,AutoTrain,Not Specified,Not Specified,Not Specified,0.813,0.455,0.798,,,737766955.0,True,4,0,['pytorch'],2022-10-24 08:55:55+00:00,2022-10-24 08:54:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859563554
- CO2 Emissions (in grams): 1.1748

## Validation Metrics

- Loss: 0.455
- Accuracy: 0.813
- Precision: 0.722
- Recall: 0.892
- AUC: 0.872
- F1: 0.798

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563554
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563554"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563554"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,628019361.5398239,0.8054301675977654,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78753,autotrain-abunawaf-cognition-auto-1859563555,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition-auto'],,1.0328202141613765,AutoTrain,Not Specified,Not Specified,Not Specified,0.866,0.379,0.834,,,438006125.0,True,5,0,['pytorch'],2022-10-24 08:55:24+00:00,2022-10-24 08:54:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859563555
- CO2 Emissions (in grams): 1.0328

## Validation Metrics

- Loss: 0.379
- Accuracy: 0.866
- Precision: 0.856
- Recall: 0.814
- AUC: 0.905
- F1: 0.834

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563555
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563555"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563555"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,424087482.9852645,0.8496988235294118,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78754,autotrain-abunawaf-cognition-auto-1859563556,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition-auto'],,0.7623869342782728,AutoTrain,Not Specified,Not Specified,Not Specified,0.829,0.404,0.809,,,433318253.0,True,4,0,['pytorch'],2022-10-24 08:55:30+00:00,2022-10-24 08:54:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859563556
- CO2 Emissions (in grams): 0.7624

## Validation Metrics

- Loss: 0.404
- Accuracy: 0.829
- Precision: 0.754
- Recall: 0.873
- AUC: 0.901
- F1: 0.809

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563556
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563556"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563556"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,568370513.0783864,0.8188778998778999,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78755,autotrain-abunawaf-cognition-auto-1859563557,['Ahmed-Abousetta/autotrain-data-abunawaf-cognition-auto'],,3.157823361506444,AutoTrain,Not Specified,Not Specified,Not Specified,0.833,0.42,0.802,,,1334461229.0,True,5,0,['pytorch'],2022-10-24 08:57:03+00:00,2022-10-24 08:55:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859563557
- CO2 Emissions (in grams): 3.1578

## Validation Metrics

- Loss: 0.420
- Accuracy: 0.833
- Precision: 0.790
- Recall: 0.814
- AUC: 0.894
- F1: 0.802

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563557
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563557"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-cognition-auto-1859563557"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,422588940.6187031,0.817206116207951,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78756,autotrain-abunawaf-information-1859863558,['Ahmed-Abousetta/autotrain-data-abunawaf-information'],,0.7147232414393694,AutoTrain,Not Specified,Not Specified,Not Specified,0.865,0.354,0.851,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:01:38+00:00,2022-10-24 09:00:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859863558
- CO2 Emissions (in grams): 0.7147

## Validation Metrics

- Loss: 0.354
- Accuracy: 0.865
- Precision: 0.817
- Recall: 0.887
- AUC: 0.931
- F1: 0.851

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-information-1859863558
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863558"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863558"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,612833191.3733582,0.8579428904428903,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78757,autotrain-abunawaf-information-1859863559,['Ahmed-Abousetta/autotrain-data-abunawaf-information'],,0.6822182565490778,AutoTrain,Not Specified,Not Specified,Not Specified,0.853,0.353,0.824,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:01:44+00:00,2022-10-24 09:00:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859863559
- CO2 Emissions (in grams): 0.6822

## Validation Metrics

- Loss: 0.353
- Accuracy: 0.853
- Precision: 0.857
- Recall: 0.792
- AUC: 0.931
- F1: 0.824

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-information-1859863559
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863559"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863559"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,642032254.040816,0.8382492546213477,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78758,autotrain-abunawaf-information-1859863560,['Ahmed-Abousetta/autotrain-data-abunawaf-information'],,1.8754846173690545,AutoTrain,Not Specified,Not Specified,Not Specified,0.878,0.331,0.86,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:01:53+00:00,2022-10-24 09:00:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859863560
- CO2 Emissions (in grams): 1.8755

## Validation Metrics

- Loss: 0.331
- Accuracy: 0.878
- Precision: 0.852
- Recall: 0.868
- AUC: 0.927
- F1: 0.860

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-information-1859863560
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863560"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863560"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,233542904.5610828,0.8689067894131185,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78759,autotrain-abunawaf-information-1859863561,['Ahmed-Abousetta/autotrain-data-abunawaf-information'],,1.588438196368296,AutoTrain,Not Specified,Not Specified,Not Specified,0.869,0.338,0.852,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:02:06+00:00,2022-10-24 09:01:02+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859863561
- CO2 Emissions (in grams): 1.5884

## Validation Metrics

- Loss: 0.338
- Accuracy: 0.869
- Precision: 0.836
- Recall: 0.868
- AUC: 0.932
- F1: 0.852

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-information-1859863561
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863561"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863561"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,275746406.75440145,0.8604160371876814,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78760,autotrain-abunawaf-information-1859863562,['Ahmed-Abousetta/autotrain-data-abunawaf-information'],,1.5985216080073748,AutoTrain,Not Specified,Not Specified,Not Specified,0.857,0.375,0.836,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:02:01+00:00,2022-10-24 09:01:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859863562
- CO2 Emissions (in grams): 1.5985

## Validation Metrics

- Loss: 0.375
- Accuracy: 0.857
- Precision: 0.832
- Recall: 0.840
- AUC: 0.912
- F1: 0.836

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-information-1859863562
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863562"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-information-1859863562"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,274007009.23023075,0.8463697578263437,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78761,autotrain-abunawaf-interaction-1859963563,['Ahmed-Abousetta/autotrain-data-abunawaf-interaction'],,0.7644156643824811,AutoTrain,Not Specified,Not Specified,Not Specified,0.91,0.244,0.935,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:05:53+00:00,2022-10-24 09:05:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859963563
- CO2 Emissions (in grams): 0.7644

## Validation Metrics

- Loss: 0.244
- Accuracy: 0.910
- Precision: 0.935
- Recall: 0.935
- AUC: 0.954
- F1: 0.935

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963563
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963563"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963563"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,572994700.9312466,0.9223306233062332,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78762,autotrain-abunawaf-interaction-1859963564,['Ahmed-Abousetta/autotrain-data-abunawaf-interaction'],,0.8413403809338463,AutoTrain,Not Specified,Not Specified,Not Specified,0.902,0.268,0.931,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:05:56+00:00,2022-10-24 09:05:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859963564
- CO2 Emissions (in grams): 0.8413

## Validation Metrics

- Loss: 0.268
- Accuracy: 0.902
- Precision: 0.905
- Recall: 0.959
- AUC: 0.954
- F1: 0.931

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963564
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963564"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963564"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,520605137.8561372,0.9162705946535734,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78763,autotrain-abunawaf-interaction-1859963565,['Ahmed-Abousetta/autotrain-data-abunawaf-interaction'],,0.6502317465394943,AutoTrain,Not Specified,Not Specified,Not Specified,0.922,0.241,0.944,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:06:06+00:00,2022-10-24 09:05:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859963565
- CO2 Emissions (in grams): 0.6502

## Validation Metrics

- Loss: 0.241
- Accuracy: 0.922
- Precision: 0.936
- Recall: 0.953
- AUC: 0.951
- F1: 0.944

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963565
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963565"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963565"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,673615410.7071055,0.9328703108252948,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78764,autotrain-abunawaf-interaction-1859963566,['Ahmed-Abousetta/autotrain-data-abunawaf-interaction'],,0.8147216061910044,AutoTrain,Not Specified,Not Specified,Not Specified,0.906,0.257,0.933,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:06:18+00:00,2022-10-24 09:05:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859963566
- CO2 Emissions (in grams): 0.8147

## Validation Metrics

- Loss: 0.257
- Accuracy: 0.906
- Precision: 0.924
- Recall: 0.941
- AUC: 0.949
- F1: 0.933

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963566
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963566"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963566"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,537614470.6015238,0.9193017944535076,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78765,autotrain-abunawaf-interaction-1859963567,['Ahmed-Abousetta/autotrain-data-abunawaf-interaction'],,1.0555869183889894,AutoTrain,Not Specified,Not Specified,Not Specified,0.91,0.263,0.934,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:06:18+00:00,2022-10-24 09:05:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1859963567
- CO2 Emissions (in grams): 1.0556

## Validation Metrics

- Loss: 0.263
- Accuracy: 0.910
- Precision: 0.945
- Recall: 0.923
- AUC: 0.945
- F1: 0.934

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963567
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963567"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-interaction-1859963567"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,414940842.2647697,0.9218438177874188,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78766,autotrain-abunawaf-performance-1860063568,['Ahmed-Abousetta/autotrain-data-abunawaf-performance'],,1.0292657249217083,AutoTrain,Not Specified,Not Specified,Not Specified,0.812,0.453,0.8,,,438006125.0,True,4,0,['pytorch'],2022-10-24 09:09:44+00:00,2022-10-24 09:09:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860063568
- CO2 Emissions (in grams): 1.0293

## Validation Metrics

- Loss: 0.453
- Accuracy: 0.812
- Precision: 0.836
- Recall: 0.767
- AUC: 0.860
- F1: 0.800

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-performance-1860063568
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063568"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063568"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,425552036.169588,0.8059553349875932,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78767,autotrain-abunawaf-performance-1860063569,['Ahmed-Abousetta/autotrain-data-abunawaf-performance'],,0.6232110285492835,AutoTrain,Not Specified,Not Specified,Not Specified,0.841,0.43,0.835,,,438006125.0,True,4,0,['pytorch'],2022-10-24 09:09:47+00:00,2022-10-24 09:09:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860063569
- CO2 Emissions (in grams): 0.6232

## Validation Metrics

- Loss: 0.430
- Accuracy: 0.841
- Precision: 0.846
- Recall: 0.825
- AUC: 0.873
- F1: 0.835

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-performance-1860063569
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063569"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063569"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,702821524.2268655,0.837989260143198,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78768,autotrain-abunawaf-performance-1860063570,['Ahmed-Abousetta/autotrain-data-abunawaf-performance'],,0.9744207053095344,AutoTrain,Not Specified,Not Specified,Not Specified,0.824,0.435,0.812,,,438006125.0,True,4,0,['pytorch'],2022-10-24 09:09:57+00:00,2022-10-24 09:09:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860063570
- CO2 Emissions (in grams): 0.9744

## Validation Metrics

- Loss: 0.435
- Accuracy: 0.824
- Precision: 0.853
- Recall: 0.775
- AUC: 0.885
- F1: 0.812

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-performance-1860063570
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063570"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063570"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,449504123.438,0.817955990220049,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78769,autotrain-abunawaf-performance-1860063571,['Ahmed-Abousetta/autotrain-data-abunawaf-performance'],,0.6594479502465727,AutoTrain,Not Specified,Not Specified,Not Specified,0.824,0.447,0.815,,,438006125.0,True,4,0,['pytorch'],2022-10-24 09:10:04+00:00,2022-10-24 09:09:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860063571
- CO2 Emissions (in grams): 0.6594

## Validation Metrics

- Loss: 0.447
- Accuracy: 0.824
- Precision: 0.841
- Recall: 0.792
- AUC: 0.886
- F1: 0.815

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-performance-1860063571
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063571"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063571"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,664201207.7469134,0.8194752898108603,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78770,autotrain-abunawaf-performance-1860063572,['Ahmed-Abousetta/autotrain-data-abunawaf-performance'],,0.8429873610442068,AutoTrain,Not Specified,Not Specified,Not Specified,0.788,0.462,0.792,,,438006125.0,True,5,0,['pytorch'],2022-10-24 09:10:04+00:00,2022-10-24 09:09:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860063572
- CO2 Emissions (in grams): 0.8430

## Validation Metrics

- Loss: 0.462
- Accuracy: 0.788
- Precision: 0.762
- Recall: 0.825
- AUC: 0.881
- F1: 0.792

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-performance-1860063572
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063572"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-performance-1860063572"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,519588009.54909056,0.7899949367088608,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78771,autotrain-24102022-cert6-1859663573,['teacookies/autotrain-data-24102022-cert6'],,19.238000251078866,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.002,0.969,,,667179825.0,True,2,0,['pytorch'],2022-10-24 09:23:05+00:00,2022-10-24 09:11:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1859663573
- CO2 Emissions (in grams): 19.2380

## Validation Metrics

- Loss: 0.002
- Accuracy: 0.999
- Precision: 0.964
- Recall: 0.974
- F1: 0.969

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-24102022-cert6-1859663573
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-24102022-cert6-1859663573"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-24102022-cert6-1859663573"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,34680310.65040581,0.9837713414634146,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78772,autotrain-abunawaf-user-1860163583,['Ahmed-Abousetta/autotrain-data-abunawaf-user'],,0.6436453501778651,AutoTrain,Not Specified,Not Specified,Not Specified,0.869,0.344,0.652,,,438006125.0,True,4,0,['pytorch'],2022-10-24 09:13:09+00:00,2022-10-24 09:12:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860163583
- CO2 Emissions (in grams): 0.6436

## Validation Metrics

- Loss: 0.344
- Accuracy: 0.869
- Precision: 0.698
- Recall: 0.612
- AUC: 0.856
- F1: 0.652

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-user-1860163583
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163583"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163583"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,680508489.463897,0.7450203813280737,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78773,autotrain-abunawaf-user-1860163584,['Ahmed-Abousetta/autotrain-data-abunawaf-user'],,1.0492185026433665,AutoTrain,Not Specified,Not Specified,Not Specified,0.89,0.34,0.733,,,438006125.0,True,4,0,['pytorch'],2022-10-24 09:13:15+00:00,2022-10-24 09:12:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860163584
- CO2 Emissions (in grams): 1.0492

## Validation Metrics

- Loss: 0.340
- Accuracy: 0.890
- Precision: 0.712
- Recall: 0.755
- AUC: 0.901
- F1: 0.733

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-user-1860163584
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163584"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163584"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,417459398.4918316,0.803906346272335,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78774,autotrain-abunawaf-user-1860163585,['Ahmed-Abousetta/autotrain-data-abunawaf-user'],,1.0008458491802985,AutoTrain,Not Specified,Not Specified,Not Specified,0.89,0.304,0.722,,,438006125.0,True,4,0,['pytorch'],2022-10-24 09:13:23+00:00,2022-10-24 09:12:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860163585
- CO2 Emissions (in grams): 1.0008

## Validation Metrics

- Loss: 0.304
- Accuracy: 0.890
- Precision: 0.729
- Recall: 0.714
- AUC: 0.889
- F1: 0.722

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-user-1860163585
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163585"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163585"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,437635950.9895863,0.7972456575682382,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78775,autotrain-abunawaf-user-1860163586,['Ahmed-Abousetta/autotrain-data-abunawaf-user'],,1.2062625201613788,AutoTrain,Not Specified,Not Specified,Not Specified,0.89,0.312,0.727,,,438006125.0,True,4,0,['pytorch'],2022-10-24 09:13:41+00:00,2022-10-24 09:12:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860163586
- CO2 Emissions (in grams): 1.2063

## Validation Metrics

- Loss: 0.312
- Accuracy: 0.890
- Precision: 0.720
- Recall: 0.735
- AUC: 0.883
- F1: 0.727

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-user-1860163586
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163586"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163586"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,363110117.1421638,0.8002844774273346,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78776,autotrain-abunawaf-user-1860163587,['Ahmed-Abousetta/autotrain-data-abunawaf-user'],,1.58506390711431,AutoTrain,Not Specified,Not Specified,Not Specified,0.878,0.339,0.688,,,438006125.0,True,4,0,['pytorch'],2022-10-24 09:13:41+00:00,2022-10-24 09:12:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1860163587
- CO2 Emissions (in grams): 1.5851

## Validation Metrics

- Loss: 0.339
- Accuracy: 0.878
- Precision: 0.702
- Recall: 0.673
- AUC: 0.852
- F1: 0.688

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Ahmed-Abousetta/autotrain-abunawaf-user-1860163587
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163587"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Ahmed-Abousetta/autotrain-abunawaf-user-1860163587"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,276333416.61120313,0.7714738186462324,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78792,autotrain-mikrotik-7-7-1860563588,['pcoloc/autotrain-data-mikrotik-7-7'],,1.538605928853103,AutoTrain,Not Specified,Not Specified,Not Specified,,48.213,,,,,True,3,0,['joblib'],2022-10-24 09:58:31+00:00,2022-10-24 09:56:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1860563588
- CO2 Emissions (in grams): 1.5386

## Validation Metrics

- Loss: 48.213
- R2: 0.654
- MSE: 2324.518
- MAE: 32.634
- RMSLE: 0.586

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78793,autotrain-mikrotik-7-7-1860563590,['pcoloc/autotrain-data-mikrotik-7-7'],,7.101169339115312,AutoTrain,Not Specified,Not Specified,Not Specified,,52.881,,,,,True,3,0,['joblib'],2022-10-24 10:15:30+00:00,2022-10-24 09:56:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1860563590
- CO2 Emissions (in grams): 7.1012

## Validation Metrics

- Loss: 52.881
- R2: 0.584
- MSE: 2796.357
- MAE: 37.116
- RMSLE: 0.518

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78794,autotrain-mikrotik-7-7-1860563597,['pcoloc/autotrain-data-mikrotik-7-7'],,1.3260818006120507,AutoTrain,Not Specified,Not Specified,Not Specified,,49.757,,,,,True,3,0,['joblib'],2022-10-24 09:59:02+00:00,2022-10-24 09:57:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1860563597
- CO2 Emissions (in grams): 1.3261

## Validation Metrics

- Loss: 49.757
- R2: 0.632
- MSE: 2475.747
- MAE: 33.327
- RMSLE: 0.587

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78796,autotrain-dragino-7-7-1860763606,['pcoloc/autotrain-data-dragino-7-7'],,0.000588925557747,AutoTrain,Not Specified,Not Specified,Not Specified,,84.433,,,,,True,3,0,['joblib'],2022-10-24 10:03:18+00:00,2022-10-24 10:02:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1860763606
- CO2 Emissions (in grams): 0.0006

## Validation Metrics

- Loss: 84.433
- R2: 0.540
- MSE: 7129.004
- MAE: 62.626
- RMSLE: 0.418

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78797,autotrain-24102022-cert7-1860363608,['teacookies/autotrain-data-24102022-cert7'],,0.0825722192587215,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.002,0.978,,,667179825.0,True,2,0,['pytorch'],2022-10-24 10:14:31+00:00,2022-10-24 10:03:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1860363608
- CO2 Emissions (in grams): 0.0826

## Validation Metrics

- Loss: 0.002
- Accuracy: 0.999
- Precision: 0.972
- Recall: 0.983
- F1: 0.978

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-24102022-cert7-1860363608
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-24102022-cert7-1860363608"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-24102022-cert7-1860363608"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,8079955110.683678,0.9883884673748102,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78798,autotrain-dragino-7-7-max_495m-1860863627,['pcoloc/autotrain-data-dragino-7-7-max_495m'],,0.0112423262668447,AutoTrain,Not Specified,Not Specified,Not Specified,,72.73,,,,,True,3,0,['joblib'],2022-10-24 10:14:36+00:00,2022-10-24 10:11:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1860863627
- CO2 Emissions (in grams): 0.0112

## Validation Metrics

- Loss: 72.730
- R2: 0.386
- MSE: 5289.600
- MAE: 60.230
- RMSLE: 0.436

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78800,autotrain-dragino-7-7-max_300m-1861063640,['pcoloc/autotrain-data-dragino-7-7-max_300m'],,0.128606860489453,AutoTrain,Not Specified,Not Specified,Not Specified,,50.918,,,,,True,3,0,['joblib'],2022-10-24 10:21:21+00:00,2022-10-24 10:20:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1861063640
- CO2 Emissions (in grams): 0.1286

## Validation Metrics

- Loss: 50.918
- R2: 0.304
- MSE: 2592.667
- MAE: 39.693
- RMSLE: 0.429

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
78832,autotrain-24102022-cert9-1861563662,['teacookies/autotrain-data-24102022-cert9'],,18.678658475473995,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.004,0.964,,,667179825.0,True,2,0,['pytorch'],2022-10-24 11:10:59+00:00,2022-10-24 11:00:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1861563662
- CO2 Emissions (in grams): 18.6787

## Validation Metrics

- Loss: 0.004
- Accuracy: 0.999
- Precision: 0.959
- Recall: 0.969
- F1: 0.964

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-24102022-cert9-1861563662
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-24102022-cert9-1861563662"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-24102022-cert9-1861563662"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,35718829.9082635,0.9811879775853284,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79140,autotrain-book_recommender-1867863842,['kroos/autotrain-data-book_recommender'],,10.620169750625417,AutoTrain,Not Specified,Not Specified,Not Specified,0.594,0.946,0.387,,,737779243.0,True,4,0,['pytorch'],2022-10-24 17:57:11+00:00,2022-10-24 17:51:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1867863842
- CO2 Emissions (in grams): 10.6202

## Validation Metrics

- Loss: 0.946
- Accuracy: 0.594
- Macro F1: 0.387
- Micro F1: 0.594
- Weighted F1: 0.574
- Macro Precision: 0.370
- Micro Precision: 0.594
- Weighted Precision: 0.567
- Macro Recall: 0.417
- Micro Recall: 0.594
- Weighted Recall: 0.594


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/kroos/autotrain-book_recommender-1867863842
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kroos/autotrain-book_recommender-1867863842"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kroos/autotrain-book_recommender-1867863842"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,69469628.10613762,0.4686605504587156,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79271,autotrain-25102022-cert1-1871763939,['teacookies/autotrain-data-25102022-cert1'],,22.29520077707259,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.002,0.975,,,667179825.0,True,2,0,['pytorch'],2022-10-25 03:32:53+00:00,2022-10-25 03:20:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1871763939
- CO2 Emissions (in grams): 22.2952

## Validation Metrics

- Loss: 0.002
- Accuracy: 0.999
- Precision: 0.972
- Recall: 0.978
- F1: 0.975

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-25102022-cert1-1871763939
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-25102022-cert1-1871763939"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-25102022-cert1-1871763939"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,29924817.976346664,0.9868541033434648,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79277,autotrain-25102022-cert2-1871863945,['teacookies/autotrain-data-25102022-cert2'],,23.303137544479885,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,0.985,,,667179825.0,True,2,0,['pytorch'],2022-10-25 04:23:23+00:00,2022-10-25 04:10:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1871863945
- CO2 Emissions (in grams): 23.3031

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 0.984
- Recall: 0.986
- F1: 0.985

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-25102022-cert2-1871863945
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-25102022-cert2-1871863945"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-25102022-cert2-1871863945"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,28630471.915059503,0.9924433249370276,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79301,bart-base-cantonese,[''],,6.29,MLCO2,second-stage pre-training,Not Specified,Google Cloud TPU v4-16,,,,,,439094139.0,False,125,2,"['jax', 'pytorch', 'bart-base-jax']",2022-11-06 15:48:58+00:00,2022-10-25 06:24:17+00:00,"
# bart-base-cantonese

This is the Cantonese model of BART base. It is obtained by a second-stage pre-training on the [LIHKG dataset](https://github.com/ayaka14732/lihkg-scraper) based on the [fnlp/bart-base-chinese](https://huggingface.co/fnlp/bart-base-chinese) model.

This project is supported by Cloud TPUs from Google's [TPU Research Cloud](https://sites.research.google/trc/about/) (TRC).

**Note**: To avoid any copyright issues, please do not use this model for any purpose.

## GitHub Links

- Dataset: [ayaka14732/lihkg-scraper](https://github.com/ayaka14732/lihkg-scraper)
- Tokeniser: [ayaka14732/bert-tokenizer-cantonese](https://github.com/ayaka14732/bert-tokenizer-cantonese)
- Base model: [ayaka14732/bart-base-jax](https://github.com/ayaka14732/bart-base-jax)
- Pre-training: [ayaka14732/bart-base-cantonese](https://github.com/ayaka14732/bart-base-cantonese)

## Usage

```python
from transformers import BertTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained('Ayaka/bart-base-cantonese')
model = BartForConditionalGeneration.from_pretrained('Ayaka/bart-base-cantonese')
text2text_generator = Text2TextGenerationPipeline(model, tokenizer)  
output = text2text_generator('聽日就要返香港，我激動到[MASK]唔着', max_length=50, do_sample=False)
print(output[0]['generated_text'].replace(' ', ''))
# output: 聽日就要返香港，我激動到瞓唔着
```

**Note**: Please use the `BertTokenizer` for the model vocabulary. DO NOT use the original `BartTokenizer`.

## Training Details

- Optimiser: SGD 0.03 + Adaptive Gradient Clipping 0.1
- Dataset: 172937863 sentences, pad or truncate to 64 tokens
- Batch size: 640
- Number of epochs: 7 epochs + 61440 steps
- Time: 44.0 hours on Google Cloud TPU v4-16

WandB link: [`1j7zs802`](https://wandb.ai/ayaka/bart-base-cantonese/runs/1j7zs802)
",,,1,[],[],NLP,2022-10,69808289.1891892,,0.0,0.0,0.0,0.0,0,0.0,0,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0,0,1,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79378,improve-a-v1,[''],,0.9899872350262614,AutoTrain,Not Specified,Not Specified,Not Specified,,0.347,,0.66429,0.66188,557969145.0,True,2,0,"['transformers', 'pytorch']",2022-12-08 09:13:15+00:00,2022-10-25 14:10:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1822063032
- CO2 Emissions (in grams): 0.9900

## Validation Metrics

- Loss: 0.347
- Rouge1: 66.429
- Rouge2: 29.419
- RougeL: 66.188
- RougeLsum: 66.183
- Gen Len: 11.256

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aalbertini90/autotrain-improve-a-1822063032
```",,,1,[],[],NLP,2022-10,563612464.1397005,0.6630828101977877,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79380,tag-h-v1,[''],,607.9833800689026,AutoTrain,Not Specified,Not Specified,Not Specified,,1.665,,0.53144,0.5266299999999999,1625537793.0,True,2,0,"['transformers', 'pytorch']",2022-12-08 09:13:25+00:00,2022-10-25 14:11:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1822163038
- CO2 Emissions (in grams): 607.9834

## Validation Metrics

- Loss: 1.665
- Rouge1: 53.144
- Rouge2: 27.768
- RougeL: 52.663
- RougeLsum: 52.645
- Gen Len: 10.722

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aalbertini90/autotrain-h-1822163038
```",,,1,[],[],NLP,2022-10,2673654.981844698,0.529024066838678,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79381,tag-caption-v1,[''],,339.29944607016967,AutoTrain,Not Specified,Not Specified,Not Specified,,2.405,,0.30426,0.29262,1625537793.0,True,2,0,"['transformers', 'pytorch']",2022-12-08 09:13:36+00:00,2022-10-25 14:11:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1872663964
- CO2 Emissions (in grams): 339.2994

## Validation Metrics

- Loss: 2.405
- Rouge1: 30.426
- Rouge2: 16.255
- RougeL: 29.262
- RougeLsum: 29.337
- Gen Len: 13.671

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aalbertini90/autotrain-caption-1872663964
```",,,1,[],[],NLP,2022-10,4790864.859425166,0.2983265018094089,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79512,Movie_review_sentiment_analysis_model,['JamesH/autotrain-data-third-project'],,6.99192089941968,AutoTrain,Not Specified,Not Specified,Not Specified,0.95,0.175,0.95,,,737766955.0,True,4,1,['pytorch'],2022-10-26 01:02:13+00:00,2022-10-26 00:58:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 1883864250
- CO2 Emissions (in grams): 6.9919

## Validation Metrics

- Loss: 0.175
- Accuracy: 0.950
- Precision: 0.950
- Recall: 0.950
- AUC: 0.986
- F1: 0.950

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/JamesH/autotrain-third-project-1883864250
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""JamesH/autotrain-third-project-1883864250"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""JamesH/autotrain-third-project-1883864250"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,105517062.5659157,0.95,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79853,autotrain-27102022-cert1-1899464570,['teacookies/autotrain-data-27102022-cert1'],,16.254745105263574,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.004,0.975,,,667179825.0,True,2,0,['pytorch'],2022-10-27 06:29:42+00:00,2022-10-27 06:19:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1899464570
- CO2 Emissions (in grams): 16.2547

## Validation Metrics

- Loss: 0.004
- Accuracy: 0.999
- Precision: 0.972
- Recall: 0.979
- F1: 0.975

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-27102022-cert1-1899464570
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-27102022-cert1-1899464570"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-27102022-cert1-1899464570"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,41045234.5256374,0.9868541033434648,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79863,autotrain-27102022-cert-1899564594,['teacookies/autotrain-data-27102022-cert'],,22.03607609264655,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.003,0.981,,,667179825.0,True,2,0,['pytorch'],2022-10-27 07:34:21+00:00,2022-10-27 07:21:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1899564594
- CO2 Emissions (in grams): 22.0361

## Validation Metrics

- Loss: 0.003
- Accuracy: 0.999
- Precision: 0.981
- Recall: 0.982
- F1: 0.981

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-27102022-cert-1899564594
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-27102022-cert-1899564594"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-27102022-cert-1899564594"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,30276707.25926737,0.9899181818181816,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79925,lojban-translation,['woctordho/autotrain-data-lojban-translation'],,53.16467716910746,AutoTrain,Not Specified,Not Specified,Not Specified,,1.367,,,,310020485.0,True,3,0,"['transformers', 'pytorch']",2023-02-25 22:06:33+00:00,2022-10-27 13:38:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1902964708
- CO2 Emissions (in grams): 53.1647

## Validation Metrics

- Loss: 1.367
- SacreBLEU: 20.194
- Gen len: 18.535",,,1,[],[],NLP,2022-10,5831324.509201467,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79952,xlm-roberta-finetuned-sentiment,['lewtun/autotrain-data-mgb-product-reviews-xlm-r'],,19.11641413955588,AutoTrain,Not Specified,Not Specified,Not Specified,0.563,1.021,0.555,,,1112261613.0,True,5,0,['pytorch'],2022-10-27 15:37:04+00:00,2022-10-27 15:17:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1904264758
- CO2 Emissions (in grams): 19.1164

## Validation Metrics

- Loss: 1.021
- Accuracy: 0.563
- Macro F1: 0.555
- Micro F1: 0.563
- Weighted F1: 0.556
- Macro Precision: 0.555
- Micro Precision: 0.563
- Weighted Precision: 0.556
- Macro Recall: 0.562
- Micro Recall: 0.563
- Weighted Recall: 0.563


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lewtun/autotrain-mgb-product-reviews-xlm-r-1904264758
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lewtun/autotrain-mgb-product-reviews-xlm-r-1904264758"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lewtun/autotrain-mgb-product-reviews-xlm-r-1904264758"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,58183590.54580728,0.5589713774597496,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
79955,distilbert-multilingual-finetuned-sentiment,['lewtun/autotrain-data-mgb-product-reviews-mbert'],,5.523107849339405,AutoTrain,Not Specified,Not Specified,Not Specified,0.514,1.135,0.504,,,541348337.0,True,5,0,['pytorch'],2022-10-27 15:43:10+00:00,2022-10-27 15:34:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1904564767
- CO2 Emissions (in grams): 5.5231

## Validation Metrics

- Loss: 1.135
- Accuracy: 0.514
- Macro F1: 0.504
- Micro F1: 0.514
- Weighted F1: 0.505
- Macro Precision: 0.506
- Micro Precision: 0.514
- Weighted Precision: 0.507
- Macro Recall: 0.513
- Micro Recall: 0.514
- Weighted Recall: 0.514


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lewtun/autotrain-mgb-product-reviews-mbert-1904564767
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lewtun/autotrain-mgb-product-reviews-mbert-1904564767"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lewtun/autotrain-mgb-product-reviews-mbert-1904564767"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,98015166.78055604,0.508950884086444,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80017,Translation_en_to_fr_project,['JamesH/autotrain-data-second-project-en2fr'],,0.6863820434350988,AutoTrain,Not Specified,Not Specified,Not Specified,,1.117,,,,4918417081.0,True,3,1,['pytorch'],2022-10-27 21:52:09+00:00,2022-10-27 19:57:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1907464829
- CO2 Emissions (in grams): 0.6864

## Validation Metrics

- Loss: 1.117
- SacreBLEU: 16.546
- Gen len: 14.511",,,1,[],[],NLP,2022-10,7165713509.03218,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80152,autotrain-28102022-1914864930,['teacookies/autotrain-data-28102022'],,19.19485186697524,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,0.983,,,667179825.0,True,2,0,['pytorch'],2022-10-28 07:41:13+00:00,2022-10-28 07:30:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1914864930
- CO2 Emissions (in grams): 19.1949

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 0.982
- Recall: 0.984
- F1: 0.983

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-28102022-1914864930
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-28102022-1914864930"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-28102022-1914864930"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,34758269.020449355,0.9914271306101864,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80201,autotrain-28102022-cert2-1916264970,['teacookies/autotrain-data-28102022-cert2'],,17.982023070008026,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,0.983,,,667179825.0,True,2,0,['pytorch'],2022-10-28 12:26:46+00:00,2022-10-28 12:15:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 1916264970
- CO2 Emissions (in grams): 17.9820

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 0.980
- Recall: 0.986
- F1: 0.983

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-28102022-cert2-1916264970
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-28102022-cert2-1916264970"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-28102022-cert2-1916264970"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,37102600.88103102,0.9914271306101864,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80308,autotrain-planes-1918465011,['kyle-lucke/autotrain-data-planes'],,0.1981134535019566,AutoTrain,Not Specified,Not Specified,Not Specified,0.997,0.011,0.916,,,,True,3,0,['joblib'],2022-10-28 19:42:45+00:00,2022-10-28 19:42:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1918465011
- CO2 Emissions (in grams): 0.1981

## Validation Metrics

- Loss: 0.011
- Accuracy: 0.997
- Macro F1: 0.916
- Micro F1: 0.997
- Weighted F1: 0.996
- Macro Precision: 0.999
- Micro Precision: 0.997
- Weighted Precision: 0.997
- Macro Recall: 0.867
- Micro Recall: 0.997
- Weighted Recall: 0.997

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-10,,0.9547851542080502,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80411,TLDR-Vegan-Studies,['vegancreativecompass/autotrain-data-scitldr-for-vegan-studies'],,57.779835625872906,AutoTrain,Not Specified,Not Specified,Not Specified,,0.711,,0.44317,0.41369,2950844807.0,True,2,0,['pytorch'],2022-10-29 11:36:42+00:00,2022-10-29 10:48:26+00:00,"# About This Model

This model has been trained to take abstracts of scientific studies about veganism & animal rights and turn them into single-sentence takeaways for vegan businesses and animal activists to apply to their activism. The dataset was curated by scraping TLDRs and abstracts from Semantic Scholar and having vegan activists and marketing professionals from VEG3 review the usefulness of a random sample of the dataset to ensure their relevance to vegan businesses and animal activists.


# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1923365100
- CO2 Emissions (in grams): 57.7798

## Validation Metrics

- Loss: 0.711
- Rouge1: 44.317
- Rouge2: 30.335
- RougeL: 41.369
- RougeLsum: 41.198
- Gen Len: 17.855

## Usage

You can use cURL to access this model:

```
curl https://api-inference.huggingface.co/models/VEG3/TLDR-Vegan-Studies \
	-X POST \
	-d '{""inputs"":""ABSTRACT""}' \
	-H ""Authorization: Bearer YOURAPIKEY""

```",,,1,[],[],NLP,2022-10,51070495.00983104,0.4279228749153888,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80471,EnglishtoBulgarian,['Tritkoman/autotrain-data-okskkakq'],,41.90097830745309,AutoTrain,Not Specified,Not Specified,Not Specified,,1.492,,,,4918417081.0,True,3,0,['pytorch'],2022-10-29 18:58:04+00:00,2022-10-29 18:28:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1926765185
- CO2 Emissions (in grams): 41.9010

## Validation Metrics

- Loss: 1.492
- SacreBLEU: 17.642
- Gen len: 12.667",,,1,[],[],NLP,2022-10,117381915.164619,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,1.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80550,English2Sardinian,['Tritkoman/autotrain-data-gatvotva'],,14.908336657166226,AutoTrain,Not Specified,Not Specified,Not Specified,,2.666,,,,340870277.0,True,7,0,['pytorch'],2022-10-30 07:41:31+00:00,2022-10-30 07:31:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 1931765297
- CO2 Emissions (in grams): 14.9083

## Validation Metrics

- Loss: 2.666
- SacreBLEU: 17.990
- Gen len: 64.922",,,1,[],[],NLP,2022-10,22864406.99849292,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80659,autotrain-fbert-singlish-2-1937065404,['DingYao/autotrain-data-fbert-singlish-2'],,1.04277637543434,AutoTrain,Not Specified,Not Specified,Not Specified,0.858,0.368,0.717,,,438009197.0,True,5,0,['pytorch'],2022-10-30 17:12:55+00:00,2022-10-30 17:11:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1937065404
- CO2 Emissions (in grams): 1.0428

## Validation Metrics

- Loss: 0.368
- Accuracy: 0.858
- Macro F1: 0.717
- Micro F1: 0.858
- Weighted F1: 0.857
- Macro Precision: 0.731
- Micro Precision: 0.858
- Weighted Precision: 0.859
- Macro Recall: 0.711
- Micro Recall: 0.858
- Weighted Recall: 0.858


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/DingYao/autotrain-fbert-singlish-2-1937065404
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""DingYao/autotrain-fbert-singlish-2-1937065404"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""DingYao/autotrain-fbert-singlish-2-1937065404"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,420041350.4933493,0.7811885714285713,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80840,improve-figcaption-v2,[''],,2.4435817864698777,AutoTrain,Not Specified,Not Specified,Not Specified,,0.282,,0.63261,0.63239,557969145.0,True,19,0,"['transformers', 'pytorch']",2022-12-08 09:13:07+00:00,2022-10-31 11:48:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1902664733
- CO2 Emissions (in grams): 2.4436

## Validation Metrics

- Loss: 0.282
- Rouge1: 63.261
- Rouge2: 29.581
- RougeL: 63.239
- RougeLsum: 63.219
- Gen Len: 9.777

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aalbertini90/autotrain-improve-figcaption-v2-1902664733
```",,,1,[],[],NLP,2022-10,228340687.46521088,0.6324999808695653,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
80889,autotrain-fbert-singlish-5-1943965533,['DingYao/autotrain-data-fbert-singlish-5'],,2.1095744631067883,AutoTrain,Not Specified,Not Specified,Not Specified,0.88,0.31,0.766,,,438009197.0,True,5,0,['pytorch'],2022-10-31 16:01:02+00:00,2022-10-31 15:59:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1943965533
- CO2 Emissions (in grams): 2.1096

## Validation Metrics

- Loss: 0.310
- Accuracy: 0.880
- Macro F1: 0.766
- Micro F1: 0.880
- Weighted F1: 0.877
- Macro Precision: 0.826
- Micro Precision: 0.880
- Weighted Precision: 0.877
- Macro Recall: 0.735
- Micro Recall: 0.880
- Weighted Recall: 0.880


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/DingYao/autotrain-fbert-singlish-5-1943965533
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""DingYao/autotrain-fbert-singlish-5-1943965533"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""DingYao/autotrain-fbert-singlish-5-1943965533"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-10,207629171.029564,0.819052247873633,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
81415,predict-page-is-junk-0.1.0,['heyharmon/autotrain-data-page-classifier-predict-junk-training'],,4.157527125888868,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,1,['pytorch'],2022-11-02 07:42:42+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
81495,href-v1,[''],,2.970316260186869,AutoTrain,Not Specified,Not Specified,Not Specified,,2.262,,0.27046,0.24913,1625537793.0,True,19,0,"['transformers', 'pytorch']",2022-12-08 09:12:16+00:00,2022-11-02 14:36:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1900964639
- CO2 Emissions (in grams): 2.9703

## Validation Metrics

- Loss: 2.262
- Rouge1: 27.046
- Rouge2: 14.251
- RougeL: 24.913
- RougeLsum: 25.284
- Gen Len: 19.888

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aalbertini90/autotrain-href-1900964639
```",,,1,[],[],NLP,2022-11,547260847.1993935,0.2593571847033237,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
81731,autotrain-hjuihu-1974565969,['MS-Go/autotrain-data-hjuihu'],,49.67104326560968,AutoTrain,Not Specified,Not Specified,Not Specified,,2.889,,0.36489,0.18766,1625533697.0,True,2,0,['pytorch'],2022-11-03 12:58:30+00:00,2022-11-03 12:26:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1974565969
- CO2 Emissions (in grams): 49.6710

## Validation Metrics

- Loss: 2.889
- Rouge1: 36.489
- Rouge2: 7.128
- RougeL: 18.766
- RougeLsum: 33.217
- Gen Len: 141.972

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/MS-Go/autotrain-hjuihu-1974565969
```",,,1,[],[],NLP,2022-11,32725982.58723221,0.2478518049045335,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
81772,autotrain-bart_normaldata-1976866012,['MS-Go/autotrain-data-bart_normaldata'],,41.152874017879256,AutoTrain,Not Specified,Not Specified,Not Specified,,2.837,,0.34318,0.1846,1625533697.0,True,2,0,['pytorch'],2022-11-03 15:20:24+00:00,2022-11-03 14:57:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 1976866012
- CO2 Emissions (in grams): 41.1529

## Validation Metrics

- Loss: 2.837
- Rouge1: 34.318
- Rouge2: 6.495
- RougeL: 18.460
- RougeLsum: 30.998
- Gen Len: 141.027

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/MS-Go/autotrain-bart_normaldata-1976866012
```",,,1,[],[],NLP,2022-11,39499882.71277898,0.240066042669294,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
81788,autotrain-testtextexists-1966366048,['orange6996/autotrain-data-testtextexists'],,0.3550338626114656,AutoTrain,Not Specified,Not Specified,Not Specified,,4911.982,,,,737763883.0,True,2,0,['pytorch'],2022-11-03 15:56:11+00:00,2022-11-03 15:55:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1966366048
- CO2 Emissions (in grams): 0.3550

## Validation Metrics

- Loss: 4911.982
- MSE: 4911.981
- MAE: 68.106
- R2: -16.962
- RMSE: 70.086
- Explained Variance: -0.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/orange6996/autotrain-testtextexists-1966366048
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""orange6996/autotrain-testtextexists-1966366048"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""orange6996/autotrain-testtextexists-1966366048"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,2078009904.6703565,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
81789,autotrain-testtextexists-1966366051,['orange6996/autotrain-data-testtextexists'],,0.8382331508369333,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,1421580269.0,False,2,0,['pytorch'],2022-11-03 16:13:42+00:00,2022-11-03 15:56:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 1966366051
- CO2 Emissions (in grams): 0.8382

## Validation Metrics

- Loss: 4927.679
- MSE: 4927.679
- MAE: 68.224
- R2: -17.019
- RMSE: 70.197
- Explained Variance: 0.001

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/orange6996/autotrain-testtextexists-1966366051
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""orange6996/autotrain-testtextexists-1966366051"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""orange6996/autotrain-testtextexists-1966366051"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,1695924657.215745,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
82021,autotrain-wine-1986366196,['navidfk/autotrain-data-wine'],,23.98337622177028,AutoTrain,Not Specified,Not Specified,Not Specified,0.705,0.792,0.345,,,,True,4,0,['joblib'],2022-11-04 15:38:15+00:00,2022-11-04 15:10:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 1986366196
- CO2 Emissions (in grams): 23.9834

## Validation Metrics

- Loss: 0.792
- Accuracy: 0.705
- Macro F1: 0.345
- Micro F1: 0.705
- Weighted F1: 0.683
- Macro Precision: 0.365
- Micro Precision: 0.705
- Weighted Precision: 0.676
- Macro Recall: 0.341
- Micro Recall: 0.705
- Weighted Recall: 0.705

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-11,,0.4632857142857142,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
82469,autotrain-english-to-interlingua-translator-2002766502,['irfanns/autotrain-data-english-to-interlingua-translator'],,19.067960229529483,AutoTrain,Not Specified,Not Specified,Not Specified,,1.241,,,,340870277.0,True,2,0,['pytorch'],2022-11-06 10:56:33+00:00,2022-11-06 10:44:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 2002766502
- CO2 Emissions (in grams): 19.0680

## Validation Metrics

- Loss: 1.241
- SacreBLEU: 42.137
- Gen len: 32.318",,,1,[],[],NLP,2022-11,17876598.907108758,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
83479,autotrain-hmaet-2037366889,['Robertooo/autotrain-data-hmaet'],,0.0405645225064915,AutoTrain,Not Specified,Not Specified,Not Specified,,0.003,,,,,True,5,0,['joblib'],2022-11-09 12:32:43+00:00,2022-11-09 12:08:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 2037366889
- CO2 Emissions (in grams): 0.0406

## Validation Metrics

- Loss: 0.003
- R2: 0.999
- MSE: 0.000
- MAE: 0.001
- RMSLE: 0.002

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-11,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
83480,autotrain-hmaet-2037366891,['Robertooo/autotrain-data-hmaet'],,0.3032763853118019,AutoTrain,Not Specified,Not Specified,Not Specified,,0.067,,,,,True,4,0,['joblib'],2022-11-09 12:09:36+00:00,2022-11-09 12:08:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 2037366891
- CO2 Emissions (in grams): 0.3033

## Validation Metrics

- Loss: 0.067
- R2: 0.486
- MSE: 0.005
- MAE: 0.055
- RMSLE: 0.036

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-11,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
83516,wikicat_ca,['projecte-aina/WikiCAT_ca'],,47.543878831739285,AutoTrain,Not Specified,Not Specified,Not Specified,0.787,0.701,0.776,,,1421617133.0,True,5,0,['pytorch'],2022-11-09 15:23:23+00:00,2022-11-09 14:18:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2036166932
- CO2 Emissions (in grams): 47.5439

## Validation Metrics

- Loss: 0.701
- Accuracy: 0.787
- Macro F1: 0.776
- Micro F1: 0.787
- Weighted F1: 0.784
- Macro Precision: 0.786
- Micro Precision: 0.787
- Weighted Precision: 0.788
- Macro Recall: 0.775
- Micro Recall: 0.787
- Weighted Recall: 0.787


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crodri/autotrain-wikicat_ca-2036166932
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crodri/wikicat_ca"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crodri/wikicat_ca"", use_auth_token=True)

inputs = tokenizer(""Una cançó és una composició musical que conté, a vegades, una part amb veu o melodia vocal, és a dir, amb text, cantada, però també pot ser simplement un conjunt de notes tocades sistemàticament, formant un ritme."", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,29901160.10582962,0.7814612923864364,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
84979,autotrain-okmjn-moo-2088667188,['micole66/autotrain-data-okmjn-moo'],,1.361181216223394,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.202,,,,1392724369.0,True,4,0,['pytorch'],2022-11-14 09:44:07+00:00,2022-11-14 09:42:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2088667188
- CO2 Emissions (in grams): 1.3612

## Validation Metrics

- Loss: 0.202
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-11,1023173367.6608634,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
84983,autotrain-pachyderms-v2-2088767193,['micole66/autotrain-data-pachyderms-v2'],,1.190285924893865,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.004,,,,1392724369.0,True,2,0,['pytorch'],2022-11-14 10:04:56+00:00,2022-11-14 10:03:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2088767193
- CO2 Emissions (in grams): 1.1903

## Validation Metrics

- Loss: 0.004
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-11,1170075475.036963,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
84992,autotrain-pachyderms-v3-2088867198,['micole66/autotrain-data-pachyderms-v3'],,1.5875017032028491,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.02,,,,1392724369.0,True,2,0,['pytorch'],2022-11-14 10:55:08+00:00,2022-11-14 10:53:02+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2088867198
- CO2 Emissions (in grams): 1.5875

## Validation Metrics

- Loss: 0.020
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-11,877305747.8868351,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85020,autotrain-14112022-cert-2086767210,['teacookies/autotrain-data-14112022-cert'],,18.953935307959163,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,0.988,,,667186033.0,True,2,0,['pytorch'],2022-11-14 12:19:48+00:00,2022-11-14 12:08:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2086767210
- CO2 Emissions (in grams): 18.9539

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 0.987
- Recall: 0.989
- F1: 0.988

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-14112022-cert-2086767210
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-14112022-cert-2086767210"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-14112022-cert-2086767210"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,35200396.23221856,0.993963782696177,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85071,autotrain-documentos-oficiais-2092367351,['famube/autotrain-data-documentos-oficiais'],,6.461431564881563,AutoTrain,Not Specified,Not Specified,Not Specified,0.986,0.059,0.0,,,1333543089.0,True,3,0,['pytorch'],2022-11-18 20:33:18+00:00,2022-11-14 15:52:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2092367351
- CO2 Emissions (in grams): 6.4614

## Validation Metrics

- Loss: 0.059
- Accuracy: 0.986
- Precision: 0.000
- Recall: 0.000
- F1: 0.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/famube/autotrain-documentos-oficiais-2092367351
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""famube/autotrain-documentos-oficiais-2092367351"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""famube/autotrain-documentos-oficiais-2092367351"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,206385082.8735727,0.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85084,autotrain-furryornot-2093267379,['micole66/autotrain-data-furryornot'],,0.0074725601621924,AutoTrain,Not Specified,Not Specified,Not Specified,0.909,0.242,,,,1392724369.0,True,7,0,['pytorch'],2022-11-14 16:38:27+00:00,2022-11-14 16:36:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2093267379
- CO2 Emissions (in grams): 0.0075

## Validation Metrics

- Loss: 0.242
- Accuracy: 0.909
- Precision: 0.750
- Recall: 1.000
- AUC: 0.958
- F1: 0.857",,,1,[],[],Computer Vision,2022-11,186378475217.43924,0.909,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85155,autotrain-disease_tokens-2095367455,['Olusegun/autotrain-data-disease_tokens'],,1.569698418187329,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,1.0,,,1336508593.0,True,2,0,['pytorch'],2022-11-14 19:41:09+00:00,2022-11-14 19:40:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2095367455
- CO2 Emissions (in grams): 1.5697

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Olusegun/autotrain-disease_tokens-2095367455
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Olusegun/autotrain-disease_tokens-2095367455"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Olusegun/autotrain-disease_tokens-2095367455"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,851442912.5458289,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85401,autotrain-15112022-cert2-2099767621,['teacookies/autotrain-data-15112022-cert2'],,30.88105111466208,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.004,0.986,,,667186033.0,True,2,0,['pytorch'],2022-11-15 05:45:30+00:00,2022-11-15 05:27:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2099767621
- CO2 Emissions (in grams): 30.8811

## Validation Metrics

- Loss: 0.004
- Accuracy: 0.999
- Precision: 0.982
- Recall: 0.990
- F1: 0.986

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-15112022-cert2-2099767621
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-15112022-cert2-2099767621"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-15112022-cert2-2099767621"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,21605029.910501502,0.9924574307304784,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85438,autotrain-15112022-cert3-2101567677,['teacookies/autotrain-data-15112022-cert3'],,0.0847161246389862,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,0.991,,,667186033.0,True,2,0,['pytorch'],2022-11-15 08:19:47+00:00,2022-11-15 08:09:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2101567677
- CO2 Emissions (in grams): 0.0847

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 0.990
- Recall: 0.992
- F1: 0.991

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-15112022-cert3-2101567677
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-15112022-cert3-2101567677"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-15112022-cert3-2101567677"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,7875549499.49826,0.995479658463084,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85470,autotrain-15112022-cert4-2103567748,['teacookies/autotrain-data-15112022-cert4'],,18.55067964060936,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,0.991,,,667186033.0,True,2,0,['pytorch'],2022-11-15 09:57:43+00:00,2022-11-15 09:47:02+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2103567748
- CO2 Emissions (in grams): 18.5507

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 0.990
- Recall: 0.992
- F1: 0.991

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-15112022-cert4-2103567748
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-15112022-cert4-2103567748"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-15112022-cert4-2103567748"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,35965584.33036926,0.995479658463084,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85490,autotrain-15112022-cert6-2103867793,['teacookies/autotrain-data-15112022-cert6'],,0.0843114319344479,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,0.99,,,667186033.0,True,2,0,['pytorch'],2022-11-15 11:52:46+00:00,2022-11-15 11:41:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2103867793
- CO2 Emissions (in grams): 0.0843

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 0.989
- Recall: 0.992
- F1: 0.990

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-15112022-cert6-2103867793
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-15112022-cert6-2103867793"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-15112022-cert6-2103867793"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,7913351934.512711,0.9949748743718592,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85533,autotrain-15112022-cert7-2105067828,['teacookies/autotrain-data-15112022-cert7'],,0.0817743318404079,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.002,0.991,,,667186033.0,True,2,0,['pytorch'],2022-11-15 14:07:19+00:00,2022-11-15 13:56:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2105067828
- CO2 Emissions (in grams): 0.0818

## Validation Metrics

- Loss: 0.002
- Accuracy: 0.999
- Precision: 0.991
- Recall: 0.992
- F1: 0.991

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-15112022-cert7-2105067828
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-15112022-cert7-2105067828"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-15112022-cert7-2105067828"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,8158868657.002187,0.9949839195979898,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85546,autotrain-15112022-cert8-2105867883,['teacookies/autotrain-data-15112022-cert8'],,0.0952584031790109,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.003,0.986,,,667186033.0,True,2,0,['pytorch'],2022-11-15 15:10:42+00:00,2022-11-15 14:59:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2105867883
- CO2 Emissions (in grams): 0.0953

## Validation Metrics

- Loss: 0.003
- Accuracy: 0.999
- Precision: 0.984
- Recall: 0.989
- F1: 0.986

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-15112022-cert8-2105867883
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-15112022-cert8-2105867883"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-15112022-cert8-2105867883"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,7003959868.466563,0.9924574307304784,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85799,autotrain-16112022-cert-2114268313,['teacookies/autotrain-data-16112022-cert'],,0.086994101215413,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.003,0.986,,,667186033.0,True,2,0,['pytorch'],2022-11-16 05:47:55+00:00,2022-11-16 05:38:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2114268313
- CO2 Emissions (in grams): 0.0870

## Validation Metrics

- Loss: 0.003
- Accuracy: 0.999
- Precision: 0.987
- Recall: 0.986
- F1: 0.986

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-16112022-cert-2114268313
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-16112022-cert-2114268313"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-16112022-cert-2114268313"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,7669324973.516626,0.9924574307304784,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85828,autotrain-16112022-cert2-2115868392,['teacookies/autotrain-data-16112022-cert2'],,20.85124428527041,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.003,0.984,,,667186033.0,True,2,0,['pytorch'],2022-11-16 07:33:45+00:00,2022-11-16 07:21:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2115868392
- CO2 Emissions (in grams): 20.8512

## Validation Metrics

- Loss: 0.003
- Accuracy: 0.999
- Precision: 0.984
- Recall: 0.984
- F1: 0.984

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-16112022-cert2-2115868392
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-16112022-cert2-2115868392"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-16112022-cert2-2115868392"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,31997420.579418797,0.9914432677760968,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
85857,autotrain-question-generation4-2116768409,['abhishekgupta/autotrain-data-question-generation4'],,4.806834090498112,AutoTrain,Not Specified,Not Specified,Not Specified,,1.092,,0.32336,0.30175,712383415.0,True,2,0,['pytorch'],2022-11-16 09:31:25+00:00,2022-11-16 09:28:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2116768409
- CO2 Emissions (in grams): 4.8068

## Validation Metrics

- Loss: 1.092
- Rouge1: 32.336
- Rouge2: 15.558
- RougeL: 30.175
- RougeLsum: 30.191
- Gen Len: 14.493

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/abhishekgupta/autotrain-question-generation4-2116768409
```",,,1,[],[],NLP,2022-11,148202205.77369222,0.3121814720609173,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
86062,welcome_message_prod,['lucieackley/autotrain-data-welcome-msg'],,0.0042327719742692,AutoTrain,Not Specified,Not Specified,Not Specified,0.958,0.244,0.968,,,438006125.0,True,4,0,['pytorch'],2022-11-16 22:04:26+00:00,2022-11-16 21:54:51+00:00,"
# Model Trained On Welcome Messages labeled by Catie

- Problem type: Binary Classification
- Model ID: 2125168670
- CO2 Emissions (in grams): 0.0042

## Validation Metrics

- Loss: 0.244
- Accuracy: 0.958
- Precision: 0.938
- Recall: 1.000
- AUC: 0.970
- F1: 0.968

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lucieackley/autotrain-welcome-msg-2125168670
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucieackley/autotrain-welcome-msg-2125168670"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucieackley/autotrain-welcome-msg-2125168670"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,103479735658.48016,0.9629740394600206,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
86174,activity-classifier,['jeveuxaider/activity-classifier'],,12.734050517307358,AutoTrain,Not Specified,Not Specified,Not Specified,0.812,0.888,0.684,,,442676333.0,True,598,0,"['transformers', 'pytorch']",2022-11-17 08:41:32+00:00,2022-11-17 08:13:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2096367492
- CO2 Emissions (in grams): 12.7341

## Validation Metrics

- Loss: 0.888
- Accuracy: 0.812
- Macro F1: 0.684
- Micro F1: 0.812
- Weighted F1: 0.808
- Macro Precision: 0.708
- Micro Precision: 0.812
- Weighted Precision: 0.813
- Macro Recall: 0.691
- Micro Recall: 0.812
- Weighted Recall: 0.812


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""Je participe à un accueil de jour""}' https://api-inference.huggingface.co/models/jeveuxaider/activity-classifier
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer
model = AutoModelForSequenceClassification.from_pretrained(""jeveuxaider/activity-classifier"", use_auth_token=True)
tokenizer = AutoTokenizer.from_pretrained(""jeveuxaider/activity-classifier"", use_auth_token=True)
inputs = tokenizer(""Je participe à un accueil de jour"", return_tensors=""pt"")
outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,34763199.06210054,0.7425240641711232,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
86552,autotrain-18112022-cert-2140369076,['teacookies/autotrain-data-18112022-cert'],,17.745493294511153,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.003,0.987,,,667186033.0,True,2,0,['pytorch'],2022-11-18 09:29:58+00:00,2022-11-18 09:20:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2140369076
- CO2 Emissions (in grams): 17.7455

## Validation Metrics

- Loss: 0.003
- Accuracy: 0.999
- Precision: 0.986
- Recall: 0.989
- F1: 0.987

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-18112022-cert-2140369076
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-18112022-cert-2140369076"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-18112022-cert-2140369076"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,37597491.48288635,0.9929637462235648,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
86647,autotrain-consumer-nature-speech_finbert-2147169289,['hr-elrond/autotrain-data-consumer-nature-speech_finbert'],,0.0043719752543122,AutoTrain,Not Specified,Not Specified,Not Specified,0.913,0.256,0.78,,,439084397.0,True,4,1,['pytorch'],2022-12-01 08:59:48+00:00,2022-11-18 15:00:49+00:00,"

# Model Trained Using AutoTrain
We trained FinBERT to identify whether firms´ talk contains consumer concepts of human nature (e.g., ""I believe consumers generally act rational."", ""Consumers must take over responsibility for the choices they make."", ""It seems consumers behave quite altruistic."") from statements that do not (e.g., ""We expect buyers to double their purchases next year."", ""We see a 5% growth in numbers compared to the previous year."").  
The training data consisted of 236 positive documents (containing concepts of consumer nature) and 1034 negative documents (not contain concepts of consumer nature) extracted from earnings call transcripts of S&P-500 companies (2015-2020).

# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2147169289
- CO2 Emissions (in grams): 0.0044

## Validation Metrics

- Loss: 0.256
- Accuracy: 0.913
- Precision: 0.736
- Recall: 0.830
- AUC: 0.956
- F1: 0.780

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/hr-elrond/autotrain-consumer-nature-speech_finbert-2147169289
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""hr-elrond/autotrain-consumer-nature-speech_finbert-2147169289"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hr-elrond/autotrain-consumer-nature-speech_finbert-2147169289"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,100431583313.95378,0.8412758417011222,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
86950,good-reads-string,['fernanda-dionello/autotrain-data-autotrain_goodreads_string'],,0.0470068041759547,AutoTrain,Not Specified,Not Specified,Not Specified,0.686,0.806,0.534,,,737779243.0,True,2,0,['pytorch'],2022-11-19 20:16:34+00:00,2022-11-19 20:11:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2164069744
- CO2 Emissions (in grams): 0.0470

## Validation Metrics

- Loss: 0.806
- Accuracy: 0.686
- Macro F1: 0.534
- Micro F1: 0.686
- Weighted F1: 0.678
- Macro Precision: 0.524
- Micro Precision: 0.686
- Weighted Precision: 0.673
- Macro Recall: 0.551
- Micro Recall: 0.686
- Weighted Recall: 0.686


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fernanda-dionello/autotrain-autotrain_goodreads_string-2164069744
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fernanda-dionello/autotrain-autotrain_goodreads_string-2164069744"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fernanda-dionello/autotrain-autotrain_goodreads_string-2164069744"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,15695158518.719185,0.6005311475409837,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87032,idk,['Carlangeloconcepcionrepoyo/autotrain-data-dambuhalang-pogi-scout'],,1.7850904815735922,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.026,,,,346858475.0,True,1,0,['pytorch'],2022-11-20 08:34:52+00:00,2022-11-20 08:32:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2169069849
- CO2 Emissions (in grams): 1.7851

## Validation Metrics

- Loss: 0.026
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-11,194308623.89352804,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87115,autotrain-goodreads_without_bookid-2171169881,['fernanda-dionello/autotrain-data-goodreads_without_bookid'],,10.018792119596627,AutoTrain,Not Specified,Not Specified,Not Specified,0.66,0.754,0.422,,,737779243.0,True,2,0,['pytorch'],2022-11-20 17:08:42+00:00,2022-11-20 17:03:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2171169881
- CO2 Emissions (in grams): 10.0188

## Validation Metrics

- Loss: 0.754
- Accuracy: 0.660
- Macro F1: 0.422
- Micro F1: 0.660
- Weighted F1: 0.637
- Macro Precision: 0.418
- Micro Precision: 0.660
- Weighted Precision: 0.631
- Macro Recall: 0.440
- Micro Recall: 0.660
- Weighted Recall: 0.660


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fernanda-dionello/autotrain-goodreads_without_bookid-2171169881
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169881"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169881"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,73639539.99573596,0.5148243992606285,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87116,autotrain-goodreads_without_bookid-2171169880,['fernanda-dionello/autotrain-data-goodreads_without_bookid'],,11.598027053629249,AutoTrain,Not Specified,Not Specified,Not Specified,0.654,0.792,0.547,,,556859119.0,True,2,0,['pytorch'],2022-11-20 17:08:53+00:00,2022-11-20 17:03:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2171169880
- CO2 Emissions (in grams): 11.5980

## Validation Metrics

- Loss: 0.792
- Accuracy: 0.654
- Macro F1: 0.547
- Micro F1: 0.654
- Weighted F1: 0.649
- Macro Precision: 0.594
- Micro Precision: 0.654
- Weighted Precision: 0.660
- Macro Recall: 0.530
- Micro Recall: 0.654
- Weighted Recall: 0.654


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fernanda-dionello/autotrain-goodreads_without_bookid-2171169880
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169880"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169880"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,48013262.63726449,0.5957335553705246,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87117,autotrain-goodreads_without_bookid-2171169882,['fernanda-dionello/autotrain-data-goodreads_without_bookid'],,6.409243088343928,AutoTrain,Not Specified,Not Specified,Not Specified,0.586,0.95,0.373,,,438018413.0,True,2,0,['pytorch'],2022-11-20 17:06:43+00:00,2022-11-20 17:03:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2171169882
- CO2 Emissions (in grams): 6.4092

## Validation Metrics

- Loss: 0.950
- Accuracy: 0.586
- Macro F1: 0.373
- Micro F1: 0.586
- Weighted F1: 0.564
- Macro Precision: 0.438
- Micro Precision: 0.586
- Weighted Precision: 0.575
- Macro Recall: 0.399
- Micro Recall: 0.586
- Weighted Recall: 0.586


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fernanda-dionello/autotrain-goodreads_without_bookid-2171169882
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169882"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169882"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,68341675.75834274,0.4558456725755996,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87118,autotrain-goodreads_without_bookid-2171169883,['fernanda-dionello/autotrain-data-goodreads_without_bookid'],,7.7592453257413565,AutoTrain,Not Specified,Not Specified,Not Specified,0.579,1.024,0.36,,,433330541.0,True,2,0,['pytorch'],2022-11-20 17:07:17+00:00,2022-11-20 17:03:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2171169883
- CO2 Emissions (in grams): 7.7592

## Validation Metrics

- Loss: 1.024
- Accuracy: 0.579
- Macro F1: 0.360
- Micro F1: 0.579
- Weighted F1: 0.560
- Macro Precision: 0.383
- Micro Precision: 0.579
- Weighted Precision: 0.553
- Macro Recall: 0.353
- Micro Recall: 0.579
- Weighted Recall: 0.579


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fernanda-dionello/autotrain-goodreads_without_bookid-2171169883
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169883"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169883"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,55846995.78481203,0.443961661341853,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87119,autotrain-goodreads_without_bookid-2171169884,['fernanda-dionello/autotrain-data-goodreads_without_bookid'],,21.014243837592847,AutoTrain,Not Specified,Not Specified,Not Specified,0.666,0.815,0.454,,,1334477613.0,True,2,0,['pytorch'],2022-11-20 17:13:39+00:00,2022-11-20 17:04:02+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2171169884
- CO2 Emissions (in grams): 21.0142

## Validation Metrics

- Loss: 0.815
- Accuracy: 0.666
- Macro F1: 0.454
- Micro F1: 0.666
- Weighted F1: 0.649
- Macro Precision: 0.465
- Micro Precision: 0.666
- Weighted Precision: 0.638
- Macro Recall: 0.454
- Micro Recall: 0.666
- Weighted Recall: 0.666


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fernanda-dionello/autotrain-goodreads_without_bookid-2171169884
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169884"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fernanda-dionello/autotrain-goodreads_without_bookid-2171169884"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,63503479.98783203,0.5399357142857143,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87809,old-beta1,['LiveEvil/autotrain-data-copuml-production'],,0.9758714074673084,AutoTrain,Not Specified,Not Specified,Not Specified,0.701,1.092,0.416,,,438030701.0,True,2,0,['pytorch'],2022-11-22 17:15:53+00:00,2022-11-22 17:14:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2205570752
- CO2 Emissions (in grams): 0.9759

## Validation Metrics

- Loss: 1.092
- Accuracy: 0.701
- Macro F1: 0.416
- Micro F1: 0.701
- Weighted F1: 0.670
- Macro Precision: 0.399
- Micro Precision: 0.701
- Weighted Precision: 0.643
- Macro Recall: 0.436
- Micro Recall: 0.701
- Weighted Recall: 0.701


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/LiveEvil/autotrain-copuml-production-2205570752
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""LiveEvil/autotrain-copuml-production-2205570752"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""LiveEvil/autotrain-copuml-production-2205570752"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,448861087.2787294,0.5221414503133394,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87813,old-beta2,['LiveEvil/autotrain-data-copuml-la-beta-demo'],,1.2815143214785871,AutoTrain,Not Specified,Not Specified,Not Specified,0.747,1.085,0.513,,,556871407.0,True,2,1,['pytorch'],2022-11-22 17:37:01+00:00,2022-11-22 17:35:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2205770755
- CO2 Emissions (in grams): 1.2815

## Validation Metrics

- Loss: 1.085
- Accuracy: 0.747
- Macro F1: 0.513
- Micro F1: 0.747
- Weighted F1: 0.715
- Macro Precision: 0.533
- Micro Precision: 0.747
- Weighted Precision: 0.691
- Macro Recall: 0.515
- Micro Recall: 0.747
- Weighted Recall: 0.747


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/LiveEvil/autotrain-copuml-la-beta-demo-2205770755
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""LiveEvil/autotrain-copuml-la-beta-demo-2205770755"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""LiveEvil/autotrain-copuml-la-beta-demo-2205770755"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,434541696.2312932,0.6082714285714286,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87850,autotrain-healthcare_summarization_uta-2207670804,['Capstone/autotrain-data-healthcare_summarization_uta'],,7.541213010226726,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-11-22 19:48:33+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87876,autotrain-my-train-2209070896,['warrormac/autotrain-data-my-train'],,48.01845367300684,AutoTrain,Not Specified,Not Specified,Not Specified,,0.94,,,,4918417081.0,True,0,0,['pytorch'],2022-11-22 22:30:08+00:00,2022-11-22 21:56:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 2209070896
- CO2 Emissions (in grams): 48.0185

## Validation Metrics

- Loss: 0.940
- SacreBLEU: 37.030
- Gen len: 11.428",,,1,[],[],NLP,2022-11,102427644.05728556,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87959,whisper-medium-nlcv11,['mozilla-foundation/common_voice_11_0'],,2930.0,MLCO2,fine-tuning,"Ghent, Belgium",1 v100 GPU,,,,,,3055754841.0,False,2,3,"['tensorboard', 'transformers', 'pytorch']",2022-12-14 16:58:58+00:00,2022-11-23 06:21:29+00:00,"
# Whisper Medium nl - GeoffVdr

This model is a fine-tuned version of [openai/whisper-medium](https://huggingface.co/openai/whisper-medium) on the Common Voice 11.0 dataset.

## Model description
More information needed
## Intended uses & limitations
More information needed
## Training and evaluation data
- Training: Mozilla CommonVoice 11 Dutch train+validation set
- Evaluation: Mozilla CommonVoice 11 Dutch test set
## Training procedure

## Training Hyperparameters
- learning_rate: 1e-5
- train_batch_size: 8
- eval_batch_size: 8
- gradient_accumulation_steps: 2
- lr_scheduler_warmup_steps: 500
- training_steps: 12000

## Training Results

| Training Loss | Epoch | Step | WER  |
|:-------------:|:-----:|:----:|:----:|
| 0.1111        | 0.39  | 1000 | 9.89 |
| 0.0884        | 0.78  | 2000 | 9.26 |
| 0.0362        | 1.17  | 3000 | 8.64 |
| 0.0359        | 1.56  | 4000 | 8.60 |
| 0.0375        | 1.95  | 5000 | 8.24 |
:
:
| 0.0015        | 4.68  | 12000| 7.51 |


### Framework versions",,,1,[],[],Audio,2022-11,1042919.7409556314,,0.0,0.0,0.0,0.0,1,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
87983,autotrain-wikicat_es-2213570987,['crodri/autotrain-data-wikicat_es'],,10.4216765068249,AutoTrain,Not Specified,Not Specified,Not Specified,0.786,0.713,0.758,,,498681837.0,True,2,0,['pytorch'],2022-11-23 08:18:56+00:00,2022-11-23 08:07:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2213570987
- CO2 Emissions (in grams): 10.4217

## Validation Metrics

- Loss: 0.713
- Accuracy: 0.786
- Macro F1: 0.758
- Micro F1: 0.786
- Weighted F1: 0.785
- Macro Precision: 0.762
- Micro Precision: 0.786
- Weighted Precision: 0.787
- Macro Recall: 0.757
- Micro Recall: 0.786
- Weighted Recall: 0.786


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crodri/autotrain-wikicat_es-2213570987
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""crodri/autotrain-wikicat_es-2213570987"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""crodri/autotrain-wikicat_es-2213570987"", use_auth_token=True)

inputs = tokenizer(""El Fútbol Club Barcelona, conocido popularmente como Barça, es una entidad polideportiva con sede en Barcelona, España."", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,47850442.93722085,0.7717461139896373,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
88074,ell-phraseology,['huynhdoo/autotrain-data-ell-phraseology'],,3.8479338857094207,AutoTrain,Not Specified,Not Specified,Not Specified,,0.254,,,,328515693.0,True,0,0,['pytorch'],2022-11-23 13:20:30+00:00,2022-11-23 13:18:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 2217871115
- CO2 Emissions (in grams): 3.8479

## Validation Metrics

- Loss: 0.254
- MSE: 0.254
- MAE: 0.393
- R2: 0.413
- RMSE: 0.504
- Explained Variance: 0.414

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/huynhdoo/autotrain-ell-phraseology-2217871115
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""huynhdoo/autotrain-ell-phraseology-2217871115"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""huynhdoo/autotrain-ell-phraseology-2217871115"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,85374567.95192143,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
88076,ell-cohesion,['huynhdoo/autotrain-data-ell-cohesion'],,4.569992504332477,AutoTrain,Not Specified,Not Specified,Not Specified,,0.259,,,,328515693.0,True,10,0,['pytorch'],2022-11-23 13:30:47+00:00,2022-11-23 13:27:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 2217971118
- CO2 Emissions (in grams): 4.5700

## Validation Metrics

- Loss: 0.259
- MSE: 0.259
- MAE: 0.407
- R2: 0.416
- RMSE: 0.509
- Explained Variance: 0.427

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/huynhdoo/autotrain-ell-cohesion-2217971118
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""huynhdoo/autotrain-ell-cohesion-2217971118"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""huynhdoo/autotrain-ell-cohesion-2217971118"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,71885389.89693269,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
88077,ell-grammar,['huynhdoo/autotrain-data-ell-grammar'],,2.437473438795388,AutoTrain,Not Specified,Not Specified,Not Specified,,0.325,,,,328515693.0,True,0,0,['pytorch'],2022-11-23 13:31:50+00:00,2022-11-23 13:29:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 2218171131
- CO2 Emissions (in grams): 2.4375

## Validation Metrics

- Loss: 0.325
- MSE: 0.325
- MAE: 0.449
- R2: 0.342
- RMSE: 0.570
- Explained Variance: 0.425

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/huynhdoo/autotrain-ell-grammar-2218171131
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""huynhdoo/autotrain-ell-grammar-2218171131"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""huynhdoo/autotrain-ell-grammar-2218171131"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,134777137.576668,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
88078,ell-vocabulary,['huynhdoo/autotrain-data-ell-vocabulary'],,2.371997852718524,AutoTrain,Not Specified,Not Specified,Not Specified,,0.228,,,,328515693.0,True,0,0,['pytorch'],2022-11-23 13:33:26+00:00,2022-11-23 13:31:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 2218271145
- CO2 Emissions (in grams): 2.3720

## Validation Metrics

- Loss: 0.228
- MSE: 0.228
- MAE: 0.383
- R2: 0.343
- RMSE: 0.478
- Explained Variance: 0.402

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/huynhdoo/autotrain-ell-vocabulary-2218271145
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""huynhdoo/autotrain-ell-vocabulary-2218271145"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""huynhdoo/autotrain-ell-vocabulary-2218271145"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,138497466.43888876,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
88079,ell-conventions,['huynhdoo/autotrain-data-ell-conventions'],,2.6341173422087247,AutoTrain,Not Specified,Not Specified,Not Specified,,0.259,,,,328515693.0,True,0,0,['pytorch'],2022-11-23 13:34:18+00:00,2022-11-23 13:32:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 2218371153
- CO2 Emissions (in grams): 2.6341

## Validation Metrics

- Loss: 0.259
- MSE: 0.259
- MAE: 0.402
- R2: 0.426
- RMSE: 0.509
- Explained Variance: 0.439

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/huynhdoo/autotrain-ell-conventions-2218371153
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""huynhdoo/autotrain-ell-conventions-2218371153"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""huynhdoo/autotrain-ell-conventions-2218371153"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,124715663.85290088,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
88080,ell-syntax,['huynhdoo/autotrain-data-ell-syntax'],,6.2662711223675815,AutoTrain,Not Specified,Not Specified,Not Specified,,0.237,,,,328515693.0,True,0,0,['pytorch'],2022-11-23 13:37:04+00:00,2022-11-23 13:33:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 2218471162
- CO2 Emissions (in grams): 6.2663

## Validation Metrics

- Loss: 0.237
- MSE: 0.237
- MAE: 0.393
- R2: 0.438
- RMSE: 0.487
- Explained Variance: 0.477

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/huynhdoo/autotrain-ell-syntax-2218471162
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""huynhdoo/autotrain-ell-syntax-2218471162"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""huynhdoo/autotrain-ell-syntax-2218471162"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,52426026.03442366,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
89008,autotrain-mt5_xlsum_msamsum-2231571360,['rolosaCBTech/autotrain-data-mt5_xlsum_msamsum'],,52.2418341683463,AutoTrain,Not Specified,Not Specified,Not Specified,,1.589,,0.43587,0.3832,2329700173.0,True,3,0,['pytorch'],2022-11-24 15:21:26+00:00,2022-11-24 14:55:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2231571360
- CO2 Emissions (in grams): 52.2418

## Validation Metrics

- Loss: 1.589
- Rouge1: 43.587
- Rouge2: 22.929
- RougeL: 38.320
- RougeLsum: 38.089
- Gen Len: 23.965

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/rolosaCBTech/autotrain-mt5_xlsum_msamsum-2231571360
```",,,1,[],[],NLP,2022-11,44594532.52526846,0.4078415373533397,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
89285,autotrain-25112022-cert-2235671445,['teacookies/autotrain-data-25112022-cert'],,18.043415297084504,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.003,0.986,,,667186033.0,True,0,0,['pytorch'],2022-11-25 03:30:26+00:00,2022-11-25 03:20:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2235671445
- CO2 Emissions (in grams): 18.0434

## Validation Metrics

- Loss: 0.003
- Accuracy: 0.999
- Precision: 0.985
- Recall: 0.987
- F1: 0.986

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-25112022-cert-2235671445
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-25112022-cert-2235671445"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-25112022-cert-2235671445"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,36976704.3552894,0.9924574307304784,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
89319,autotrain-25112022-cert2-2236171465,['teacookies/autotrain-data-25112022-cert2'],,17.128542622750768,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.003,0.986,,,667186033.0,True,27,0,['pytorch'],2022-11-25 05:41:55+00:00,2022-11-25 05:32:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2236171465
- CO2 Emissions (in grams): 17.1285

## Validation Metrics

- Loss: 0.003
- Accuracy: 0.999
- Precision: 0.985
- Recall: 0.986
- F1: 0.986

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-25112022-cert2-2236171465
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-25112022-cert2-2236171465"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-25112022-cert2-2236171465"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,38951710.46915682,0.9924574307304784,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
90152,autotrain-analyze-vehicle-images-2256371855,['johnhabeck/autotrain-data-analyze-vehicle-images'],,2.9415774139015696,AutoTrain,Not Specified,Not Specified,Not Specified,0.932,0.336,0.87,,,347649727.0,True,1,0,['pytorch'],2022-11-27 17:40:25+00:00,2022-11-27 17:37:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2256371855
- CO2 Emissions (in grams): 2.9416

## Validation Metrics

- Loss: 0.336
- Accuracy: 0.932
- Macro F1: 0.870
- Micro F1: 0.932
- Weighted F1: 0.926
- Macro Precision: 0.896
- Micro Precision: 0.932
- Weighted Precision: 0.937
- Macro Recall: 0.864
- Micro Recall: 0.932
- Weighted Recall: 0.932",,,1,[],[],Computer Vision,2022-11,118184796.1427246,0.8999334073251943,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
90425,autotrain-country-to-country-code-2-2266072007,['AiBototicus/autotrain-data-country-to-country-code-2'],,0.0233638657506133,AutoTrain,Not Specified,Not Specified,Not Specified,0.94,0.556,0.741,,,1335014701.0,True,2,1,['pytorch'],2022-11-28 13:03:30+00:00,2022-11-28 12:59:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2266072007
- CO2 Emissions (in grams): 0.0234

## Validation Metrics

- Loss: 0.556
- Accuracy: 0.940
- Macro F1: 0.741
- Micro F1: 0.940
- Weighted F1: 0.923
- Macro Precision: 0.732
- Micro Precision: 0.940
- Weighted Precision: 0.911
- Macro Recall: 0.759
- Micro Recall: 0.940
- Weighted Recall: 0.940


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/AiBototicus/autotrain-country-to-country-code-2-2266072007
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""AiBototicus/autotrain-country-to-country-code-2-2266072007"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""AiBototicus/autotrain-country-to-country-code-2-2266072007"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-11,57140146037.90282,0.8287209994051159,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
90601,led-large-legal-summary,['0-hero/autotrain-data-legal-summarisation'],,0.1413928133684925,AutoTrain,Not Specified,Not Specified,Not Specified,,2.098,,0.36855,0.33547,1839604721.0,True,2,0,['pytorch'],2022-11-28 20:49:11+00:00,2022-11-28 20:34:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2269972136
- CO2 Emissions (in grams): 0.1414

## Validation Metrics

- Loss: 2.098
- Rouge1: 36.855
- Rouge2: 22.050
- RougeL: 33.547
- RougeLsum: 34.607
- Gen Len: 27.633

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/0-hero/autotrain-legal-summarisation-2269972136
```",,,1,[],[],NLP,2022-11,13010595639.013796,0.3512328300332377,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
91838,bert-autotrain-1,['NicholasSynovic/autotrain-data-test'],,0.6063524119726759,AutoTrain,Not Specified,Not Specified,Not Specified,0.876,0.345,0.914,,,438006125.0,True,2,0,['pytorch'],2022-12-01 17:54:20+00:00,2022-12-01 17:53:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2306572976
- CO2 Emissions (in grams): 0.6064

## Validation Metrics

- Loss: 0.345
- Accuracy: 0.876
- Precision: 0.909
- Recall: 0.920
- AUC: 0.924
- F1: 0.914

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/NicholasSynovic/autotrain-test-2306572976
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""NicholasSynovic/autotrain-test-2306572976"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""NicholasSynovic/autotrain-test-2306572976"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,722362303.4251869,0.8945966480446926,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
91854,anchored-ant,['abdalrahmanshahrour/autotrain-data-shahroursummarizer'],,69.23178007707939,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-12-01 18:57:22+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
91877,auto_train,['alanila/autotrain-data-training'],,1.2620473255629745,AutoTrain,Not Specified,Not Specified,Not Specified,0.517,1.279,0.549,,,433345901.0,True,3,0,['pytorch'],2022-12-01 20:25:12+00:00,2022-12-01 19:29:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2307973004
- CO2 Emissions (in grams): 1.2620

## Validation Metrics

- Loss: 1.279
- Accuracy: 0.517
- Macro F1: 0.549
- Micro F1: 0.517
- Weighted F1: 0.443
- Macro Precision: 0.585
- Micro Precision: 0.517
- Weighted Precision: 0.480
- Macro Recall: 0.572
- Micro Recall: 0.517
- Weighted Recall: 0.517


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/alanila/autotrain-training-2307973004
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alanila/auto_train"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""alanila/auto_train"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,343367393.7755804,0.5325196998123828,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
91878,autotrain-training-2307973005,['alanila/autotrain-data-training'],,3.7679548759427006,AutoTrain,Not Specified,Not Specified,Not Specified,0.508,1.098,0.559,,,1334498093.0,True,2,0,['pytorch'],2022-12-01 19:32:39+00:00,2022-12-01 19:29:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2307973005
- CO2 Emissions (in grams): 3.7680

## Validation Metrics

- Loss: 1.098
- Accuracy: 0.508
- Macro F1: 0.559
- Micro F1: 0.508
- Weighted F1: 0.452
- Macro Precision: 0.610
- Micro Precision: 0.508
- Weighted Precision: 0.537
- Macro Recall: 0.581
- Micro Recall: 0.508
- Weighted Recall: 0.508


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/alanila/autotrain-training-2307973005
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alanila/autotrain-training-2307973005"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""alanila/autotrain-training-2307973005"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,354170401.96536946,0.5322811621368323,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
91889,test2,['NicholasSynovic/autotrain-data-test2'],,0.9679834304164028,AutoTrain,Not Specified,Not Specified,Not Specified,0.812,0.415,0.872,,,439084397.0,True,2,0,['pytorch'],2022-12-01 19:54:15+00:00,2022-12-01 19:53:21+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2308373008
- CO2 Emissions (in grams): 0.9680

## Validation Metrics

- Loss: 0.415
- Accuracy: 0.812
- Precision: 0.851
- Recall: 0.895
- AUC: 0.861
- F1: 0.872

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/NicholasSynovic/autotrain-test2-2308373008
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""NicholasSynovic/autotrain-test2-2308373008"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""NicholasSynovic/autotrain-test2-2308373008"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,453607348.2281785,0.8409311163895489,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92050,autotrain-faseiii_diciembre-2311773112,['al02783013/autotrain-data-faseiii_diciembre'],,4.041080293052415,AutoTrain,Not Specified,Not Specified,Not Specified,,5487.957,,,,,True,2,0,['joblib'],2022-12-02 05:51:59+00:00,2022-12-02 05:47:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 2311773112
- CO2 Emissions (in grams): 4.0411

## Validation Metrics

- Loss: 5487.957
- R2: 0.960
- MSE: 30117668.000
- MAE: 2082.499
- RMSLE: 1.918

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-12,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92071,autotrain-faseiii_final-2312773135,['al02783013/autotrain-data-faseiii_final'],,2.814484312003443,AutoTrain,Not Specified,Not Specified,Not Specified,0.996,0.03,0.985,,,433318253.0,True,3,0,['pytorch'],2022-12-02 07:36:28+00:00,2022-12-02 07:35:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2312773135
- CO2 Emissions (in grams): 2.8145

## Validation Metrics

- Loss: 0.030
- Accuracy: 0.996
- Precision: 1.000
- Recall: 0.971
- AUC: 0.993
- F1: 0.985

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/al02783013/autotrain-faseiii_final-2312773135
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""al02783013/autotrain-faseiii_final-2312773135"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""al02783013/autotrain-faseiii_final-2312773135"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,153960088.2307103,0.9904694598687532,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92159,autotrain-textcat-paul-2315373253,['poldham/autotrain-data-textcat-paul'],,7.014613433979796,AutoTrain,Not Specified,Not Specified,Not Specified,0.944,0.183,0.942,,,737766955.0,True,2,0,['pytorch'],2022-12-02 12:03:44+00:00,2022-12-02 12:00:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2315373253
- CO2 Emissions (in grams): 7.0146

## Validation Metrics

- Loss: 0.183
- Accuracy: 0.944
- Precision: 0.953
- Recall: 0.931
- AUC: 0.974
- F1: 0.942

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/poldham/autotrain-textcat-paul-2315373253
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""poldham/autotrain-textcat-paul-2315373253"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""poldham/autotrain-textcat-paul-2315373253"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,105175710.95595244,0.9429989395546128,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92295,autotrain-what-animal-are-you-2318373345,['micole66/autotrain-data-what-animal-are-you'],,0.442974318420001,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.003,1.0,,,344458929.0,True,0,1,['pytorch'],2022-12-02 18:10:28+00:00,2022-12-02 18:09:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2318373345
- CO2 Emissions (in grams): 0.4430

## Validation Metrics

- Loss: 0.003
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2022-12,777604738.4160209,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92865,autotrain-4-2331273589,['cnulatienpo/autotrain-data-4'],,0.9127749760143646,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-12-04 04:06:26+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92867,autotrain-whatsapp_chat_summarization-2331373597,['dippatel11/autotrain-data-whatsapp_chat_summarization'],,8.016185733770428,AutoTrain,Not Specified,Not Specified,Not Specified,,1.547,,0.45685,0.38559,1625537793.0,True,11,0,['pytorch'],2022-12-04 04:50:24+00:00,2022-12-04 04:46:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2331373597
- CO2 Emissions (in grams): 8.0162

## Validation Metrics

- Loss: 1.547
- Rouge1: 45.685
- Rouge2: 22.879
- RougeL: 38.559
- RougeLsum: 42.130
- Gen Len: 18.555

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/dippatel11/autotrain-whatsapp_chat_summarization-2331373597
```",,,1,[],[],NLP,2022-12,202781952.28835168,0.4182061428707088,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92882,autotrain-dippatel_summarizer-2331873599,['dippatel11/autotrain-data-dippatel_summarizer'],,68.41274041098731,AutoTrain,Not Specified,Not Specified,Not Specified,,1.513,,0.49434,0.41176,2279605745.0,True,1,0,['pytorch'],2022-12-04 07:13:37+00:00,2022-12-04 06:39:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2331873599
- CO2 Emissions (in grams): 68.4127

## Validation Metrics

- Loss: 1.513
- Rouge1: 49.434
- Rouge2: 24.817
- RougeL: 41.176
- RougeLsum: 44.737
- Gen Len: 18.258

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/dippatel11/autotrain-dippatel_summarizer-2331873599
```",,,1,[],[],NLP,2022-12,33321362.823727608,0.4492869184416731,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92883,autotrain-dippatel_summarizer-2331873598,['dippatel11/autotrain-data-dippatel_summarizer'],,71.50478540100151,AutoTrain,Not Specified,Not Specified,Not Specified,,1.513,,0.49414,0.41157,2279605745.0,True,0,0,['pytorch'],2022-12-04 07:13:50+00:00,2022-12-04 06:40:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2331873598
- CO2 Emissions (in grams): 71.5048

## Validation Metrics

- Loss: 1.513
- Rouge1: 49.414
- Rouge2: 24.843
- RougeL: 41.157
- RougeLsum: 44.732
- Gen Len: 18.258

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/dippatel11/autotrain-dippatel_summarizer-2331873598
```",,,1,[],[],NLP,2022-12,31880464.114616744,0.4490912097691314,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92893,autotrain-bart-2332573622,['dippatel11/autotrain-data-bart'],,17.308721714114615,AutoTrain,Not Specified,Not Specified,Not Specified,,1.46,,0.40163,0.30916,1625533697.0,True,1,0,['pytorch'],2022-12-04 08:02:59+00:00,2022-12-04 07:53:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2332573622
- CO2 Emissions (in grams): 17.3087

## Validation Metrics

- Loss: 1.460
- Rouge1: 40.163
- Rouge2: 20.060
- RougeL: 30.916
- RougeLsum: 37.538
- Gen Len: 60.370

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/dippatel11/autotrain-bart-2332573622
```",,,1,[],[],NLP,2022-12,93914139.00163628,0.3493800723139042,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92901,autotrain-bart-large-samsum-lid-2333073627,['dippatel11/autotrain-data-bart-large-samsum-lid'],,4.671853339537159,AutoTrain,Not Specified,Not Specified,Not Specified,,1.499,,0.47617,0.39771,557969145.0,True,1,0,['pytorch'],2022-12-04 08:27:27+00:00,2022-12-04 08:24:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2333073627
- CO2 Emissions (in grams): 4.6719

## Validation Metrics

- Loss: 1.499
- Rouge1: 47.617
- Rouge2: 23.262
- RougeL: 39.771
- RougeLsum: 43.344
- Gen Len: 18.088

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/dippatel11/autotrain-bart-large-samsum-lid-2333073627
```",,,1,[],[],NLP,2022-12,119432076.40488093,0.4334177935185609,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
92916,autotrain-t5-base-samsum-2333773650,['jdminor/autotrain-data-t5-base-samsum'],,49.83285188931273,AutoTrain,Not Specified,Not Specified,Not Specified,,0.987,,0.52508,0.44903,3132789733.0,True,1,0,['pytorch'],2022-12-04 10:48:14+00:00,2022-12-04 10:21:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2333773650
- CO2 Emissions (in grams): 49.8329

## Validation Metrics

- Loss: 0.987
- Rouge1: 52.508
- Rouge2: 29.367
- RougeL: 44.903
- RougeLsum: 48.446
- Gen Len: 17.123

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/jdminor/autotrain-t5-base-samsum-2333773650
```",,,1,[],[],NLP,2022-12,62865953.16596491,0.48408634014639,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
93133,autotrain-acc-2337673709,['alanila/autotrain-data-acc'],,1.6543357301983936,AutoTrain,Not Specified,Not Specified,Not Specified,0.492,1.331,0.457,,,438033773.0,True,3,0,['pytorch'],2022-12-05 01:34:58+00:00,2022-12-05 01:33:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2337673709
- CO2 Emissions (in grams): 1.6543

## Validation Metrics

- Loss: 1.331
- Accuracy: 0.492
- Macro F1: 0.457
- Micro F1: 0.492
- Weighted F1: 0.423
- Macro Precision: 0.464
- Micro Precision: 0.492
- Weighted Precision: 0.420
- Macro Recall: 0.484
- Micro Recall: 0.492
- Weighted Recall: 0.492


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/alanila/autotrain-acc-2337673709
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alanila/autotrain-acc-2337673709"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""alanila/autotrain-acc-2337673709"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,264779249.4619393,0.473854583772392,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
93141,autotrain-t5-large-summary-2338073717,['jdminor/autotrain-data-t5-large-summary'],,0.2958140546196442,AutoTrain,Not Specified,Not Specified,Not Specified,,1.536,,0.45911,0.36497,990450547.0,True,132,0,['pytorch'],2022-12-05 02:30:14+00:00,2022-12-05 02:00:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2338073717
- CO2 Emissions (in grams): 0.2958

## Validation Metrics

- Loss: 1.536
- Rouge1: 45.911
- Rouge2: 18.396
- RougeL: 36.497
- RougeLsum: 40.822
- Gen Len: 23.070

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/jdminor/autotrain-t5-large-summary-2338073717
```",,,1,[],[],NLP,2022-12,3348220044.086529,0.4066628887972041,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
93147,autotrain-pegasus-large-summary-2.0-2338573727,['jdminor/autotrain-data-pegasus-large-summary-2.0'],,74.34647142824745,AutoTrain,Not Specified,Not Specified,Not Specified,,1.562,,0.45675,0.3675,2274842165.0,True,0,0,['pytorch'],2022-12-05 03:04:02+00:00,2022-12-05 02:28:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2338573727
- CO2 Emissions (in grams): 74.3465

## Validation Metrics

- Loss: 1.562
- Rouge1: 45.675
- Rouge2: 19.602
- RougeL: 36.750
- RougeLsum: 40.715
- Gen Len: 17.977

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/jdminor/autotrain-pegasus-large-summary-2.0-2338573727
```",,,1,[],[],NLP,2022-12,30597849.787605237,0.4072929936305732,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
93512,autotrain-ciap-2345673819,['famube/autotrain-data-ciap'],,4.04378848235129,AutoTrain,Not Specified,Not Specified,Not Specified,0.631,2.342,0.542,,,1338184941.0,True,2,0,['pytorch'],2022-12-05 20:28:40+00:00,2022-12-05 19:41:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2345673819
- CO2 Emissions (in grams): 4.0438

## Validation Metrics

- Loss: 2.342
- Accuracy: 0.631
- Macro F1: 0.542
- Micro F1: 0.631
- Weighted F1: 0.546
- Macro Precision: 0.514
- Micro Precision: 0.631
- Weighted Precision: 0.515
- Macro Recall: 0.617
- Micro Recall: 0.631
- Weighted Recall: 0.631


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/famube/autotrain-ciap-2345673819
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""famube/autotrain-ciap-2345673819"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""famube/autotrain-ciap-2345673819"", use_auth_token=True)

inputs = tokenizer(""dor de cabeça"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,330923574.9694561,0.5831236146632566,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
93581,autotrain-acc_keys-2347073860,['alanila/autotrain-data-acc_keys'],,1.3599341780747405,AutoTrain,Not Specified,Not Specified,Not Specified,0.5,1.255,0.445,,,438033773.0,True,3,0,"['transformers', 'pytorch']",2022-12-05 22:34:34+00:00,2022-12-05 22:27:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2347073860
- CO2 Emissions (in grams): 1.3599

## Validation Metrics

- Loss: 1.255
- Accuracy: 0.500
- Macro F1: 0.445
- Micro F1: 0.500
- Weighted F1: 0.421
- Macro Precision: 0.498
- Micro Precision: 0.500
- Weighted Precision: 0.508
- Macro Recall: 0.481
- Micro Recall: 0.500
- Weighted Recall: 0.500


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/alanila/autotrain-acc_keys-2347073860
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alanila/autotrain-acc_keys-2347073860"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""alanila/autotrain-acc_keys-2347073860"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,322099245.7297636,0.4708994708994709,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
93589,autotrain-ciap2-2347173866,['famube/autotrain-data-ciap2'],,4.825567476024859,AutoTrain,Not Specified,Not Specified,Not Specified,0.681,1.932,0.609,,,1338184941.0,True,2,0,['pytorch'],2022-12-05 23:32:01+00:00,2022-12-05 22:49:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2347173866
- CO2 Emissions (in grams): 4.8256

## Validation Metrics

- Loss: 1.932
- Accuracy: 0.681
- Macro F1: 0.609
- Micro F1: 0.681
- Weighted F1: 0.622
- Macro Precision: 0.592
- Micro Precision: 0.681
- Weighted Precision: 0.610
- Macro Recall: 0.669
- Micro Recall: 0.681
- Weighted Recall: 0.681


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/famube/autotrain-ciap2-2347173866
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""famube/autotrain-ciap2-2347173866"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""famube/autotrain-ciap2-2347173866"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,277311414.18052495,0.6429906976744186,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
93686,autotrain-tc_ac-2349273884,['alanila/autotrain-data-tc_ac'],,1.196433244085964,AutoTrain,Not Specified,Not Specified,Not Specified,0.517,1.271,0.465,,,438033773.0,True,2,0,['pytorch'],2022-12-06 04:51:06+00:00,2022-12-06 04:49:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2349273884
- CO2 Emissions (in grams): 1.1964

## Validation Metrics

- Loss: 1.271
- Accuracy: 0.517
- Macro F1: 0.465
- Micro F1: 0.517
- Weighted F1: 0.437
- Macro Precision: 0.495
- Micro Precision: 0.517
- Weighted Precision: 0.488
- Macro Recall: 0.501
- Micro Recall: 0.517
- Weighted Recall: 0.517


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/alanila/autotrain-tc_ac-2349273884
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alanila/autotrain-tc_ac-2349273884"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""alanila/autotrain-tc_ac-2349273884"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,366116350.54878765,0.489623217922607,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
93758,short-description-generator-6k,['giuliobrugnaro/autotrain-data-short-description-generator'],,135.05744541394728,AutoTrain,Not Specified,Not Specified,Not Specified,,2.306,,0.30629,0.27506,2950844807.0,True,65,0,['pytorch'],2022-12-06 09:28:54+00:00,2022-12-06 08:27:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2350573905
- CO2 Emissions (in grams): 135.0574

## Validation Metrics

- Loss: 2.306
- Rouge1: 30.629
- Rouge2: 10.966
- RougeL: 27.506
- RougeLsum: 27.571
- Gen Len: 18.191

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/giuliobrugnaro/autotrain-short-description-generator-2350573905
```",,,1,[],[],NLP,2022-12,21848812.54014352,0.2898361654769072,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
94463,autotrain-07122022-2-exam_cert-2364774382,['teacookies/autotrain-data-07122022-2-exam_cert'],,24.71153691821318,AutoTrain,Not Specified,Not Specified,Not Specified,0.995,0.021,0.924,,,667210609.0,True,0,0,['pytorch'],2022-12-07 08:44:08+00:00,2022-12-07 08:29:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2364774382
- CO2 Emissions (in grams): 24.7115

## Validation Metrics

- Loss: 0.021
- Accuracy: 0.995
- Precision: 0.917
- Recall: 0.932
- F1: 0.924

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-07122022-2-exam_cert-2364774382
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-07122022-2-exam_cert-2364774382"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-07122022-2-exam_cert-2364774382"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,26999964.07379441,0.9581865554976552,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
94880,autotrain-07-12-2022-exam-cert3-2365574505,['teacookies/autotrain-data-07-12-2022-exam-cert3'],,26.625950331996258,AutoTrain,Not Specified,Not Specified,Not Specified,0.995,0.02,0.937,,,667210609.0,True,0,0,['pytorch'],2022-12-08 02:10:10+00:00,2022-12-08 01:55:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2365574505
- CO2 Emissions (in grams): 26.6260

## Validation Metrics

- Loss: 0.020
- Accuracy: 0.995
- Precision: 0.929
- Recall: 0.945
- F1: 0.937

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-07-12-2022-exam-cert3-2365574505
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-07-12-2022-exam-cert3-2365574505"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-07-12-2022-exam-cert3-2365574505"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,25058658.965431057,0.9651293995859214,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95033,subtitle-v2,[''],,4.325625308048844,AutoTrain,Not Specified,Not Specified,Not Specified,,2.08,,0.4188199999999999,0.39859,1625537793.0,True,26,0,"['transformers', 'pytorch']",2022-12-08 09:22:52+00:00,2022-12-08 09:09:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2343773834
- CO2 Emissions (in grams): 4.3256

## Validation Metrics

- Loss: 2.080
- Rouge1: 41.882
- Rouge2: 18.838
- RougeL: 39.859
- RougeLsum: 39.919
- Gen Len: 14.611

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Alfred-o/autotrain-title-2343773834
```",,,1,[],[],NLP,2022-12,375792556.5062939,0.4084546648560697,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95036,improve-a-v2,[''],,1984.9185935944208,AutoTrain,Not Specified,Not Specified,Not Specified,,0.279,,0.63536,0.63217,557969145.0,True,18,0,"['transformers', 'pytorch']",2022-12-08 09:25:58+00:00,2022-12-08 09:23:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2346173853
- CO2 Emissions (in grams): 1984.9186

## Validation Metrics

- Loss: 0.279
- Rouge1: 63.536
- Rouge2: 40.297
- RougeL: 63.217
- RougeLsum: 63.205
- Gen Len: 13.697

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Alfred-o/autotrain-improve-a-2346173853
```",,,1,[],[],NLP,2022-12,281104.29656945926,0.6337609858543782,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95037,tag-h-v2,[''],,2510.751427379945,AutoTrain,Not Specified,Not Specified,Not Specified,,1.66,,0.52842,0.52252,1625537793.0,True,27,0,"['transformers', 'pytorch']",2022-12-08 09:32:35+00:00,2022-12-08 09:25:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2346673849
- CO2 Emissions (in grams): 2510.7514

## Validation Metrics

- Loss: 1.660
- Rouge1: 52.842
- Rouge2: 28.064
- RougeL: 52.252
- RougeLsum: 52.203
- Gen Len: 11.330

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Alfred-o/autotrain-tag-h-2346673849
```",,,1,[],[],NLP,2022-12,647430.7951290519,0.5254534386358879,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95038,href-v2,[''],,2295.967887402507,AutoTrain,Not Specified,Not Specified,Not Specified,,2.208,,0.2691,0.2480399999999999,1625537793.0,True,0,0,"['transformers', 'pytorch']",2022-12-08 09:33:29+00:00,2022-12-08 09:26:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2346773844
- CO2 Emissions (in grams): 2295.9679

## Validation Metrics

- Loss: 2.208
- Rouge1: 26.910
- Rouge2: 14.562
- RougeL: 24.804
- RougeLsum: 25.155
- Gen Len: 19.930

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Alfred-o/autotrain-tag-href-2346773844
```",,,1,[],[],NLP,2022-12,707996.7459122507,0.2581411764705882,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95040,tag-caption-v2,[''],,5.33165635098582,AutoTrain,Not Specified,Not Specified,Not Specified,,2.315,,0.39655,0.38522,891700799.0,True,20,0,"['transformers', 'pytorch']",2022-12-08 09:31:31+00:00,2022-12-08 09:27:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2343573836
- CO2 Emissions (in grams): 5.3317

## Validation Metrics

- Loss: 2.315
- Rouge1: 39.655
- Rouge2: 17.178
- RougeL: 38.522
- RougeLsum: 38.554
- Gen Len: 12.059

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Alfred-o/autotrain-tag-figcaption-2343573836
```",,,1,[],[],NLP,2022-12,167246487.8264566,0.3908028985507246,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95073,autotrain-visaboxx-2379774567,['vargha/autotrain-data-visaboxx'],,34.58613148731854,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-12-08 11:42:33+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95265,autotrain-finance-consumer-complaints-issue-classification-2380974646,['ChrisPCz/autotrain-data-finance-consumer-complaints-issue-classification'],,89.45199951708611,AutoTrain,Not Specified,Not Specified,Not Specified,0.782,0.606,0.78,,,438015341.0,True,2,0,['pytorch'],2022-12-08 18:03:52+00:00,2022-12-08 17:20:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2380974646
- CO2 Emissions (in grams): 89.4520

## Validation Metrics

- Loss: 0.606
- Accuracy: 0.782
- Macro F1: 0.780
- Micro F1: 0.782
- Weighted F1: 0.780
- Macro Precision: 0.780
- Micro Precision: 0.782
- Weighted Precision: 0.780
- Macro Recall: 0.782
- Micro Recall: 0.782
- Weighted Recall: 0.782


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ChrisPCz/autotrain-finance-consumer-complaints-issue-classification-2380974646
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ChrisPCz/autotrain-finance-consumer-complaints-issue-classification-2380974646"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ChrisPCz/autotrain-finance-consumer-complaints-issue-classification-2380974646"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,4896652.32040269,0.7809987195902689,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95298,autotrain-test_row2-2384274652,['Efimov6886/autotrain-data-test_row2'],,1.2351509468215165,AutoTrain,Not Specified,Not Specified,Not Specified,0.97,0.178,0.965,,,347608767.0,True,0,0,['pytorch'],2022-12-08 18:42:15+00:00,2022-12-08 18:41:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2384274652
- CO2 Emissions (in grams): 1.2352

## Validation Metrics

- Loss: 0.178
- Accuracy: 0.970
- Macro F1: 0.965
- Micro F1: 0.970
- Weighted F1: 0.970
- Macro Precision: 0.977
- Micro Precision: 0.970
- Weighted Precision: 0.972
- Macro Recall: 0.956
- Micro Recall: 0.970
- Weighted Recall: 0.970",,,1,[],[],Computer Vision,2022-12,281430191.098927,0.9674935400516796,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95394,autotrain-medical-2387774761,['qiaokuoyuan/autotrain-data-medical-cfa966ee'],,0.7237073793849912,AutoTrain,Not Specified,Not Specified,Not Specified,0.99,0.032,0.0,,,406784817.0,True,1,0,['pytorch'],2022-12-08 23:15:46+00:00,2022-12-08 23:15:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2387774761
- CO2 Emissions (in grams): 0.7237

## Validation Metrics

- Loss: 0.032
- Accuracy: 0.990
- Precision: 0.000
- Recall: 0.000
- F1: 0.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/qiaokuoyuan/autotrain-medical-2387774761
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""qiaokuoyuan/autotrain-medical-2387774761"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""qiaokuoyuan/autotrain-medical-2387774761"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,562084660.993352,0.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95555,autotrain-fine_tune_tscholak-2392374839,['Aman6917/autotrain-data-fine_tune_tscholak'],,11.023749088725204,AutoTrain,Not Specified,Not Specified,Not Specified,,0.128,,0.94982,0.94629,3132576741.0,True,0,0,['pytorch'],2022-12-09 09:39:56+00:00,2022-12-09 09:30:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2392374839
- CO2 Emissions (in grams): 11.0237

## Validation Metrics

- Loss: 0.128
- Rouge1: 94.982
- Rouge2: 91.105
- RougeL: 94.629
- RougeLsum: 94.535
- Gen Len: 30.359

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Aman6917/autotrain-fine_tune_tscholak-2392374839
```",,,1,[],[],NLP,2022-12,284166186.63825685,0.9480517140883178,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95556,autotrain-fine_tune_tscholak-2392374841,['Aman6917/autotrain-data-fine_tune_tscholak'],,11.39130569365672,AutoTrain,Not Specified,Not Specified,Not Specified,,0.138,,0.92363,0.92026,3132576741.0,True,0,0,['pytorch'],2022-12-09 09:40:56+00:00,2022-12-09 09:30:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2392374841
- CO2 Emissions (in grams): 11.3913

## Validation Metrics

- Loss: 0.138
- Rouge1: 92.363
- Rouge2: 88.743
- RougeL: 92.026
- RougeLsum: 91.574
- Gen Len: 35.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Aman6917/autotrain-fine_tune_tscholak-2392374841
```",,,1,[],[],NLP,2022-12,274997162.3309507,0.921941920396553,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95690,autotrain-dreambooth-marsupilami,['julien-c/autotrain-dreambooth-marsupilami-data'],,56.66697547427793,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,1,0,['diffusers'],2023-01-27 17:22:24+00:00,2022-12-09 15:11:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Dreambooth
- Model ID: 17516376
- CO2 Emissions (in grams): 56.6670

Prompt token: `mrspilmi`

TODO(Add some generated images here)",,,1,[],[],Multimodal,2022-12,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95702,khu-text-classification-roberta-base-sept-2022,['sabhashanki/autotrain-data-khul-classify'],,2.8092927891228863,AutoTrain,Not Specified,Not Specified,Not Specified,0.84,0.635,0.834,,,498708213.0,True,2,0,['pytorch'],2022-12-14 17:31:08+00:00,2022-12-09 15:52:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2396974951
- CO2 Emissions (in grams): 2.8093

## Validation Metrics

- Loss: 0.635
- Accuracy: 0.840
- Macro F1: 0.834
- Micro F1: 0.840
- Weighted F1: 0.837
- Macro Precision: 0.839
- Micro Precision: 0.840
- Weighted Precision: 0.840
- Macro Recall: 0.836
- Micro Recall: 0.840
- Weighted Recall: 0.840


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sabhashanki/autotrain-topic-prediction-latest-2396974951
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sabhashanki/autotrain-topic-prediction-latest-2396974951"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sabhashanki/autotrain-topic-prediction-latest-2396974951"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,177520910.22015047,0.836989247311828,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
95703,text_classification-roberta_base_sept2022,['sabhashanki/autotrain-data-topic-prediction-latest'],,2.029312802544045,AutoTrain,Not Specified,Not Specified,Not Specified,0.846,0.656,0.826,,,498709549.0,True,3,0,['pytorch'],2022-12-14 17:21:54+00:00,2022-12-09 15:52:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2396974952
- CO2 Emissions (in grams): 2.0293

## Validation Metrics

- Loss: 0.656
- Accuracy: 0.846
- Macro F1: 0.826
- Micro F1: 0.846
- Weighted F1: 0.842
- Macro Precision: 0.867
- Micro Precision: 0.846
- Weighted Precision: 0.861
- Macro Recall: 0.829
- Micro Recall: 0.846
- Weighted Recall: 0.846


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sabhashanki/autotrain-topic-prediction-latest-2396974952
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sabhashanki/autotrain-topic-prediction-latest-2396974952"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sabhashanki/autotrain-topic-prediction-latest-2396974952"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,245752921.0749538,0.8358803827751198,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
96042,autotrain-disparities_pubmed_mit-2407875110,['eber/autotrain-data-disparities_pubmed_mit'],,0.7254914669916961,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-12-10 16:16:47+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
96207,row3_96,['Efimov6886/autotrain-data-row3'],,0.3575974781341657,AutoTrain,Not Specified,Not Specified,Not Specified,0.96,0.268,0.946,,,343276209.0,True,1,0,['pytorch'],2022-12-10 21:31:29+00:00,2022-12-10 21:30:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2405775204
- CO2 Emissions (in grams): 0.3576

## Validation Metrics

- Loss: 0.268
- Accuracy: 0.960
- Macro F1: 0.946
- Micro F1: 0.960
- Weighted F1: 0.960
- Macro Precision: 0.966
- Micro Precision: 0.960
- Weighted Precision: 0.964
- Macro Recall: 0.934
- Micro Recall: 0.960
- Weighted Recall: 0.960",,,1,[],[],Computer Vision,2022-12,959951425.807336,0.9529485834207766,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
96347,chinese_spam_detect,['paulkm/autotrain-data-spam_full'],,0.8147876753762673,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-12-11 05:02:49+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
96585,autotrain-edsa-sa-language-classification-2420175384,['techmalik/autotrain-data-edsa-sa-language-classification'],,2.255966091405494,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-12-11 13:24:03+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
96593,autotrain-lottery_v2-2420075390,['paulkm/autotrain-data-lottery_v2'],,0.0139531447303239,AutoTrain,Not Specified,Not Specified,Not Specified,0.966,0.117,0.962,,,409147757.0,True,3,0,['pytorch'],2022-12-11 13:32:25+00:00,2022-12-11 13:30:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2420075390
- CO2 Emissions (in grams): 0.0140

## Validation Metrics

- Loss: 0.117
- Accuracy: 0.966
- Precision: 0.965
- Recall: 0.960
- AUC: 0.990
- F1: 0.962

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/paulkm/autotrain-lottery_v2-2420075390
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""paulkm/autotrain-lottery_v2-2420075390"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""paulkm/autotrain-lottery_v2-2420075390"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,29322978074.67108,0.9639958506224068,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
96594,autotrain-lottery_v2-2420075389,['paulkm/autotrain-data-lottery_v2'],,0.0604793403284594,AutoTrain,Not Specified,Not Specified,Not Specified,0.965,0.122,0.961,,,1302235053.0,True,3,0,['pytorch'],2022-12-11 13:36:25+00:00,2022-12-11 13:31:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2420075389
- CO2 Emissions (in grams): 0.0605

## Validation Metrics

- Loss: 0.122
- Accuracy: 0.965
- Precision: 0.976
- Recall: 0.946
- AUC: 0.988
- F1: 0.961

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/paulkm/autotrain-lottery_v2-2420075389
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""paulkm/autotrain-lottery_v2-2420075389"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""paulkm/autotrain-lottery_v2-2420075389"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,21531899090.29505,0.9629958463136032,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
96692,yahoo-answers-small,['breadlicker45/autotrain-data-yahoo-answer-small'],,1.56957741717827,AutoTrain,Not Specified,Not Specified,Not Specified,,3.47,,0.11323,0.09397,307908421.0,True,0,0,['pytorch'],2023-03-12 13:45:33+00:00,2022-12-11 16:53:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2422175450
- CO2 Emissions (in grams): 1.5696

## Validation Metrics

- Loss: 3.470
- Rouge1: 11.323
- Rouge2: 2.075
- RougeL: 9.397
- RougeLsum: 10.236
- Gen Len: 16.882

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/breadlicker45/autotrain-yahoo-answer-small-2422175450
```",,,1,[],[],Not Specified,2022-12,196172815.45344016,0.1027048561776061,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97062,autotrain-intent_classification_chope-2429575593,['bibekbehera/autotrain-data-intent_classification_chope'],,4.711456517910571,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-12-12 06:37:16+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97231,autotrain-sentiment-analysis-2435575634,['fyhao/autotrain-data-sentiment-analysis'],,0.0803280731181239,AutoTrain,Not Specified,Not Specified,Not Specified,0.873,0.186,0.87,,,,True,1,0,['joblib'],2022-12-12 15:06:05+00:00,2022-12-12 14:18:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2435575634
- CO2 Emissions (in grams): 0.0803

## Validation Metrics

- Loss: 0.186
- Accuracy: 0.873
- Macro F1: 0.870
- Micro F1: 0.873
- Weighted F1: 0.868
- Macro Precision: 0.938
- Micro Precision: 0.873
- Weighted Precision: 0.896
- Macro Recall: 0.833
- Micro Recall: 0.873
- Weighted Recall: 0.873

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2022-12,,0.8714974182444063,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97406,autotrain-rustance-stance-xlmr-2440275732,['feralvam/autotrain-data-rustance-stance-xlmr'],,2.3986554105301314,AutoTrain,Not Specified,Not Specified,Not Specified,0.861,0.466,0.455,,,1112258541.0,True,5,0,['pytorch'],2022-12-12 17:59:39+00:00,2022-12-12 17:57:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2440275732
- CO2 Emissions (in grams): 2.3987

## Validation Metrics

- Loss: 0.466
- Accuracy: 0.861
- Macro F1: 0.455
- Micro F1: 0.861
- Weighted F1: 0.809
- Macro Precision: 0.425
- Micro Precision: 0.861
- Weighted Precision: 0.764
- Macro Recall: 0.491
- Micro Recall: 0.861
- Weighted Recall: 0.861


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/feralvam/autotrain-rustance-stance-xlmr-2440275732
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""feralvam/autotrain-rustance-stance-xlmr-2440275732"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""feralvam/autotrain-rustance-stance-xlmr-2440275732"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,463700845.11396223,0.595372340425532,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97468,autotrain-preesmetextclassifier-2437575785,['acrowth/autotrain-data-preesmetextclassifier'],,3.321230337361748,AutoTrain,Not Specified,Not Specified,Not Specified,0.97,0.142,0.572,,,442568685.0,True,2,0,['pytorch'],2022-12-12 19:25:02+00:00,2022-12-12 19:23:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2437575785
- CO2 Emissions (in grams): 3.3212

## Validation Metrics

- Loss: 0.142
- Accuracy: 0.970
- Macro F1: 0.572
- Micro F1: 0.970
- Weighted F1: 0.960
- Macro Precision: 0.592
- Micro Precision: 0.970
- Weighted Precision: 0.951
- Macro Recall: 0.556
- Micro Recall: 0.970
- Weighted Recall: 0.970


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/acrowth/autotrain-preesmetextclassifier-2437575785
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""acrowth/autotrain-preesmetextclassifier-2437575785"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""acrowth/autotrain-preesmetextclassifier-2437575785"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,133254438.8810921,0.7196368352788586,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97636,autotrain-hamantest-2444675858,['lehiko/autotrain-data-hamantest'],,0.3689978392965571,AutoTrain,Not Specified,Not Specified,Not Specified,0.8,0.634,,,,347596479.0,True,0,0,['pytorch'],2022-12-13 00:06:37+00:00,2022-12-13 00:06:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2444675858
- CO2 Emissions (in grams): 0.3690

## Validation Metrics

- Loss: 0.634
- Accuracy: 0.800
- Precision: 0.000
- Recall: 0.000
- AUC: 0.000
- F1: 0.000",,,1,[],[],Computer Vision,2022-12,942001393.9990656,0.8,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97851,autotrain-victormautotraindreambooth-FS8JGUBRYX-2450175922,['victor/autotrain-data-victormautotraindreambooth-FS8JGUBRYX'],,60.20045215291253,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,1,0,['diffusers'],2022-12-13 09:28:46+00:00,,,,,1,[],[],Multimodal,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97874,autotrain-ner-only-effect-2451175943,['qiaokuoyuan/autotrain-data-ner-only-effect'],,0.8232221958822463,AutoTrain,Not Specified,Not Specified,Not Specified,0.965,0.1,0.0,,,406784817.0,True,0,0,['pytorch'],2022-12-13 09:39:04+00:00,2022-12-13 09:38:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2451175943
- CO2 Emissions (in grams): 0.8232

## Validation Metrics

- Loss: 0.100
- Accuracy: 0.965
- Precision: 0.000
- Recall: 0.000
- F1: 0.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/qiaokuoyuan/autotrain-ner-only-effect-2451175943
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""qiaokuoyuan/autotrain-ner-only-effect-2451175943"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""qiaokuoyuan/autotrain-ner-only-effect-2451175943"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,494137328.9431891,0.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,0,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97918,autotrain-recipes-2451975972,['davanstrien/autotrain-data-recipes'],,9.62915730999643,AutoTrain,Not Specified,Not Specified,Not Specified,0.989,0.053,0.933,,,438006125.0,True,2,0,['pytorch'],2022-12-13 11:30:35+00:00,2022-12-13 11:25:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2451975972
- CO2 Emissions (in grams): 9.6292

## Validation Metrics

- Loss: 0.053
- Accuracy: 0.989
- Macro F1: 0.933
- Micro F1: 0.989
- Weighted F1: 0.989
- Macro Precision: 0.941
- Micro Precision: 0.989
- Weighted Precision: 0.989
- Macro Recall: 0.926
- Micro Recall: 0.989
- Weighted Recall: 0.989


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/davanstrien/autotrain-recipes-2451975972
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""davanstrien/autotrain-recipes-2451975972"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""davanstrien/autotrain-recipes-2451975972"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,45487482.53861089,0.96018418314256,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97919,autotrain-recipes-2451975973,['davanstrien/autotrain-data-recipes'],,6.990639915807625,AutoTrain,Not Specified,Not Specified,Not Specified,0.989,0.046,0.936,,,433318253.0,True,3,0,['pytorch'],2022-12-13 16:43:31+00:00,2022-12-13 11:25:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2451975973
- CO2 Emissions (in grams): 6.9906

## Validation Metrics

- Loss: 0.046
- Accuracy: 0.989
- Macro F1: 0.936
- Micro F1: 0.989
- Weighted F1: 0.989
- Macro Precision: 0.929
- Micro Precision: 0.989
- Weighted Precision: 0.989
- Macro Recall: 0.943
- Micro Recall: 0.989
- Weighted Recall: 0.989


## Usage


This model has been trained to predict whether an article from a historic newspaper is a 'recipe' or 'not a recipe'. 
This model was trained on data generated by carrying out a keyword search of food terms and annotating examples results to indicate whether they were a recipe. 

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/davanstrien/autotrain-recipes-2451975973
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""davanstrien/autotrain-recipes-2451975973"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""davanstrien/autotrain-recipes-2451975973"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,61985491.77453077,0.9617703896103896,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
97931,MassiveCatalanIntents,['crodri/autotrain-data-massive-4-catalan'],,13.789236303098791,AutoTrain,Not Specified,Not Specified,Not Specified,0.882,0.546,0.855,,,1421809837.0,True,2,0,['pytorch'],2022-12-14 08:32:53+00:00,2022-12-13 11:53:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2452075980
- CO2 Emissions (in grams): 13.7892

## Validation Metrics

- Loss: 0.546
- Accuracy: 0.882
- Macro F1: 0.855
- Micro F1: 0.882
- Weighted F1: 0.881
- Macro Precision: 0.862
- Micro Precision: 0.882
- Weighted Precision: 0.886
- Macro Recall: 0.858
- Micro Recall: 0.882
- Weighted Recall: 0.882


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/crodri/MassiveCatalanIntents
```

Or Python API:

```
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""crodri/MassiveCatalanIntents"", use_auth_token=True)

model = AutoModelForSequenceClassification.from_pretrained(""crodri/MassiveCatalanIntents"", use_auth_token=True)

pipe = pipeline(""text-classification"",model=model,tokenizer=tokenizer)

result = pipe(""afegeix a la llista de la compra un litre de llet"")
```",,,1,[],[],NLP,2022-12,103110121.96378732,0.8682901554404144,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
98696,autotrain-feet-typelok-2473576411,['micole66/autotrain-data-feet-typelok'],,0.5534616165654265,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.279,,,,780988623.0,True,0,0,['pytorch'],2022-12-14 18:19:38+00:00,2022-12-14 18:18:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2473576411
- CO2 Emissions (in grams): 0.5535

## Validation Metrics

- Loss: 0.279
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-12,1411098077.3093536,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
98796,pickonai-best,['LiveEvil/autotrain-data-pickonai-alpha'],,0.482684204457421,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-12-19 17:19:31+00:00,,,,,1,[],[],Multimodal,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
98853,autotrain-text2itinerary-2479576495,['eubinecto/autotrain-data-text2itinerary'],,35.1760357885458,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,['pytorch'],2022-12-15 06:17:48+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99072,autotrain-medical-reports-summarizer-2484176581,['Abdulkader/autotrain-data-medical-reports-summarizer'],,0.0185081541168912,AutoTrain,Not Specified,Not Specified,Not Specified,,1.728,,0.44555,0.44168,990406605.0,True,220,0,['pytorch'],2022-12-15 14:15:11+00:00,2022-12-15 14:11:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2484176581
- CO2 Emissions (in grams): 0.0185

## Validation Metrics

- Loss: 1.728
- Rouge1: 44.555
- Rouge2: 34.430
- RougeL: 44.168
- RougeLsum: 43.895
- Gen Len: 8.930

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Abdulkader/autotrain-medical-reports-summarizer-2484176581
```",,,1,[],[],NLP,2022-12,53511906089.874176,0.4436065597421187,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99084,autotrain-butterflies-new-17716424,['abhishek/autotrain-data-butterflies-new'],,111.21012328795236,AutoTrain,Not Specified,Not Specified,Not Specified,0.317,4.305,0.043,,,123976577.0,True,1,0,['pytorch'],2022-12-15 16:52:39+00:00,2022-12-15 14:41:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 17716424
- CO2 Emissions (in grams): 111.2101

## Validation Metrics

- Loss: 4.305
- Accuracy: 0.317
- Macro F1: 0.043
- Micro F1: 0.317
- Weighted F1: 0.224
- Macro Precision: 0.044
- Micro Precision: 0.317
- Weighted Precision: 0.192
- Macro Recall: 0.053
- Micro Recall: 0.317
- Weighted Recall: 0.317",,,1,[],[],Computer Vision,2022-12,1114795.787780865,0.0757277777777777,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99085,autotrain-butterflies-new-17716422,['abhishek/autotrain-data-butterflies-new'],,138.53332005624384,AutoTrain,Not Specified,Not Specified,Not Specified,0.496,2.762,0.204,,,121503343.0,True,0,0,['pytorch'],2022-12-15 16:40:50+00:00,2022-12-15 14:41:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 17716422
- CO2 Emissions (in grams): 138.5333

## Validation Metrics

- Loss: 2.762
- Accuracy: 0.496
- Macro F1: 0.204
- Micro F1: 0.496
- Weighted F1: 0.438
- Macro Precision: 0.199
- Micro Precision: 0.496
- Weighted Precision: 0.409
- Macro Recall: 0.230
- Micro Recall: 0.496
- Weighted Recall: 0.496",,,1,[],[],Computer Vision,2022-12,877069.4512386641,0.2890971428571429,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99086,autotrain-butterflies-new-17716423,['abhishek/autotrain-data-butterflies-new'],,185.36475571171792,AutoTrain,Not Specified,Not Specified,Not Specified,0.46,3.193,0.146,,,354377457.0,True,0,0,['pytorch'],2022-12-15 17:04:25+00:00,2022-12-15 14:41:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 17716423
- CO2 Emissions (in grams): 185.3648

## Validation Metrics

- Loss: 3.193
- Accuracy: 0.460
- Macro F1: 0.146
- Micro F1: 0.460
- Weighted F1: 0.392
- Macro Precision: 0.145
- Micro Precision: 0.460
- Weighted Precision: 0.360
- Macro Recall: 0.166
- Micro Recall: 0.460
- Weighted Recall: 0.460",,,1,[],[],Computer Vision,2022-12,1911784.4470452257,0.2216501650165016,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99087,autotrain-butterflies-new-17716425,['abhishek/autotrain-data-butterflies-new'],,150.8589874762091,AutoTrain,Not Specified,Not Specified,Not Specified,0.548,2.461,0.259,,,362405631.0,True,0,0,['pytorch'],2022-12-15 16:24:53+00:00,2022-12-15 14:41:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 17716425
- CO2 Emissions (in grams): 150.8590

## Validation Metrics

- Loss: 2.461
- Accuracy: 0.548
- Macro F1: 0.259
- Micro F1: 0.548
- Weighted F1: 0.495
- Macro Precision: 0.253
- Micro Precision: 0.548
- Weighted Precision: 0.470
- Macro Recall: 0.289
- Micro Recall: 0.548
- Weighted Recall: 0.548",,,1,[],[],Computer Vision,2022-12,2402280.67987764,0.3517521685254027,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99088,autotrain-butterflies-new-17716426,['abhishek/autotrain-data-butterflies-new'],,0.8632390143249431,AutoTrain,Not Specified,Not Specified,Not Specified,0.497,2.804,0.179,,,357968939.0,True,0,0,['pytorch'],2022-12-15 17:05:35+00:00,2022-12-15 14:41:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 17716426
- CO2 Emissions (in grams): 0.8632

## Validation Metrics

- Loss: 2.804
- Accuracy: 0.497
- Macro F1: 0.179
- Micro F1: 0.497
- Weighted F1: 0.434
- Macro Precision: 0.173
- Micro Precision: 0.497
- Weighted Precision: 0.402
- Macro Recall: 0.205
- Micro Recall: 0.497
- Weighted Recall: 0.497",,,1,[],[],Computer Vision,2022-12,414681140.518114,0.2632041420118343,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99244,row4_accu100,['Efimov6886/autotrain-data-onlykaggle'],,0.0039350798740081,AutoTrain,Not Specified,Not Specified,Not Specified,0.99,0.021,0.99,,,110402095.0,True,1,0,['pytorch'],2022-12-15 19:22:21+00:00,2022-12-15 19:21:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2477076724
- CO2 Emissions (in grams): 0.0039

## Validation Metrics

- Loss: 0.021
- Accuracy: 0.990
- Macro F1: 0.990
- Micro F1: 0.990
- Weighted F1: 0.990
- Macro Precision: 0.990
- Micro Precision: 0.990
- Weighted Precision: 0.990
- Macro Recall: 0.990
- Micro Recall: 0.990
- Weighted Recall: 0.990",,,1,[],[],Computer Vision,2022-12,28055871426.962746,0.99,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99245,row4_98,['Efimov6886/autotrain-data-onlykaggle'],,1.893737751807574,AutoTrain,Not Specified,Not Specified,Not Specified,0.98,0.047,0.98,,,346867691.0,True,0,0,['pytorch'],2022-12-15 19:24:08+00:00,2022-12-15 19:22:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2477076728
- CO2 Emissions (in grams): 1.8937

## Validation Metrics

- Loss: 0.047
- Accuracy: 0.980
- Macro F1: 0.980
- Micro F1: 0.980
- Weighted F1: 0.980
- Macro Precision: 0.980
- Micro Precision: 0.980
- Weighted Precision: 0.980
- Macro Recall: 0.980
- Micro Recall: 0.980
- Weighted Recall: 0.980",,,1,[],[],Computer Vision,2022-12,183165641.95275432,0.98,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99295,told_br_binary_sm,['alexandreteles/told_br_binary_sm'],,4.429755329718354,AutoTrain,Not Specified,Not Specified,Not Specified,0.8,0.432,0.759,,,435769709.0,True,3,0,"['transformers', 'pytorch']",2023-01-20 00:52:44+00:00,2022-12-15 21:36:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2489276793
- Base model: bert-base-multilingual-cased
- Parameters: 109M
- Model size: 416MB
- CO2 Emissions (in grams): 4.4298

## Validation Metrics

- Loss: 0.432
- Accuracy: 0.800
- Precision: 0.823
- Recall: 0.704
- AUC: 0.891
- F1: 0.759

## Usage

This model was trained on a random subset of the [told-br](https://huggingface.co/datasets/told-br) dataset (1/3 of the original size). Our main objective is to provide a small
model that can be used to classify Brazilian Portuguese tweets in a binary way ('toxic' or 'non toxic').

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/alexandreteles/autotrain-told_br_binary_sm-2489276793
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alexandreteles/told_br_binary_sm"")

tokenizer = AutoTokenizer.from_pretrained(""alexandreteles/told_br_binary_sm"")

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,98373313.32421614,0.7789608723540732,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99297,row5_100,['Efimov6886/autotrain-data-row5_db'],,2.647537931755994,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,1.0,,,347608767.0,True,0,0,['pytorch'],2022-12-15 21:42:46+00:00,2022-12-15 21:41:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2488976797
- CO2 Emissions (in grams): 2.6475

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2022-12,131295103.58684328,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99316,told_br_binary_sm_bertimbau,['alexandreteles/told_br_binary_sm'],,1.778776476039011,AutoTrain,Not Specified,Not Specified,Not Specified,0.815,0.412,0.793,,,435769709.0,True,3,0,"['transformers', 'pytorch']",2023-01-20 00:50:43+00:00,2022-12-15 22:31:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2489776826
- Base model: bert-base-portuguese-cased
- Parameters: 109M
- Model size: 416MB
- CO2 Emissions (in grams): 1.7788

## Validation Metrics

- Loss: 0.412
- Accuracy: 0.815
- Precision: 0.793
- Recall: 0.794
- AUC: 0.895
- F1: 0.793

## Usage

This model was trained on a random subset of the [told-br](https://huggingface.co/datasets/told-br) dataset (1/3 of the original size). Our main objective is to provide a small
model that can be used to classify Brazilian Portuguese tweets in a binary way ('toxic' or 'non toxic').

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/alexandreteles/autotrain-told_br_binary_sm_bertimbau-2489776826
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""alexandreteles/autotrain-told_br_binary_sm_bertimbau-2489776826"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""alexandreteles/autotrain-told_br_binary_sm_bertimbau-2489776826"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,244982837.84951684,0.8038495024875622,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99332,autotrain-butterfly-similarity-2490576840,['sasha/autotrain-data-butterfly-similarity'],,21.263808199884835,AutoTrain,Not Specified,Not Specified,Not Specified,0.609,1.818,0.409,,,345438641.0,True,5,0,['pytorch'],2022-12-16 00:06:07+00:00,2022-12-15 23:48:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2490576840
- CO2 Emissions (in grams): 21.2638

## Validation Metrics

- Loss: 1.818
- Accuracy: 0.609
- Macro F1: 0.409
- Micro F1: 0.609
- Weighted F1: 0.559
- Macro Precision: 0.404
- Micro Precision: 0.609
- Weighted Precision: 0.542
- Macro Recall: 0.446
- Micro Recall: 0.609
- Weighted Recall: 0.609",,,1,[],[],Computer Vision,2022-12,16245379.837552847,0.4893536345776031,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99374,autotrain-Consequenv05-WEW6KM47ET-2492376867,['taskmasterpeace/autotrain-data-Consequenv05-WEW6KM47ET'],,39.499488037662175,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,2,0,['diffusers'],2022-12-16 03:39:39+00:00,2022-12-16 03:18:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Dreambooth
- Model ID: 2492376867
- CO2 Emissions (in grams): 39.4995",,,1,[],[],Multimodal,2022-12,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99595,yahoo-answers-test-model,['breadlicker45/autotrain-data-test2'],,3.128325675589278,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,557969145.0,False,0,0,['pytorch'],2022-12-16 13:20:45+00:00,2022-12-16 13:16:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2496476946
- CO2 Emissions (in grams): 3.1283

## Validation Metrics

- Loss: 3.511
- Rouge1: 14.002
- Rouge2: 2.968
- RougeL: 11.022
- RougeLsum: 12.335
- Gen Len: 18.900

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/breadlicker45/autotrain-test2-2496476946
```",,,1,[],[],Not Specified,2022-12,178360312.46807325,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99608,autotrain-butterfly_similarity_swin-2490776951,['sasha/autotrain-data-butterfly_similarity_swin'],,28.296015693616063,AutoTrain,Not Specified,Not Specified,Not Specified,0.689,1.385,0.488,,,350491071.0,True,1,0,['pytorch'],2022-12-16 14:05:38+00:00,2022-12-16 13:45:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2490776951
- CO2 Emissions (in grams): 28.2960

## Validation Metrics

- Loss: 1.385
- Accuracy: 0.689
- Macro F1: 0.488
- Micro F1: 0.689
- Weighted F1: 0.641
- Macro Precision: 0.483
- Micro Precision: 0.689
- Weighted Precision: 0.628
- Macro Recall: 0.528
- Micro Recall: 0.689
- Weighted Recall: 0.689",,,1,[],[],Computer Vision,2022-12,12386587.383716896,0.5713372982158029,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99715,autotrain-sea-slug-similarity-2498977005,['sasha/autotrain-data-sea-slug-similarity'],,13.759124872304856,AutoTrain,Not Specified,Not Specified,Not Specified,0.837,0.757,0.778,,,348592767.0,True,6,0,['pytorch'],2022-12-16 17:53:22+00:00,2022-12-16 17:42:21+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2498977005
- CO2 Emissions (in grams): 13.7591

## Validation Metrics

- Loss: 0.757
- Accuracy: 0.837
- Macro F1: 0.778
- Micro F1: 0.837
- Weighted F1: 0.816
- Macro Precision: 0.787
- Micro Precision: 0.837
- Weighted Precision: 0.825
- Macro Recall: 0.796
- Micro Recall: 0.837
- Weighted Recall: 0.837",,,1,[],[],Computer Vision,2022-12,25335387.986896407,0.8064222910216717,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
99717,andro-micro,['sabhashanki/autotrain-data-micro-dataset-text-classification'],,2.843258817349137,AutoTrain,Not Specified,Not Specified,Not Specified,0.901,0.366,0.897,,,498706477.0,True,3,0,['pytorch'],2022-12-16 18:08:33+00:00,2022-12-16 18:02:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2499077017
- CO2 Emissions (in grams): 2.8433

## Validation Metrics

- Loss: 0.366
- Accuracy: 0.901
- Macro F1: 0.897
- Micro F1: 0.901
- Weighted F1: 0.901
- Macro Precision: 0.914
- Micro Precision: 0.901
- Weighted Precision: 0.907
- Macro Recall: 0.888
- Micro Recall: 0.901
- Weighted Recall: 0.901


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sabhashanki/autotrain-micro-dataset-text-classification-2499077017
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sabhashanki/autotrain-micro-dataset-text-classification-2499077017"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sabhashanki/autotrain-micro-dataset-text-classification-2499077017"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,175399606.2394912,0.8989955506117909,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
100133,CovidAutoTrainTest,['MohamedSaad/autotrain-data-covid'],,1.7646991170797304,AutoTrain,Not Specified,Not Specified,Not Specified,0.319,1.861,0.231,,,540875117.0,True,2,0,['pytorch'],2022-12-17 21:01:18+00:00,2022-12-17 20:59:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2509577239
- CO2 Emissions (in grams): 1.7647

## Validation Metrics

- Loss: 1.861
- Accuracy: 0.319
- Macro F1: 0.231
- Micro F1: 0.319
- Weighted F1: 0.337
- Macro Precision: 0.270
- Micro Precision: 0.319
- Weighted Precision: 0.613
- Macro Recall: 0.346
- Micro Recall: 0.319
- Weighted Recall: 0.319


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/MohamedSaad/autotrain-covid-2509577239
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""MohamedSaad/autotrain-covid-2509577239"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""MohamedSaad/autotrain-covid-2509577239"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,306497074.63732064,0.26796,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
100173,autotrain-twitter-covid-19-spam-detection-2512177276,['pittawat/autotrain-data-twitter-covid-19-spam-detection'],,1.0218403202204225,AutoTrain,Not Specified,Not Specified,Not Specified,0.906,0.275,0.945,,,433318253.0,True,2,0,['pytorch'],2022-12-18 00:20:04+00:00,2022-12-18 00:19:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2512177276
- CO2 Emissions (in grams): 1.0218

## Validation Metrics

- Loss: 0.275
- Accuracy: 0.906
- Precision: 0.930
- Recall: 0.960
- AUC: 0.882
- F1: 0.945

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pittawat/autotrain-twitter-covid-19-spam-detection-2512177276
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pittawat/autotrain-twitter-covid-19-spam-detection-2512177276"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pittawat/autotrain-twitter-covid-19-spam-detection-2512177276"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,424056718.4768442,0.9250891410048624,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
100175,biomedical_text_summarization,['sumedh/MeQSum'],,3198.3976606503647,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,3132667369.0,False,31,1,"['transformers', 'pytorch']",2022-12-18 03:11:14+00:00,2022-12-18 00:23:04+00:00,,,,1,[],[],NLP,2022-12,979448.9933321802,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
100177,autotrain-sima-2512277279,['BigSneed/autotrain-data-sima'],,1.7528609470694885,AutoTrain,Not Specified,Not Specified,Not Specified,,4.313,,0.10778,0.0828599999999999,891700799.0,True,1,0,['pytorch'],2022-12-18 00:42:23+00:00,2022-12-18 00:40:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2512277279
- CO2 Emissions (in grams): 1.7529

## Validation Metrics

- Loss: 4.313
- Rouge1: 10.778
- Rouge2: 3.220
- RougeL: 8.286
- RougeLsum: 9.532
- Gen Len: 13.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/BigSneed/autotrain-sima-2512277279
```",,,1,[],[],NLP,2022-12,508711658.21269816,0.093691258917331,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
100531,autotrained_spoof_detector,['venuv62/autotrain-data-real_or_fake'],,2.251071897861615,AutoTrain,Not Specified,Not Specified,Not Specified,0.73,0.502,,,,347599761.0,True,1,0,['pytorch'],2022-12-19 00:39:11+00:00,2022-12-19 00:37:02+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2522377421
- CO2 Emissions (in grams): 2.2511

## Validation Metrics

- Loss: 0.502
- Accuracy: 0.730
- Precision: 0.717
- Recall: 0.760
- AUC: 0.790
- F1: 0.738",,,1,[],[],Computer Vision,2022-12,154415219.40289834,0.7300000000000001,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
100575,autotrain-rf_auto_gen-2522877431,['venuv62/autotrain-data-rf_auto_gen'],,2.118580608507536,AutoTrain,Not Specified,Not Specified,Not Specified,0.79,0.572,,,,347599761.0,True,0,0,['pytorch'],2022-12-19 02:58:17+00:00,2022-12-19 02:55:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2522877431
- CO2 Emissions (in grams): 2.1186

## Validation Metrics

- Loss: 0.572
- Accuracy: 0.790
- Precision: 0.872
- Recall: 0.680
- AUC: 0.854
- F1: 0.764",,,1,[],[],Computer Vision,2022-12,164072001.6052972,0.79,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
100610,honor,['freddiezhang/autotrain-data-honor'],,14.46129742532204,AutoTrain,Not Specified,Not Specified,Not Specified,0.989,0.055,0.989,,,433320053.0,True,4,0,"['transformers', 'pytorch']",2022-12-31 04:32:00+00:00,2022-12-19 05:04:04+00:00,"
<!-- Wikipedia text from https://en.wikipedia.org/wiki/GPT-3 -->

HonOR, standing for ""Hyper-parameter tuned computer-generated text objectification utilizing BERTForSeqenceClassification"" is a binary text classification model built with BertForSequenceClassification. This model was built to explore possibilities for zero-shot classification of texts in a wide range of domains.

For more information, please see the [model card](https://huggingface.co/freddiezhang/honor/blob/main/modelcard.md).

# Model information

- Problem type: Binary Classification
- Model ID: 2514377451
- CO2 Emissions (in grams): 14.4613

## Validation metrics

- Loss: 0.055
- Accuracy: 0.989
- Precision: 0.995
- Recall: 0.983
- AUC: 0.998
- F1: 0.989

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/freddiezhang/autotrain-honor-2514377451
```

Or a Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""freddiezhang/autotrain-honor-2514377451"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""freddiezhang/autotrain-honor-2514377451"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,29964120.110084128,0.989,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101091,autotrain-20-12-2022-2540377772,['teacookies/autotrain-data-20-12-2022'],,14.305842162020754,AutoTrain,Not Specified,Not Specified,Not Specified,0.994,0.019,0.828,,,667193901.0,True,0,0,"['transformers', 'pytorch']",2022-12-20 04:13:06+00:00,2022-12-20 04:05:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2540377772
- CO2 Emissions (in grams): 14.3058

## Validation Metrics

- Loss: 0.019
- Accuracy: 0.994
- Precision: 0.804
- Recall: 0.853
- F1: 0.828

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-20-12-2022-2540377772
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-20-12-2022-2540377772"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-20-12-2022-2540377772"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,46637862.590940006,0.9034379802414928,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101098,CatsandDogsPOC-Resnet,['BirdL/DalleCatsAndDogs'],49662722.0,1.6189148718411266,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.465,,,,94374989.0,True,0,0,"['transformers', 'pytorch']",2022-12-20 05:17:03+00:00,2022-12-20 04:53:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2540477800
- CO2 Emissions (in grams): 1.6189

## Validation Metrics

- Loss: 0.465
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-12,58295214.06068198,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101099,CatsandDogsPOC-Swin,['puffy310/autotrain-data-synth-cats-or-dogs'],,1.165641024416945,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,,,,347599761.0,True,6,0,"['transformers', 'pytorch']",2022-12-20 05:06:59+00:00,2022-12-20 04:53:45+00:00,"
Resnet is more lightweight but this is better in terms of loss, at the cost of being 3.5X the size. 

# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2540477801
- CO2 Emissions (in grams): 1.1656

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-12,298204810.67391205,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101119,autotrain-20-12-2022-exam4-2541677824,['teacookies/autotrain-data-20-12-2022-exam4'],,11.026710036149494,AutoTrain,Not Specified,Not Specified,Not Specified,0.994,0.02,0.804,,,667206189.0,True,0,0,"['transformers', 'pytorch']",2022-12-20 06:27:35+00:00,2022-12-20 06:20:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2541677824
- CO2 Emissions (in grams): 11.0267

## Validation Metrics

- Loss: 0.020
- Accuracy: 0.994
- Precision: 0.768
- Recall: 0.843
- F1: 0.804

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-20-12-2022-exam4-2541677824
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-20-12-2022-exam4-2541677824"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-20-12-2022-exam4-2541677824"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,60508183.022194274,0.8889610678531703,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101137,autotrain-12-20-2022_rated_speed_only_exam-2542677876,['teacookies/autotrain-data-12-20-2022_rated_speed_only_exam'],,7.9382600757577935,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.002,0.666,,,667150829.0,True,5,0,"['transformers', 'pytorch']",2022-12-20 07:11:03+00:00,2022-12-20 07:05:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2542677876
- CO2 Emissions (in grams): 7.9383

## Validation Metrics

- Loss: 0.002
- Accuracy: 0.999
- Precision: 0.629
- Recall: 0.706
- F1: 0.666

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-12-20-2022_rated_speed_only_exam-2542677876
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-12-20-2022_rated_speed_only_exam-2542677876"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-12-20-2022_rated_speed_only_exam-2542677876"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,84042450.44041508,0.7992,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101148,autotrain-20-12-2022_rated_speed_exam-2543177886,['teacookies/autotrain-data-20-12-2022_rated_speed_exam'],,16.824773531345276,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,0.897,,,667150829.0,True,0,0,"['transformers', 'pytorch']",2022-12-20 07:56:05+00:00,2022-12-20 07:46:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2543177886
- CO2 Emissions (in grams): 16.8248

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Precision: 0.894
- Recall: 0.899
- F1: 0.897

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-20-12-2022_rated_speed_exam-2543177886
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-20-12-2022_rated_speed_exam-2543177886"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-20-12-2022_rated_speed_exam-2543177886"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,39652886.129912496,0.9457037427517132,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101162,autotrain-20-12-2022_rated_speed_exam2-2543577904,['teacookies/autotrain-data-20-12-2022_rated_speed_exam2'],,18.10539257374316,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,0.846,,,667150829.0,True,0,0,"['transformers', 'pytorch']",2022-12-20 08:40:50+00:00,2022-12-20 08:31:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2543577904
- CO2 Emissions (in grams): 18.1054

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 0.816
- Recall: 0.877
- F1: 0.846

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-20-12-2022_rated_speed_exam2-2543577904
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-20-12-2022_rated_speed_exam2-2543577904"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-20-12-2022_rated_speed_exam2-2543577904"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,36848183.56093073,0.9165763813651138,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101174,autotrain-20-12-2022_general_info_exam-2543777917,['teacookies/autotrain-data-20-12-2022_general_info_exam'],,21.038593211432406,AutoTrain,Not Specified,Not Specified,Not Specified,0.998,0.007,0.945,,,667187757.0,True,0,0,"['transformers', 'pytorch']",2022-12-20 09:22:01+00:00,2022-12-20 09:10:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2543777917
- CO2 Emissions (in grams): 21.0386

## Validation Metrics

- Loss: 0.007
- Accuracy: 0.998
- Precision: 0.937
- Recall: 0.952
- F1: 0.945

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-20-12-2022_general_info_exam-2543777917
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-20-12-2022_general_info_exam-2543777917"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-20-12-2022_general_info_exam-2543777917"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,31712565.108081896,0.9707771487390632,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101192,autotrain-20-12-2022_exam_part3-2543877946,['teacookies/autotrain-data-20-12-2022_exam_part3'],,21.733051144065605,AutoTrain,Not Specified,Not Specified,Not Specified,0.998,0.01,0.762,,,667206189.0,True,0,0,"['transformers', 'pytorch']",2022-12-20 10:12:28+00:00,2022-12-20 10:01:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2543877946
- CO2 Emissions (in grams): 21.7331

## Validation Metrics

- Loss: 0.010
- Accuracy: 0.998
- Precision: 0.739
- Recall: 0.786
- F1: 0.762

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-20-12-2022_exam_part3-2543877946
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-20-12-2022_exam_part3-2543877946"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-20-12-2022_exam_part3-2543877946"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,30700069.88789452,0.8641772727272728,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101308,autotrain-tscholak_finetune_2-2548477985,['Aman6917/autotrain-data-tscholak_finetune_2'],,17.282664720294886,AutoTrain,Not Specified,Not Specified,Not Specified,,0.09,,0.96687,0.96016,3132580677.0,True,2,2,"['transformers', 'pytorch']",2022-12-20 14:58:10+00:00,2022-12-20 14:42:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2548477985
- CO2 Emissions (in grams): 17.2827

## Validation Metrics

- Loss: 0.090
- Rouge1: 96.687
- Rouge2: 93.600
- RougeL: 96.016
- RougeLsum: 96.170
- Gen Len: 32.733

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Aman6917/autotrain-tscholak_finetune_2-2548477985
```",,,1,[],[],NLP,2022-12,181255652.857828,0.9635033177480372,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101539,autotrain-csgo_dust2_or_mirage-2555378112,['Lunibo/autotrain-data-csgo_dust2_or_mirage'],,3.586661186521151,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.004,,,,343268717.0,True,1,0,"['transformers', 'pytorch']",2022-12-20 22:41:17+00:00,2022-12-20 22:37:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2555378112
- CO2 Emissions (in grams): 3.5867

## Validation Metrics

- Loss: 0.004
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2022-12,95707037.5897285,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101594,autotrain-20-12-2022_rated_speed3_exam-2544978148,['teacookies/autotrain-data-20-12-2022_rated_speed3_exam'],,17.12192796383268,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,0.835,,,667150829.0,True,0,0,"['transformers', 'pytorch']",2022-12-21 02:08:00+00:00,2022-12-21 01:59:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2544978148
- CO2 Emissions (in grams): 17.1219

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Precision: 0.815
- Recall: 0.855
- F1: 0.835

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-20-12-2022_rated_speed3_exam-2544978148
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-20-12-2022_rated_speed3_exam-2544978148"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-20-12-2022_rated_speed3_exam-2544978148"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,38964702.480307646,0.9100817438692098,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101625,autotrain-21-12-2022_rated_speed2-2557778169,['teacookies/autotrain-data-21-12-2022_rated_speed2'],,14.90637346423708,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,0.991,,,667150829.0,True,0,0,"['transformers', 'pytorch']",2022-12-21 03:24:14+00:00,2022-12-21 03:15:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2557778169
- CO2 Emissions (in grams): 14.9064

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Precision: 0.991
- Recall: 0.992
- F1: 0.991

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-21-12-2022_rated_speed2-2557778169
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-21-12-2022_rated_speed2-2557778169"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-21-12-2022_rated_speed2-2557778169"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,44756079.0423377,0.995479658463084,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101637,autotrain-21-12-2022_exam_part3_1-2557478179,['teacookies/autotrain-data-21-12-2022_exam_part3_1'],,20.48292260935143,AutoTrain,Not Specified,Not Specified,Not Specified,0.998,0.009,0.802,,,667206189.0,True,0,0,"['transformers', 'pytorch']",2022-12-21 03:55:58+00:00,2022-12-21 03:44:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2557478179
- CO2 Emissions (in grams): 20.4829

## Validation Metrics

- Loss: 0.009
- Accuracy: 0.998
- Precision: 0.796
- Recall: 0.808
- F1: 0.802

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-21-12-2022_exam_part3_1-2557478179
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-21-12-2022_exam_part3_1-2557478179"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-21-12-2022_exam_part3_1-2557478179"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,32573778.73875228,0.889328888888889,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101641,autotrain-21-12-2022_exam_part5-2557978193,['teacookies/autotrain-data-21-12-2022_exam_part5'],,11.403028098792594,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,0.993,,,667172333.0,True,0,0,"['transformers', 'pytorch']",2022-12-21 03:57:45+00:00,2022-12-21 03:51:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2557978193
- CO2 Emissions (in grams): 11.4030

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Precision: 0.988
- Recall: 0.998
- F1: 0.993

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-21-12-2022_exam_part5-2557978193
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-21-12-2022_exam_part5-2557978193"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-21-12-2022_exam_part5-2557978193"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,58508347.71429207,0.9964877069744104,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101642,autotrain-21-12-2022_overspeed_governor-2557878199,['teacookies/autotrain-data-21-12-2022_overspeed_governor'],,0.0497938898991906,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,0.992,,,667193901.0,True,0,0,"['transformers', 'pytorch']",2022-12-21 03:58:32+00:00,2022-12-21 03:51:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2557878199
- CO2 Emissions (in grams): 0.0498

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Precision: 0.990
- Recall: 0.993
- F1: 0.992

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-21-12-2022_overspeed_governor-2557878199
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-21-12-2022_overspeed_governor-2557878199"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-21-12-2022_overspeed_governor-2557878199"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,13399111865.948944,0.995983935742972,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101665,autotrain-21-12-2022_exam_part4_1-2558278221,['teacookies/autotrain-data-21-12-2022_exam_part4_1'],,17.974413129541247,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,0.984,,,667200045.0,True,0,0,"['transformers', 'pytorch']",2022-12-21 05:54:19+00:00,2022-12-21 05:43:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2558278221
- CO2 Emissions (in grams): 17.9744

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Precision: 0.984
- Recall: 0.985
- F1: 0.984

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-21-12-2022_exam_part4_1-2558278221
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-21-12-2022_exam_part4_1-2558278221"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-21-12-2022_exam_part4_1-2558278221"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,37119434.17520796,0.9919354838709676,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101700,autotrain-21-12-2022_exam_part3-2559178277,['teacookies/autotrain-data-21-12-2022_exam_part3'],,20.464916479903486,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.002,0.955,,,667206189.0,True,0,0,"['transformers', 'pytorch']",2022-12-21 07:39:06+00:00,2022-12-21 07:27:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2559178277
- CO2 Emissions (in grams): 20.4649

## Validation Metrics

- Loss: 0.002
- Accuracy: 0.999
- Precision: 0.944
- Recall: 0.967
- F1: 0.955

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/teacookies/autotrain-21-12-2022_exam_part3-2559178277
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""teacookies/autotrain-21-12-2022_exam_part3-2559178277"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""teacookies/autotrain-21-12-2022_exam_part3-2559178277"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,32602438.89366445,0.9765046059365404,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
101988,autotrain-GoodhuesChairDemo-KPRJBJ8D95-2568578395,['Jeronimotani/autotrain-data-GoodhuesChairDemo-KPRJBJ8D95'],,26.147409089438984,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,2,0,['diffusers'],2022-12-21 20:11:47+00:00,2022-12-21 19:58:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Dreambooth
- Model ID: 2568578395
- CO2 Emissions (in grams): 26.1474",,,1,[],[],Multimodal,2022-12,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
102473,auto-arabic-summarization,['abdalrahmanshahrour/autotrain-data-auto-arabic-summarization'],,23.93485567770492,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,557175853.0,False,70,3,"['transformers', 'pytorch']",2023-02-08 11:10:23+00:00,2022-12-22 19:22:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2581378622
- CO2 Emissions (in grams): 23.9349

## Validation Metrics

- Loss: 0.829
- Rouge1: 1.132
- Rouge2: 0.127
- RougeL: 1.137
- RougeLsum: 1.129

### Framework versions

- Transformers 4.25.1
- Pytorch 1.13.0+cu116
- Datasets 2.7.1
- Tokenizers 0.13.2

  ",,,1,[],[],NLP,2022-12,23278847.40575243,,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,1,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,1,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,2,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,2,0.0,0,0,0,0,0.0,1,0,0,0.0,2,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
102939,English-to-Aramaic-or-Syriac,['Tritkoman/autotrain-data-abjsbjajabs'],,186.95204500157936,AutoTrain,Not Specified,Not Specified,Not Specified,,1.098,,,,4918420761.0,True,14,0,"['transformers', 'pytorch']",2022-12-23 20:56:50+00:00,2022-12-23 19:05:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 2590578831
- CO2 Emissions (in grams): 186.9520

## Validation Metrics

- Loss: 1.098
- SacreBLEU: 0.819
- Gen len: 19.000",,,1,[],[],NLP,2022-12,26308461.9425182,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
103227,friedeberg,['clem/autotrain-data-friedeberg-0OIYU5UZXE'],,23.966933198902527,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,4,0,['diffusers'],2022-12-24 18:21:14+00:00,2022-12-24 18:01:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Dreambooth
- Model ID: 2603979056
- CO2 Emissions (in grams): 23.9669",,,1,[],[],Multimodal,2022-12,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
103239,maxdekdt,['clem/autotrain-data-maxdekdt-AER2SE090K'],,86.12664300406121,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,3,2,['diffusers'],2023-01-05 20:11:04+00:00,2022-12-24 19:16:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Dreambooth
- Model ID: 2604679068
- CO2 Emissions (in grams): 86.1266",,,1,[],[],Multimodal,2022-12,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
103673,drug-recommend-reco-v1,['hunk3000/autotrain-data-drugrecommendreco'],,2.748256947165376,AutoTrain,Not Specified,Not Specified,Not Specified,0.987,0.051,0.988,,,409149557.0,True,2,0,"['transformers', 'pytorch']",2022-12-26 09:12:06+00:00,2022-12-26 09:10:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2617579252
- CO2 Emissions (in grams): 2.7483

## Validation Metrics

- Loss: 0.051
- Accuracy: 0.987
- Precision: 0.985
- Recall: 0.991
- AUC: 0.999
- F1: 0.988

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/hunk3000/autotrain-drugrecommendreco-2617579252
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""hunk3000/autotrain-drugrecommendreco-2617579252"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hunk3000/autotrain-drugrecommendreco-2617579252"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,148876020.27969313,0.987499746835443,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
103689,autotrain-drugrecommendtiny-2617979261,['hunk3000/autotrain-data-drugrecommendtiny'],,1.30441791792016,AutoTrain,Not Specified,Not Specified,Not Specified,0.923,0.217,0.928,,,46187471.0,True,2,0,"['transformers', 'pytorch']",2022-12-26 09:53:37+00:00,2022-12-26 09:52:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2617979261
- CO2 Emissions (in grams): 1.3044

## Validation Metrics

- Loss: 0.217
- Accuracy: 0.923
- Precision: 0.931
- Recall: 0.925
- AUC: 0.977
- F1: 0.928

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/hunk3000/autotrain-drugrecommendtiny-2617979261
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""hunk3000/autotrain-drugrecommendtiny-2617979261"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hunk3000/autotrain-drugrecommendtiny-2617979261"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,35408491.68466192,0.9254932468935712,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
103961,autotrain-lottery_prod-2626879382,['paulkm/autotrain-data-lottery_prod'],,11.554897545219454,AutoTrain,Not Specified,Not Specified,Not Specified,0.96,0.146,0.955,,,1302236789.0,True,25,0,"['transformers', 'pytorch']",2022-12-27 04:33:46+00:00,2022-12-27 04:27:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2626879382
- CO2 Emissions (in grams): 11.5549

## Validation Metrics

- Loss: 0.146
- Accuracy: 0.960
- Precision: 0.967
- Recall: 0.944
- AUC: 0.986
- F1: 0.955

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/paulkm/autotrain-lottery_prod-2626879382
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""paulkm/autotrain-lottery_prod-2626879382"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""paulkm/autotrain-lottery_prod-2626879382"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2022-12,112699985.77691998,0.9574934725848564,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
104314,mT5-OrangeSum,['ell-hol/autotrain-data-test-orangesum'],,675.7789931017469,AutoTrain,Not Specified,Not Specified,Not Specified,,1.631,,0.33348,0.2421,2329702453.0,True,1,1,"['transformers', 'pytorch']",2023-02-08 14:34:07+00:00,2022-12-27 22:06:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2638979565
- CO2 Emissions (in grams): 675.7790

## Validation Metrics

- Loss: 1.631
- Rouge1: 33.348
- Rouge2: 14.481
- RougeL: 24.210
- RougeLsum: 25.514
- Gen Len: 48.497

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/ell-hol/autotrain-test-orangesum-2638979565
```",,,1,[],[],NLP,2022-12,3447432.484260774,0.28053618263317,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,1,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
104489,mT5-dialogSum,['ell-hol/autotrain-data-mt5-dialogsum'],,248.06396898781733,AutoTrain,Not Specified,Not Specified,Not Specified,,1.316,,0.40914,0.33122,2329702453.0,True,4,1,"['transformers', 'pytorch']",2022-12-28 13:30:17+00:00,2022-12-28 11:19:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2644579647
- CO2 Emissions (in grams): 248.0640

## Validation Metrics

- Loss: 1.316
- Rouge1: 40.914
- Rouge2: 16.140
- RougeL: 33.122
- RougeLsum: 35.661
- Gen Len: 34.075

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/ell-hol/autotrain-mt5-dialogsum-2644579647
```",,,1,[],[],NLP,2022-12,9391539.055453934,0.3660796120806094,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
104518,autotrain-t5-cnn-v6,['Gowtham2003/autotrain-data-autotrain-t5-cnn-v6'],,6.1132277010358,AutoTrain,Not Specified,Not Specified,Not Specified,,1.798,,0.23476,0.1926599999999999,242071641.0,True,1,0,"['transformers', 'onnx', 'pytorch']",2022-12-30 04:08:06+00:00,2022-12-28 12:42:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2646779711
- CO2 Emissions (in grams): 6.1132

## Validation Metrics

- Loss: 1.798
- Rouge1: 23.476
- Rouge2: 10.592
- RougeL: 19.266
- RougeLsum: 22.050
- Gen Len: 18.988

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Gowtham2003/autotrain-autotrain-t5-cnn-v6-2646779711
```",,,1,[],[],NLP,2022-12,39598008.25985664,0.2116366178466145,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,1,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
104924,autotrain-age3-2658279907,['ivensamdh/autotrain-data-age3'],,5.065800795931951,AutoTrain,Not Specified,Not Specified,Not Specified,0.77,0.895,0.768,,,1392751597.0,True,1,0,"['transformers', 'pytorch']",2022-12-29 13:25:52+00:00,2022-12-29 13:21:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2658279907
- CO2 Emissions (in grams): 5.0658

## Validation Metrics

- Loss: 0.895
- Accuracy: 0.770
- Macro F1: 0.768
- Micro F1: 0.770
- Weighted F1: 0.768
- Macro Precision: 0.773
- Micro Precision: 0.770
- Weighted Precision: 0.773
- Macro Recall: 0.770
- Micro Recall: 0.770
- Weighted Recall: 0.770",,,1,[],[],Computer Vision,2022-12,274932168.2997163,0.7689986996098831,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
105026,autotrain-t5-cnn,['Gowtham2003/autotrain-data-autotrain-t5-cnn'],,10.839262225533137,AutoTrain,Not Specified,Not Specified,Not Specified,,1.685,,0.2386499999999999,0.19696,242071641.0,True,1,0,"['transformers', 'pytorch']",2022-12-29 17:54:59+00:00,2022-12-29 17:48:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2662379993
- CO2 Emissions (in grams): 10.8393

## Validation Metrics

- Loss: 1.685
- Rouge1: 23.865
- Rouge2: 10.983
- RougeL: 19.696
- RougeLsum: 22.456
- Gen Len: 18.990

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Gowtham2003/autotrain-autotrain-t5-cnn-2662379993
```",,,1,[],[],NLP,2022-12,22332852.17786984,0.2158100319092766,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
105543,English2AlgerianArabic,['Tritkoman/autotrain-data-ajiomqmkoao'],,90.58305184650668,AutoTrain,Not Specified,Not Specified,Not Specified,,2.518,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2022-12-31 12:55:14+00:00,2022-12-31 11:55:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 2667480110
- CO2 Emissions (in grams): 90.5831

## Validation Metrics

- Loss: 2.518
- SacreBLEU: 1.532
- Gen len: 17.661",,,1,[],[],NLP,2022-12,54297362.0422315,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
105758,English2AlgerianArabicV2,['Tritkoman/autotrain-data-jqqjjqjo9jqjqj'],,60.07025137574803,AutoTrain,Not Specified,Not Specified,Not Specified,,2.663,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-01-01 10:18:43+00:00,2023-01-01 09:37:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 2684380215
- CO2 Emissions (in grams): 60.0703

## Validation Metrics

- Loss: 2.663
- SacreBLEU: 7.486
- Gen len: 12.545",,,1,[],[],NLP,2023-01,81877812.20082755,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
106590,autotrain-finance_data_classification-2694580522,['samaresh55/autotrain-data-finance_data_classification'],,4.221526489857838,AutoTrain,Not Specified,Not Specified,Not Specified,0.95,0.227,0.931,,,438017141.0,True,13,1,"['transformers', 'pytorch']",2023-01-06 08:53:08+00:00,2023-01-03 05:28:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2694580522
- CO2 Emissions (in grams): 4.2215

## Validation Metrics

- Loss: 0.227
- Accuracy: 0.950
- Macro F1: 0.931
- Micro F1: 0.950
- Weighted F1: 0.950
- Macro Precision: 0.956
- Micro Precision: 0.950
- Weighted Precision: 0.950
- Macro Recall: 0.914
- Micro Recall: 0.950
- Weighted Recall: 0.950


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/samaresh55/autotrain-finance_data_classification-2694580522
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""samaresh55/autotrain-finance_data_classification-2694580522"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""samaresh55/autotrain-finance_data_classification-2694580522"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"",truncation=True)

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,103757998.92582232,0.9404040404040404,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
106610,autotrain-fine_tune_table_tm2-2695480537,['Aman6917/autotrain-data-fine_tune_table_tm2'],,6.3826736622439215,AutoTrain,Not Specified,Not Specified,Not Specified,,1.227,,0.5006499999999999,0.46018,3132580677.0,True,0,0,"['transformers', 'pytorch']",2023-01-03 06:52:09+00:00,2023-01-03 06:46:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2695480537
- CO2 Emissions (in grams): 6.3827

## Validation Metrics

- Loss: 1.227
- Rouge1: 50.065
- Rouge2: 24.621
- RougeL: 46.018
- RougeLsum: 46.230
- Gen Len: 111.647

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Aman6917/autotrain-fine_tune_table_tm2-2695480537
```",,,1,[],[],NLP,2023-01,490794429.2264969,0.4795627051611627,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
106697,autotrain-genderage-2709480568,['molsen/autotrain-data-genderage'],,8.240977060159542,AutoTrain,Not Specified,Not Specified,Not Specified,0.56,1.277,0.56,,,1392792557.0,True,23,0,"['transformers', 'pytorch']",2023-01-03 10:46:37+00:00,2023-01-03 10:41:02+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2709480568
- CO2 Emissions (in grams): 8.2410

## Validation Metrics

- Loss: 1.277
- Accuracy: 0.560
- Macro F1: 0.560
- Micro F1: 0.560
- Weighted F1: 0.560
- Macro Precision: 0.570
- Micro Precision: 0.560
- Weighted Precision: 0.570
- Macro Recall: 0.560
- Micro Recall: 0.560
- Weighted Recall: 0.560",,,1,[],[],Computer Vision,2023-01,169008182.74733022,0.56,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
106759,autotrain-tm3_model-2711480628,['Aman6917/autotrain-data-tm3_model'],,9.38482304577412,AutoTrain,Not Specified,Not Specified,Not Specified,,0.088,,0.94638,0.93188,3132580677.0,True,0,0,"['transformers', 'pytorch']",2023-01-03 13:02:33+00:00,2023-01-03 12:54:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2711480628
- CO2 Emissions (in grams): 9.3848

## Validation Metrics

- Loss: 0.088
- Rouge1: 94.638
- Rouge2: 90.173
- RougeL: 93.188
- RougeLsum: 93.163
- Gen Len: 66.529

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Aman6917/autotrain-tm3_model-2711480628
```",,,1,[],[],NLP,2023-01,333792194.2396736,0.9390740306453844,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
106760,autotrain-tm3_model-2711480629,['Aman6917/autotrain-data-tm3_model'],,8.016071286117265,AutoTrain,Not Specified,Not Specified,Not Specified,,0.088,,0.94701,0.92992,3132580677.0,True,0,1,"['transformers', 'pytorch']",2023-01-03 13:02:29+00:00,2023-01-03 12:54:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2711480629
- CO2 Emissions (in grams): 8.0161

## Validation Metrics

- Loss: 0.088
- Rouge1: 94.701
- Rouge2: 89.907
- RougeL: 92.992
- RougeLsum: 93.163
- Gen Len: 66.529

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Aman6917/autotrain-tm3_model-2711480629
```",,,1,[],[],NLP,2023-01,390787527.3546032,0.9383871952603452,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
106761,autotrain-tm3_model-2711480631,['Aman6917/autotrain-data-tm3_model'],,9.180873432477254,AutoTrain,Not Specified,Not Specified,Not Specified,,0.088,,0.94701,0.93006,3132580677.0,True,0,0,"['transformers', 'pytorch']",2023-01-03 13:02:37+00:00,2023-01-03 12:54:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2711480631
- CO2 Emissions (in grams): 9.1809

## Validation Metrics

- Loss: 0.088
- Rouge1: 94.701
- Rouge2: 90.005
- RougeL: 93.006
- RougeLsum: 93.078
- Gen Len: 66.529

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Aman6917/autotrain-tm3_model-2711480631
```",,,1,[],[],NLP,2023-01,341207260.94736534,0.9384584704885808,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
106878,indobert-sentiment-analysis,['dafex/autotrain-data-indobert-sentiment-analysis'],,1.3428141985163928,AutoTrain,Not Specified,Not Specified,Not Specified,0.96,0.132,0.969,,,442311797.0,True,51,0,"['transformers', 'pytorch']",2023-01-03 16:51:59+00:00,2023-01-03 16:50:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2713480683
- CO2 Emissions (in grams): 1.3428

## Validation Metrics

- Loss: 0.132
- Accuracy: 0.960
- Precision: 0.966
- Recall: 0.973
- AUC: 0.993
- F1: 0.969

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dafex/autotrain-indobert-sentiment-analysis-2713480683
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dafex/autotrain-indobert-sentiment-analysis-2713480683"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dafex/autotrain-indobert-sentiment-analysis-2713480683"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,329391659.3142133,0.9644790046656296,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
107006,Pacc,['ongp/autotrain-data-test1'],,4.8390309824523134,AutoTrain,Not Specified,Not Specified,Not Specified,0.708,0.663,0.698,,,347612049.0,True,6,0,"['transformers', 'pytorch']",2023-01-03 21:41:35+00:00,2023-01-03 21:36:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2718280758
- CO2 Emissions (in grams): 4.8390

## Validation Metrics

- Loss: 0.663
- Accuracy: 0.708
- Macro F1: 0.698
- Micro F1: 0.708
- Weighted F1: 0.712
- Macro Precision: 0.703
- Micro Precision: 0.708
- Weighted Precision: 0.717
- Macro Recall: 0.695
- Micro Recall: 0.708
- Weighted Recall: 0.708",,,1,[],[],Computer Vision,2023-01,71835053.39406568,0.7029644381223329,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
107182,70btclassification,['ongp/autotrain-data-312312'],,0.0112914208925239,AutoTrain,Not Specified,Not Specified,Not Specified,0.708,0.877,0.695,,,343277933.0,True,7,0,"['transformers', 'pytorch']",2023-01-04 03:06:39+00:00,2023-01-04 03:03:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2723080856
- CO2 Emissions (in grams): 0.0113

## Validation Metrics

- Loss: 0.877
- Accuracy: 0.708
- Macro F1: 0.695
- Micro F1: 0.708
- Weighted F1: 0.704
- Macro Precision: 0.703
- Micro Precision: 0.708
- Weighted Precision: 0.711
- Macro Recall: 0.699
- Micro Recall: 0.708
- Weighted Recall: 0.708",,,1,[],[],Computer Vision,2023-01,30401659478.24032,0.70143977191732,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
107401,autotrain-tamil_emotion_11_tamilbert-2710380899,['jusgowiturs/autotrain-data-tamil_emotion_11_tamilbert'],,0.0198551858623125,AutoTrain,Not Specified,Not Specified,Not Specified,0.434,1.77,0.238,,,1112281781.0,True,5,0,"['transformers', 'pytorch']",2023-01-04 11:22:25+00:00,2023-01-04 11:19:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2710380899
- CO2 Emissions (in grams): 0.0199

## Validation Metrics

- Loss: 1.770
- Accuracy: 0.434
- Macro F1: 0.238
- Micro F1: 0.434
- Weighted F1: 0.385
- Macro Precision: 0.310
- Micro Precision: 0.434
- Weighted Precision: 0.397
- Macro Recall: 0.241
- Micro Recall: 0.434
- Weighted Recall: 0.434


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jusgowiturs/autotrain-tamil_emotion_11_tamilbert-2710380899
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jusgowiturs/autotrain-tamil_emotion_11_tamilbert-2710380899"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jusgowiturs/autotrain-tamil_emotion_11_tamilbert-2710380899"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,56019711359.7029,0.3074166666666666,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
107732,autotrain-disaster_tweets_autotrain-2730481027,['venetis/autotrain-data-disaster_tweets_autotrain'],,3.296830635621752,AutoTrain,Not Specified,Not Specified,Not Specified,0.856,0.352,0.818,,,556848625.0,True,2,0,"['transformers', 'pytorch']",2023-01-04 23:36:25+00:00,2023-01-04 23:34:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2730481027
- CO2 Emissions (in grams): 3.2968

## Validation Metrics

- Loss: 0.352
- Accuracy: 0.856
- Precision: 0.898
- Recall: 0.751
- AUC: 0.908
- F1: 0.818

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/venetis/autotrain-disaster_tweets_autotrain-2730481027
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""venetis/autotrain-disaster_tweets_autotrain-2730481027"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""venetis/autotrain-disaster_tweets_autotrain-2730481027"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,168904225.4653107,0.8365686977299881,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
108043,autotrain-animalsbull-2741281174,['micole66/autotrain-data-animalsbull'],,0.7219356485793302,AutoTrain,Not Specified,Not Specified,Not Specified,0.5,0.708,0.667,,,1740396281.0,True,3,0,"['transformers', 'pytorch']",2023-01-05 12:35:05+00:00,2023-01-05 12:34:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2741281174
- CO2 Emissions (in grams): 0.7219

## Validation Metrics

- Loss: 0.708
- Accuracy: 0.500
- Precision: 0.500
- Recall: 1.000
- AUC: 0.125
- F1: 0.667

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/micole66/autotrain-animalsbull-2741281174
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""micole66/autotrain-animalsbull-2741281174"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""micole66/autotrain-animalsbull-2741281174"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,2410736032.255589,0.5715509854327335,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
108090,autotrain-jayefam-XTVOGKX3RD-2742681219,['curiosityone/autotrain-data-jayefam-XTVOGKX3RD'],,34.13925460566729,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,3,0,['diffusers'],2023-01-05 14:28:55+00:00,2023-01-05 14:05:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Dreambooth
- Model ID: 2742681219
- CO2 Emissions (in grams): 34.1393",,,1,[],[],Multimodal,2023-01,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
108199,python-code-explainer,['sagard21/autotrain-data-code-explainer'],,5.393079045128973,AutoTrain,Not Specified,Not Specified,Not Specified,,2.156,,0.29375,0.25445,2950733825.0,True,159,3,"['transformers', 'safetensors', 'pytorch']",2023-03-19 08:28:10+00:00,2023-01-05 18:04:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2745581349
- CO2 Emissions (in grams): 5.3931

# Model Description

This model is an attempt to simplify code understanding by generating line by line explanation of a source code. This model was fine-tuned using the Salesforce/codet5-large model. Currently it is trained on a small subset of Python snippets.

# Model Usage

```py
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    AutoConfig,
    pipeline,
)

model_name = ""sagard21/python-code-explainer""

tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

config = AutoConfig.from_pretrained(model_name)

model.eval()

pipe = pipeline(""summarization"", model=model_name, config=config, tokenizer=tokenizer)

raw_code = """"""
def preprocess(text: str) -> str:
    text = str(text)
    text = text.replace(""\n"", "" "")
    tokenized_text = text.split("" "")
    preprocessed_text = "" "".join([token for token in tokenized_text if token])

    return preprocessed_text
""""""

print(pipe(raw_code)[0][""summary_text""])

```

## Validation Metrics

- Loss: 2.156
- Rouge1: 29.375
- Rouge2: 18.128
- RougeL: 25.445
- RougeLsum: 28.084
- Gen Len: 19.000
",,,1,[],[],NLP,2023-01,547133427.9190849,0.2726913079168186,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
108633,autotrain-real-vs-fake-news-2757281767,['Eip/autotrain-data-real-vs-fake-news'],,2.0552688377356976,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,1.0,,,267855533.0,True,2,0,"['transformers', 'pytorch']",2023-01-06 12:23:02+00:00,2023-01-06 12:21:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2757281767
- CO2 Emissions (in grams): 2.0553

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Eip/autotrain-real-vs-fake-news-2757281767
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281767"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281767"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,130326275.6102983,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
108634,autotrain-real-vs-fake-news-2757281768,['Eip/autotrain-data-real-vs-fake-news'],,1.4525417907263476,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,1.0,,,267855533.0,True,3,0,"['transformers', 'pytorch']",2023-01-06 12:22:39+00:00,2023-01-06 12:21:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2757281768
- CO2 Emissions (in grams): 1.4525

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Eip/autotrain-real-vs-fake-news-2757281768
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281768"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281768"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,184404699.8923577,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
108635,autotrain-real-vs-fake-news-2757281769,['Eip/autotrain-data-real-vs-fake-news'],,1.5370516791351636,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,1.0,,,267855533.0,True,3,0,"['transformers', 'pytorch']",2023-01-06 12:22:45+00:00,2023-01-06 12:21:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2757281769
- CO2 Emissions (in grams): 1.5371

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Eip/autotrain-real-vs-fake-news-2757281769
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281769"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281769"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,174265795.11673376,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
108636,autotrain-real-vs-fake-news-2757281770,['Eip/autotrain-data-real-vs-fake-news'],,1.1122429329446866,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,1.0,,,267855533.0,True,3,0,"['transformers', 'pytorch']",2023-01-06 12:22:49+00:00,2023-01-06 12:21:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2757281770
- CO2 Emissions (in grams): 1.1122

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Eip/autotrain-real-vs-fake-news-2757281770
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281770"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281770"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,240824666.1463129,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
108637,autotrain-real-vs-fake-news-2757281771,['Eip/autotrain-data-real-vs-fake-news'],,2.3993982522584325,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,1.0,,,267855533.0,True,7,0,"['transformers', 'pytorch']",2023-01-06 12:23:34+00:00,2023-01-06 12:22:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2757281771
- CO2 Emissions (in grams): 2.3994

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Eip/autotrain-real-vs-fake-news-2757281771
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281771"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Eip/autotrain-real-vs-fake-news-2757281771"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,111634461.9939108,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
109157,autotrain-gend-ma-classification-2764081811,['mjaydenkim/autotrain-data-gend-ma-classification'],,1.0453005361524643,AutoTrain,Not Specified,Not Specified,Not Specified,0.839,0.418,0.774,,,438007925.0,True,2,0,"['transformers', 'pytorch']",2023-01-07 06:50:15+00:00,2023-01-07 06:49:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2764081811
- CO2 Emissions (in grams): 1.0453

## Validation Metrics

- Loss: 0.418
- Accuracy: 0.839
- Precision: 0.779
- Recall: 0.769
- AUC: 0.884
- F1: 0.774

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/mjaydenkim/autotrain-gend-ma-classification-2764081811
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mjaydenkim/autotrain-gend-ma-classification-2764081811"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mjaydenkim/autotrain-gend-ma-classification-2764081811"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,419025830.22889936,0.8051903285802852,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
109254,autotrain-sherlockholmes20230107002-2767381827,['vikey10/autotrain-data-sherlockholmes20230107002'],,3.2051992210410623,AutoTrain,Not Specified,Not Specified,Not Specified,0.745,0.522,0.803,,,1334464117.0,True,2,0,"['transformers', 'pytorch']",2023-01-07 12:47:03+00:00,2023-01-07 12:45:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2767381827
- CO2 Emissions (in grams): 3.2052

## Validation Metrics

- Loss: 0.522
- Accuracy: 0.745
- Precision: 0.777
- Recall: 0.831
- AUC: 0.809
- F1: 0.803

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/vikey10/autotrain-sherlockholmes20230107002-2767381827
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""vikey10/autotrain-sherlockholmes20230107002-2767381827"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""vikey10/autotrain-sherlockholmes20230107002-2767381827"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,416343579.5939575,0.7729134366925065,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
109258,autotrain-sherlockholmes20230107003-2767481829,['vikey10/autotrain-data-sherlockholmes20230107003'],,1.7166828546754431,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-07 12:49:27+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
109302,autotrain-ag-news-classification-2680881849,['Hrishikesh332/autotrain-data-ag-news-classification'],,12.806489367828096,AutoTrain,Not Specified,Not Specified,Not Specified,0.882,0.39,0.765,,,737777977.0,True,1,0,"['transformers', 'pytorch']",2023-01-07 15:12:23+00:00,2023-01-07 15:05:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2680881849
- CO2 Emissions (in grams): 12.8065

## Validation Metrics

- Loss: 0.390
- Accuracy: 0.882
- Macro F1: 0.765
- Micro F1: 0.882
- Weighted F1: 0.880
- Macro Precision: 0.927
- Micro Precision: 0.882
- Weighted Precision: 0.887
- Macro Recall: 0.705
- Micro Recall: 0.882
- Weighted Recall: 0.882


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Hrishikesh332/autotrain-ag-news-classification-2680881849
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Hrishikesh332/autotrain-ag-news-classification-2680881849"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Hrishikesh332/autotrain-ag-news-classification-2680881849"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,57609697.38150204,0.819344262295082,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
110064,arabertv2_flodusta,['MMars/autotrain-data-arabertv2_flodusta'],,3.6263155149619295,AutoTrain,Not Specified,Not Specified,Not Specified,0.953,0.144,0.951,,,540858485.0,True,3,0,"['transformers', 'pytorch']",2023-01-08 18:53:51+00:00,2023-01-08 18:22:55+00:00,"# Labels Mapping
0 non event 

1 flood 

2 dust storm 

3 traffic accident


# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2782582128
- CO2 Emissions (in grams): 3.6263

## Validation Metrics

- Loss: 0.144
- Accuracy: 0.953
- Macro F1: 0.951
- Micro F1: 0.953
- Weighted F1: 0.953
- Macro Precision: 0.951
- Micro Precision: 0.953
- Weighted Precision: 0.953
- Macro Recall: 0.952
- Micro Recall: 0.953
- Weighted Recall: 0.953


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/MMars/autotrain-arabertv2_flodusta-2782582128
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""MMars/arabertv2_flodusta"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""MMars/arabertv2_flodusta"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,149148214.70124564,0.951998949579832,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
110075,marbertv2_flodusta,['MMars/autotrain-data-marbertv2_flodusta'],,0.0261715151601056,AutoTrain,Not Specified,Not Specified,Not Specified,0.948,0.157,0.945,,,651450485.0,True,3,0,"['transformers', 'pytorch']",2023-01-08 18:54:16+00:00,2023-01-08 18:32:48+00:00,"# Labels Mapping
0 non event 

1 flood 

2 dust storm 

3 traffic accident


# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2782682135
- CO2 Emissions (in grams): 0.0262

## Validation Metrics

- Loss: 0.157
- Accuracy: 0.948
- Macro F1: 0.945
- Micro F1: 0.948
- Weighted F1: 0.948
- Macro Precision: 0.940
- Micro Precision: 0.948
- Weighted Precision: 0.949
- Macro Recall: 0.952
- Micro Recall: 0.948
- Weighted Recall: 0.948


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/MMars/autotrain-marbertv2_flodusta-2782682135
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""MMars/marbertv2_flodusta"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""MMars/marbertv2_flodusta"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,24891584648.986423,0.946497622820919,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
110095,camelbert-mix_flodusta,['MMars/autotrain-data-camelbert-mix_flodusta'],,0.010214592292905,AutoTrain,Not Specified,Not Specified,Not Specified,0.949,0.149,0.946,,,436410485.0,True,3,0,"['transformers', 'pytorch']",2023-01-08 19:11:48+00:00,2023-01-08 19:01:39+00:00,"# Labels Mapping
0 non event 

1 flood 

2 dust storm 

3 traffic accident


# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2783082152
- CO2 Emissions (in grams): 0.0102

## Validation Metrics

- Loss: 0.149
- Accuracy: 0.949
- Macro F1: 0.946
- Micro F1: 0.949
- Weighted F1: 0.949
- Macro Precision: 0.942
- Micro Precision: 0.949
- Weighted Precision: 0.950
- Macro Recall: 0.951
- Micro Recall: 0.949
- Weighted Recall: 0.949


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/MMars/autotrain-camelbert-mix_flodusta-2783082152
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""MMars/camelbert-mix_flodusta"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""MMars/camelbert-mix_flodusta"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,42724219673.76302,0.9474976253298152,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
110669,autotrain-pidgintranslation_-2795382481,['jamm55/autotrain-data-pidgintranslation_'],,62.58086434891094,AutoTrain,Not Specified,Not Specified,Not Specified,,1.647,,,,4918420761.0,True,0,0,"['transformers', 'pytorch']",2023-01-09 16:10:27+00:00,2023-01-09 15:31:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 2795382481
- CO2 Emissions (in grams): 62.5809

## Validation Metrics

- Loss: 1.647
- SacreBLEU: 15.789
- Gen len: 14.926",,,1,[],[],NLP,2023-01,78593046.1678833,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
110687,navigation-chinese,['Kunologist/autotrain-data-nav-chinese'],,2.1130584322221275,AutoTrain,Not Specified,Not Specified,Not Specified,0.998,0.02,0.984,,,406832685.0,True,6,0,"['transformers', 'pytorch']",2023-01-09 16:02:30+00:00,2023-01-09 16:01:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 2795482484
- CO2 Emissions (in grams): 2.1131

## Validation Metrics

- Loss: 0.020
- Accuracy: 0.998
- Precision: 0.984
- Recall: 0.984
- F1: 0.984

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Kunologist/autotrain-nav-chinese-2795482484
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Kunologist/autotrain-nav-chinese-2795482484"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Kunologist/autotrain-nav-chinese-2795482484"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,192532624.1793361,0.9909505549949544,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
110745,autotrain-lots_of_text-2797882537,['team-marmalade/autotrain-data-lots_of_text'],,2.7585473731043173,AutoTrain,Not Specified,Not Specified,Not Specified,0.97,0.076,,,,347599761.0,True,3,0,"['transformers', 'pytorch']",2023-01-09 18:41:47+00:00,2023-01-09 18:40:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2797882537
- CO2 Emissions (in grams): 2.7585

## Validation Metrics

- Loss: 0.076
- Accuracy: 0.970
- Precision: 0.980
- Recall: 0.988
- AUC: 0.991
- F1: 0.984",,,1,[],[],Computer Vision,2023-01,126008262.31554992,0.97,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
110752,autotrain-pidgintranslation2-2798082543,['jamm55/autotrain-data-pidgintranslation2'],,5.960829912309611,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-09 18:54:26+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
110796,autotrain-pidgintranslationmix-2798982563,['jamm55/autotrain-data-pidgintranslationmix'],,9.975347552307484,AutoTrain,Not Specified,Not Specified,Not Specified,,1.76,,,,295863749.0,True,0,0,"['transformers', 'pytorch']",2023-01-09 20:24:11+00:00,2023-01-09 20:17:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 2798982563
- CO2 Emissions (in grams): 9.9753

## Validation Metrics

- Loss: 1.760
- SacreBLEU: 17.015
- Gen len: 23.459",,,1,[],[],NLP,2023-01,29659492.80950729,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
110855,multiclass_message_classifier,['lucieackley/autotrain-data-multi_msg_4'],,1.531642698505257,AutoTrain,Not Specified,Not Specified,Not Specified,0.95,0.251,0.949,,,737774905.0,True,2,0,"['transformers', 'pytorch']",2023-01-09 22:09:05+00:00,2023-01-09 22:07:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2799882586
- CO2 Emissions (in grams): 1.5316

## Validation Metrics

- Loss: 0.251
- Accuracy: 0.950
- Macro F1: 0.949
- Micro F1: 0.950
- Weighted F1: 0.950
- Macro Precision: 0.962
- Micro Precision: 0.950
- Weighted Precision: 0.953
- Macro Recall: 0.938
- Micro Recall: 0.950
- Weighted Recall: 0.950


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lucieackley/autotrain-multi_msg_4-2799882586
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucieackley/autotrain-multi_msg_4-2799882586"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucieackley/autotrain-multi_msg_4-2799882586"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,481688650.83220834,0.949499736703528,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
111149,GPTxLege_FoxHunter,['PoseyATX/autotrain-data-gptxlege_kyrieproject'],,74.07891296294748,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-11 00:41:33+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
111195,autotrain-15-2806382679,['onevholy/autotrain-data-15'],,5.399037757901585,AutoTrain,Not Specified,Not Specified,Not Specified,,0.404,,0.97949,0.97949,2279610349.0,True,0,0,"['transformers', 'pytorch']",2023-01-10 09:53:53+00:00,2023-01-10 09:48:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2806382679
- CO2 Emissions (in grams): 5.3990

## Validation Metrics

- Loss: 0.404
- Rouge1: 97.949
- Rouge2: 94.872
- RougeL: 97.949
- RougeLsum: 97.949
- Gen Len: 4.154

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/onevholy/autotrain-15-2806382679
```",,,1,[],[],NLP,2023-01,422225302.21496433,0.97949,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
111224,autotrain-software_picture_preselection_classifier-2804582686,['BIDEQUITY/autotrain-data-software_picture_preselection_classifier'],,2.0734204068239874,AutoTrain,Not Specified,Not Specified,Not Specified,0.973,0.209,0.98,,,111352101.0,True,171,0,"['transformers', 'pytorch']",2023-01-10 11:08:27+00:00,2023-01-10 11:06:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2804582686
- CO2 Emissions (in grams): 2.0734

## Validation Metrics

- Loss: 0.209
- Accuracy: 0.973
- Macro F1: 0.980
- Micro F1: 0.973
- Weighted F1: 0.973
- Macro Precision: 0.980
- Micro Precision: 0.973
- Weighted Precision: 0.973
- Macro Recall: 0.980
- Micro Recall: 0.973
- Weighted Recall: 0.973",,,1,[],[],Computer Vision,2023-01,53704545.70309082,0.9764874551971326,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
111559,FoxHunter_V0.02,['PoseyATX/autotrain-data-fenrir_zero_test_two'],,183.00320092703035,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-11 01:57:30+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
111560,Fenrir59-072,['PoseyATX/autotrain-data-fenrir_zero_test_two'],,392.8528382524423,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-11 03:40:06+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
112089,autotrain-improved-pidgin-model-2837583189,['jamm55/autotrain-data-improved-pidgin-model'],,4.315660252959388,AutoTrain,Not Specified,Not Specified,Not Specified,,0.753,,,,295863749.0,True,49,1,"['transformers', 'pytorch']",2023-02-02 11:31:34+00:00,2023-01-11 17:45:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 2837583189
- CO2 Emissions (in grams): 4.3157

## Validation Metrics

- Loss: 0.753
- SacreBLEU: 46.837
- Gen len: 21.250
- 
- ## English to Pidgin

- This model will translate English to pidgin
- Pidgin, a simplified version of english. Mostly used in Africa",,,1,[],[],NLP,2023-01,68555848.15721224,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
112572,autotrain-clauses_classifier-2847083403,['ranajoy98/autotrain-data-clauses_classifier'],,2.3063999928355314,AutoTrain,Not Specified,Not Specified,Not Specified,0.807,0.693,0.827,,,556867057.0,True,2,0,"['transformers', 'pytorch']",2023-01-12 08:00:05+00:00,2023-01-12 07:58:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2847083403
- CO2 Emissions (in grams): 2.3064

## Validation Metrics

- Loss: 0.693
- Accuracy: 0.807
- Macro F1: 0.827
- Micro F1: 0.807
- Weighted F1: 0.807
- Macro Precision: 0.820
- Micro Precision: 0.807
- Weighted Precision: 0.818
- Macro Recall: 0.849
- Micro Recall: 0.807
- Weighted Recall: 0.807


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ranajoy98/autotrain-clauses_classifier-2847083403
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ranajoy98/autotrain-clauses_classifier-2847083403"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ranajoy98/autotrain-clauses_classifier-2847083403"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,241444267.5727627,0.8168776009791923,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
112573,autotrain-clauses_classifier-2847083405,['ranajoy98/autotrain-data-clauses_classifier'],,0.712310551029896,AutoTrain,Not Specified,Not Specified,Not Specified,0.795,0.642,0.81,,,438026357.0,True,5,0,"['transformers', 'pytorch']",2023-01-12 07:59:14+00:00,2023-01-12 07:58:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2847083405
- CO2 Emissions (in grams): 0.7123

## Validation Metrics

- Loss: 0.642
- Accuracy: 0.795
- Macro F1: 0.810
- Micro F1: 0.795
- Weighted F1: 0.796
- Macro Precision: 0.807
- Micro Precision: 0.795
- Weighted Precision: 0.802
- Macro Recall: 0.819
- Micro Recall: 0.795
- Weighted Recall: 0.795


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ranajoy98/autotrain-clauses_classifier-2847083405
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ranajoy98/autotrain-clauses_classifier-2847083405"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ranajoy98/autotrain-clauses_classifier-2847083405"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,614937342.0998446,0.8024299065420561,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
112725,autotrain-productkit-customer-insights-2851583532,['paulmbw/autotrain-data-productkit-customer-insights'],,0.3252141705527131,AutoTrain,Not Specified,Not Specified,Not Specified,,1.044,,0.43528,0.34718,1625537293.0,True,26,0,"['transformers', 'pytorch']",2023-01-12 16:19:42+00:00,2023-01-12 13:01:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2851583532
- CO2 Emissions (in grams): 0.3252

## Validation Metrics

- Loss: 1.044
- Rouge1: 43.528
- Rouge2: 24.232
- RougeL: 34.718
- RougeLsum: 40.574
- Gen Len: 60.206

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/paulmbw/autotrain-productkit-customer-insights-2851583532
```",,,1,[],[],NLP,2023-01,4998359358.81066,0.3862702512588503,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
113027,autotrain-2023011302-2864483894,['vikey10/autotrain-data-2023011302'],,1.594123660832492,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-13 02:33:43+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
113470,FoxHunter_PigIron,['PoseyATX/autotrain-data-foxhunterirontesting'],,25.44757706430333,AutoTrain,Not Specified,Not Specified,Not Specified,,1.027,,0.60232,0.4791499999999999,2283804653.0,True,0,0,"['transformers', 'pytorch']",2023-01-13 20:25:36+00:00,2023-01-13 20:12:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2874884135
- CO2 Emissions (in grams): 25.4476

## Validation Metrics

- Loss: 1.027
- Rouge1: 60.232
- Rouge2: 42.909
- RougeL: 47.915
- RougeLsum: 54.128
- Gen Len: 193.351

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/PoseyATX/autotrain-foxhunterirontesting-2874884135
```",,,1,[],[],NLP,2023-01,89745465.63820468,0.5337210056682108,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
113686,autotrain-cuad-document-type-2883884341,['jogoni/autotrain-data-cuad-document-type'],,4.141997048700727,AutoTrain,Not Specified,Not Specified,Not Specified,0.938,0.541,0.869,,,438084789.0,True,2,0,"['transformers', 'pytorch']",2023-01-14 10:27:54+00:00,2023-01-14 10:24:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2883884341
- CO2 Emissions (in grams): 4.1420

## Validation Metrics

- Loss: 0.541
- Accuracy: 0.938
- Macro F1: 0.869
- Micro F1: 0.938
- Weighted F1: 0.925
- Macro Precision: 0.875
- Micro Precision: 0.938
- Weighted Precision: 0.927
- Macro Recall: 0.884
- Micro Recall: 0.938
- Weighted Recall: 0.938


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jogoni/autotrain-cuad-document-type-2883884341
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jogoni/autotrain-cuad-document-type-2883884341"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jogoni/autotrain-cuad-document-type-2883884341"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,105766562.32466888,0.9021826231322634,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
113696,autotrain-cuad-document-type-cleaned-2883984346,['jogoni/autotrain-data-cuad-document-type-cleaned'],,4.9342799418113215,AutoTrain,Not Specified,Not Specified,Not Specified,0.982,0.304,0.949,,,438078645.0,True,2,0,"['transformers', 'pytorch']",2023-01-14 10:54:32+00:00,2023-01-14 10:51:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2883984346
- CO2 Emissions (in grams): 4.9343

## Validation Metrics

- Loss: 0.304
- Accuracy: 0.982
- Macro F1: 0.949
- Micro F1: 0.982
- Weighted F1: 0.978
- Macro Precision: 0.946
- Micro Precision: 0.982
- Weighted Precision: 0.976
- Macro Recall: 0.954
- Micro Recall: 0.982
- Weighted Recall: 0.982


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jogoni/autotrain-cuad-document-type-cleaned-2883984346
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jogoni/autotrain-cuad-document-type-cleaned-2883984346"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jogoni/autotrain-cuad-document-type-cleaned-2883984346"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,88782689.7067348,0.9652180217503884,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
113770,autotrain-books-rating-analysis-2885184365,['LewisShanghai/autotrain-data-books-rating-analysis'],,13.050690238461922,AutoTrain,Not Specified,Not Specified,Not Specified,0.652,0.797,0.425,,,737781049.0,True,2,0,"['transformers', 'pytorch']",2023-01-14 14:38:30+00:00,2023-01-14 14:32:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2885184365
- CO2 Emissions (in grams): 13.0507

## Validation Metrics

- Loss: 0.797
- Accuracy: 0.652
- Macro F1: 0.425
- Micro F1: 0.652
- Weighted F1: 0.637
- Macro Precision: 0.396
- Micro Precision: 0.652
- Weighted Precision: 0.634
- Macro Recall: 0.478
- Micro Recall: 0.652
- Weighted Recall: 0.652


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/LewisShanghai/autotrain-books-rating-analysis-2885184365
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""LewisShanghai/autotrain-books-rating-analysis-2885184365"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""LewisShanghai/autotrain-books-rating-analysis-2885184365"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,56531956.204559386,0.5145775301764159,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
113779,autotrain-text-sentiment-indonlu-smse-2885384370,['mkhairil/autotrain-data-text-sentiment-indonlu-smse'],,5.395117116799661,AutoTrain,Not Specified,Not Specified,Not Specified,0.9,0.27,0.866,,,711495797.0,True,2,0,"['transformers', 'pytorch']",2023-01-21 21:45:46+00:00,2023-01-14 14:56:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- fine tuned with indonlp/indonlu dataset. (10000 rows from https://huggingface.co/datasets/indonlp/indonlu/viewer/smsa/train)
- Model ID: 2885384370
- CO2 Emissions (in grams): 5.3951

## Validation Metrics

- Loss: 0.270
- Accuracy: 0.900
- Macro F1: 0.866
- Micro F1: 0.900
- Weighted F1: 0.899
- Macro Precision: 0.874
- Micro Precision: 0.900
- Weighted Precision: 0.899
- Macro Recall: 0.859
- Micro Recall: 0.900
- Weighted Recall: 0.900


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/mkhairil/autotrain-text-sentiment-indonlu-smse-2885384370
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mkhairil/autotrain-text-sentiment-indonlu-smse-2885384370"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mkhairil/autotrain-text-sentiment-indonlu-smse-2885384370"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,131877729.7316677,0.8826727066817668,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
113793,autotrain-test_auto_nlp-2885884378,['owsgfwnlgjuz/autotrain-data-test_auto_nlp'],,1.139809838906873,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.079,,,,347599761.0,True,3,0,"['transformers', 'pytorch']",2023-01-14 15:20:42+00:00,2023-01-14 15:19:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2885884378
- CO2 Emissions (in grams): 1.1398

## Validation Metrics

- Loss: 0.079
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-01,304962941.30375576,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
115495,autotrain-contract_types-2926484993,['ranajoy98/autotrain-data-contract_types'],,0.0041854392608065,AutoTrain,Not Specified,Not Specified,Not Specified,0.981,0.106,0.977,,,433338485.0,True,2,0,"['transformers', 'pytorch']",2023-01-17 11:29:35+00:00,2023-01-17 11:28:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2926484993
- CO2 Emissions (in grams): 0.0042

## Validation Metrics

- Loss: 0.106
- Accuracy: 0.981
- Macro F1: 0.977
- Micro F1: 0.981
- Weighted F1: 0.980
- Macro Precision: 0.983
- Micro Precision: 0.981
- Weighted Precision: 0.982
- Macro Recall: 0.975
- Micro Recall: 0.981
- Weighted Recall: 0.981


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ranajoy98/autotrain-contract_types-2926484993
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ranajoy98/autotrain-contract_types-2926484993"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ranajoy98/autotrain-contract_types-2926484993"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,103534768514.7148,0.9789959141981616,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
115508,autotrain-mm-2927885005,['swww/autotrain-data-mm'],,0.3584667794035356,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.015,,,,86733677.0,True,1,0,"['transformers', 'pytorch']",2023-01-17 12:01:46+00:00,2023-01-17 12:01:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2927885005
- CO2 Emissions (in grams): 0.3585

## Validation Metrics

- Loss: 0.015
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-01,241957363.92733228,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
115509,autotrain-mm-2927885009,['swww/autotrain-data-mm'],,0.3747546351485631,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,,,,86733677.0,True,0,0,"['transformers', 'pytorch']",2023-01-17 12:02:10+00:00,2023-01-17 12:01:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2927885009
- CO2 Emissions (in grams): 0.3748

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-01,231441238.78712365,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
115516,test,['swww/autotrain-data-test'],,1.9626027616152408,AutoTrain,Not Specified,Not Specified,Not Specified,0.925,0.226,0.925,,,343274861.0,True,0,0,"['transformers', 'pytorch']",2023-01-17 12:20:37+00:00,2023-01-17 12:18:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2928085012
- CO2 Emissions (in grams): 1.9626

## Validation Metrics

- Loss: 0.226
- Accuracy: 0.925
- Macro F1: 0.925
- Micro F1: 0.925
- Weighted F1: 0.925
- Macro Precision: 0.929
- Micro Precision: 0.925
- Weighted Precision: 0.929
- Macro Recall: 0.925
- Micro Recall: 0.925
- Weighted Recall: 0.925",,,1,[],[],Computer Vision,2023-01,174907967.98710376,0.9250000000000002,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
115685,autotrain-let-2932785109,['AdamOswald1/autotrain-data-let'],,0.0171096411570498,AutoTrain,Not Specified,Not Specified,Not Specified,0.372,1.241,0.228,,,343321005.0,True,2,0,"['transformers', 'pytorch']",2023-01-17 17:38:28+00:00,2023-01-17 17:34:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2932785109
- CO2 Emissions (in grams): 0.0171

## Validation Metrics

- Loss: 1.241
- Accuracy: 0.372
- Macro F1: 0.228
- Micro F1: 0.372
- Weighted F1: 0.344
- Macro Precision: 0.190
- Micro Precision: 0.372
- Weighted Precision: 0.337
- Macro Recall: 0.355
- Micro Recall: 0.372
- Weighted Recall: 0.372",,,1,[],[],Computer Vision,2023-01,20065938370.57413,0.28272,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
115686,autotrain-let-2932785111,['AdamOswald1/autotrain-data-let'],,3.216116887212137,AutoTrain,Not Specified,Not Specified,Not Specified,0.376,1.165,0.269,,,347669457.0,True,7,0,"['transformers', 'pytorch']",2023-01-17 17:38:08+00:00,2023-01-17 17:34:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2932785111
- CO2 Emissions (in grams): 3.2161

## Validation Metrics

- Loss: 1.165
- Accuracy: 0.376
- Macro F1: 0.269
- Micro F1: 0.376
- Weighted F1: 0.349
- Macro Precision: 0.235
- Micro Precision: 0.376
- Weighted Precision: 0.354
- Macro Recall: 0.413
- Micro Recall: 0.376
- Weighted Recall: 0.376",,,1,[],[],Computer Vision,2023-01,108102245.40730987,0.3136248062015503,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
115946,Humiliated_Dolphin,['PoseyATX/autotrain-data-awfulnewdatatrain'],,0.3524550149496619,AutoTrain,Not Specified,Not Specified,Not Specified,,1.182,,0.69284,0.61472,2283804653.0,True,0,0,"['transformers', 'pytorch']",2023-01-18 13:45:34+00:00,2023-01-18 04:40:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2940685240
- CO2 Emissions (in grams): 0.3525

## Validation Metrics

- Loss: 1.182
- Rouge1: 69.284
- Rouge2: 55.274
- RougeL: 61.472
- RougeLsum: 66.749
- Gen Len: 130.111

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/PoseyATX/autotrain-awfulnewdatatrain-2940685240
```",,,1,[],[],NLP,2023-01,6479705369.850323,0.6514463654440332,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
115964,Self-Reliant_Bison,['PoseyATX/autotrain-data-letsgettoseventy'],,160.69662192250064,AutoTrain,Not Specified,Not Specified,Not Specified,,0.85,,0.6587099999999999,0.55782,2283804653.0,True,1,0,"['transformers', 'pytorch']",2023-01-18 14:21:26+00:00,2023-01-18 05:33:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2941585271
- CO2 Emissions (in grams): 160.6966

## Validation Metrics

- Loss: 0.850
- Rouge1: 65.871
- Rouge2: 50.714
- RougeL: 55.782
- RougeLsum: 60.308
- Gen Len: 127.369

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/PoseyATX/autotrain-letsgettoseventy-2941585271
```",,,1,[],[],NLP,2023-01,14211902.06538015,0.6040814648220759,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
116140,autotrain-modelo_hate_detection-2946985361,['Pafebla/autotrain-data-modelo_hate_detection'],,0.9675589686679076,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.063,1.0,,,347603857.0,True,6,0,"['transformers', 'pytorch']",2023-01-18 13:08:20+00:00,2023-01-18 13:07:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2946985361
- CO2 Emissions (in grams): 0.9676

## Validation Metrics

- Loss: 0.063
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-01,359258575.71096224,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
116156,autotrain-convnext_test_masterage-2947785378,['ivensamdh/autotrain-data-convnext_test_masterage'],,2.59724118879757,AutoTrain,Not Specified,Not Specified,Not Specified,0.408,1.47,0.261,,,350417325.0,True,0,0,"['transformers', 'pytorch']",2023-01-18 13:44:57+00:00,2023-01-18 13:42:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2947785378
- CO2 Emissions (in grams): 2.5972

## Validation Metrics

- Loss: 1.470
- Accuracy: 0.408
- Macro F1: 0.261
- Micro F1: 0.408
- Weighted F1: 0.392
- Macro Precision: 0.285
- Micro Precision: 0.408
- Weighted Precision: 0.421
- Macro Recall: 0.266
- Micro Recall: 0.408
- Weighted Recall: 0.408",,,1,[],[],Computer Vision,2023-01,134919054.3070937,0.3183497757847533,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
116185,Wobbly-Caribou,['PoseyATX/autotrain-data-cleanedfutherfaster'],,84.43005367170522,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-02-02 21:26:49+00:00,2023-01-18 14:48:29+00:00,,,,1,[],[],NLP,2023-01,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
116300,autotrain-flyswot-jan-2950385442,['davanstrien/autotrain-data-flyswot-jan'],,3.560791710013544,AutoTrain,Not Specified,Not Specified,Not Specified,0.941,0.206,0.919,,,347624337.0,True,3,0,"['transformers', 'pytorch']",2023-01-18 18:12:48+00:00,2023-01-18 18:09:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2950385442
- CO2 Emissions (in grams): 3.5608

## Validation Metrics

- Loss: 0.206
- Accuracy: 0.941
- Macro F1: 0.919
- Micro F1: 0.941
- Weighted F1: 0.940
- Macro Precision: 0.941
- Micro Precision: 0.941
- Weighted Precision: 0.940
- Macro Recall: 0.901
- Micro Recall: 0.941
- Weighted Recall: 0.941",,,1,[],[],Computer Vision,2023-01,97625574.6783565,0.9298698924731182,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
116532,autotrain-pos_neg_v4-2955385528,['docoolthing/autotrain-data-pos_neg_v4'],,0.745848768023924,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.322,1.0,,,433320053.0,True,11,0,"['transformers', 'pytorch']",2023-01-19 01:03:49+00:00,2023-01-19 01:03:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2955385528
- CO2 Emissions (in grams): 0.7458

## Validation Metrics

- Loss: 0.322
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/docoolthing/autotrain-pos_neg_v4-2955385528
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""docoolthing/autotrain-pos_neg_v4-2955385528"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""docoolthing/autotrain-pos_neg_v4-2955385528"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,580975757.5226038,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
116672,autotrain-contract-new-classifier-19thjan-2958385563,['ranajoy98/autotrain-data-contract-new-classifier-19thjan'],,5.453836274077357,AutoTrain,Not Specified,Not Specified,Not Specified,0.965,0.159,0.964,,,438023285.0,True,2,0,"['transformers', 'pytorch']",2023-01-19 08:27:04+00:00,2023-01-19 08:24:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2958385563
- CO2 Emissions (in grams): 5.4538

## Validation Metrics

- Loss: 0.159
- Accuracy: 0.965
- Macro F1: 0.964
- Micro F1: 0.965
- Weighted F1: 0.965
- Macro Precision: 0.964
- Micro Precision: 0.965
- Weighted Precision: 0.965
- Macro Recall: 0.964
- Micro Recall: 0.965
- Weighted Recall: 0.965


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ranajoy98/autotrain-contract-new-classifier-19thjan-2958385563
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ranajoy98/autotrain-contract-new-classifier-19thjan-2958385563"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ranajoy98/autotrain-contract-new-classifier-19thjan-2958385563"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,80314711.14781527,0.9644997407983412,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
116732,autotrain-reconocimiento_banderas-2960885598,['Pafebla/autotrain-data-reconocimiento_banderas'],,1.3323699904171715,AutoTrain,Not Specified,Not Specified,Not Specified,0.95,0.266,0.943,,,343271789.0,True,1,0,"['transformers', 'pytorch']",2023-01-19 10:47:53+00:00,2023-01-19 10:46:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2960885598
- CO2 Emissions (in grams): 1.3324

## Validation Metrics

- Loss: 0.266
- Accuracy: 0.950
- Macro F1: 0.943
- Micro F1: 0.950
- Weighted F1: 0.949
- Macro Precision: 0.963
- Micro Precision: 0.950
- Weighted Precision: 0.956
- Macro Recall: 0.933
- Micro Recall: 0.950
- Weighted Recall: 0.950",,,1,[],[],Computer Vision,2023-01,257639988.4933763,0.94648705758056,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
116797,autotrain-enchondroma-vs-low-grade-chondrosarcoma-histology-2962985627,['logannyeMD/autotrain-data-enchondroma-vs-low-grade-chondrosarcoma-histology'],,3.6593488665934646,AutoTrain,Not Specified,Not Specified,Not Specified,0.887,0.229,,,,343268717.0,True,1,0,"['transformers', 'pytorch']",2023-01-19 13:29:49+00:00,2023-01-19 13:25:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2962985627
- CO2 Emissions (in grams): 3.6593

## Validation Metrics

- Loss: 0.229
- Accuracy: 0.887
- Precision: 0.939
- Recall: 0.821
- AUC: 0.969
- F1: 0.876",,,1,[],[],Computer Vision,2023-01,93805955.51676749,0.887,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117315,moderate-101-rejection-examiner,['Awesome7749/autotrain-data-patent-101'],,3.752263652532065,AutoTrain,Not Specified,Not Specified,Not Specified,0.851,0.353,0.566,,,556848625.0,True,6,4,"['transformers', 'pytorch']",2023-01-20 03:07:35+00:00,2023-01-20 03:05:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2952885909
- CO2 Emissions (in grams): 3.7523

## Validation Metrics

- Loss: 0.353
- Accuracy: 0.851
- Precision: 0.598
- Recall: 0.537
- AUC: 0.846
- F1: 0.566

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Awesome7749/autotrain-patent-101-2952885909
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Awesome7749/autotrain-patent-101-2952885909"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Awesome7749/autotrain-patent-101-2952885909"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,148403384.3475346,0.6798390966831334,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117316,conservative-101-rejection-examiner,['Awesome7749/autotrain-data-patent-101'],,1.761467513119125,AutoTrain,Not Specified,Not Specified,Not Specified,0.854,0.359,0.424,,,438007925.0,True,2,0,"['transformers', 'pytorch']",2023-01-20 03:06:39+00:00,2023-01-20 03:05:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2952885911
- CO2 Emissions (in grams): 1.7615

## Validation Metrics

- Loss: 0.359
- Accuracy: 0.854
- Precision: 0.744
- Recall: 0.296
- AUC: 0.832
- F1: 0.424

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Awesome7749/autotrain-patent-101-2952885911
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Awesome7749/autotrain-patent-101-2952885911"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Awesome7749/autotrain-patent-101-2952885911"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,248660802.2786613,0.566660406885759,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117359,WaspImageRecComap,['Arm627/autotrain-data-wasp_classification_2.0'],,0.8068778967193102,AutoTrain,Not Specified,Not Specified,Not Specified,0.697,0.647,,,,343268717.0,True,0,0,"['transformers', 'pytorch']",2023-01-20 05:56:48+00:00,2023-01-20 05:55:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2977085925
- CO2 Emissions (in grams): 0.8069

## Validation Metrics

- Loss: 0.647
- Accuracy: 0.697
- Precision: 0.065
- Recall: 0.667
- AUC: 0.601
- F1: 0.118",,,1,[],[],Computer Vision,2023-01,425428331.0965617,0.697,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117409,autotrain-contract_risk_identification-2978385936,['samaresh55/autotrain-data-contract_risk_identification'],,1.583281503071558,AutoTrain,Not Specified,Not Specified,Not Specified,0.67,0.771,0.667,,,438010997.0,True,5,0,"['transformers', 'pytorch']",2023-01-20 08:33:21+00:00,2023-01-20 08:31:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2978385936
- CO2 Emissions (in grams): 1.5833

## Validation Metrics

- Loss: 0.771
- Accuracy: 0.670
- Macro F1: 0.667
- Micro F1: 0.670
- Weighted F1: 0.667
- Macro Precision: 0.673
- Micro Precision: 0.670
- Weighted Precision: 0.673
- Macro Recall: 0.670
- Micro Recall: 0.670
- Weighted Recall: 0.670


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/samaresh55/autotrain-contract_risk_identification-2978385936
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""samaresh55/autotrain-contract_risk_identification-2978385936"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""samaresh55/autotrain-contract_risk_identification-2978385936"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,276647580.4525354,0.6684966342557966,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117480,autotrain-test2-2979285951,['r-kaichi/autotrain-data-test2'],,22.168745481272524,AutoTrain,Not Specified,Not Specified,Not Specified,,0.254,,,,2950733825.0,True,0,0,"['transformers', 'pytorch']",2023-01-20 11:50:58+00:00,2023-01-20 11:40:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 2979285951
- CO2 Emissions (in grams): 22.1687

## Validation Metrics

- Loss: 0.254
- SacreBLEU: 7.587
- Gen len: 19.000",,,1,[],[],NLP,2023-01,133103329.07618472,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117577,rottentomato-classifier,['tkurtulus/autotrain-data-rottentomato'],,0.7137118018641835,AutoTrain,Not Specified,Not Specified,Not Specified,0.808,0.416,0.808,,,267855533.0,True,2,0,"['transformers', 'pytorch']",2023-01-20 14:44:38+00:00,2023-01-20 14:44:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2981285985
- CO2 Emissions (in grams): 0.7137

## Validation Metrics

- Loss: 0.416
- Accuracy: 0.808
- Macro F1: 0.808
- Micro F1: 0.808
- Weighted F1: 0.808
- Macro Precision: 0.809
- Micro Precision: 0.808
- Weighted Precision: 0.809
- Macro Recall: 0.808
- Micro Recall: 0.808
- Weighted Recall: 0.808


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/tkurtulus/autotrain-rottentomato-2981285985
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""tkurtulus/autotrain-rottentomato-2981285985"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""tkurtulus/autotrain-rottentomato-2981285985"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,375299290.6946099,0.808,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117655,autotrain-retrain-db16d58-2983986070,['sbrandeis-test-org/autotrain-data-retrain-db16d58'],,0.5759791564661282,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,1.0,,,110397937.0,True,3,0,"['transformers', 'pytorch']",2023-01-20 17:49:13+00:00,2023-01-20 17:48:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2983986070
- CO2 Emissions (in grams): 0.5760

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-01,191670020.9732194,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117681,Bronze_Buffalo_89,['PoseyATX/autotrain-data-caribou2billsum'],,611.8280914546775,AutoTrain,Not Specified,Not Specified,Not Specified,,0.199,,0.89446,0.87385,2283804653.0,True,0,0,"['transformers', 'pytorch']",2023-01-21 02:10:27+00:00,2023-01-20 19:16:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2988086239
- CO2 Emissions (in grams): 611.8281

## Validation Metrics

- Loss: 0.199
- Rouge1: 89.446
- Rouge2: 86.014
- RougeL: 87.385
- RougeLsum: 88.542
- Gen Len: 155.343

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/PoseyATX/autotrain-caribou2billsum-2988086239
```",,,1,[],[],NLP,2023-01,3732755.46660508,0.8840348932031148,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117707,autotrain-artunit-50-500-2970786288,['RowanTELSCorp/autotrain-data-artunit-50-500'],,43.92017027932009,AutoTrain,Not Specified,Not Specified,Not Specified,0.207,3.426,0.197,,,268003181.0,True,2,0,"['transformers', 'pytorch']",2023-01-20 21:04:43+00:00,2023-01-20 20:38:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2970786288
- CO2 Emissions (in grams): 43.9202

## Validation Metrics

- Loss: 3.426
- Accuracy: 0.207
- Macro F1: 0.197
- Micro F1: 0.207
- Weighted F1: 0.197
- Macro Precision: 0.219
- Micro Precision: 0.207
- Weighted Precision: 0.219
- Macro Recall: 0.207
- Micro Recall: 0.207
- Weighted Recall: 0.207


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/RowanTELSCorp/autotrain-artunit-50-500-2970786288
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""RowanTELSCorp/autotrain-artunit-50-500-2970786288"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""RowanTELSCorp/autotrain-artunit-50-500-2970786288"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,6102052.412264665,0.2018762376237623,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117708,autotrain-artunit-50-500-2970786289,['RowanTELSCorp/autotrain-data-artunit-50-500'],,35.90162156358881,AutoTrain,Not Specified,Not Specified,Not Specified,0.208,3.398,0.207,,,263315309.0,True,2,0,"['transformers', 'pytorch']",2023-01-20 21:00:00+00:00,2023-01-20 20:38:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 2970786289
- CO2 Emissions (in grams): 35.9016

## Validation Metrics

- Loss: 3.398
- Accuracy: 0.208
- Macro F1: 0.207
- Micro F1: 0.208
- Weighted F1: 0.207
- Macro Precision: 0.227
- Micro Precision: 0.208
- Weighted Precision: 0.227
- Macro Recall: 0.208
- Micro Recall: 0.208
- Weighted Recall: 0.208


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/RowanTELSCorp/autotrain-artunit-50-500-2970786289
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""RowanTELSCorp/autotrain-artunit-50-500-2970786289"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""RowanTELSCorp/autotrain-artunit-50-500-2970786289"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,7334356.988127039,0.2074987951807229,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117808,Whole-Ostrich88_157,['PoseyATX/autotrain-data-billwork_second_half'],,674.392653539903,AutoTrain,Not Specified,Not Specified,Not Specified,,0.19,,0.8879900000000001,0.8645499999999999,2283804653.0,True,1,0,"['transformers', 'pytorch']",2023-01-21 04:56:51+00:00,2023-01-20 23:52:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2990886304
- CO2 Emissions (in grams): 674.3927

## Validation Metrics

- Loss: 0.190
- Rouge1: 88.799
- Rouge2: 85.137
- RougeL: 86.455
- RougeLsum: 87.865
- Gen Len: 157.049

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/PoseyATX/autotrain-billwork_second_half-2990886304
```",,,1,[],[],NLP,2023-01,3386461.345645234,0.8761132464879547,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117898,autotrain-230121_t5_lcw99-2991486314,['Maeji/autotrain-data-230121_t5_lcw99'],,49.15616288291169,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-21 04:42:27+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117914,autotrain-230121_lcw99_test2-2993186318,['Maeji/autotrain-data-230121_lcw99_test2'],,44.24408554718138,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-21 06:11:26+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
117971,Not-My-Hat,['yigitkucuk/autotrain-data-row-eval'],,3.243662516902224,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,737768761.0,False,2,0,"['transformers', 'pytorch']",2023-01-21 11:01:32+00:00,2023-01-21 09:36:20+00:00,"
# Model

- Problem type: Multi-class Classification
- Model ID: 2994886333
- CO2 Emissions (in grams): 3.2437

## Validation Metrics

- Loss: 0.537
- Accuracy: 0.721
- Macro F1: 0.720
- Micro F1: 0.721
- Weighted F1: 0.720
- Macro Precision: 0.723
- Micro Precision: 0.721
- Weighted Precision: 0.723
- Macro Recall: 0.721
- Micro Recall: 0.721
- Weighted Recall: 0.721


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""Your text here""}' https://api-inference.huggingface.co/models/yigitkucuk/Not-My-Hat
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""yigitkucuk/Not-My-Hat"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""yigitkucuk/Not-My-Hat"", use_auth_token=True)

inputs = tokenizer(""Your text here"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,227449297.56273997,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
118075,testwebhook,['pile-of-law/pile-of-law'],,0.2345,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,,False,0,1,['diffusers'],2023-01-31 15:25:21+00:00,2023-01-21 15:49:15+00:00,,,,1,[],[],Multimodal,2023-01,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
118408,EnglishtoAncientGreekV4,['Tritkoman/autotrain-data-ancientgreek'],,0.220278718686126,AutoTrain,Not Specified,Not Specified,Not Specified,,1.954,,,,4918420761.0,True,0,0,"['transformers', 'pytorch']",2023-01-22 07:39:39+00:00,2023-01-22 06:59:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3005186440
- CO2 Emissions (in grams): 0.2203

## Validation Metrics

- Loss: 1.954
- SacreBLEU: 7.006
- Gen len: 15.616",,,1,[],[],NLP,2023-01,22328170375.860195,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
118923,flan-t5-large,['Jonnylaw/autotrain-data-flan-t5-tunned'],,4.95420834932979,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,3132793669.0,False,0,0,"['transformers', 'pytorch']",2023-02-16 23:10:01+00:00,2023-01-23 04:52:39+00:00,"
# Flan-T5 large, trained to a lot of tasks.



## Validation Metrics

- Loss: 1.344
- Rouge1: 62.583
- Rouge2: 52.337
- RougeL: 59.779
- RougeLsum: 60.437
- Gen Len: 15.639

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Jonnylaw/autotrain-flan-t5-tunned-3016686642
```",,,1,[],[],NLP,2023-01,632350003.8959417,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
118933,autotrain-8-class-contract-classify-3016886646,['ranajoy98/autotrain-data-8-class-contract-classify'],,9.023147775262212,AutoTrain,Not Specified,Not Specified,Not Specified,0.968,0.172,0.972,,,737787193.0,True,12,0,"['transformers', 'pytorch']",2023-01-23 05:18:41+00:00,2023-01-23 05:14:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3016886646
- CO2 Emissions (in grams): 9.0231

## Validation Metrics

- Loss: 0.172
- Accuracy: 0.968
- Macro F1: 0.972
- Micro F1: 0.968
- Weighted F1: 0.968
- Macro Precision: 0.973
- Micro Precision: 0.968
- Weighted Precision: 0.969
- Macro Recall: 0.971
- Micro Recall: 0.968
- Weighted Recall: 0.968


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ranajoy98/autotrain-8-class-contract-classify-3016886646
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ranajoy98/autotrain-8-class-contract-classify-3016886646"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ranajoy98/autotrain-8-class-contract-classify-3016886646"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,81766054.5273027,0.9699958762886596,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
118968,autotrain-contract_risk_identification_3-3017886667,['samaresh55/autotrain-data-contract_risk_identification_3'],,1.5048732544943952,AutoTrain,Not Specified,Not Specified,Not Specified,0.823,0.465,0.836,,,438007925.0,True,2,0,"['transformers', 'pytorch']",2023-01-23 07:22:20+00:00,2023-01-23 07:21:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3017886667
- CO2 Emissions (in grams): 1.5049

## Validation Metrics

- Loss: 0.465
- Accuracy: 0.823
- Precision: 0.834
- Recall: 0.839
- AUC: 0.893
- F1: 0.836

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/samaresh55/autotrain-contract_risk_identification_3-3017886667
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""samaresh55/autotrain-contract_risk_identification_3-3017886667"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""samaresh55/autotrain-contract_risk_identification_3-3017886667"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,291059678.07711565,0.8294490657022301,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
119040,autotrain-auto-retrain-190471b-3020886685,['sbrandeis-test-org/autotrain-data-auto-retrain-190471b'],,0.5727250562055856,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,1.0,,,110397937.0,True,3,0,"['transformers', 'pytorch']",2023-01-23 11:04:26+00:00,2023-01-23 11:03:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3020886685
- CO2 Emissions (in grams): 0.5727

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-01,192759048.69852865,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
119091,autotrain-big_tm4-3021286705,['Aman6917/autotrain-data-big_tm4'],,14.606662709582208,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0,,1.0,1.0,3132580677.0,True,0,1,"['transformers', 'pytorch']",2023-01-23 12:59:40+00:00,2023-01-23 12:48:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3021286705
- CO2 Emissions (in grams): 14.6067

## Validation Metrics

- Loss: 0.000
- Rouge1: 100.000
- Rouge2: 100.000
- RougeL: 100.000
- RougeLsum: 100.000
- Gen Len: 110.556

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Aman6917/autotrain-big_tm4-3021286705
```",,,1,[],[],NLP,2023-01,214462450.40935847,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
119154,TurkishAirlines-SentimentAnalysisModel,['tkurtulus/thycomments'],,1.271844016424588,AutoTrain,Not Specified,Not Specified,Not Specified,0.839,0.489,0.767,,,429738117.0,True,68,0,"['transformers', 'pytorch']",2023-01-30 14:51:41+00:00,2023-01-23 15:01:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3023686751
- CO2 Emissions (in grams): 1.2718

## Validation Metrics

- Loss: 0.489
- Accuracy: 0.839
- Macro F1: 0.767
- Micro F1: 0.839
- Weighted F1: 0.832
- Macro Precision: 0.782
- Micro Precision: 0.839
- Weighted Precision: 0.845
- Macro Recall: 0.770
- Micro Recall: 0.839
- Weighted Recall: 0.839


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love to fly with Turkish Airlines""}' https://api-inference.huggingface.co/models/tkurtulus/TurkishAirlines-SentimentAnalysisModel
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""tkurtulus/TurkishAirlines-SentimentAnalysisModel"")

tokenizer = AutoTokenizer.from_pretrained(""tkurtulus/TurkishAirlines-SentimentAnalysisModel"")

inputs = tokenizer(""I love to fly with Turkish Airlines"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,337885866.0734837,0.8013860523038605,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
119231,autotrain-final_model-3026786824,['Amal98/autotrain-data-final_model'],,3.5122512070831804,AutoTrain,Not Specified,Not Specified,Not Specified,0.94,0.221,0.532,,,1336683629.0,True,3,0,"['transformers', 'pytorch']",2023-01-23 18:08:42+00:00,2023-01-23 18:06:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3026786824
- CO2 Emissions (in grams): 3.5123

## Validation Metrics

- Loss: 0.221
- Accuracy: 0.940
- Precision: 0.557
- Recall: 0.509
- F1: 0.532

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Amal98/autotrain-final_model-3026786824
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Amal98/autotrain-final_model-3026786824"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Amal98/autotrain-final_model-3026786824"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,380577455.9360393,0.6794565217391304,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
119500,autotrain-tm4_2_big-3033986980,['Aman6917/autotrain-data-tm4_2_big'],,14.618973710629987,AutoTrain,Not Specified,Not Specified,Not Specified,,0.0,,1.0,1.0,3132580677.0,True,0,0,"['transformers', 'pytorch']",2023-01-24 07:07:27+00:00,2023-01-24 06:55:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3033986980
- CO2 Emissions (in grams): 14.6190

## Validation Metrics

- Loss: 0.000
- Rouge1: 100.000
- Rouge2: 100.000
- RougeL: 100.000
- RougeLsum: 100.000
- Gen Len: 110.456

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Aman6917/autotrain-tm4_2_big-3033986980
```",,,1,[],[],NLP,2023-01,214281846.25041,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120003,autotrain-enterprise_v_consumer-3052187265,['braedennorris/autotrain-data-enterprise_v_consumer'],,1.1718652256627062,AutoTrain,Not Specified,Not Specified,Not Specified,0.824,0.428,0.848,,,737768761.0,True,1,0,"['transformers', 'pytorch']",2023-01-25 20:36:45+00:00,2023-01-25 03:19:47+00:00,"
Enterprise = 1
Consumer = 0

# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3052187265
- CO2 Emissions (in grams): 1.1719

## Validation Metrics

- Loss: 0.428
- Accuracy: 0.824
- Precision: 0.805
- Recall: 0.896
- AUC: 0.891
- F1: 0.848

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/braedennorris/autotrain-enterprise_v_consumer-3052187265
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""braedennorris/autotrain-enterprise_v_consumer-3052187265"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""braedennorris/autotrain-enterprise_v_consumer-3052187265"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,629567927.133243,0.8358277511961721,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120373,GPT-DMV-125m,['DarwinAnim8or/DMV-Plate-Review'],,20.0,MLCO2,fine-tuning,"Oregon, USA","1 T4, Google Colab",,,,,,551186797.0,False,16,0,"['transformers', 'safetensors', 'pytorch']",2023-03-22 22:38:31+00:00,2023-01-25 20:30:04+00:00,"
# GPT-DMV-125m
A finetuned version of [GPT-Neo-125M](https://huggingface.co/EleutherAI/gpt-neo-125M) on the 'DMV' dataset. (Linked above)
A demo is available [here](https://huggingface.co/spaces/DarwinAnim8or/GPT-DMV-Playground)

(I recommend using the demo playground rather than the Inference window on the right here)

# Training Procedure
This was trained on the 'DMV' dataset, using the ""HappyTransformers"" library on Google Colab.
This model was trained for 5 epochs with learning rate 1e-2.

# Biases & Limitations
This likely contains the same biases and limitations as the original GPT-Neo-125M that it is based on, and additionally heavy biases from the DMV dataset.

# Intended Use
This model is meant for fun, nothing else.

# Sample Use
```python
#Import model:
from happytransformer import HappyGeneration
happy_gen = HappyGeneration(""GPT-NEO"", ""DarwinAnim8or/GPT-DMV-125m"")

#Set generation settings:
from happytransformer import GENSettings
args_top_k = GENSettings(no_repeat_ngram_size=3, do_sample=True,top_k=80, temperature=0.4, max_length=50, early_stopping=False)

#Generate a response:
result = happy_gen.generate_text(""""""PLATE: LUCH
REVIEW REASON CODE: """""", args=args_top_k)

print(result)
print(result.text)
```",,,1,[],[],NLP,2023-01,27559339.85,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120394,GPT-Greentext-125m,['DarwinAnim8or/greentext'],,60.0,MLCO2,fine-tuning,"Oregon, USA","1 T4, Google Colab",,,,,,551186797.0,False,4,1,"['transformers', 'pytorch']",2023-01-29 04:27:35+00:00,2023-01-25 21:38:55+00:00,"
# GPT-Greentext-125m
A finetuned version of [GPT-Neo-125M](https://huggingface.co/EleutherAI/gpt-neo-125M) on the 'greentext' dataset. (Linked above)
Do also take a look at [GPT-Greentext-355m](https://huggingface.co/DarwinAnim8or/GPT-Greentext-355m), the larger size model of this project, it will produce better-quality greentexts than this model can. 
A demo is available [here](https://huggingface.co/spaces/DarwinAnim8or/GPT-Greentext-Playground)
The demo playground is recommended over the inference box on the right, as it uses the larger 355m model.

# Training Procedure
This was trained on the 'greentext' dataset, using the ""HappyTransformers"" library on Google Colab.
This model was trained for 15 epochs with learning rate 1e-2.

# Biases & Limitations
This likely contains the same biases and limitations as the original GPT-Neo-125M that it is based on, and additionally heavy biases from the greentext dataset.
It likely will generate offensive output. 

# Intended Use
This model is meant for fun, nothing else.

# Sample Use
```python
#Import model:
from happytransformer import HappyGeneration
happy_gen = HappyGeneration(""GPT-NEO"", ""DarwinAnim8or/GPT-Greentext-125m"")

#Set generation settings:
from happytransformer import GENSettings
args_top_k = GENSettings(no_repeat_ngram_size=2, do_sample=True,top_k=80, temperature=0.4, max_length=150, early_stopping=False)

#Generate a response:
result = happy_gen.generate_text("""""">be me
>"""""", args=args_top_k)

print(result)
print(result.text)
```",,,1,[],[],NLP,2023-01,9186446.616666667,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120606,autotrain-text2sql-t5-3071587538,['charanhu/autotrain-data-text2sql-t5'],,37.36786127948564,AutoTrain,Not Specified,Not Specified,Not Specified,,0.163,,,,3132580677.0,True,0,1,"['transformers', 'pytorch']",2023-01-26 06:44:11+00:00,2023-01-26 06:16:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3071587538
- CO2 Emissions (in grams): 37.3679

## Validation Metrics

- Loss: 0.163
- SacreBLEU: 75.074
- Gen len: 41.531",,,1,[],[],NLP,2023-01,83830879.5242648,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120627,text_to_sql_5,['charanhu/autotrain-data-text_to_sql_finetune'],,14.683238550750524,AutoTrain,Not Specified,Not Specified,Not Specified,,0.159,,,,3132580677.0,True,82,0,"['transformers', 'pytorch']",2023-01-26 07:50:52+00:00,2023-01-26 07:40:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3073487572
- CO2 Emissions (in grams): 14.6832

## Validation Metrics

- Loss: 0.159
- SacreBLEU: 72.889
- Gen len: 40.580",,,1,[],[],NLP,2023-01,213343988.53309372,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120628,text_to_sql_2,['spider'],5425876.0,14.712742960025288,AutoTrain,Not Specified,Not Specified,Not Specified,,0.14,,,,3132580677.0,True,118,0,"['transformers', 'pytorch']",2023-01-27 08:43:56+00:00,2023-01-26 07:40:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3073487568
- CO2 Emissions (in grams): 14.7127

## Validation Metrics

- Loss: 0.140
- SacreBLEU: 77.653
- Gen len: 42.019",,,1,[],[],NLP,2023-01,212916156.11794904,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120629,text_to_sql_3,['charanhu/autotrain-data-text_to_sql_finetune'],,20.566492426746724,AutoTrain,Not Specified,Not Specified,Not Specified,,0.16,,,,3132580677.0,True,3,0,"['transformers', 'pytorch']",2023-01-26 07:56:17+00:00,2023-01-26 07:40:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3073487570
- CO2 Emissions (in grams): 20.5665

## Validation Metrics

- Loss: 0.160
- SacreBLEU: 76.002
- Gen len: 38.850",,,1,[],[],NLP,2023-01,152314775.50960895,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120630,text_to_sql_1,['charanhu/autotrain-data-text_to_sql_finetune'],,16.03787641705279,AutoTrain,Not Specified,Not Specified,Not Specified,,0.14,,,,3132580677.0,True,8,0,"['transformers', 'pytorch']",2023-01-26 07:52:04+00:00,2023-01-26 07:40:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3073487571
- CO2 Emissions (in grams): 16.0379

## Validation Metrics

- Loss: 0.140
- SacreBLEU: 77.653
- Gen len: 42.019",,,1,[],[],NLP,2023-01,195323906.70308337,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120631,text_to_sql_4,['charanhu/autotrain-data-text_to_sql_finetune'],,15.216605611144294,AutoTrain,Not Specified,Not Specified,Not Specified,,0.159,,,,3132580677.0,True,6,0,"['transformers', 'pytorch']",2023-01-26 07:51:46+00:00,2023-01-26 07:40:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3073487569
- CO2 Emissions (in grams): 15.2166

## Validation Metrics

- Loss: 0.159
- SacreBLEU: 72.889
- Gen len: 40.580",,,1,[],[],NLP,2023-01,205865930.75040135,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120683,sunnishia_textclass,['asaderu/autotrain-data-sunnishia'],,2.79648714853368,AutoTrain,Not Specified,Not Specified,Not Specified,0.98,0.073,0.979,,,442548341.0,True,2,0,"['transformers', 'pytorch']",2023-01-26 09:16:39+00:00,2023-01-26 09:01:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3075387600
- CO2 Emissions (in grams): 2.7965

## Validation Metrics

- Loss: 0.073
- Accuracy: 0.980
- Precision: 0.986
- Recall: 0.972
- AUC: 0.999
- F1: 0.979

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/asaderu/autotrain-sunnishia-3075387600
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""asaderu/autotrain-sunnishia-3075387600"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""asaderu/autotrain-sunnishia-3075387600"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,158251519.6724746,0.9794997447677388,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120759,r-nr-categorization-patent-deberta,['eeshan/autotrain-data-r-nr-categorization'],,17.1013640357776,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-26 11:32:42+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
120933,biomedical-ner-all,"['tner/bc5cdr', 'commanderstrife/jnlpba', 'bc2gm_corpus', 'drAbreu/bc4chemd_ner', 'linnaeus', 'chintagunta85/ncbi_disease']",18933861.0,0.0279399890043426,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,265743541.0,False,9,1,"['transformers', 'pytorch']",2023-02-01 03:39:22+00:00,2023-01-26 15:41:19+00:00,"
## About the Model
An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased

- Dataset: Maccrobat https://figshare.com/articles/dataset/MACCROBAT2018/9764942
- Carbon emission: 0.0279399890043426 Kg
- Training time: 30.16527 minutes
- GPU used : 1 x GeForce RTX 3060 Laptop GPU

Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18

## Usage
The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.
```python
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained(""d4data/biomedical-ner-all"")
model = AutoModelForTokenClassification.from_pretrained(""d4data/biomedical-ner-all"")

pipe = pipeline(""ner"", model=model, tokenizer=tokenizer, aggregation_strategy=""simple"") # pass device=0 if using gpu
pipe(""""""The patient reported no recurrence of palpitations at follow-up 6 months after the ablation."""""")
```

## Author
This model is part of the Research topic ""AI in Biomedical field"" conducted by Deepak John Reji, Shaina Raza. If you use this work (code, model or dataset), please star at:
> https://github.com/dreji18/Bio-Epidemiology-NER",,,1,[],[],NLP,2023-01,9511225682.969902,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,2,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
121026,Moist-Pony,['PoseyATX/autotrain-data-dbarttrain2'],,140.6871460520222,AutoTrain,Not Specified,Not Specified,Not Specified,,1.413,,0.5792499999999999,0.44952,1222363741.0,True,0,0,"['transformers', 'pytorch']",2023-01-26 20:14:29+00:00,2023-01-26 18:57:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3083787793
- CO2 Emissions (in grams): 140.6871

## Validation Metrics

- Loss: 1.413
- Rouge1: 57.925
- Rouge2: 36.683
- RougeL: 44.952
- RougeLsum: 50.807
- Gen Len: 120.034

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/PoseyATX/autotrain-dbarttrain2-3083787793
```",,,1,[],[],NLP,2023-01,8688524.682617443,0.5062053909037005,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
121207,ssclass_best,['asaderu/autotrain-data-sunnishia_2'],,3.830888580049552,AutoTrain,Not Specified,Not Specified,Not Specified,0.98,0.073,0.981,,,442551413.0,True,2,0,"['transformers', 'pytorch']",2023-01-27 03:07:55+00:00,2023-01-27 03:05:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3090387920
- CO2 Emissions (in grams): 3.8309

## Validation Metrics

- Loss: 0.073
- Accuracy: 0.980
- Macro F1: 0.981
- Micro F1: 0.980
- Weighted F1: 0.980
- Macro Precision: 0.981
- Micro Precision: 0.980
- Weighted Precision: 0.980
- Macro Recall: 0.982
- Micro Recall: 0.980
- Weighted Recall: 0.980


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/asaderu/autotrain-sunnishia_2-3090387920
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""asaderu/autotrain-sunnishia_2-3090387920"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""asaderu/autotrain-sunnishia_2-3090387920"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,115521870.1229561,0.9804997450280468,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
121403,Poem-Sentimentale,['yigitkucuk/poem-sentimentale-dataset'],,0.7402856123778213,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,433326197.0,False,4,0,"['transformers', 'pytorch']",2023-01-27 14:59:42+00:00,2023-01-27 12:59:06+00:00,"
# Model

- Problem type: Multi-class Classification
- Model ID: 3099088026
- CO2 Emissions (in grams): 0.7403

## Validation Metrics

- Loss: 0.576
- Accuracy: 0.827
- Macro F1: 0.711
- Micro F1: 0.827
- Weighted F1: 0.827
- Macro Precision: 0.708
- Micro Precision: 0.827
- Weighted Precision: 0.828
- Macro Recall: 0.716
- Micro Recall: 0.827
- Weighted Recall: 0.827


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/yigitkucuk/Poem-Sentimentale
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""yigitkucuk/Poem-Sentimentale"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""yigitkucuk/poem-sentimentale-dataset"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,585350018.6342164,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
121911,autotrain-lucy-light-control-3122788375,['ankleBowl/autotrain-data-lucy-light-control'],,0.5335980780308736,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.003,1.0,,,265507877.0,True,61,0,"['transformers', 'pytorch']",2023-01-28 20:02:30+00:00,2023-01-28 20:01:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3122788375
- CO2 Emissions (in grams): 0.5336

## Validation Metrics

- Loss: 0.003
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ankleBowl/autotrain-lucy-light-control-3122788375
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""ankleBowl/autotrain-lucy-light-control-3122788375"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ankleBowl/autotrain-lucy-light-control-3122788375"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,497580272.3649202,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
121921,autotrain-ex-and-pt-3122688386,['Lloviant/autotrain-data-ex-and-pt'],,0.6202842405816136,AutoTrain,Not Specified,Not Specified,Not Specified,0.571,1.338,0.389,,,110407153.0,True,5,0,"['transformers', 'pytorch']",2023-01-28 20:41:18+00:00,2023-01-28 20:40:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3122688386
- CO2 Emissions (in grams): 0.6203

## Validation Metrics

- Loss: 1.338
- Accuracy: 0.571
- Macro F1: 0.389
- Micro F1: 0.571
- Weighted F1: 0.429
- Macro Precision: 0.333
- Micro Precision: 0.571
- Weighted Precision: 0.357
- Macro Recall: 0.500
- Micro Recall: 0.571
- Weighted Recall: 0.571",,,1,[],[],Computer Vision,2023-01,177994451.21558467,0.4627479166666666,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
121922,autotrain-ex-and-pt-3122688387,['Lloviant/autotrain-data-ex-and-pt'],,0.5722366196083666,AutoTrain,Not Specified,Not Specified,Not Specified,0.571,1.749,0.444,,,343281005.0,True,2,0,"['transformers', 'pytorch']",2023-01-28 20:41:18+00:00,2023-01-28 20:40:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3122688387
- CO2 Emissions (in grams): 0.5722

## Validation Metrics

- Loss: 1.749
- Accuracy: 0.571
- Macro F1: 0.444
- Micro F1: 0.571
- Weighted F1: 0.476
- Macro Precision: 0.417
- Micro Precision: 0.571
- Weighted Precision: 0.429
- Macro Recall: 0.500
- Micro Recall: 0.571
- Weighted Recall: 0.571",,,1,[],[],Computer Vision,2023-01,599893458.8194274,0.4995546798029556,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
121923,autotrain-ex-and-pt-3122688388,['Lloviant/autotrain-data-ex-and-pt'],,0.2158114227532694,AutoTrain,Not Specified,Not Specified,Not Specified,0.0,1.818,0.0,,,94407757.0,True,2,0,"['transformers', 'pytorch']",2023-01-28 20:40:53+00:00,2023-01-28 20:40:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3122688388
- CO2 Emissions (in grams): 0.2158

## Validation Metrics

- Loss: 1.818
- Accuracy: 0.000
- Macro F1: 0.000
- Micro F1: 0.000
- Weighted F1: 0.000
- Macro Precision: 0.000
- Micro Precision: 0.000
- Weighted Precision: 0.000
- Macro Recall: 0.000
- Micro Recall: 0.000
- Weighted Recall: 0.000",,,1,[],[],Computer Vision,2023-01,437454865.8989821,0.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
121924,autotrain-ex-and-pt-3122688389,['Lloviant/autotrain-data-ex-and-pt'],,0.7206152092702812,AutoTrain,Not Specified,Not Specified,Not Specified,0.286,1.599,0.25,,,347616145.0,True,5,0,"['transformers', 'pytorch']",2023-01-28 20:41:40+00:00,2023-01-28 20:40:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3122688389
- CO2 Emissions (in grams): 0.7206

## Validation Metrics

- Loss: 1.599
- Accuracy: 0.286
- Macro F1: 0.250
- Micro F1: 0.286
- Weighted F1: 0.286
- Macro Precision: 0.250
- Micro Precision: 0.286
- Weighted Precision: 0.286
- Macro Recall: 0.250
- Micro Recall: 0.286
- Weighted Recall: 0.286",,,1,[],[],Computer Vision,2023-01,482388021.4129918,0.2667910447761194,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
121925,autotrain-ex-and-pt-3122688390,['Lloviant/autotrain-data-ex-and-pt'],,0.4228512772358779,AutoTrain,Not Specified,Not Specified,Not Specified,0.286,1.919,0.214,,,346872697.0,True,2,0,"['transformers', 'pytorch']",2023-01-28 20:41:24+00:00,2023-01-28 20:40:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3122688390
- CO2 Emissions (in grams): 0.4229

## Validation Metrics

- Loss: 1.919
- Accuracy: 0.286
- Macro F1: 0.214
- Micro F1: 0.286
- Weighted F1: 0.184
- Macro Precision: 0.194
- Micro Precision: 0.286
- Weighted Precision: 0.167
- Macro Recall: 0.333
- Micro Recall: 0.286
- Weighted Recall: 0.286",,,1,[],[],Computer Vision,2023-01,820318432.6826687,0.2448159999999999,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
122001,is-this-furry,[''],,2.875222895985932,AutoTrain,Not Specified,Not Specified,Not Specified,0.933,0.175,0.938,,,347599761.0,True,8,0,"['transformers', 'pytorch']",2023-01-29 01:53:17+00:00,2023-01-29 01:53:17+00:00,"
This detects furry images, mostly profile pictures, although it may be able detect any sort of furry picture (I haven't tried it, though).

# Dataset Info

This was trained on scraped pfp images from Mastodon, with some non-pfp images thrown in for ""balancing"" (i.e ensuring pokemon, kemonomimi (catgirls/foxgirls/etc), and normal animals weren't classified as 'furry')

**Furry images**: 551  
**Non-furry images**: 641  

# Disclaimer

Please do not ruin this by using this to harass anyone.  
This is *not* intended to be used for targeted harrassement, and I will explicitly condemn any use that attempts to do so.

If you're wondering why I made this public in the first place?  
I believe in freedom of *information* - this image classification model has various perfectly valid uses, and it's kinda useless to keep it private.

# Statistics

## Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 2890884434
- CO2 Emissions (in grams): 2.8752

## Validation Metrics

- Loss: 0.175
- Accuracy: 0.933
- Precision: 0.938
- Recall: 0.938
- AUC: 0.975
- F1: 0.938
",,,1,[],[],Computer Vision,2023-01,120894891.83091868,0.9354933190807057,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
122039,GPT-Greentext-355m,['DarwinAnim8or/greentext'],,60.0,MLCO2,fine-tuning,"Oregon, USA","1 T4, Google Colab",,,,,,1444569373.0,False,34,0,"['transformers', 'safetensors', 'pytorch']",2023-03-20 10:55:09+00:00,2023-01-29 02:47:49+00:00,"
# GPT-Greentext-355m
A finetuned version of [GPT2-Medium](https://huggingface.co/gpt2-medium) on the 'greentext' dataset. (Linked above)
A demo is available [here](https://huggingface.co/spaces/DarwinAnim8or/GPT-Greentext-Playground)
The demo playground is recommended over the inference box on the right. 

# Training Procedure
This was trained on the 'greentext' dataset, using the ""HappyTransformers"" library on Google Colab.
This model was trained for 15 epochs with learning rate 1e-2.

# Biases & Limitations
This likely contains the same biases and limitations as the original GPT2 that it is based on, and additionally heavy biases from the greentext dataset.
It likely will generate offensive output. 

# Intended Use
This model is meant for fun, nothing else.

# Sample Use
```python
#Import model:
from happytransformer import HappyGeneration
happy_gen = HappyGeneration(""GPT2"", ""DarwinAnim8or/GPT-Greentext-355m"")

#Set generation settings:
from happytransformer import GENSettings
args_top_k = GENSettingsGENSettings(no_repeat_ngram_size=3, do_sample=True, top_k=80, temperature=0.8, max_length=150, early_stopping=False)

#Generate a response:
result = happy_gen.generate_text("""""">be me
>"""""", args=args_top_k)

print(result)
print(result.text)
```",,,1,[],[],NLP,2023-01,24076156.216666665,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,1,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
122110,Reviews-Sentiment-Analysis,['Kaludi/data-reviews-sentiment-analysis'],,24.76716845191504,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,737768761.0,False,267,0,"['transformers', 'pytorch']",2023-02-25 19:34:11+00:00,2023-01-29 06:18:53+00:00,"
# Reviews Sentiment Analysis

A tool that analyzes the overall sentiment of customer reviews for a specific product or service, whether it’s positive or negative. This analysis is performed by using natural language processing algorithms and machine learning from the model ‘Reviews-Sentiment-Analysis’ trained by Kaludi, allowing businesses to gain valuable insights into customer satisfaction and improve their products and services accordingly.

## Training Procedure

- learning_rate = 1e-5
- batch_size = 32
- warmup = 600
- max_seq_length = 128
- num_train_epochs = 10.0

## Validation Metrics

- Loss: 0.159
- Accuracy: 0.952
- Precision: 0.965
- Recall: 0.938
- AUC: 0.988
- F1: 0.951

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I don't feel like you trust me to do my job.""}' https://api-inference.huggingface.co/models/Kaludi/Reviews-Sentiment-Analysis
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Kaludi/Reviews-Sentiment-Analysis"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Kaludi/Reviews-Sentiment-Analysis"", use_auth_token=True)

inputs = tokenizer(""I don't feel like you trust me to do my job."", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,29788175.5208458,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
122317,autotrain-chest-xray-demo-3129688461,['learnsolana/autotrain-data-chest-xray-demo'],,8.032313770239128,AutoTrain,Not Specified,Not Specified,Not Specified,0.774,0.445,,,,94374989.0,True,2,0,"['transformers', 'pytorch']",2023-01-29 15:53:06+00:00,2023-01-29 15:43:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3129688461
- CO2 Emissions (in grams): 8.0323

## Validation Metrics

- Loss: 0.445
- Accuracy: 0.774
- Precision: 0.738
- Recall: 0.990
- AUC: 0.920
- F1: 0.846",,,1,[],[],Computer Vision,2023-01,11749415.13735094,0.774,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
122373,Food-Classification,['Kaludi/data-food-classification'],,2.774520323133161,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,347620241.0,False,10,0,"['transformers', 'pytorch']",2023-01-31 01:15:08+00:00,2023-01-29 18:45:51+00:00,"
# Food Classification

This is a Food Image Classifier model that has been trained by [Kaludi](https://huggingface.co/Kaludi) to recognize 7 different types of popular foods, including **apple pie**, **falafel**, **french toast**, **ice cream**, **ramen**, **sushi**, and **tiramisu**. It can accurately classify an image of food into one of these categories by analyzing its visual features. This model can be used by food bloggers, restaurants, and recipe websites to quickly categorize and sort their food images, making it easier to manage their content and provide a better user experience.

### Gradio

Tis model supports a [Gradio](https://github.com/gradio-app/gradio) Web UI to run the data-food-classification model:
[![Open In HF Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/Kaludi/Food-Classification_App)


## Validation Metrics

- Loss: 0.094
- Accuracy: 0.977
- Macro F1: 0.977
- Micro F1: 0.977
- Weighted F1: 0.977
- Macro Precision: 0.978
- Micro Precision: 0.977
- Weighted Precision: 0.978
- Macro Recall: 0.977
- Micro Recall: 0.977
- Weighted Recall: 0.977",,,1,[],[],Computer Vision,2023-01,125290212.54652248,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123062,garbage-image-detection,['kripsjaviya/autotrain-data-ssip2'],,1.817950842757937,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-01-30 19:26:37+00:00,,,,,1,[],[],Computer Vision,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123075,ClaimBuster-DeBERTaV2,['lucafrost/autotrain-data-claimbuster'],,23.10234958653748,AutoTrain,Not Specified,Not Specified,Not Specified,0.842,0.405,0.753,,,737771833.0,True,166,1,"['transformers', 'pytorch']",2023-01-30 20:14:39+00:00,2023-01-30 19:52:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3165789318
- CO2 Emissions (in grams): 23.1023

## Validation Metrics

- Loss: 0.405
- Accuracy: 0.842
- Macro F1: 0.753
- Micro F1: 0.842
- Weighted F1: 0.843
- Macro Precision: 0.750
- Micro Precision: 0.842
- Weighted Precision: 0.844
- Macro Recall: 0.756
- Micro Recall: 0.842
- Weighted Recall: 0.842


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lucafrost/ClaimBuster-DeBERTaV2
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucafrost/ClaimBuster-DeBERTaV2"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucafrost/ClaimBuster-DeBERTaV2"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,31934926.36913107,0.7950169278996865,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123223,helloworld,['scy99/autotrain-data-todo'],,1.5063043935583178,AutoTrain,Not Specified,Not Specified,Not Specified,0.848,0.339,0.7,,,409150133.0,True,2,0,"['transformers', 'pytorch']",2023-01-31 03:20:21+00:00,2023-01-31 03:19:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3171489424
- CO2 Emissions (in grams): 1.5063

## Validation Metrics

- Loss: 0.339
- Accuracy: 0.848
- Precision: 0.679
- Recall: 0.721
- AUC: 0.906
- F1: 0.700

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/scy99/autotrain-todo-3171489424
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""scy99/autotrain-todo-3171489424"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""scy99/autotrain-todo-3171489424"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,271625134.1692441,0.7669250645994832,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,1.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123317,autotrain-test-text-classification-3175589570,['Sushovan/autotrain-data-test-text-classification'],,3.2260052742267447,AutoTrain,Not Specified,Not Specified,Not Specified,0.665,1.111,0.424,,,433369269.0,True,2,1,"['transformers', 'pytorch']",2023-01-31 08:20:56+00:00,2023-01-31 08:18:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3175589570
- CO2 Emissions (in grams): 3.2260

## Validation Metrics

- Loss: 1.111
- Accuracy: 0.665
- Macro F1: 0.424
- Micro F1: 0.665
- Weighted F1: 0.638
- Macro Precision: 0.427
- Micro Precision: 0.665
- Weighted Precision: 0.622
- Macro Recall: 0.434
- Micro Recall: 0.665
- Weighted Recall: 0.665


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Sushovan/autotrain-test-text-classification-3175589570
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Sushovan/autotrain-test-text-classification-3175589570"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Sushovan/autotrain-test-text-classification-3175589570"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,134336193.57732642,0.5178328741965106,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123324,autotrain-hello_summarization-3171289572,['scy99/autotrain-data-hello_summarization'],,25.535336151027007,AutoTrain,Not Specified,Not Specified,Not Specified,,3.536,,0.38529,0.38154,2329702453.0,True,0,0,"['transformers', 'pytorch']",2023-01-31 08:45:52+00:00,2023-01-31 08:31:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3171289572
- CO2 Emissions (in grams): 25.5353

## Validation Metrics

- Loss: 3.536
- Rouge1: 38.529
- Rouge2: 6.769
- RougeL: 38.154
- RougeLsum: 37.958
- Gen Len: 18.864

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/scy99/autotrain-hello_summarization-3171289572
```",,,1,[],[],NLP,2023-01,91234454.06087992,0.3834058307577951,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123386,pneumo_v3,['ashutoshmondal/autotrain-data-pneumo'],,1.9594067819084715,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.017,,,,343268717.0,True,2,0,"['transformers', 'pytorch']",2023-01-31 10:50:06+00:00,2023-01-31 10:47:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3177689678
- CO2 Emissions (in grams): 1.9594

## Validation Metrics

- Loss: 0.017
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-01,175190123.9545852,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123422,autotrain-pneumo-v3-3180589690,['ashutoshmondal/autotrain-data-pneumo-v3'],,3.4021330886626298,AutoTrain,Not Specified,Not Specified,Not Specified,0.964,0.131,,,,343268717.0,True,2,0,"['transformers', 'pytorch']",2023-01-31 12:31:16+00:00,2023-01-31 12:27:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3180589690
- CO2 Emissions (in grams): 3.4021

## Validation Metrics

- Loss: 0.131
- Accuracy: 0.964
- Precision: 0.964
- Recall: 0.964
- AUC: 0.994
- F1: 0.964",,,1,[],[],Computer Vision,2023-01,100898086.01077925,0.964,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123643,autotrain-text-classification-ingredients-3187189756,['happycart/autotrain-data-text-classification-ingredients'],,0.0381073097260659,AutoTrain,Not Specified,Not Specified,Not Specified,0.66,2.762,0.5,,,444163765.0,True,6,0,"['transformers', 'pytorch']",2023-01-31 21:09:44+00:00,2023-01-31 21:04:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3187189756
- CO2 Emissions (in grams): 0.0381

## Validation Metrics

- Loss: 2.762
- Accuracy: 0.660
- Macro F1: 0.500
- Micro F1: 0.660
- Weighted F1: 0.581
- Macro Precision: 0.484
- Micro Precision: 0.660
- Weighted Precision: 0.546
- Macro Recall: 0.548
- Micro Recall: 0.660
- Weighted Recall: 0.660


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/happycart/autotrain-text-classification-ingredients-3187189756
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""happycart/autotrain-text-classification-ingredients-3187189756"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""happycart/autotrain-text-classification-ingredients-3187189756"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-01,11655605399.406776,0.5689655172413793,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123886,wine_quality,['reesu/autotrain-data-wine_quality'],,8.276808778335907,AutoTrain,Not Specified,Not Specified,Not Specified,0.569,0.995,0.296,,,,True,0,0,"['transformers', 'joblib']",2023-02-01 07:54:12+00:00,2023-02-01 07:44:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3195889861
- CO2 Emissions (in grams): 8.2768

## Validation Metrics

- Loss: 0.995
- Accuracy: 0.569
- Macro F1: 0.296
- Micro F1: 0.569
- Weighted F1: 0.543
- Macro Precision: 0.447
- Micro Precision: 0.569
- Weighted Precision: 0.558
- Macro Recall: 0.283
- Micro Recall: 0.569
- Weighted Recall: 0.569

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-02,,0.3894196531791907,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123887,autotrain-wine_quality-3195889865,['reesu/autotrain-data-wine_quality'],,0.8738507920594603,AutoTrain,Not Specified,Not Specified,Not Specified,0.545,15.722,0.226,,,,True,0,0,"['transformers', 'joblib']",2023-02-01 07:46:47+00:00,2023-02-01 07:44:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3195889865
- CO2 Emissions (in grams): 0.8739

## Validation Metrics

- Loss: 15.722
- Accuracy: 0.545
- Macro F1: 0.226
- Micro F1: 0.545
- Weighted F1: 0.500
- Macro Precision: 0.260
- Micro Precision: 0.545
- Weighted Precision: 0.507
- Macro Recall: 0.240
- Micro Recall: 0.545
- Weighted Recall: 0.545

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-02,,0.3195071335927367,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123927,autotrain-okr_iptal-3196789879,['ekincanozcelik/autotrain-data-okr_iptal'],,2.66060887304261,AutoTrain,Not Specified,Not Specified,Not Specified,0.941,0.207,0.95,,,1112254133.0,True,7,0,"['transformers', 'pytorch']",2023-02-01 09:19:11+00:00,2023-02-01 09:17:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3196789879
- CO2 Emissions (in grams): 2.6606

## Validation Metrics

- Loss: 0.207
- Accuracy: 0.941
- Precision: 0.947
- Recall: 0.953
- AUC: 0.980
- F1: 0.950

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ekincanozcelik/autotrain-okr_iptal-3196789879
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ekincanozcelik/autotrain-okr_iptal-3196789879"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ekincanozcelik/autotrain-okr_iptal-3196789879"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,418044961.1626125,0.9454785827604442,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
123958,autotrain-text_cla_2-3198889905,['K304/autotrain-data-text_cla_2'],,3.141305501873425,AutoTrain,Not Specified,Not Specified,Not Specified,0.964,0.165,0.966,,,409167989.0,True,4,0,"['transformers', 'pytorch']",2023-02-01 10:38:12+00:00,2023-02-01 10:36:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3198889905
- CO2 Emissions (in grams): 3.1413

## Validation Metrics

- Loss: 0.165
- Accuracy: 0.964
- Macro F1: 0.966
- Micro F1: 0.964
- Weighted F1: 0.964
- Macro Precision: 0.959
- Micro Precision: 0.964
- Weighted Precision: 0.965
- Macro Recall: 0.975
- Micro Recall: 0.964
- Weighted Recall: 0.964


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/K304/autotrain-text_cla_2-3198889905
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""K304/autotrain-text_cla_2-3198889905"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""K304/autotrain-text_cla_2-3198889905"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,130254121.65610084,0.96499896373057,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
124004,autotrain-wzd_111-3200189938,['K304/autotrain-data-wzd_111'],,0.0141117061535212,AutoTrain,Not Specified,Not Specified,Not Specified,0.993,0.035,0.994,,,409167989.0,True,7,1,"['transformers', 'pytorch']",2023-02-01 11:50:43+00:00,2023-02-01 11:48:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3200189938
- CO2 Emissions (in grams): 0.0141

## Validation Metrics

- Loss: 0.035
- Accuracy: 0.993
- Macro F1: 0.994
- Micro F1: 0.993
- Weighted F1: 0.993
- Macro Precision: 0.996
- Micro Precision: 0.993
- Weighted Precision: 0.993
- Macro Recall: 0.993
- Micro Recall: 0.993
- Weighted Recall: 0.993


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/K304/autotrain-wzd_111-3200189938
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""K304/autotrain-wzd_111-3200189938"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""K304/autotrain-wzd_111-3200189938"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,28994934031.97763,0.9934997483643684,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
124177,rose_charlotte,['Kanr1u/autotrain-data-emma2'],,2.1409787540187346,AutoTrain,Not Specified,Not Specified,Not Specified,0.846,0.303,,,,347599761.0,True,8,0,"['transformers', 'pytorch']",2023-02-01 17:42:27+00:00,2023-02-01 17:39:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3206689984
- CO2 Emissions (in grams): 2.1410

## Validation Metrics

- Loss: 0.303
- Accuracy: 0.846
- Precision: 0.846
- Recall: 0.846
- AUC: 0.929
- F1: 0.846",,,1,[],[],Computer Vision,2023-02,162355539.65565336,0.8460000000000001,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
124263,RecipeGPT,[''],,21.35830579428,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,,False,0,0,"['transformers', 'pytorch']",2023-02-01 22:51:09+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
124402,autotrain-koles_score-3215890190,['DioLiu/autotrain-data-koles_score'],,0.0090072003921208,AutoTrain,Not Specified,Not Specified,Not Specified,0.542,1.187,0.368,,,1334476405.0,True,8,0,"['transformers', 'pytorch']",2023-02-02 05:02:45+00:00,2023-02-02 05:01:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3215890190
- CO2 Emissions (in grams): 0.0090

## Validation Metrics

- Loss: 1.187
- Accuracy: 0.542
- Macro F1: 0.368
- Micro F1: 0.542
- Weighted F1: 0.482
- Macro Precision: 0.331
- Micro Precision: 0.542
- Weighted Precision: 0.434
- Macro Recall: 0.414
- Micro Recall: 0.542
- Weighted Recall: 0.542


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/DioLiu/autotrain-koles_score-3215890190
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""DioLiu/autotrain-koles_score-3215890190"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""DioLiu/autotrain-koles_score-3215890190"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,148156624356.59317,0.4383648351648351,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
124776,autotrain-sns-fake-news-3229590413,['koolerkx/autotrain-data-sns-fake-news'],,0.0156658667652406,AutoTrain,Not Specified,Not Specified,Not Specified,0.832,0.413,0.836,,,737768761.0,True,4,0,"['transformers', 'pytorch']",2023-02-02 18:54:55+00:00,2023-02-02 18:53:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3229590413
- CO2 Emissions (in grams): 0.0157

## Validation Metrics

- Loss: 0.413
- Accuracy: 0.832
- Precision: 0.814
- Recall: 0.860
- AUC: 0.910
- F1: 0.836

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/koolerkx/autotrain-sns-fake-news-3229590413
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""koolerkx/autotrain-sns-fake-news-3229590413"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""koolerkx/autotrain-sns-fake-news-3229590413"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,47094027547.64646,0.8339952038369305,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,1.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
124879,csgo-weapon-classification,['Kaludi/data-csgo-weapon-classification'],,0.0421564161796381,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,347636625.0,False,31,0,"['transformers', 'pytorch']",2023-02-02 23:24:33+00:00,2023-02-02 22:49:14+00:00,"
# CSGO Weapon Classification

This is a CSGO Weapon Classifier Model that has been trained by [Kaludi](https://huggingface.co/Kaludi) to recognize **11** different types of Counter-Strike: Global Offensive (CSGO) Weapons, which include **AK-47,AWP,Famas,Galil-AR,Glock,M4A1,M4A4,P-90,SG-553,UMP,USP**. The model is capable of accurately classifying the weapon name present in an image. With its deep understanding of the characteristics of each weapon in the game, the model is a valuable tool for players and fans of CSGO.

### Gradio

Tis model supports a [Gradio](https://github.com/gradio-app/gradio) Web UI to run the csgo-weapon-classification model:
[![Open In HF Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/Kaludi/CSGO-Weapon-Classification_App)


## Validation Metrics

- Loss: 0.282
- Accuracy: 0.945
- Macro F1: 0.946
- Micro F1: 0.945
- Weighted F1: 0.946
- Macro Precision: 0.948
- Micro Precision: 0.945
- Weighted Precision: 0.948
- Macro Recall: 0.945
- Micro Recall: 0.945
- Weighted Recall: 0.945",,,1,[],[],Computer Vision,2023-02,8246351481.080391,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
124941,food-category-classification,['Kaludi/data-food-classification'],,0.0202275837591701,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,347636625.0,False,6,0,"['transformers', 'pytorch']",2023-02-03 02:32:13+00:00,2023-02-03 01:53:32+00:00,"
# Food Category Classification

This is a Food Category Image Classifier model that has been trained by [Kaludi](https://huggingface.co/Kaludi) to recognize 11 different categories of foods, including **Bread**, **Dairy Product**, **Dessert**, **Egg**, **Fried Food**, **Meat**, **Noodles-Pasta**, **Rice**, **Seafood**, **Soup**, and **Vegetable-Fruit**. It can accurately classify an image of food into one of these categories by analyzing its visual features. This model can be used by food bloggers, restaurants, and recipe websites to quickly categorize and sort their food images, making it easier to manage their content and provide a better user experience.

### Gradio

Tis model supports a [Gradio](https://github.com/gradio-app/gradio) Web UI to run the data-food-classification model:
[![Open In HF Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/Kaludi/Food-Category-Classification_App)


## Validation Metrics

- Loss: 0.079
- Accuracy: 0.978
- Macro F1: 0.978
- Micro F1: 0.978
- Weighted F1: 0.978
- Macro Precision: 0.979
- Micro Precision: 0.978
- Weighted Precision: 0.979
- Macro Recall: 0.978
- Micro Recall: 0.978
- Weighted Recall: 0.978",,,1,[],[],Computer Vision,2023-02,17186265504.519306,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
125376,autotrain-ai-generated-image-classification-3250490787,['NehaBardeDUKE/autotrain-data-ai-generated-image-classification'],,0.0106218167696346,AutoTrain,Not Specified,Not Specified,Not Specified,0.941,0.217,,,,110394865.0,True,10,0,"['transformers', 'pytorch']",2023-02-03 19:20:12+00:00,2023-02-03 19:17:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3250490787
- CO2 Emissions (in grams): 0.0106

## Validation Metrics

- Loss: 0.217
- Accuracy: 0.941
- Precision: 0.929
- Recall: 1.000
- AUC: 1.000
- F1: 0.963",,,1,[],[],Computer Vision,2023-02,10393218730.301792,0.9409999999999998,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
125702,2_Labels,['Ailyth/autotrain-data-2labels'],,2.038789255434584,AutoTrain,Not Specified,Not Specified,Not Specified,0.97,0.044,,,,347599761.0,True,8,0,"['transformers', 'pytorch']",2023-02-10 10:12:52+00:00,2023-02-04 14:58:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3268491180
- CO2 Emissions (in grams): 2.0388

## Validation Metrics

- Loss: 0.044
- Accuracy: 0.970
- Precision: 0.966
- Recall: 0.982
- AUC: 0.998
- F1: 0.974",,,1,[],[],Computer Vision,2023-02,170493227.81814757,0.97,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
126155,Quick-Summarization,['Kaludi/data-quick-summarization'],,460.6785690944488,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,2283804653.0,False,21,0,"['transformers', 'pytorch']",2023-02-05 20:38:18+00:00,2023-02-05 08:57:31+00:00,"
# Quick Summarization

This is a Text Summarization Model that has been trained by [Kaludi](https://huggingface.co/Kaludi) to Transform long and complex texts into concise and meaningful summaries. Get a quick and accurate overview of any document in seconds, saving you time and effort.

### Gradio

Tis model supports a [Gradio](https://github.com/gradio-app/gradio) Web UI to run the data-food-classification model:
[![Open In HF Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/Kaludi/Quick-Summarizer_App)


## Validation Metrics

- Loss: 1.629
- Rouge1: 41.066
- Rouge2: 19.231
- RougeL: 28.295
- RougeLsum: 37.746
- Gen Len: 98.873

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Kaludi/autotrain-quik-sum-3280991391
```",,,1,[],[],NLP,2023-02,4957479.69672054,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,1.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
127246,UltraSound-Lung,['hamdan07/autotrain-data-lungultrasound'],,1.3971381846584354,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,1.0,,,343271789.0,True,3,0,"['transformers', 'pytorch']",2023-02-06 22:26:48+00:00,2023-02-06 22:25:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3310291874
- CO2 Emissions (in grams): 1.3971

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-02,245696376.1848089,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
127817,autotrain-encyclopaedia-illustrations-blog-post-3327992158,['biglam/encyclopaedia_britannica_illustrated'],,13.45295564861545,AutoTrain,Not Specified,Not Specified,Not Specified,0.992,0.025,,,,344436077.0,True,5,0,"['transformers', 'pytorch']",2023-02-22 19:46:17+00:00,2023-02-07 19:00:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3327992158
- CO2 Emissions (in grams): 13.4530

## Validation Metrics

- Loss: 0.025
- Accuracy: 0.992
- Precision: 0.998
- Recall: 0.994
- AUC: 0.998
- F1: 0.996",,,1,[],[],Computer Vision,2023-02,25603003.97893965,0.992,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
127818,autotrain-encyclopaedia-illustrations-blog-post-3327992159,['davanstrien/autotrain-data-encyclopaedia-illustrations-blog-post'],,4.608389135708385,AutoTrain,Not Specified,Not Specified,Not Specified,0.992,0.04,,,,111349029.0,True,3,0,"['transformers', 'pytorch']",2023-02-07 19:04:23+00:00,2023-02-07 19:00:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3327992159
- CO2 Emissions (in grams): 4.6084

## Validation Metrics

- Loss: 0.040
- Accuracy: 0.992
- Precision: 0.994
- Recall: 0.998
- AUC: 0.993
- F1: 0.996",,,1,[],[],Computer Vision,2023-02,24162245.35753833,0.992,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
128055,autotrain-document-text-language-ar-en-zh-3338392240,['ernie-ai/autotrain-data-document-text-language-ar-en-zh'],,2.2266908460523576,AutoTrain,Not Specified,Not Specified,Not Specified,0.882,0.267,0.862,,,110401009.0,True,29,1,"['transformers', 'pytorch']",2023-02-08 19:12:02+00:00,2023-02-08 06:41:29+00:00,"# finetuned-MS-swin-doc-text-classifer

This model is a fine-tuned version of Microsoft’s Swin Transformer tiny-sized model [microsoft/swin-tiny-patch4-window7-224](https://huggingface.co/microsoft/swin-tiny-patch4-window7-224) on the ernie-ai/image-text-examples-ar-cn-latin-notext dataset.
It achieves the following results on the evaluation set:
- Loss: 0.267
- Accuracy: 0.882

## Model description

It is an image classificatin model fine-tuned to predict whether an images contains text and if that text is Latin script, Chinese or Arabic. It also classifies non-text images.

## Training and evaluation data

Dataset: [ernie-ai/image-text-examples-ar-cn-latin-notext]

# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3338392240
- CO2 Emissions (in grams): 2.2267

## Validation Metrics

- Loss: 0.267
- Accuracy: 0.882
- Macro F1: 0.862
- Micro F1: 0.882
- Weighted F1: 0.880
- Macro Precision: 0.877
- Micro Precision: 0.882
- Weighted Precision: 0.883
- Macro Recall: 0.856
- Micro Recall: 0.882
- Weighted Recall: 0.882",,,1,[],[],Computer Vision,2023-02,49580753.06939312,0.8718853211009173,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
128109,3_Labels,['Ailyth/autotrain-data-3lables'],,2.650072914067399,AutoTrain,Not Specified,Not Specified,Not Specified,0.95,0.133,0.951,,,347603857.0,True,8,0,"['transformers', 'pytorch']",2023-02-08 08:57:43+00:00,2023-02-08 08:55:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3341092265
- CO2 Emissions (in grams): 2.6501

## Validation Metrics

- Loss: 0.133
- Accuracy: 0.950
- Macro F1: 0.951
- Micro F1: 0.950
- Weighted F1: 0.950
- Macro Precision: 0.951
- Micro Precision: 0.950
- Weighted Precision: 0.950
- Macro Recall: 0.951
- Micro Recall: 0.950
- Weighted Recall: 0.950",,,1,[],[],Computer Vision,2023-02,131167657.74813676,0.9504997369805364,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
128405,autotrain-greek-sentiment-analysis-3351392404,['kkarpou/autotrain-data-greek-sentiment-analysis'],,4.129267471119826,AutoTrain,Not Specified,Not Specified,Not Specified,0.844,0.479,0.844,,,737768761.0,True,27,0,"['transformers', 'pytorch']",2023-02-08 18:22:33+00:00,2023-02-08 18:20:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3351392404
- CO2 Emissions (in grams): 4.1293

## Validation Metrics

- Loss: 0.479
- Accuracy: 0.844
- Macro F1: 0.844
- Micro F1: 0.844
- Weighted F1: 0.843
- Macro Precision: 0.847
- Micro Precision: 0.844
- Weighted Precision: 0.849
- Macro Recall: 0.846
- Micro Recall: 0.844
- Weighted Recall: 0.844


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/kkarpou/autotrain-greek-sentiment-analysis-3351392404
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kkarpou/autotrain-greek-sentiment-analysis-3351392404"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kkarpou/autotrain-greek-sentiment-analysis-3351392404"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,178668193.85277617,0.844,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
128470,food-category-classification-v2.0,['Kaludi/food-category-classification-v2.0'],,12.456278925446483,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,347640721.0,False,1976,4,"['transformers', 'pytorch']",2023-02-09 19:20:59+00:00,2023-02-08 20:35:47+00:00,"
# Food Category Classification v2.0

This is an updated Food Category Image Classifier model of the [old](https://huggingface.co/Kaludi/food-category-classification) model that has been trained by [Kaludi](https://huggingface.co/Kaludi) to recognize **12** different categories of foods, which includes **Bread**, **Dairy**, **Dessert**, **Egg**, **Fried Food**, **Fruit**, **Meat**, **Noodles**, **Rice**, **Seafood**, **Soup**, and **Vegetable**. It can accurately classify an image of food into one of these categories by analyzing its visual features. This model can be used by food bloggers, restaurants, and recipe websites to quickly categorize and sort their food images, making it easier to manage their content and provide a better user experience.

### Gradio

This model supports a [Gradio](https://github.com/gradio-app/gradio) Web UI to run the data-food-classification model:
[![Open In HF Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/Kaludi/Food-Category-Classification_V2_App)


## Validation Metrics
- Problem type: Multi-class Classification
- Model ID: 3353292434
- CO2 Emissions (in grams): 12.4563
- Loss: 0.144
- Accuracy: 0.960
- Macro F1: 0.959
- Micro F1: 0.960
- Weighted F1: 0.959
- Macro Precision: 0.962
- Micro Precision: 0.960
- Weighted Precision: 0.962
- Macro Recall: 0.960
- Micro Recall: 0.960
- Weighted Recall: 0.960",,,1,[],[],Computer Vision,2023-02,27908874.1574193,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
128992,autotrain-ma-detection-test-3372892714,['mjaydenkim/autotrain-data-ma-detection-test'],,1.2555854454965398,AutoTrain,Not Specified,Not Specified,Not Specified,0.941,0.153,0.928,,,438007925.0,True,5,0,"['transformers', 'pytorch']",2023-02-09 19:08:48+00:00,2023-02-09 19:07:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3372892714
- CO2 Emissions (in grams): 1.2556

## Validation Metrics

- Loss: 0.153
- Accuracy: 0.941
- Precision: 0.892
- Recall: 0.966
- AUC: 0.988
- F1: 0.928

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/mjaydenkim/autotrain-ma-detection-test-3372892714
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mjaydenkim/autotrain-ma-detection-test-3372892714"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mjaydenkim/autotrain-ma-detection-test-3372892714"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,348847564.75237995,0.934454788657036,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129010,autotrain-builty-2-table-searcher-3373492718,['marac5/autotrain-data-builty-2-table-searcher'],,7.738626953271278,AutoTrain,Not Specified,Not Specified,Not Specified,0.871,0.36,0.824,,,,True,3,0,"['transformers', 'joblib']",2023-02-09 20:00:58+00:00,2023-02-09 19:40:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3373492718
- CO2 Emissions (in grams): 7.7386

## Validation Metrics

- Loss: 0.360
- Accuracy: 0.871
- Precision: 0.871
- Recall: 0.783
- AUC: 0.916
- F1: 0.824

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-02,,0.846848377581121,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129236,resnet50_mask_classification,['AllanOuii/autotrain-data-resnet50_mask_classification'],,1.5544780289204296,AutoTrain,Not Specified,Not Specified,Not Specified,0.977,0.138,,,,94374989.0,True,3,0,"['transformers', 'pytorch']",2023-02-10 08:26:05+00:00,2023-02-10 08:11:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3387392923
- CO2 Emissions (in grams): 1.5545

## Validation Metrics

- Loss: 0.138
- Accuracy: 0.977
- Precision: 0.958
- Recall: 1.000
- AUC: 0.996
- F1: 0.979",,,1,[],[],Computer Vision,2023-02,60711690.5123082,0.977,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129350,autotrain-dataset-mentions-3390592983,['davanstrien/autotrain-data-dataset-mentions'],,0.0089996665628707,AutoTrain,Not Specified,Not Specified,Not Specified,0.997,0.014,0.998,,,263167661.0,True,5,0,"['transformers', 'pytorch']",2023-02-10 14:01:28+00:00,2023-02-10 11:19:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3390592983
- CO2 Emissions (in grams): 0.0090

## Validation Metrics

- Loss: 0.014
- Accuracy: 0.997
- Precision: 0.998
- Recall: 0.997
- AUC: 1.000
- F1: 0.998

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/davanstrien/autotrain-dataset-mentions-3390592983
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""davanstrien/autotrain-dataset-mentions-3390592983"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""davanstrien/autotrain-dataset-mentions-3390592983"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,29241934594.08069,0.9974997493734336,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129425,autotrain-histopathological_image_classification-3393093035,['JoffreyMa/autotrain-data-histopathological_image_classification'],,4.012874943915816,AutoTrain,Not Specified,Not Specified,Not Specified,0.933,0.183,0.931,,,110413297.0,True,7,0,"['transformers', 'pytorch']",2023-02-10 13:31:08+00:00,2023-02-10 13:26:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3393093035
- CO2 Emissions (in grams): 4.0129

## Validation Metrics

- Loss: 0.183
- Accuracy: 0.933
- Macro F1: 0.931
- Micro F1: 0.933
- Weighted F1: 0.933
- Macro Precision: 0.927
- Micro Precision: 0.933
- Weighted Precision: 0.935
- Macro Recall: 0.939
- Micro Recall: 0.933
- Weighted Recall: 0.933",,,1,[],[],Computer Vision,2023-02,27514761.49721657,0.9319989270386266,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
129426,autotrain-histopathological_image_classification-3393093036,['JoffreyMa/autotrain-data-histopathological_image_classification'],,4.820448255825163,AutoTrain,Not Specified,Not Specified,Not Specified,0.933,0.186,0.933,,,343287149.0,True,7,0,"['transformers', 'pytorch']",2023-02-10 13:31:25+00:00,2023-02-10 13:27:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3393093036
- CO2 Emissions (in grams): 4.8204

## Validation Metrics

- Loss: 0.186
- Accuracy: 0.933
- Macro F1: 0.933
- Micro F1: 0.933
- Weighted F1: 0.932
- Macro Precision: 0.929
- Micro Precision: 0.933
- Weighted Precision: 0.934
- Macro Recall: 0.941
- Micro Recall: 0.933
- Weighted Recall: 0.933",,,1,[],[],Computer Vision,2023-02,71214777.29486306,0.933,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129427,autotrain-histopathological_image_classification-3393093038,['JoffreyMa/autotrain-data-histopathological_image_classification'],,3.5030531186190697,AutoTrain,Not Specified,Not Specified,Not Specified,0.966,0.179,0.959,,,347624337.0,True,10,0,"['transformers', 'pytorch']",2023-02-10 13:30:31+00:00,2023-02-10 13:27:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3393093038
- CO2 Emissions (in grams): 3.5031

## Validation Metrics

- Loss: 0.179
- Accuracy: 0.966
- Macro F1: 0.959
- Micro F1: 0.966
- Weighted F1: 0.966
- Macro Precision: 0.969
- Micro Precision: 0.966
- Weighted Precision: 0.969
- Macro Recall: 0.954
- Micro Recall: 0.966
- Weighted Recall: 0.966",,,1,[],[],Computer Vision,2023-02,99234674.7904971,0.9624872727272726,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
129428,autotrain-histopathological_image_classification-3393093039,['JoffreyMa/autotrain-data-histopathological_image_classification'],,3.927473671872376,AutoTrain,Not Specified,Not Specified,Not Specified,0.91,0.243,0.928,,,346878841.0,True,4,0,"['transformers', 'pytorch']",2023-02-10 13:31:34+00:00,2023-02-10 13:27:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3393093039
- CO2 Emissions (in grams): 3.9275

## Validation Metrics

- Loss: 0.243
- Accuracy: 0.910
- Macro F1: 0.928
- Micro F1: 0.910
- Weighted F1: 0.910
- Macro Precision: 0.929
- Micro Precision: 0.910
- Weighted Precision: 0.914
- Macro Recall: 0.929
- Micro Recall: 0.910
- Weighted Recall: 0.910",,,1,[],[],Computer Vision,2023-02,88321111.73252746,0.918911860718172,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129430,autotrain-histopathological_image_classification-3393093037,['JoffreyMa/autotrain-data-histopathological_image_classification'],,4.00147854080629,AutoTrain,Not Specified,Not Specified,Not Specified,0.348,1.988,0.21,,,94424141.0,True,4,0,"['transformers', 'pytorch']",2023-02-10 13:33:24+00:00,2023-02-10 13:28:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3393093037
- CO2 Emissions (in grams): 4.0015

## Validation Metrics

- Loss: 1.988
- Accuracy: 0.348
- Macro F1: 0.210
- Micro F1: 0.348
- Weighted F1: 0.279
- Macro Precision: 0.217
- Micro Precision: 0.348
- Weighted Precision: 0.278
- Macro Recall: 0.245
- Micro Recall: 0.348
- Weighted Recall: 0.348",,,1,[],[],Computer Vision,2023-02,23597312.852507196,0.2619354838709677,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129701,FoxHunterSwift,['PoseyATX/autotrain-data-again'],,17.381617143515218,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-02-13 23:22:50+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129736,kebersihan_jalan_detection,[''],,1.5317579633796956,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,344436077.0,False,3,0,"['transformers', 'pytorch']",2023-02-11 02:07:02+00:00,2023-02-11 01:58:14+00:00,"

## Validation Metrics

- Loss: 0.004
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-02,224863252.05062467,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129796,autotrain-lottery_prod_v3-3409393337,['paulkm/autotrain-data-lottery_prod_v3'],,3.67386840637788,AutoTrain,Not Specified,Not Specified,Not Specified,0.909,0.244,0.898,,,409149557.0,True,17,0,"['transformers', 'pytorch']",2023-02-11 05:23:31+00:00,2023-02-11 05:21:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3409393337
- CO2 Emissions (in grams): 3.6739

## Validation Metrics

- Loss: 0.244
- Accuracy: 0.909
- Precision: 0.922
- Recall: 0.875
- AUC: 0.953
- F1: 0.898

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/paulkm/autotrain-lottery_prod_v3-3409393337
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""paulkm/autotrain-lottery_prod_v3-3409393337"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""paulkm/autotrain-lottery_prod_v3-3409393337"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,111367504.69606136,0.9034665190924184,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
129923,autotrain-nlp-exercise-3413793400,['hanselgm/autotrain-data-nlp-exercise'],,7.21478572426289,AutoTrain,Not Specified,Not Specified,Not Specified,0.896,0.311,0.861,,,1334468213.0,True,5,0,"['transformers', 'pytorch']",2023-02-11 10:08:01+00:00,2023-02-11 09:58:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3413793400
- CO2 Emissions (in grams): 7.2148

## Validation Metrics

- Loss: 0.311
- Accuracy: 0.896
- Macro F1: 0.861
- Micro F1: 0.896
- Weighted F1: 0.892
- Macro Precision: 0.912
- Micro Precision: 0.896
- Weighted Precision: 0.898
- Macro Recall: 0.828
- Micro Recall: 0.896
- Weighted Recall: 0.896


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/hanselgm/autotrain-nlp-exercise-3413793400
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""hanselgm/autotrain-nlp-exercise-3413793400"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hanselgm/autotrain-nlp-exercise-3413793400"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,184962972.42928004,0.8781513944223107,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
130002,autotrain-ia_covers-3416193421,['davanstrien/autotrain-data-ia_covers'],,1.69724123660189,AutoTrain,Not Specified,Not Specified,Not Specified,0.904,0.213,,,,343268717.0,True,3,0,"['transformers', 'pytorch']",2023-02-11 13:50:03+00:00,2023-02-11 13:48:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3416193421
- CO2 Emissions (in grams): 1.6972

## Validation Metrics

- Loss: 0.213
- Accuracy: 0.904
- Precision: 0.714
- Recall: 0.875
- AUC: 0.948
- F1: 0.787",,,1,[],[],Computer Vision,2023-02,202250988.01350775,0.904,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
130659,dappradar-categories-prediction,['Mantas/autotrain-data-dappradar-clean-long-desc'],,2.3855196066520623,AutoTrain,Not Specified,Not Specified,Not Specified,0.801,0.73,0.771,,,267877037.0,True,5,0,"['transformers', 'pytorch']",2023-02-13 02:44:03+00:00,2023-02-13 02:42:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3446293922
- CO2 Emissions (in grams): 2.3855

## Validation Metrics

- Loss: 0.730
- Accuracy: 0.801
- Macro F1: 0.771
- Micro F1: 0.801
- Weighted F1: 0.801
- Macro Precision: 0.783
- Micro Precision: 0.801
- Weighted Precision: 0.803
- Macro Recall: 0.762
- Micro Recall: 0.801
- Weighted Recall: 0.801


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Mantas/autotrain-dappradar-clean-long-desc-3446293922
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Mantas/autotrain-dappradar-clean-long-desc-3446293922"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Mantas/autotrain-dappradar-clean-long-desc-3446293922"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,112292951.29372248,0.7857137404580152,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
130735,pdf-classification-multi,['badalsahani/autotrain-data-pdf-classification'],,6.061459826922492,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.005,1.0,,,1334505077.0,True,9,0,"['transformers', 'safetensors', 'pytorch']",2023-03-19 02:13:25+00:00,2023-02-13 06:48:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3447993987
- CO2 Emissions (in grams): 6.0615

## Validation Metrics

- Loss: 0.005
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/badalsahani/autotrain-pdf-classification-3447993987
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""badalsahani/autotrain-pdf-classification-3447993987"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""badalsahani/autotrain-pdf-classification-3447993987"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,220162323.12102136,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
130742,kualitas_lemon,[''],,2.02289641653325,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,347599761.0,False,6,0,"['transformers', 'pytorch']",2023-02-13 07:13:09+00:00,2023-02-13 07:07:17+00:00,"

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-02,171832703.91852343,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
130760,autotrain-customers_email_sentiment-3449294006,['zabiullah/autotrain-data-customers_email_sentiment'],,26.328823791893843,AutoTrain,Not Specified,Not Specified,Not Specified,0.991,0.053,0.986,,,1334468213.0,True,25,0,"['transformers', 'pytorch']",2023-02-13 08:19:28+00:00,2023-02-13 08:07:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3449294006
- CO2 Emissions (in grams): 26.3288

## Validation Metrics

- Loss: 0.053
- Accuracy: 0.991
- Macro F1: 0.986
- Micro F1: 0.991
- Weighted F1: 0.991
- Macro Precision: 0.986
- Micro Precision: 0.991
- Weighted Precision: 0.991
- Macro Recall: 0.986
- Micro Recall: 0.991
- Weighted Recall: 0.991


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/zabiullah/autotrain-customers_email_sentiment-3449294006
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""zabiullah/autotrain-customers_email_sentiment-3449294006"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""zabiullah/autotrain-customers_email_sentiment-3449294006"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,50684687.76075208,0.9884936772888216,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
130833,autotrain-bert-nlp-3450894022,['hanselgm/autotrain-data-bert-nlp'],,1.9463833241540096,AutoTrain,Not Specified,Not Specified,Not Specified,0.833,0.431,0.8,,,1340718709.0,True,5,0,"['transformers', 'pytorch']",2023-02-13 10:26:07+00:00,2023-02-13 10:24:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3450894022
- CO2 Emissions (in grams): 1.9464

## Validation Metrics

- Loss: 0.431
- Accuracy: 0.833
- Macro F1: 0.800
- Micro F1: 0.833
- Weighted F1: 0.827
- Macro Precision: 0.857
- Micro Precision: 0.833
- Weighted Precision: 0.835
- Macro Recall: 0.765
- Micro Recall: 0.833
- Weighted Recall: 0.833


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/hanselgm/autotrain-bert-nlp-3450894022
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""hanselgm/autotrain-bert-nlp-3450894022"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""hanselgm/autotrain-bert-nlp-3450894022"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,688825624.6146889,0.8161665646050215,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
130848,autotrain-preesmefirstpageclassificationnew-3451994032,['acrowth/autotrain-data-preesmefirstpageclassificationnew'],,8.769306773648797,AutoTrain,Not Specified,Not Specified,Not Specified,0.978,0.128,0.953,,,442576565.0,True,5,0,"['transformers', 'pytorch']",2023-02-13 10:58:47+00:00,2023-02-13 10:54:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3451994032
- CO2 Emissions (in grams): 8.7693

## Validation Metrics

- Loss: 0.128
- Accuracy: 0.978
- Macro F1: 0.953
- Micro F1: 0.978
- Weighted F1: 0.978
- Macro Precision: 0.957
- Micro Precision: 0.978
- Weighted Precision: 0.979
- Macro Recall: 0.954
- Micro Recall: 0.978
- Weighted Recall: 0.978


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/acrowth/autotrain-preesmefirstpageclassificationnew-3451994032
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""acrowth/autotrain-preesmefirstpageclassificationnew-3451994032"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""acrowth/autotrain-preesmefirstpageclassificationnew-3451994032"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,50468819.99041408,0.9653381667529776,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,1,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
131189,FoxHunterSwift2,['PoseyATX/autotrain-data-secondpassbafnbs'],,17.222411531644617,AutoTrain,Not Specified,Not Specified,Not Specified,,1.531,,0.51336,0.4389099999999999,1222363741.0,True,7,0,"['transformers', 'pytorch']",2023-02-13 22:51:44+00:00,2023-02-13 22:41:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3464494202
- CO2 Emissions (in grams): 17.2224

## Validation Metrics

- Loss: 1.531
- Rouge1: 51.336
- Rouge2: 35.638
- RougeL: 43.891
- RougeLsum: 44.994
- Gen Len: 82.349

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/PoseyATX/autotrain-secondpassbafnbs-3464494202
```",,,1,[],[],NLP,2023-02,70975178.98431458,0.4732246896363426,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
131555,autotrain-ant-bee-3482194557,['1024khandsom/autotrain-data-ant-bee'],,0.7388274047348641,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.013,,,,347599761.0,True,14,1,"['transformers', 'pytorch']",2023-02-15 02:42:45+00:00,2023-02-14 16:03:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3482194557
- CO2 Emissions (in grams): 0.7388

## Validation Metrics

- Loss: 0.013
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-02,470474915.754837,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
131585,text-classification-multi,['badalsahani/autotrain-data-text-classification'],,7.761992510873142,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.008,1.0,,,1334509173.0,True,130,5,"['transformers', 'pytorch']",2023-02-14 17:59:32+00:00,2023-02-14 17:32:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3486594647
- CO2 Emissions (in grams): 7.7620

## Validation Metrics

- Loss: 0.008
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000


## Usage

You can use cURL to access this model:

```curl
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/badalsahani/text-classification-multi
```

Or Python API:

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""badalsahani/text-classification-multi"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""badalsahani/text-classification-multi"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,171928685.9824452,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
131928,autotrain-flant5_jobs_description_summary-3501894907,['zaib32/autotrain-data-flant5_jobs_description_summary'],,14.862574077492916,AutoTrain,Not Specified,Not Specified,Not Specified,,0.921,,0.24927,0.23311,3132793669.0,True,1,0,"['transformers', 'pytorch']",2023-02-15 10:01:19+00:00,2023-02-15 09:53:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3501894907
- CO2 Emissions (in grams): 14.8626

## Validation Metrics

- Loss: 0.921
- Rouge1: 24.927
- Rouge2: 17.927
- RougeL: 23.311
- RougeLsum: 24.351
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-flant5_jobs_description_summary-3501894907
```",,,1,[],[],NLP,2023-02,210784057.5034801,0.2409193154774244,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132272,autotrain-finetune_17-0-3516595138,['zaib32/autotrain-data-finetune_17-0'],,0.1345018657300824,AutoTrain,Not Specified,Not Specified,Not Specified,,1.229,,0.52561,0.3747399999999999,990452905.0,True,1,0,"['transformers', 'pytorch']",2023-02-15 23:46:54+00:00,2023-02-15 23:34:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3516595138
- CO2 Emissions (in grams): 0.1345

## Validation Metrics

- Loss: 1.229
- Rouge1: 52.561
- Rouge2: 25.355
- RougeL: 37.474
- RougeLsum: 48.677
- Gen Len: 186.719

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-finetune_17-0-3516595138
```",,,1,[],[],NLP,2023-02,7363859970.44558,0.4375344952518465,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132403,autotrain-glenn_epa_second_pooled_25-3519195196,['gjbooth2/autotrain-data-glenn_epa_second_pooled_25'],,0.020216018970584,AutoTrain,Not Specified,Not Specified,Not Specified,0.534,1.733,0.343,,,556919345.0,True,5,0,"['transformers', 'pytorch']",2023-02-16 04:14:19+00:00,2023-02-16 04:11:21+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3519195196
- CO2 Emissions (in grams): 0.0202

## Validation Metrics

- Loss: 1.733
- Accuracy: 0.534
- Macro F1: 0.343
- Micro F1: 0.534
- Weighted F1: 0.473
- Macro Precision: 0.371
- Micro Precision: 0.534
- Weighted Precision: 0.477
- Macro Recall: 0.375
- Micro Recall: 0.534
- Weighted Recall: 0.534


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/gjbooth2/autotrain-glenn_epa_second_pooled_25-3519195196
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""gjbooth2/autotrain-glenn_epa_second_pooled_25-3519195196"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""gjbooth2/autotrain-glenn_epa_second_pooled_25-3519195196"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,27548418202.929283,0.4177012542759407,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132553,autotrain-dataset-mentions-160223-3522695252,['davanstrien/autotrain-data-dataset-mentions-160223'],,0.1275346561915165,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,1.0,,,433320053.0,True,5,0,"['transformers', 'pytorch']",2023-02-16 09:45:30+00:00,2023-02-16 09:30:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3522695252
- CO2 Emissions (in grams): 0.1275

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/davanstrien/autotrain-dataset-mentions-160223-3522695252
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""davanstrien/autotrain-dataset-mentions-160223-3522695252"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""davanstrien/autotrain-dataset-mentions-160223-3522695252"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,3397665120.524504,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132563,autotrain-bbc-news-classifier-3523995259,['Saripudin/autotrain-data-bbc-news-classifier'],,0.0058878580675376,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.422,1.0,,,267864749.0,True,7,1,"['transformers', 'pytorch']",2023-02-16 09:56:51+00:00,2023-02-16 09:55:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3523995259
- CO2 Emissions (in grams): 0.0059

## Validation Metrics

- Loss: 0.422
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Saripudin/autotrain-bbc-news-classifier-3523995259
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Saripudin/autotrain-bbc-news-classifier-3523995259"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Saripudin/autotrain-bbc-news-classifier-3523995259"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,45494430390.0358,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132580,autotrain-koles_score2-3525195294,['DioLiu/autotrain-data-koles_score2'],,0.0093307307788541,AutoTrain,Not Specified,Not Specified,Not Specified,0.55,1.074,0.502,,,1334476405.0,True,17,0,"['transformers', 'pytorch']",2023-02-16 10:28:22+00:00,2023-02-16 10:26:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3525195294
- CO2 Emissions (in grams): 0.0093

## Validation Metrics

- Loss: 1.074
- Accuracy: 0.550
- Macro F1: 0.502
- Micro F1: 0.550
- Weighted F1: 0.538
- Macro Precision: 0.545
- Micro Precision: 0.550
- Weighted Precision: 0.549
- Macro Recall: 0.495
- Micro Recall: 0.550
- Weighted Recall: 0.550


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/DioLiu/autotrain-koles_score2-3525195294
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""DioLiu/autotrain-koles_score2-3525195294"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""DioLiu/autotrain-koles_score2-3525195294"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,143019495110.10178,0.5249049429657795,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132635,1000_respostas-MODELO_2,['pedro-m4u/autotrain-data-new_1000_respostas'],,1.7141641973570885,AutoTrain,Not Specified,Not Specified,Not Specified,0.863,0.483,0.821,,,435796085.0,True,6,0,"['transformers', 'pytorch']",2023-02-16 12:22:09+00:00,2023-02-16 12:20:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3526695346
- CO2 Emissions (in grams): 1.7142

## Validation Metrics

- Loss: 0.483
- Accuracy: 0.863
- Macro F1: 0.821
- Micro F1: 0.863
- Weighted F1: 0.858
- Macro Precision: 0.876
- Micro Precision: 0.863
- Weighted Precision: 0.866
- Macro Recall: 0.813
- Micro Recall: 0.863
- Weighted Recall: 0.863


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pedro-m4u/autotrain-new_1000_respostas-3526695346
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pedro-m4u/autotrain-new_1000_respostas-3526695346"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pedro-m4u/autotrain-new_1000_respostas-3526695346"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,254232404.14886376,0.8414762470308788,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132636,1000_respostas-MODELO_1,['pedro-m4u/autotrain-data-new_1000_respostas'],,0.0070902146820632,AutoTrain,Not Specified,Not Specified,Not Specified,0.863,0.468,0.823,,,435796085.0,True,3,0,"['transformers', 'pytorch']",2023-02-16 12:21:59+00:00,2023-02-16 12:21:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3526695349
- CO2 Emissions (in grams): 0.0071

## Validation Metrics

- Loss: 0.468
- Accuracy: 0.863
- Macro F1: 0.823
- Micro F1: 0.863
- Weighted F1: 0.860
- Macro Precision: 0.882
- Micro Precision: 0.863
- Weighted Precision: 0.873
- Macro Recall: 0.814
- Micro Recall: 0.863
- Weighted Recall: 0.863


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/pedro-m4u/autotrain-new_1000_respostas-3526695349
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""pedro-m4u/autotrain-new_1000_respostas-3526695349"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""pedro-m4u/autotrain-new_1000_respostas-3526695349"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,61464441422.69421,0.8425255041518386,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132641,autotrain-reklambox-3527295358,['fathyshalab/autotrain-data-reklambox'],,3.5346662598120697,AutoTrain,Not Specified,Not Specified,Not Specified,0.572,1.428,0.209,,,504056501.0,True,5,0,"['transformers', 'pytorch']",2023-02-16 12:41:25+00:00,2023-02-16 12:39:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3527295358
- CO2 Emissions (in grams): 3.5347

## Validation Metrics

- Loss: 1.428
- Accuracy: 0.572
- Macro F1: 0.209
- Micro F1: 0.572
- Weighted F1: 0.513
- Macro Precision: 0.206
- Micro Precision: 0.572
- Weighted Precision: 0.469
- Macro Recall: 0.220
- Micro Recall: 0.572
- Weighted Recall: 0.572


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fathyshalab/autotrain-reklambox-3527295358
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fathyshalab/autotrain-reklambox-3527295358"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fathyshalab/autotrain-reklambox-3527295358"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,142603703.98499787,0.3061408450704225,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132642,autotrain-reklambox-3527295357,['fathyshalab/autotrain-data-reklambox'],,5.001923750904775,AutoTrain,Not Specified,Not Specified,Not Specified,0.57,1.503,0.162,,,1343140597.0,True,5,0,"['transformers', 'pytorch']",2023-02-16 12:42:58+00:00,2023-02-16 12:39:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3527295357
- CO2 Emissions (in grams): 5.0019

## Validation Metrics

- Loss: 1.503
- Accuracy: 0.570
- Macro F1: 0.162
- Micro F1: 0.570
- Weighted F1: 0.489
- Macro Precision: 0.174
- Micro Precision: 0.570
- Weighted Precision: 0.449
- Macro Recall: 0.176
- Micro Recall: 0.570
- Weighted Recall: 0.570


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fathyshalab/autotrain-reklambox-3527295357
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fathyshalab/autotrain-reklambox-3527295357"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fathyshalab/autotrain-reklambox-3527295357"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,268524804.4329035,0.2522950819672131,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
132977,autotrain-jobs_description_distilbart-cnn-12-6-3538095538,['zaib32/autotrain-data-jobs_description_distilbart-cnn-12-6'],,3.180367417858415,AutoTrain,Not Specified,Not Specified,Not Specified,,1.327,,0.64059,0.48572,1222363741.0,True,15,0,"['transformers', 'pytorch']",2023-02-17 00:22:08+00:00,2023-02-17 00:19:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3538095538
- CO2 Emissions (in grams): 3.1804

## Validation Metrics

- Loss: 1.327
- Rouge1: 64.059
- Rouge2: 39.347
- RougeL: 48.572
- RougeLsum: 60.889
- Gen Len: 129.657

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-jobs_description_distilbart-cnn-12-6-3538095538
```",,,1,[],[],NLP,2023-02,384346706.0240201,0.552507524216246,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
133282,autotrain-wilderv2-3544295625,['ashutoshmondal/autotrain-data-wilderv2'],,2.829794634796424,AutoTrain,Not Specified,Not Specified,Not Specified,0.94,0.159,,,,1214905133.0,True,3,0,"['transformers', 'pytorch']",2023-02-17 09:17:10+00:00,2023-02-17 09:15:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3544295625
- CO2 Emissions (in grams): 2.8298

## Validation Metrics

- Loss: 0.159
- Accuracy: 0.940
- Precision: 0.923
- Recall: 0.960
- AUC: 0.988
- F1: 0.941",,,1,[],[],Computer Vision,2023-02,429326255.0083959,0.94,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
133337,EnglishtoChurchSlavonicV1,['Tritkoman/autotrain-data-agahata'],,0.6456799104854907,AutoTrain,Not Specified,Not Specified,Not Specified,,1.659,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-17 12:59:26+00:00,2023-02-17 11:39:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3547595703
- CO2 Emissions (in grams): 0.6457

## Validation Metrics

- Loss: 1.659
- SacreBLEU: 2.851
- Gen len: 18.977",,,1,[],[],NLP,2023-02,7617428823.67489,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
133338,autotrain-patentmatch-3547495705,['kkmkorea/autotrain-data-patentmatch'],,54.78280971868554,AutoTrain,Not Specified,Not Specified,Not Specified,0.948,0.226,0.948,,,556848625.0,True,23,1,"['transformers', 'pytorch']",2023-02-17 12:05:44+00:00,2023-02-17 11:41:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3547495705
- CO2 Emissions (in grams): 54.7828

## Validation Metrics

- Loss: 0.226
- Accuracy: 0.948
- Precision: 0.945
- Recall: 0.952
- AUC: 0.986
- F1: 0.948

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/kkmkorea/autotrain-patentmatch-3547495705
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kkmkorea/autotrain-patentmatch-3547495705"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kkmkorea/autotrain-patentmatch-3547495705"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,10164659.824121213,0.948,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
133388,EnglishtoAncientGreekV5,['Tritkoman/autotrain-data-apapaqjajq'],,0.1070018436405666,AutoTrain,Not Specified,Not Specified,Not Specified,,1.703,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-17 13:56:46+00:00,2023-02-17 13:35:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3548795734
- CO2 Emissions (in grams): 0.1070

## Validation Metrics

- Loss: 1.703
- SacreBLEU: 7.516
- Gen len: 25.710",,,1,[],[],NLP,2023-02,45965757165.09734,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
133483,EnglishtoAncientGreekV6,['Tritkoman/autotrain-data-abagaga'],,10.516087938863189,AutoTrain,Not Specified,Not Specified,Not Specified,,1.732,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-17 15:43:06+00:00,2023-02-17 15:35:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3551595787
- CO2 Emissions (in grams): 10.5161

## Validation Metrics

- Loss: 1.732
- SacreBLEU: 12.665
- Gen len: 19.955",,,1,[],[],NLP,2023-02,467704415.3295367,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
133626,EnglishtoRusynV1,['Tritkoman/autotrain-data-thisisforalesson'],,13.36932256664444,AutoTrain,Not Specified,Not Specified,Not Specified,,5.22,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-17 18:17:07+00:00,2023-02-17 18:05:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3554595856
- CO2 Emissions (in grams): 13.3693

## Validation Metrics

- Loss: 5.220
- SacreBLEU: 0.379
- Gen len: 3.235",,,1,[],[],NLP,2023-02,367888555.04699457,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
133636,EnglishtoRusynV2,['Tritkoman/autotrain-data-rusyntest'],,0.0518521274361566,AutoTrain,Not Specified,Not Specified,Not Specified,,2.858,,,,4918420761.0,True,2,0,"['transformers', 'pytorch']",2023-02-17 18:50:43+00:00,2023-02-17 18:40:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3555695871
- CO2 Emissions (in grams): 0.0519

## Validation Metrics

- Loss: 2.858
- SacreBLEU: 1.820
- Gen len: 5.265",,,1,[],[],NLP,2023-02,94854753395.71844,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
133647,EnglishtoChurchSlavonicV2,['Tritkoman/autotrain-data-apaqaqa'],,152.26214749444304,AutoTrain,Not Specified,Not Specified,Not Specified,,1.392,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-17 20:39:06+00:00,2023-02-17 19:01:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3556095881
- CO2 Emissions (in grams): 152.2621

## Validation Metrics

- Loss: 1.392
- SacreBLEU: 20.121
- Gen len: 45.832",,,1,[],[],NLP,2023-02,32302320.976915836,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
134206,autotrain-pegasus_jobs_description-3576596204,['zaib32/autotrain-data-pegasus_jobs_description'],,0.1123734297287905,AutoTrain,Not Specified,Not Specified,Not Specified,,1.169,,0.50657,0.39248,2283804653.0,True,1,0,"['transformers', 'pytorch']",2023-02-18 21:52:39+00:00,2023-02-18 21:38:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3576596204
- CO2 Emissions (in grams): 0.1124

## Validation Metrics

- Loss: 1.169
- Rouge1: 50.657
- Rouge2: 28.360
- RougeL: 39.248
- RougeLsum: 46.279
- Gen Len: 148.200

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-pegasus_jobs_description-3576596204
```",,,1,[],[],NLP,2023-02,20323350978.179504,0.4422859542850786,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
134375,EnglishtoArliRomaniV1,['Tritkoman/autotrain-data-romaniarli'],,60.82575206712663,AutoTrain,Not Specified,Not Specified,Not Specified,,2.848,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-19 08:36:52+00:00,2023-02-19 08:00:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3583096229
- CO2 Emissions (in grams): 60.8258

## Validation Metrics

- Loss: 2.848
- SacreBLEU: 1.153
- Gen len: 19.000",,,1,[],[],NLP,2023-02,80860829.3995623,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
134387,EnglishtoArliRomaniV2,['Tritkoman/autotrain-data-romaniv2'],,71.97851742122822,AutoTrain,Not Specified,Not Specified,Not Specified,,2.284,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-19 09:41:00+00:00,2023-02-19 08:51:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3584296276
- CO2 Emissions (in grams): 71.9785

## Validation Metrics

- Loss: 2.284
- SacreBLEU: 8.048
- Gen len: 49.335",,,1,[],[],NLP,2023-02,68331787.55567752,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135022,e621TagAutocomplete,['0Tick/E621-Random-PostsTag-Scrape'],,100.0,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,333970169.0,False,19,0,"['tensorboard', 'transformers', 'safetensors', 'pytorch']",2023-03-22 22:18:25+00:00,2023-02-20 09:36:19+00:00,"
## Model description

This is a fine-tuned version of [distilgpt2](https://huggingface.co/distilgpt2) which is intended to be used with the [promptgen](https://github.com/AUTOMATIC1111/stable-diffusion-webui-promptgen) extension inside the AUTOMATIC1111 WebUI.
It is trained on the raw tags of e621 with underscores and spaces


# Training

This model is a fine-tuned version of [distilgpt2](https://huggingface.co/distilgpt2) on a dataset of the tags of 116k random posts of e621.net.
It achieves the following results on the evaluation set:
- Loss: 4.3983
- Accuracy: 0.3865


## Training and evaluation data


Use this collab notebook to train your own model. Also used to train this model
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/0Tick/stable-diffusion-tools/blob/main/distilgpt2train.ipynb)

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 6
- eval_batch_size: 6
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3.0

## Intended uses & limitations

Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. 

The developers of GPT-2 state in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md) that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including: 

> - *Writing assistance: Grammar assistance, autocompletion (for normal prose or code)*
> - *Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.*
> - *Entertainment: Creation of games, chat bots, and amusing generations.*

Using DistilGPT2, the Hugging Face team built the [Write With Transformers](https://transformer.huggingface.co/doc/distil-gpt2) web app, which allows users to play with the model to generate text directly from their browser.

#### Out-of-scope Uses

OpenAI states in the GPT-2 [model card](https://github.com/openai/gpt-2/blob/master/model_card.md): 

> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.
>
> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.



### Framework versions

- Transformers 4.27.0.dev0
- Pytorch 1.13.1+cu116
- Datasets 2.9.0
- Tokenizers 0.13.2",,,1,[],[],NLP,2023-02,3339701.69,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,1,1,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,1,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135156,autotrain-ant-image-classification-3599096563,['Dwarni/autotrain-data-ant-image-classification'],,4.384842539782406,AutoTrain,Not Specified,Not Specified,Not Specified,0.975,0.09,0.966,,,347607953.0,True,6,0,"['transformers', 'pytorch']",2023-02-20 13:51:42+00:00,2023-02-20 13:46:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3599096563
- CO2 Emissions (in grams): 4.3848

## Validation Metrics

- Loss: 0.090
- Accuracy: 0.975
- Macro F1: 0.966
- Micro F1: 0.975
- Weighted F1: 0.975
- Macro Precision: 0.966
- Micro Precision: 0.975
- Weighted Precision: 0.975
- Macro Recall: 0.966
- Micro Recall: 0.975
- Weighted Recall: 0.975",,,1,[],[],Computer Vision,2023-02,79274899.80455483,0.9704791344667696,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
135157,autotrain-ant-image-classification-3599096564,['Dwarni/autotrain-data-ant-image-classification'],,6.825955089798696,AutoTrain,Not Specified,Not Specified,Not Specified,0.975,0.078,0.966,,,346866553.0,True,6,0,"['transformers', 'pytorch']",2023-02-20 13:54:08+00:00,2023-02-20 13:46:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3599096564
- CO2 Emissions (in grams): 6.8260

## Validation Metrics

- Loss: 0.078
- Accuracy: 0.975
- Macro F1: 0.966
- Micro F1: 0.975
- Weighted F1: 0.975
- Macro Precision: 0.966
- Micro Precision: 0.975
- Weighted Precision: 0.975
- Macro Recall: 0.966
- Micro Recall: 0.975
- Weighted Recall: 0.975",,,1,[],[],Computer Vision,2023-02,50815827.00688841,0.9704791344667696,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135205,autotrain-cat-vs-dog-3608196586,['Kluuking/autotrain-data-cat-vs-dog'],,0.0041382370503284,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.003,,,,110394865.0,True,6,0,"['transformers', 'pytorch']",2023-02-20 15:07:35+00:00,2023-02-20 15:06:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3608196586
- CO2 Emissions (in grams): 0.0041

## Validation Metrics

- Loss: 0.003
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-02,26676786191.17273,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
135206,autotrain-cat-vs-dog-3608196590,['Kluuking/autotrain-data-cat-vs-dog'],,0.0069014005878672,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.005,,,,346860409.0,True,3,0,"['transformers', 'pytorch']",2023-02-20 15:08:17+00:00,2023-02-20 15:06:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3608196590
- CO2 Emissions (in grams): 0.0069

## Validation Metrics

- Loss: 0.005
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-02,50259422646.728775,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135207,autotrain-cat-vs-dog-3608196587,['Kluuking/autotrain-data-cat-vs-dog'],,0.7752428902322911,AutoTrain,Not Specified,Not Specified,Not Specified,0.99,0.017,,,,343268717.0,True,3,0,"['transformers', 'pytorch']",2023-02-20 15:07:40+00:00,2023-02-20 15:06:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3608196587
- CO2 Emissions (in grams): 0.7752

## Validation Metrics

- Loss: 0.017
- Accuracy: 0.990
- Precision: 0.980
- Recall: 1.000
- AUC: 1.000
- F1: 0.990",,,1,[],[],Computer Vision,2023-02,442788603.8363333,0.99,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135208,autotrain-cat-vs-dog-3608196589,['Kluuking/autotrain-data-cat-vs-dog'],,0.9014000947977684,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,,,,347599761.0,True,6,0,"['transformers', 'pytorch']",2023-02-20 15:07:45+00:00,2023-02-20 15:06:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3608196589
- CO2 Emissions (in grams): 0.9014

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-02,385622059.51174766,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
135224,autotrain-summarization-pubmed-sample-3609596599,['LeaBresson/autotrain-data-summarization-pubmed-sample'],,132.759647304653,AutoTrain,Not Specified,Not Specified,Not Specified,,1.922,,0.13684,0.1176,2950848513.0,True,28,0,"['transformers', 'pytorch']",2023-02-20 16:29:31+00:00,2023-02-20 15:28:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3609596599
- CO2 Emissions (in grams): 132.7596

## Validation Metrics

- Loss: 1.922
- Rouge1: 13.684
- Rouge2: 5.645
- RougeL: 11.760
- RougeLsum: 12.632
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/LeaBresson/autotrain-summarization-pubmed-sample-3609596599
```",,,1,[],[],NLP,2023-02,22226998.73726297,0.1264925640622543,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135262,EnglishtoOttomanTurkishV1,['Tritkoman/autotrain-data-ottomanturkish'],,38.89288850572544,AutoTrain,Not Specified,Not Specified,Not Specified,,3.108,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-20 17:14:31+00:00,2023-02-20 16:47:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3598696634
- CO2 Emissions (in grams): 38.8929

## Validation Metrics

- Loss: 3.108
- SacreBLEU: 0.858
- Gen len: 11.035",,,1,[],[],NLP,2023-02,126460670.57415798,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135278,EnglishtoOttomanTurkishV2,['Tritkoman/autotrain-data-ottoman2'],,31.152945095580463,AutoTrain,Not Specified,Not Specified,Not Specified,,2.823,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-20 17:48:13+00:00,2023-02-20 17:25:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3611696648
- CO2 Emissions (in grams): 31.1529

## Validation Metrics

- Loss: 2.823
- SacreBLEU: 2.784
- Gen len: 15.622",,,1,[],[],NLP,2023-02,157879800.6387446,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135308,autotrain-traductor-en-es-2023-3608896666,['robertrengel/autotrain-data-traductor-en-es-2023'],,0.0100931012359453,AutoTrain,Not Specified,Not Specified,Not Specified,,0.118,,,,931126725.0,True,3,0,"['transformers', 'pytorch']",2023-02-20 18:14:00+00:00,2023-02-20 18:12:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3608896666
- CO2 Emissions (in grams): 0.0101

## Validation Metrics

- Loss: 0.118
- SacreBLEU: 85.088
- Gen len: 10.172",,,1,[],[],NLP,2023-02,92253778420.8396,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135310,autotrain-traductor-en-es-2023-3608896670,['robertrengel/autotrain-data-traductor-en-es-2023'],,2.5094872306394733,AutoTrain,Not Specified,Not Specified,Not Specified,,0.118,,,,931126725.0,True,1,0,"['transformers', 'pytorch']",2023-02-20 18:14:34+00:00,2023-02-20 18:12:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3608896670
- CO2 Emissions (in grams): 2.5095

## Validation Metrics

- Loss: 0.118
- SacreBLEU: 85.088
- Gen len: 10.172",,,1,[],[],NLP,2023-02,371042623.22256494,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135316,EnglishtoOttomanTurkishV3,['Tritkoman/autotrain-data-ottomanturk'],,11.116575217857822,AutoTrain,Not Specified,Not Specified,Not Specified,,2.863,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-20 18:31:16+00:00,2023-02-20 18:22:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3612596685
- CO2 Emissions (in grams): 11.1166

## Validation Metrics

- Loss: 2.863
- SacreBLEU: 5.756
- Gen len: 12.000",,,1,[],[],NLP,2023-02,442440289.8024727,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135466,autotrain-flight-delay-3621096840,['Kluuking/autotrain-data-flight-delay'],,3.325994852017075,AutoTrain,Not Specified,Not Specified,Not Specified,0.748,0.531,0.271,,,,True,5,0,"['transformers', 'joblib']",2023-02-21 02:34:58+00:00,2023-02-21 02:31:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3621096840
- CO2 Emissions (in grams): 3.3260

## Validation Metrics

- Loss: 0.531
- Accuracy: 0.748
- Precision: 0.609
- Recall: 0.174
- AUC: 0.690
- F1: 0.271

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-02,,0.3978567222767419,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135471,BetterMTL-mk9,['cyy0/open-mantra-bettermtl'],,0.0156907328034992,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,,False,0,1,"['transformers', 'pytorch']",2023-02-21 11:44:08+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135474,autotrain-glenn_ntsa_1-3621496854,['gjbooth2/autotrain-data-glenn_ntsa_1'],,7.937797482362119,AutoTrain,Not Specified,Not Specified,Not Specified,0.905,0.353,0.714,,,1334533813.0,True,7,0,"['transformers', 'pytorch']",2023-02-21 02:54:30+00:00,2023-02-21 02:50:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3621496854
- CO2 Emissions (in grams): 7.9378

## Validation Metrics

- Loss: 0.353
- Accuracy: 0.905
- Macro F1: 0.714
- Micro F1: 0.905
- Weighted F1: 0.890
- Macro Precision: 0.712
- Micro Precision: 0.905
- Weighted Precision: 0.887
- Macro Recall: 0.743
- Micro Recall: 0.905
- Weighted Recall: 0.905


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/gjbooth2/autotrain-glenn_ntsa_1-3621496854
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""gjbooth2/autotrain-glenn_ntsa_1-3621496854"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""gjbooth2/autotrain-glenn_ntsa_1-3621496854"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,168123943.19272444,0.7982334774552192,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135615,EnglishtoOldEastSlavicV2,['Tritkoman/autotrain-data-oldeastslav'],,0.0912491426275244,AutoTrain,Not Specified,Not Specified,Not Specified,,3.471,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-21 08:41:41+00:00,2023-02-21 08:23:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3625296906
- CO2 Emissions (in grams): 0.0912

## Validation Metrics

- Loss: 3.471
- SacreBLEU: 0.470
- Gen len: 10.508",,,1,[],[],NLP,2023-02,53901007936.88342,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135641,EnglishtoOldEastSlavicV3,['Tritkoman/autotrain-data-oldeastslavie'],,19.11966832204106,AutoTrain,Not Specified,Not Specified,Not Specified,,2.519,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-21 09:08:42+00:00,2023-02-21 08:52:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3627096933
- CO2 Emissions (in grams): 19.1197

## Validation Metrics

- Loss: 2.519
- SacreBLEU: 4.219
- Gen len: 15.836",,,1,[],[],NLP,2023-02,257244042.00725952,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135661,EnglishtoOldEastSlavicV4,['Tritkoman/autotrain-data-oldeast33'],,0.0310110956466163,AutoTrain,Not Specified,Not Specified,Not Specified,,2.489,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-21 09:40:20+00:00,2023-02-21 09:33:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3627796950
- CO2 Emissions (in grams): 0.0310

## Validation Metrics

- Loss: 2.489
- SacreBLEU: 6.935
- Gen len: 12.672",,,1,[],[],NLP,2023-02,158601966762.0696,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135675,EnglishtoOldEastSlavicV5,['Tritkoman/autotrain-data-oldeastyav'],,0.0242105702870484,AutoTrain,Not Specified,Not Specified,Not Specified,,2.537,,,,4918420761.0,True,11,0,"['transformers', 'pytorch']",2023-02-21 09:51:44+00:00,2023-02-21 09:47:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3627896959
- CO2 Emissions (in grams): 0.0242

## Validation Metrics

- Loss: 2.537
- SacreBLEU: 6.867
- Gen len: 11.940",,,1,[],[],NLP,2023-02,203151792902.2573,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135678,autotrain-email_tagger-3627996965,['abhibagda/autotrain-data-email_tagger'],,2.342053571382714,AutoTrain,Not Specified,Not Specified,Not Specified,0.667,1.029,0.55,,,1334476405.0,True,30,0,"['transformers', 'pytorch']",2023-02-21 09:51:49+00:00,2023-02-21 09:49:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3627996965
- CO2 Emissions (in grams): 2.3421

## Validation Metrics

- Loss: 1.029
- Accuracy: 0.667
- Macro F1: 0.550
- Micro F1: 0.667
- Weighted F1: 0.651
- Macro Precision: 0.545
- Micro Precision: 0.667
- Weighted Precision: 0.644
- Macro Recall: 0.563
- Micro Recall: 0.667
- Weighted Recall: 0.667


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/abhibagda/autotrain-email_tagger-3627996965
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""abhibagda/autotrain-email_tagger-3627996965"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""abhibagda/autotrain-email_tagger-3627996965"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,569789018.1957473,0.6028759244042728,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135695,autotrain-cat-vs-dog250_250-3628796986,['Kluuking/autotrain-data-cat-vs-dog250_250'],,0.7636424791869872,AutoTrain,Not Specified,Not Specified,Not Specified,0.992,0.053,,,,110394865.0,True,6,0,"['transformers', 'pytorch']",2023-02-21 10:18:40+00:00,2023-02-21 10:17:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3628796986
- CO2 Emissions (in grams): 0.7636

## Validation Metrics

- Loss: 0.053
- Accuracy: 0.992
- Precision: 0.984
- Recall: 1.000
- AUC: 0.993
- F1: 0.992",,,1,[],[],Computer Vision,2023-02,144563546.43541047,0.992,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
135696,autotrain-cat-vs-dog250_250-3628796988,['Kluuking/autotrain-data-cat-vs-dog250_250'],,0.3777240528794812,AutoTrain,Not Specified,Not Specified,Not Specified,0.72,0.661,,,,94374989.0,True,3,0,"['transformers', 'pytorch']",2023-02-21 10:18:28+00:00,2023-02-21 10:17:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3628796988
- CO2 Emissions (in grams): 0.3777

## Validation Metrics

- Loss: 0.661
- Accuracy: 0.720
- Precision: 0.899
- Recall: 0.496
- AUC: 0.738
- F1: 0.639",,,1,[],[],Computer Vision,2023-02,249851679.50136292,0.72,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135697,autotrain-cat-vs-dog250_250-3628796987,['Kluuking/autotrain-data-cat-vs-dog250_250'],,1.9499455569359816,AutoTrain,Not Specified,Not Specified,Not Specified,0.992,0.047,,,,343268717.0,True,3,0,"['transformers', 'pytorch']",2023-02-21 10:19:00+00:00,2023-02-21 10:17:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3628796987
- CO2 Emissions (in grams): 1.9499

## Validation Metrics

- Loss: 0.047
- Accuracy: 0.992
- Precision: 0.984
- Recall: 1.000
- AUC: 0.995
- F1: 0.992",,,1,[],[],Computer Vision,2023-02,176040154.44379395,0.992,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135698,autotrain-cat-vs-dog250_250-3628796989,['Kluuking/autotrain-data-cat-vs-dog250_250'],,0.8217037020492457,AutoTrain,Not Specified,Not Specified,Not Specified,0.996,0.038,,,,347599761.0,True,6,0,"['transformers', 'pytorch']",2023-02-21 10:19:02+00:00,2023-02-21 10:18:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3628796989
- CO2 Emissions (in grams): 0.8217

## Validation Metrics

- Loss: 0.038
- Accuracy: 0.996
- Precision: 0.992
- Recall: 1.000
- AUC: 0.995
- F1: 0.996",,,1,[],[],Computer Vision,2023-02,423023238.3438476,0.996,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
135699,autotrain-cat-vs-dog250_250-3628796990,['Kluuking/autotrain-data-cat-vs-dog250_250'],,1.1778070728273002,AutoTrain,Not Specified,Not Specified,Not Specified,0.976,0.091,,,,346860409.0,True,3,0,"['transformers', 'pytorch']",2023-02-21 10:19:38+00:00,2023-02-21 10:18:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3628796990
- CO2 Emissions (in grams): 1.1778

## Validation Metrics

- Loss: 0.091
- Accuracy: 0.976
- Precision: 0.984
- Recall: 0.968
- AUC: 0.990
- F1: 0.976",,,1,[],[],Computer Vision,2023-02,294496795.78453296,0.976,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135775,autotrain-reklam-filtered-3631097041,['fathyshalab/autotrain-data-reklam-filtered'],,6.219004242367904,AutoTrain,Not Specified,Not Specified,Not Specified,0.563,1.549,0.175,,,1343140597.0,True,7,0,"['transformers', 'pytorch']",2023-02-21 13:10:27+00:00,2023-02-21 13:07:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3631097041
- CO2 Emissions (in grams): 6.2190

## Validation Metrics

- Loss: 1.549
- Accuracy: 0.563
- Macro F1: 0.175
- Micro F1: 0.563
- Weighted F1: 0.498
- Macro Precision: 0.179
- Micro Precision: 0.563
- Weighted Precision: 0.456
- Macro Recall: 0.181
- Micro Recall: 0.563
- Weighted Recall: 0.563


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fathyshalab/autotrain-reklam-filtered-3631097041
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fathyshalab/autotrain-reklam-filtered-3631097041"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fathyshalab/autotrain-reklam-filtered-3631097041"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,215973577.86792493,0.2670054200542005,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135799,autotrain-dappradar-long-desc-summariation-3632397064,['Mantas/autotrain-data-dappradar-long-desc-summariation'],,25.51459781019821,AutoTrain,Not Specified,Not Specified,Not Specified,,1.832,,0.5262100000000001,0.50804,557971229.0,True,6,0,"['transformers', 'pytorch']",2023-02-21 14:22:01+00:00,2023-02-21 14:08:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3632397064
- CO2 Emissions (in grams): 25.5146

## Validation Metrics

- Loss: 1.832
- Rouge1: 52.621
- Rouge2: 42.313
- RougeL: 50.804
- RougeLsum: 51.151
- Gen Len: 18.679

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Mantas/autotrain-dappradar-long-desc-summariation-3632397064
```",,,1,[],[],NLP,2023-02,21868705.63865907,0.5169653921198937,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135827,danbooruTagAutocomplete,['0Tick/Danbooru-Random-Posts-Scrape'],,100.0,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,333970169.0,False,19,0,"['tensorboard', 'transformers', 'safetensors', 'pytorch']",2023-03-20 05:58:04+00:00,2023-02-21 14:49:06+00:00,"
## Model description

This is a fine-tuned version of [distilgpt2](https://huggingface.co/distilgpt2) which is intended to be used with the [promptgen](https://github.com/AUTOMATIC1111/stable-diffusion-webui-promptgen) extension inside the AUTOMATIC1111 WebUI.
It is trained on the raw tags of danbooru with underscores and spaces. Only posts with a rating higher than ""General"" were included in the dataset.


# Training

This model is a fine-tuned version of [distilgpt2](https://huggingface.co/distilgpt2) on a dataset of the tags of 118k random posts of (danbooru)[danbooru.donmai.us] .
It achieves the following results on the evaluation set:
- Loss: 3.6934
- Accuracy: 0.4650


## Training and evaluation data


Use this collab notebook to train your own model. Also used to train this model
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/0Tick/stable-diffusion-tools/blob/main/distilgpt2train.ipynb)

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 6
- eval_batch_size: 6
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3.0

## Intended uses & limitations

Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. 

The developers of GPT-2 state in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md) that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including: 

> - *Writing assistance: Grammar assistance, autocompletion (for normal prose or code)*
> - *Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.*
> - *Entertainment: Creation of games, chat bots, and amusing generations.*

Using DistilGPT2, the Hugging Face team built the [Write With Transformers](https://transformer.huggingface.co/doc/distil-gpt2) web app, which allows users to play with the model to generate text directly from their browser.

#### Out-of-scope Uses

OpenAI states in the GPT-2 [model card](https://github.com/openai/gpt-2/blob/master/model_card.md): 

> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.
>
> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.


### Framework versions

- Transformers 4.27.0.dev0
- Pytorch 1.13.1+cu116
- Datasets 2.9.0
- Tokenizers 0.13.2",,,1,[],[],NLP,2023-02,3339701.69,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,1,1,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,1,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135850,autotrain-touring3-3635197158,['acrowth/autotrain-data-touring3'],,0.1003350588418796,AutoTrain,Not Specified,Not Specified,Not Specified,,1.136,,0.46956,0.46087,2950848513.0,True,5,0,"['transformers', 'pytorch']",2023-02-21 16:05:46+00:00,2023-02-21 15:55:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3635197158
- CO2 Emissions (in grams): 0.1003

## Validation Metrics

- Loss: 1.136
- Rouge1: 46.956
- Rouge2: 0.000
- RougeL: 46.087
- RougeLsum: 46.956
- Gen Len: 6.174

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/acrowth/autotrain-touring3-3635197158
```",,,1,[],[],NLP,2023-02,29409944510.5256,0.4651744187096289,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
135906,EnglishtoAncientHebrewV1,['Tritkoman/autotrain-data-ancienthebrew'],,0.0657901960270338,AutoTrain,Not Specified,Not Specified,Not Specified,,3.478,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-21 18:20:16+00:00,2023-02-21 18:06:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3638197251
- CO2 Emissions (in grams): 0.0658

## Validation Metrics

- Loss: 3.478
- SacreBLEU: 0.108
- Gen len: 19.000",,,1,[],[],NLP,2023-02,74759174740.54912,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136011,adultcontentclassifier,['ziadA123/autotrain-data-adult-classification'],,0.0043891444981216,AutoTrain,Not Specified,Not Specified,Not Specified,0.925,0.244,0.925,,,651444341.0,True,36,0,"['transformers', 'pytorch']",2023-02-21 21:58:29+00:00,2023-02-21 21:57:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3642997339
- CO2 Emissions (in grams): 0.0044

## Validation Metrics

- Loss: 0.244
- Accuracy: 0.925
- Precision: 0.924
- Recall: 0.927
- AUC: 0.971
- F1: 0.925

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ziadA123/autotrain-adult-classification-3642997339
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ziadA123/autotrain-adult-classification-3642997339"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ziadA123/autotrain-adult-classification-3642997339"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,148421712085.0761,0.9250000000000002,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136050,skillber-ner,['Andrei95/autotrain-data-skill4'],,5.36905635369359,AutoTrain,Not Specified,Not Specified,Not Specified,0.88,0.362,0.516,,,439426541.0,True,17,0,"['transformers', 'pytorch']",2023-02-21 23:30:54+00:00,2023-02-21 23:28:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3644697368
- CO2 Emissions (in grams): 5.3691

## Validation Metrics

- Loss: 0.362
- Accuracy: 0.880
- Precision: 0.506
- Recall: 0.527
- F1: 0.516

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-skill4-3644697368
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-skill4-3644697368"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-skill4-3644697368"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,81844278.03550634,0.6505444126074498,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136060,jobbert,['Andrei95/autotrain-data-jobbert2'],,0.0147709560047834,AutoTrain,Not Specified,Not Specified,Not Specified,0.907,0.274,0.591,,,430972397.0,True,13,0,"['transformers', 'pytorch']",2023-02-22 00:25:21+00:00,2023-02-22 00:23:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3645897432
- CO2 Emissions (in grams): 0.0148

## Validation Metrics

- Loss: 0.274
- Accuracy: 0.907
- Precision: 0.575
- Recall: 0.608
- F1: 0.591

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert2-3645897432
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert2-3645897432"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert2-3645897432"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,29177014464.089844,0.7156702269692924,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136068,jobbert3-60,['Andrei95/autotrain-data-jobbert3'],,0.0168866724818305,AutoTrain,Not Specified,Not Specified,Not Specified,0.908,0.28,0.602,,,430972397.0,True,20,0,"['transformers', 'pytorch']",2023-02-22 00:40:02+00:00,2023-02-22 00:37:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3646297463
- CO2 Emissions (in grams): 0.0169

## Validation Metrics

- Loss: 0.280
- Accuracy: 0.908
- Precision: 0.581
- Recall: 0.626
- F1: 0.602

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert3-3646297463
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert3-3646297463"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert3-3646297463"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,25521451752.185757,0.723994701986755,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136071,autotrain-jobbert4-3646397468,['Andrei95/autotrain-data-jobbert4'],,2.4992396645881394,AutoTrain,Not Specified,Not Specified,Not Specified,0.908,0.308,0.62,,,430972397.0,True,13,0,"['transformers', 'pytorch']",2023-02-22 00:48:47+00:00,2023-02-22 00:47:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3646397468
- CO2 Emissions (in grams): 2.4992

## Validation Metrics

- Loss: 0.308
- Accuracy: 0.908
- Precision: 0.573
- Recall: 0.676
- F1: 0.620

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert4-3646397468
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert4-3646397468"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert4-3646397468"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,172441404.1224101,0.7368586387434555,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136072,jobbert-61,['Andrei95/autotrain-data-jobbert4'],,0.0161303759558398,AutoTrain,Not Specified,Not Specified,Not Specified,0.912,0.306,0.618,,,430972397.0,True,17,0,"['transformers', 'pytorch']",2023-02-22 00:50:15+00:00,2023-02-22 00:47:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3646397474
- CO2 Emissions (in grams): 0.0161

## Validation Metrics

- Loss: 0.306
- Accuracy: 0.912
- Precision: 0.597
- Recall: 0.641
- F1: 0.618

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert4-3646397474
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert4-3646397474"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert4-3646397474"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,26718062751.908264,0.7367529411764706,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136123,autotrain-prepaid_test-3647497488,['sonyTW/autotrain-data-prepaid_test'],,0.007422353485058,AutoTrain,Not Specified,Not Specified,Not Specified,0.975,0.078,0.975,,,409149557.0,True,13,0,"['transformers', 'pytorch']",2023-02-22 02:56:49+00:00,2023-02-22 02:55:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3647497488
- CO2 Emissions (in grams): 0.0074

## Validation Metrics

- Loss: 0.078
- Accuracy: 0.975
- Precision: 0.971
- Recall: 0.979
- AUC: 0.997
- F1: 0.975

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sonyTW/autotrain-prepaid_test-3647497488
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sonyTW/autotrain-prepaid_test-3647497488"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sonyTW/autotrain-prepaid_test-3647497488"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,55123965440.83521,0.975,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136124,autotrain-intent-classification-roberta-3647697496,['harshasurampudi/autotrain-data-intent-classification-roberta'],,1.1197250732252992,AutoTrain,Not Specified,Not Specified,Not Specified,0.875,1.379,0.867,,,498692789.0,True,22,0,"['transformers', 'safetensors', 'pytorch']",2023-03-20 14:02:22+00:00,2023-02-22 02:56:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3647697496
- CO2 Emissions (in grams): 1.1197

## Validation Metrics

- Loss: 1.379
- Accuracy: 0.875
- Macro F1: 0.867
- Micro F1: 0.875
- Weighted F1: 0.867
- Macro Precision: 0.917
- Micro Precision: 0.875
- Weighted Precision: 0.917
- Macro Recall: 0.875
- Micro Recall: 0.875
- Weighted Recall: 0.875


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/harshasurampudi/autotrain-intent-classification-roberta-3647697496
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""harshasurampudi/autotrain-intent-classification-roberta-3647697496"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""harshasurampudi/autotrain-intent-classification-roberta-3647697496"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,445370744.055544,0.8709816303099885,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136458,autotrain-jobbert-8-3660397715,['Andrei95/autotrain-data-jobbert-8'],,1.190898950142825,AutoTrain,Not Specified,Not Specified,Not Specified,0.957,0.119,0.7,,,430960109.0,True,18,0,"['transformers', 'pytorch']",2023-02-22 13:56:16+00:00,2023-02-22 13:55:10+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3660397715
- CO2 Emissions (in grams): 1.1909

## Validation Metrics

- Loss: 0.119
- Accuracy: 0.957
- Precision: 0.669
- Recall: 0.733
- F1: 0.700

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert-8-3660397715
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert-8-3660397715"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert-8-3660397715"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,361877982.1313259,0.8085697042848522,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136459,autotrain-jobbert-8-3660397718,['Andrei95/autotrain-data-jobbert-8'],,5.24940369998092,AutoTrain,Not Specified,Not Specified,Not Specified,0.958,0.119,0.699,,,430960109.0,True,13,0,"['transformers', 'pytorch']",2023-02-22 13:58:24+00:00,2023-02-22 13:55:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3660397718
- CO2 Emissions (in grams): 5.2494

## Validation Metrics

- Loss: 0.119
- Accuracy: 0.958
- Precision: 0.656
- Recall: 0.747
- F1: 0.699

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert-8-3660397718
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert-8-3660397718"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert-8-3660397718"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,82096964.46123326,0.808258298129149,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136466,autotrain-jobbert-9-3660597735,['Andrei95/autotrain-data-jobbert-9'],,1.8006765324830052,AutoTrain,Not Specified,Not Specified,Not Specified,0.958,0.123,0.677,,,430960109.0,True,13,0,"['transformers', 'pytorch']",2023-02-22 14:12:14+00:00,2023-02-22 14:11:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3660597735
- CO2 Emissions (in grams): 1.8007

## Validation Metrics

- Loss: 0.123
- Accuracy: 0.958
- Precision: 0.627
- Recall: 0.735
- F1: 0.677

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert-9-3660597735
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert-9-3660597735"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert-9-3660597735"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,239332329.3916296,0.7933529051987768,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136469,jobbert-skill,['Andrei95/autotrain-data-jobbert-10'],,1.0574234641160185,AutoTrain,Not Specified,Not Specified,Not Specified,0.935,0.175,0.357,,,430960109.0,True,24,0,"['transformers', 'pytorch']",2023-02-22 14:18:46+00:00,2023-02-22 14:17:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3660697741
- CO2 Emissions (in grams): 1.0574

## Validation Metrics

- Loss: 0.175
- Accuracy: 0.935
- Precision: 0.299
- Recall: 0.442
- F1: 0.357

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert-10-3660697741
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert-10-3660697741"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert-10-3660697741"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,407556786.49543935,0.5167105263157895,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136473,autotrain-jobbert11-3660997753,['Andrei95/autotrain-data-jobbert11'],,0.0147402482426394,AutoTrain,Not Specified,Not Specified,Not Specified,0.905,0.284,0.622,,,430966253.0,True,13,0,"['transformers', 'pytorch']",2023-02-22 14:26:57+00:00,2023-02-22 14:25:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3660997753
- CO2 Emissions (in grams): 0.0147

## Validation Metrics

- Loss: 0.284
- Accuracy: 0.905
- Precision: 0.568
- Recall: 0.686
- F1: 0.622

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert11-3660997753
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert11-3660997753"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert11-3660997753"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,29237380938.628677,0.7372757039947611,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136478,autotrain-jobbert-12-3661497769,['Andrei95/autotrain-data-jobbert-12'],,1.6328250714339845,AutoTrain,Not Specified,Not Specified,Not Specified,0.91,0.243,0.561,,,430960109.0,True,13,0,"['transformers', 'pytorch']",2023-02-22 14:37:12+00:00,2023-02-22 14:35:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3661497769
- CO2 Emissions (in grams): 1.6328

## Validation Metrics

- Loss: 0.243
- Accuracy: 0.910
- Precision: 0.501
- Recall: 0.638
- F1: 0.561

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert-12-3661497769
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert-12-3661497769"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert-12-3661497769"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,263935259.5324378,0.6940992522093815,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136593,autotrain-finetuned_distillbart-3664997842,['zaib32/autotrain-data-finetuned_distillbart'],,0.0276297623136641,AutoTrain,Not Specified,Not Specified,Not Specified,,1.011,,0.69451,0.5820799999999999,1222363741.0,True,3,0,"['transformers', 'pytorch']",2023-02-22 17:25:44+00:00,2023-02-22 17:23:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3664997842
- CO2 Emissions (in grams): 0.0276

## Validation Metrics

- Loss: 1.011
- Rouge1: 69.451
- Rouge2: 48.200
- RougeL: 58.208
- RougeLsum: 66.308
- Gen Len: 123.738

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-finetuned_distillbart-3664997842
```",,,1,[],[],NLP,2023-02,44240834471.29362,0.6333441133018431,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136617,autotrain-ia-useful-covers-3665397856,[''],,0.0048131167261957,AutoTrain,Not Specified,Not Specified,Not Specified,0.924,0.193,,,,110394865.0,True,17,0,"['transformers', 'pytorch']",2023-02-22 19:44:19+00:00,2023-02-22 18:15:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3665397856
- CO2 Emissions (in grams): 0.0048

## Validation Metrics

- Loss: 0.193
- Accuracy: 0.924
- Precision: 0.778
- Recall: 0.875
- AUC: 0.962
- F1: 0.824",,,1,[],[],Computer Vision,2023-02,22936253425.803864,0.924,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
136630,autotrain-209_distillbart-3666897870,['zaib32/autotrain-data-209_distillbart'],,3.665817434216617,AutoTrain,Not Specified,Not Specified,Not Specified,,1.268,,0.68184,0.5602900000000001,1222363741.0,True,7,0,"['transformers', 'pytorch']",2023-02-22 18:45:46+00:00,2023-02-22 18:43:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3666897870
- CO2 Emissions (in grams): 3.6658

## Validation Metrics

- Loss: 1.268
- Rouge1: 68.184
- Rouge2: 44.748
- RougeL: 56.029
- RougeLsum: 64.611
- Gen Len: 122.191

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-209_distillbart-3666897870
```",,,1,[],[],NLP,2023-02,333449159.13992274,0.6151177953998374,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136636,autotrain-translate-large-3667097880,['lebi376/autotrain-data-translate-large'],,3.0879484646170616,AutoTrain,Not Specified,Not Specified,Not Specified,,0.59,,,,314181957.0,True,6,0,"['transformers', 'pytorch']",2023-02-22 18:58:42+00:00,2023-02-22 18:56:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3667097880
- CO2 Emissions (in grams): 3.0879

## Validation Metrics

- Loss: 0.590
- SacreBLEU: 54.883
- Gen len: 10.377",,,1,[],[],NLP,2023-02,101744559.72954908,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136641,autotrain-translate-big-3667697890,['lebi376/autotrain-data-translate-big'],,0.1342238084031298,AutoTrain,Not Specified,Not Specified,Not Specified,,1.088,,,,2950733825.0,True,8,0,"['transformers', 'pytorch']",2023-02-22 19:29:38+00:00,2023-02-22 19:10:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3667697890
- CO2 Emissions (in grams): 0.1342

## Validation Metrics

- Loss: 1.088
- SacreBLEU: 32.018
- Gen len: 10.017",,,1,[],[],NLP,2023-02,21983684266.63712,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136683,autotrain-selenophake-3668397922,['CharlemagneDeer/autotrain-data-selenophake'],,0.5710813789332319,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.022,,,,343268717.0,True,3,0,"['transformers', 'pytorch']",2023-02-22 20:17:23+00:00,2023-02-22 20:16:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3668397922
- CO2 Emissions (in grams): 0.5711

## Validation Metrics

- Loss: 0.022
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-02,601085466.3852266,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136715,jobbert-short-sentences-f60,['Andrei95/autotrain-data-jobbert15'],,1.6240218297608988,AutoTrain,Not Specified,Not Specified,Not Specified,0.902,0.293,0.603,,,430972397.0,True,13,0,"['transformers', 'pytorch']",2023-02-22 21:27:57+00:00,2023-02-22 21:26:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3669997969
- CO2 Emissions (in grams): 1.6240

## Validation Metrics

- Loss: 0.293
- Accuracy: 0.902
- Precision: 0.547
- Recall: 0.672
- F1: 0.603

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobbert15-3669997969
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobbert15-3669997969"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobbert15-3669997969"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,265373524.6055474,0.7227986710963454,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136768,autotrain-jobberta-20-3670698025,['Andrei95/autotrain-data-jobberta-20'],,0.0305760639185388,AutoTrain,Not Specified,Not Specified,Not Specified,0.917,0.235,0.649,,,2235531373.0,True,3,0,"['transformers', 'pytorch']",2023-02-22 22:23:40+00:00,2023-02-22 22:19:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3670698025
- CO2 Emissions (in grams): 0.0306

## Validation Metrics

- Loss: 0.235
- Accuracy: 0.917
- Precision: 0.602
- Recall: 0.703
- F1: 0.649

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobberta-20-3670698025
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobberta-20-3670698025"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobberta-20-3670698025"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,73113772229.01991,0.7600676883780332,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,1,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136769,jobberta-large-f66,['Andrei95/autotrain-data-jobberta-20'],,3.956742446552856,AutoTrain,Not Specified,Not Specified,Not Specified,0.92,0.232,0.664,,,2235531373.0,True,51,1,"['transformers', 'pytorch']",2023-02-22 22:23:10+00:00,2023-02-22 22:19:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3670698024
- CO2 Emissions (in grams): 3.9567

## Validation Metrics

- Loss: 0.232
- Accuracy: 0.920
- Precision: 0.618
- Recall: 0.717
- F1: 0.664

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobberta-20-3670698024
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobberta-20-3670698024"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobberta-20-3670698024"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,564992895.8473433,0.7713131313131314,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,1,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136788,autotrain-flags-3670798043,['opiljain/autotrain-data-flags'],,4.145044266449912,AutoTrain,Not Specified,Not Specified,Not Specified,0.949,0.203,0.954,,,343296365.0,True,1,0,"['transformers', 'pytorch']",2023-02-22 22:38:52+00:00,2023-02-22 22:35:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3670798043
- CO2 Emissions (in grams): 4.1450

## Validation Metrics

- Loss: 0.203
- Accuracy: 0.949
- Macro F1: 0.954
- Micro F1: 0.949
- Weighted F1: 0.950
- Macro Precision: 0.956
- Micro Precision: 0.949
- Weighted Precision: 0.955
- Macro Recall: 0.956
- Micro Recall: 0.949
- Weighted Recall: 0.949",,,1,[],[],Computer Vision,2023-02,82820916.48059082,0.9514934314240672,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136849,autotrain-jobberta-23-3671398065,['Andrei95/autotrain-data-jobberta-23'],,4.051202274340627,AutoTrain,Not Specified,Not Specified,Not Specified,0.915,0.248,0.603,,,1330285549.0,True,8,0,"['transformers', 'pytorch']",2023-02-22 23:09:22+00:00,2023-02-22 23:06:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 3671398065
- CO2 Emissions (in grams): 4.0512

## Validation Metrics

- Loss: 0.248
- Accuracy: 0.915
- Precision: 0.570
- Recall: 0.639
- F1: 0.603

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Andrei95/autotrain-jobberta-23-3671398065
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Andrei95/autotrain-jobberta-23-3671398065"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Andrei95/autotrain-jobberta-23-3671398065"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,328368089.0055082,0.7269367588932807,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
136960,trainModel_p1,['ziadA123/autotrain-data-test_prepreocessing2'],,0.0092549938060457,AutoTrain,Not Specified,Not Specified,Not Specified,0.972,0.112,0.972,,,651444341.0,True,13,0,"['transformers', 'pytorch']",2023-02-22 23:52:17+00:00,2023-02-22 23:51:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3672198102
- CO2 Emissions (in grams): 0.0093

## Validation Metrics

- Loss: 0.112
- Accuracy: 0.972
- Precision: 0.964
- Recall: 0.980
- AUC: 0.990
- F1: 0.972

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ziadA123/autotrain-test_prepreocessing2-3672198102
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ziadA123/autotrain-test_prepreocessing2-3672198102"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ziadA123/autotrain-test_prepreocessing2-3672198102"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,70388414584.83232,0.972,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137181,autotrain-bart_jobs_description-3667398231,['zaib32/autotrain-data-bart_jobs_description'],,5.188589459184297,AutoTrain,Not Specified,Not Specified,Not Specified,,1.129,,0.66484,0.53467,1625537293.0,True,1,0,"['transformers', 'pytorch']",2023-02-23 08:10:25+00:00,2023-02-23 08:06:57+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3667398231
- CO2 Emissions (in grams): 5.1886

## Validation Metrics

- Loss: 1.129
- Rouge1: 66.484
- Rouge2: 42.519
- RougeL: 53.467
- RougeLsum: 62.459
- Gen Len: 129.500

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-bart_jobs_description-3667398231
```",,,1,[],[],NLP,2023-02,313290790.45223826,0.5926920205750682,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137232,EnglishtoOldEnglishV1,['Tritkoman/autotrain-data-oldenglish'],,7.273007332989732,AutoTrain,Not Specified,Not Specified,Not Specified,,4.128,,,,310022533.0,True,1,0,"['transformers', 'pytorch']",2023-02-23 09:30:22+00:00,2023-02-23 09:25:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3679898272
- CO2 Emissions (in grams): 7.2730

## Validation Metrics

- Loss: 4.128
- SacreBLEU: 0.545
- Gen len: 25.544",,,1,[],[],NLP,2023-02,42626456.81570602,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137234,EnglishtoOldEnglishV3,['Tritkoman/autotrain-data-oldenglish'],,29.02386292235669,AutoTrain,Not Specified,Not Specified,Not Specified,,4.134,,,,4918420761.0,True,4,0,"['transformers', 'pytorch']",2023-02-23 09:48:29+00:00,2023-02-23 09:27:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3679898271
- CO2 Emissions (in grams): 29.0239

## Validation Metrics

- Loss: 4.134
- SacreBLEU: 0.903
- Gen len: 9.430",,,1,[],[],NLP,2023-02,169461273.0964701,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137255,EnglishtoOldEnglishV2,['Tritkoman/autotrain-data-oldenglish2'],,5.451467518019884,AutoTrain,Not Specified,Not Specified,Not Specified,,3.265,,,,310022533.0,True,1,0,"['transformers', 'pytorch']",2023-02-23 09:59:25+00:00,2023-02-23 09:56:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3680498282
- CO2 Emissions (in grams): 5.4515

## Validation Metrics

- Loss: 3.265
- SacreBLEU: 6.433
- Gen len: 16.747",,,1,[],[],NLP,2023-02,56869555.21154941,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137280,EnglishtoOldEnglishV4,['Tritkoman/autotrain-data-oldenglish4'],,29.249758702505805,AutoTrain,Not Specified,Not Specified,Not Specified,,3.007,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-23 11:10:20+00:00,2023-02-23 10:44:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3681498294
- CO2 Emissions (in grams): 29.2498

## Validation Metrics

- Loss: 3.007
- SacreBLEU: 6.124
- Gen len: 19.114",,,1,[],[],NLP,2023-02,168152524.30026516,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137328,EnglishtoOldEnglishV5,['Tritkoman/autotrain-data-oldenglish5'],,10.382242558236785,AutoTrain,Not Specified,Not Specified,Not Specified,,2.959,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-02-23 12:44:28+00:00,2023-02-23 12:36:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3684798314
- CO2 Emissions (in grams): 10.3822

## Validation Metrics

- Loss: 2.959
- SacreBLEU: 11.287
- Gen len: 13.759",,,1,[],[],NLP,2023-02,473733948.46164095,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137359,autotrain-i-bert-twitter-sentiment-3685798328,['derek-thomas/autotrain-data-i-bert-twitter-sentiment'],,0.1576902654453995,AutoTrain,Not Specified,Not Specified,Not Specified,0.745,0.591,0.728,,,995331233.0,True,10,0,"['transformers', 'pytorch']",2023-02-23 14:04:52+00:00,2023-02-23 13:48:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3685798328
- CO2 Emissions (in grams): 0.1577

## Validation Metrics

- Loss: 0.591
- Accuracy: 0.745
- Macro F1: 0.728
- Micro F1: 0.745
- Weighted F1: 0.744
- Macro Precision: 0.738
- Micro Precision: 0.745
- Weighted Precision: 0.744
- Macro Recall: 0.721
- Micro Recall: 0.745
- Weighted Recall: 0.745


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/derek-thomas/autotrain-i-bert-twitter-sentiment-3685798328
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""derek-thomas/autotrain-i-bert-twitter-sentiment-3685798328"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""derek-thomas/autotrain-i-bert-twitter-sentiment-3685798328"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,6311938344.37824,0.7364019008825525,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137492,autotrain-idnt-3689198408,['modamsko/autotrain-data-idnt'],,0.0151076048500345,AutoTrain,Not Specified,Not Specified,Not Specified,0.66,1.231,0.341,,,467189493.0,True,17,0,"['transformers', 'pytorch']",2023-02-23 16:29:49+00:00,2023-02-23 16:28:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3689198408
- CO2 Emissions (in grams): 0.0151

## Validation Metrics

- Loss: 1.231
- Accuracy: 0.660
- Macro F1: 0.341
- Micro F1: 0.660
- Weighted F1: 0.617
- Macro Precision: 0.377
- Micro Precision: 0.660
- Weighted Precision: 0.614
- Macro Recall: 0.359
- Micro Recall: 0.660
- Weighted Recall: 0.660


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/modamsko/autotrain-idnt-3689198408
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""modamsko/autotrain-idnt-3689198408"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""modamsko/autotrain-idnt-3689198408"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,30924127129.18773,0.4496703296703297,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137567,woc_coach_only,['lucieackley/autotrain-data-woc_coach'],,0.003342450858119,AutoTrain,Not Specified,Not Specified,Not Specified,0.824,0.439,0.87,,,433320053.0,True,15,0,"['transformers', 'pytorch']",2023-02-23 18:17:29+00:00,2023-02-23 18:16:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 3693198466
- CO2 Emissions (in grams): 0.0033

## Validation Metrics

- Loss: 0.439
- Accuracy: 0.824
- Precision: 0.833
- Recall: 0.909
- AUC: 0.879
- F1: 0.870

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lucieackley/autotrain-woc_coach-3693198466
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucieackley/autotrain-woc_coach-3693198466"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucieackley/autotrain-woc_coach-3693198466"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,129641413260.40482,0.8463754427390792,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137717,autotrain-twitter-currency-regression-3698798560,['tizan25/autotrain-data-twitter-currency-regression'],,5.819688265631011,AutoTrain,Not Specified,Not Specified,Not Specified,,7.583,,,,439479413.0,True,25,0,"['transformers', 'pytorch']",2023-02-24 02:36:39+00:00,2023-02-24 02:29:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 3698798560
- CO2 Emissions (in grams): 5.8197

## Validation Metrics

- Loss: 7.583
- MSE: 7.583
- MAE: 1.332
- R2: 0.005
- RMSE: 2.754
- Explained Variance: 0.005

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/tizan25/autotrain-twitter-currency-regression-3698798560
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""tizan25/autotrain-twitter-currency-regression-3698798560"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""tizan25/autotrain-twitter-currency-regression-3698798560"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,75515971.46455552,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137763,SDOHv7,['reachosen/autotrain-data-sdohv7'],,0.011347632206498,AutoTrain,Not Specified,Not Specified,Not Specified,0.99,0.057,0.99,,,737833337.0,True,87,0,"['transformers', 'pytorch']",2023-03-14 13:25:11+00:00,2023-02-24 04:50:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3701198597
- CO2 Emissions (in grams): 0.0113

## Validation Metrics

- Loss: 0.057
- Accuracy: 0.990
- Macro F1: 0.990
- Micro F1: 0.990
- Weighted F1: 0.990
- Macro Precision: 0.990
- Micro Precision: 0.990
- Weighted Precision: 0.991
- Macro Recall: 0.990
- Micro Recall: 0.990
- Weighted Recall: 0.990


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/reachosen/autotrain-sdohv7-3701198597
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""reachosen/autotrain-sdohv7-3701198597"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""reachosen/autotrain-sdohv7-3701198597"", use_auth_token=True)

inputs = tokenizer(""The Patient is homeless"", return_tensors=""pt"")

outputs = model(**inputs)
```
",,,1,[],[],NLP,2023-02,65020906879.36591,0.99,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137844,t5base_en_re,['lambdarw/autotrain-data-pret5-base-re'],,0.037152189882447,AutoTrain,Not Specified,Not Specified,Not Specified,,1.662,,0.38448,0.36453,891702929.0,True,8,0,"['transformers', 'pytorch']",2023-02-24 07:31:27+00:00,2023-02-24 07:26:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3702698638
- CO2 Emissions (in grams): 0.0372

## Validation Metrics

- Loss: 1.662
- Rouge1: 38.448
- Rouge2: 24.331
- RougeL: 36.453
- RougeLsum: 36.463
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/lambdarw/autotrain-pret5-base-re-3702698638
```",,,1,[],[],NLP,2023-02,24001355823.746365,0.3742393142948692,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137854,autotrain-flan_t5_jobs_description_209-3703198648,['zaib32/autotrain-data-flan_t5_jobs_description_209'],,5.965451127503901,AutoTrain,Not Specified,Not Specified,Not Specified,,1.211,,0.23661,0.21084,990408885.0,True,17,0,"['transformers', 'pytorch']",2023-02-24 07:56:06+00:00,2023-02-24 07:52:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3703198648
- CO2 Emissions (in grams): 5.9655

## Validation Metrics

- Loss: 1.211
- Rouge1: 23.661
- Rouge2: 14.326
- RougeL: 21.084
- RougeLsum: 22.577
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-flan_t5_jobs_description_209-3703198648
```",,,1,[],[],NLP,2023-02,166024138.63281664,0.2229829138451223,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
137865,autotrain-flan_t5_large_jobs_description_209-3703498672,['zaib32/autotrain-data-flan_t5_large_jobs_description_209'],,0.071803380785605,AutoTrain,Not Specified,Not Specified,Not Specified,,0.914,,0.2506299999999999,0.2277799999999999,3132793669.0,True,6,0,"['transformers', 'pytorch']",2023-02-24 08:15:52+00:00,2023-02-24 08:06:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 3703498672
- CO2 Emissions (in grams): 0.0718

## Validation Metrics

- Loss: 0.914
- Rouge1: 25.063
- Rouge2: 16.559
- RougeL: 22.778
- RougeLsum: 23.757
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-flan_t5_large_jobs_description_209-3703498672
```",,,1,[],[],NLP,2023-02,43630169425.50505,0.2386593148136535,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138010,autotrain-tax_issues-3708498778,['ram119900/autotrain-data-tax_issues'],,0.0441028878242611,AutoTrain,Not Specified,Not Specified,Not Specified,0.968,0.111,0.967,,,2239906741.0,True,20,0,"['transformers', 'pytorch']",2023-02-24 13:43:42+00:00,2023-02-24 13:36:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3708498778
- CO2 Emissions (in grams): 0.0441

## Validation Metrics

- Loss: 0.111
- Accuracy: 0.968
- Macro F1: 0.967
- Micro F1: 0.968
- Weighted F1: 0.969
- Macro Precision: 0.968
- Micro Precision: 0.968
- Weighted Precision: 0.971
- Macro Recall: 0.968
- Micro Recall: 0.968
- Weighted Recall: 0.968


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/ram119900/autotrain-tax_issues-3708498778
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""ram119900/autotrain-tax_issues-3708498778"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""ram119900/autotrain-tax_issues-3708498778"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,50788210285.128365,0.967499741602067,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,1,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138302,autotrain-weather-classification-3723199087,['8kkillian/autotrain-data-weather-classification'],,0.0034994299454859,AutoTrain,Not Specified,Not Specified,Not Specified,0.41,1.564,0.4,,,94399565.0,True,3,0,"['transformers', 'pytorch']",2023-02-25 01:50:39+00:00,2023-02-25 01:50:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3723199087
- CO2 Emissions (in grams): 0.0035

## Validation Metrics

- Loss: 1.564
- Accuracy: 0.410
- Macro F1: 0.400
- Micro F1: 0.410
- Weighted F1: 0.400
- Macro Precision: 0.577
- Micro Precision: 0.410
- Weighted Precision: 0.577
- Macro Recall: 0.410
- Micro Recall: 0.410
- Weighted Recall: 0.410",,,1,[],[],Computer Vision,2023-02,26975697890.95821,0.4049382716049382,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138303,autotrain-weather-classification-3723199086,['8kkillian/autotrain-data-weather-classification'],,0.0063152072167613,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.02,1.0,,,343277933.0,True,3,0,"['transformers', 'pytorch']",2023-02-25 01:51:40+00:00,2023-02-25 01:50:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3723199086
- CO2 Emissions (in grams): 0.0063

## Validation Metrics

- Loss: 0.020
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-02,54357350632.7552,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138304,autotrain-weather-classification-3723199089,['8kkillian/autotrain-data-weather-classification'],,0.0094567553421842,AutoTrain,Not Specified,Not Specified,Not Specified,0.99,0.043,0.99,,,346869625.0,True,3,0,"['transformers', 'pytorch']",2023-02-25 01:51:31+00:00,2023-02-25 01:50:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3723199089
- CO2 Emissions (in grams): 0.0095

## Validation Metrics

- Loss: 0.043
- Accuracy: 0.990
- Macro F1: 0.990
- Micro F1: 0.990
- Weighted F1: 0.990
- Macro Precision: 0.990
- Micro Precision: 0.990
- Weighted Precision: 0.990
- Macro Recall: 0.990
- Micro Recall: 0.990
- Weighted Recall: 0.990",,,1,[],[],Computer Vision,2023-02,36679560002.22424,0.99,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138305,autotrain-weather-classification-3723199088,['8kkillian/autotrain-data-weather-classification'],,0.0067954562891757,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.007,1.0,,,347612049.0,True,6,0,"['transformers', 'pytorch']",2023-02-25 01:51:22+00:00,2023-02-25 01:50:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3723199088
- CO2 Emissions (in grams): 0.0068

## Validation Metrics

- Loss: 0.007
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-02,51153599435.802704,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
138411,autotrain-codet5_base_cpsl-3727399183,['ritheshwar/autotrain-data-codet5_base_cpsl'],,5.24175202670419,AutoTrain,Not Specified,Not Specified,Not Specified,,0.153,,,,891616913.0,True,11,0,"['transformers', 'pytorch']",2023-02-25 09:19:51+00:00,2023-02-25 09:16:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3727399183
- CO2 Emissions (in grams): 5.2418

## Validation Metrics

- Loss: 0.153
- SacreBLEU: 2.407
- Gen len: 19.000",,,1,[],[],NLP,2023-02,170099025.75658736,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138412,autotrain-codet5_base_cpsl-3727399184,['ritheshwar/autotrain-data-codet5_base_cpsl'],,0.0204505645552222,AutoTrain,Not Specified,Not Specified,Not Specified,,0.175,,,,891616913.0,True,11,0,"['transformers', 'pytorch']",2023-02-25 09:19:53+00:00,2023-02-25 09:16:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3727399184
- CO2 Emissions (in grams): 0.0205

## Validation Metrics

- Loss: 0.175
- SacreBLEU: 3.275
- Gen len: 19.000",,,1,[],[],NLP,2023-02,43598645435.55201,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138413,autotrain-codet5_base_cpsl-3727399185,['ritheshwar/autotrain-data-codet5_base_cpsl'],,0.0146478647124334,AutoTrain,Not Specified,Not Specified,Not Specified,,0.16,,,,891616913.0,True,11,0,"['transformers', 'pytorch']",2023-02-25 09:19:35+00:00,2023-02-25 09:17:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3727399185
- CO2 Emissions (in grams): 0.0146

## Validation Metrics

- Loss: 0.160
- SacreBLEU: 16.612
- Gen len: 19.000",,,1,[],[],NLP,2023-02,60870094754.71041,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138414,autotrain-codet5_base_cpsl-3727399186,['ritheshwar/autotrain-data-codet5_base_cpsl'],,3.846331276578152,AutoTrain,Not Specified,Not Specified,Not Specified,,0.223,,,,891616913.0,True,11,0,"['transformers', 'pytorch']",2023-02-25 09:19:59+00:00,2023-02-25 09:17:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3727399186
- CO2 Emissions (in grams): 3.8463

## Validation Metrics

- Loss: 0.223
- SacreBLEU: 2.566
- Gen len: 19.000",,,1,[],[],NLP,2023-02,231809703.5555444,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138415,autotrain-codet5_base_cpsl-3727399187,['ritheshwar/autotrain-data-codet5_base_cpsl'],,0.016495049956139,AutoTrain,Not Specified,Not Specified,Not Specified,,0.232,,,,891616913.0,True,13,0,"['transformers', 'pytorch']",2023-02-25 09:19:44+00:00,2023-02-25 09:17:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 3727399187
- CO2 Emissions (in grams): 0.0165

## Validation Metrics

- Loss: 0.232
- SacreBLEU: 3.396
- Gen len: 19.000",,,1,[],[],NLP,2023-02,54053604891.8218,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138503,autotrain-pick_a_card-3726099223,['rwcuffney/autotrain-data-pick_a_card'],,0.07713046775481,AutoTrain,Not Specified,Not Specified,Not Specified,0.792,1.034,0.785,,,94792973.0,True,1,0,"['transformers', 'pytorch']",2023-02-25 12:49:28+00:00,2023-02-25 12:34:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3726099223
- CO2 Emissions (in grams): 0.0771

## Validation Metrics

- Loss: 1.034
- Accuracy: 0.792
- Macro F1: 0.785
- Micro F1: 0.792
- Weighted F1: 0.785
- Macro Precision: 0.812
- Micro Precision: 0.792
- Weighted Precision: 0.812
- Macro Recall: 0.792
- Micro Recall: 0.792
- Weighted Recall: 0.792",,,1,[],[],Computer Vision,2023-02,1228995178.6800687,0.7884844641724794,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138504,autotrain-pick_a_card-3726099221,['rwcuffney/autotrain-data-pick_a_card'],,0.0660443407031469,AutoTrain,Not Specified,Not Specified,Not Specified,0.981,0.061,0.98,,,110551729.0,True,7,0,"['transformers', 'pytorch']",2023-02-25 12:44:44+00:00,2023-02-25 12:34:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3726099221
- CO2 Emissions (in grams): 0.0660

## Validation Metrics

- Loss: 0.061
- Accuracy: 0.981
- Macro F1: 0.980
- Micro F1: 0.981
- Weighted F1: 0.980
- Macro Precision: 0.984
- Micro Precision: 0.981
- Weighted Precision: 0.984
- Macro Recall: 0.981
- Micro Recall: 0.981
- Weighted Recall: 0.981",,,1,[],[],Computer Vision,2023-02,1673901621.5924222,0.9804997450280468,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
138505,autotrain-pick_a_card-3726099222,['rwcuffney/autotrain-data-pick_a_card'],,0.0850092610285532,AutoTrain,Not Specified,Not Specified,Not Specified,0.909,0.314,0.904,,,343425581.0,True,1,0,"['transformers', 'pytorch']",2023-02-25 12:47:19+00:00,2023-02-25 12:34:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3726099222
- CO2 Emissions (in grams): 0.0850

## Validation Metrics

- Loss: 0.314
- Accuracy: 0.909
- Macro F1: 0.904
- Micro F1: 0.909
- Weighted F1: 0.904
- Macro Precision: 0.926
- Micro Precision: 0.909
- Weighted Precision: 0.926
- Macro Recall: 0.909
- Micro Recall: 0.909
- Weighted Recall: 0.909",,,1,[],[],Computer Vision,2023-02,4039860796.868344,0.9064931053502482,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138506,autotrain-pick_a_card-3726099224,['rwcuffney/autotrain-data-pick_a_card'],,22.59524540097308,AutoTrain,Not Specified,Not Specified,Not Specified,0.989,0.037,0.989,,,347808849.0,True,14,0,"['transformers', 'pytorch']",2023-02-25 12:47:27+00:00,2023-02-25 12:34:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3726099224
- CO2 Emissions (in grams): 22.5952

## Validation Metrics

- Loss: 0.037
- Accuracy: 0.989
- Macro F1: 0.989
- Micro F1: 0.989
- Weighted F1: 0.989
- Macro Precision: 0.991
- Micro Precision: 0.989
- Weighted Precision: 0.991
- Macro Recall: 0.989
- Micro Recall: 0.989
- Weighted Recall: 0.989",,,1,[],[],Computer Vision,2023-02,15393010.46869893,0.989,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
138507,autotrain-pick_a_card-3726099225,['rwcuffney/autotrain-data-pick_a_card'],,14.334546926203066,AutoTrain,Not Specified,Not Specified,Not Specified,0.974,0.085,0.973,,,347017273.0,True,1,0,"['transformers', 'pytorch']",2023-02-25 12:43:26+00:00,2023-02-25 12:34:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3726099225
- CO2 Emissions (in grams): 14.3345

## Validation Metrics

- Loss: 0.085
- Accuracy: 0.974
- Macro F1: 0.973
- Micro F1: 0.974
- Weighted F1: 0.973
- Macro Precision: 0.979
- Micro Precision: 0.974
- Weighted Precision: 0.979
- Macro Recall: 0.974
- Micro Recall: 0.974
- Weighted Recall: 0.974",,,1,[],[],Computer Vision,2023-02,24208457.70616329,0.9734997431946584,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138533,indianidproofclassification,['SakshamSudhanshu/autotrain-data-image_classification_id_proof'],,0.0060102285277883,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-02-25 13:22:42+00:00,,,,,1,[],[],Computer Vision,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
138805,autotrain-weather-classification-3739699406,['dazzle-nu/autotrain-data-weather-classification'],,0.0610247517795645,AutoTrain,Not Specified,Not Specified,Not Specified,0.945,0.167,0.949,,,110422513.0,True,4,0,"['transformers', 'pytorch']",2023-02-26 00:36:00+00:00,2023-02-26 00:22:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3739699406
- CO2 Emissions (in grams): 0.0610

## Validation Metrics

- Loss: 0.167
- Accuracy: 0.945
- Macro F1: 0.949
- Micro F1: 0.945
- Weighted F1: 0.945
- Macro Precision: 0.953
- Micro Precision: 0.945
- Weighted Precision: 0.946
- Macro Recall: 0.947
- Micro Recall: 0.945
- Weighted Recall: 0.945",,,1,[],[],Computer Vision,2023-02,1809470907.786264,0.9469957761351636,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
138806,autotrain-weather-classification-3739699408,['dazzle-nu/autotrain-data-weather-classification'],,18.10129353152818,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-02-26 00:43:27+00:00,,,,,1,[],[],Computer Vision,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138807,autotrain-weather-classification-3739699407,['dazzle-nu/autotrain-data-weather-classification'],,0.0526165458286675,AutoTrain,Not Specified,Not Specified,Not Specified,0.952,0.149,0.957,,,343296365.0,True,1,0,"['transformers', 'pytorch']",2023-02-26 00:34:16+00:00,2023-02-26 00:22:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3739699407
- CO2 Emissions (in grams): 0.0526

## Validation Metrics

- Loss: 0.149
- Accuracy: 0.952
- Macro F1: 0.957
- Micro F1: 0.952
- Weighted F1: 0.952
- Macro Precision: 0.961
- Micro Precision: 0.952
- Weighted Precision: 0.953
- Macro Recall: 0.955
- Micro Recall: 0.952
- Weighted Recall: 0.952",,,1,[],[],Computer Vision,2023-02,6524494521.511502,0.954493452069146,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138808,autotrain-weather-classification-3739699410,['dazzle-nu/autotrain-data-weather-classification'],,12.172961992218216,AutoTrain,Not Specified,Not Specified,Not Specified,0.949,0.162,0.954,,,346888057.0,True,1,0,"['transformers', 'pytorch']",2023-02-26 00:32:02+00:00,2023-02-26 00:22:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3739699410
- CO2 Emissions (in grams): 12.1730

## Validation Metrics

- Loss: 0.162
- Accuracy: 0.949
- Macro F1: 0.954
- Micro F1: 0.949
- Weighted F1: 0.949
- Macro Precision: 0.958
- Micro Precision: 0.949
- Weighted Precision: 0.950
- Macro Recall: 0.951
- Micro Recall: 0.949
- Weighted Recall: 0.949",,,1,[],[],Computer Vision,2023-02,28496602.324212827,0.9514934314240672,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138809,autotrain-weather-classification-3739699409,['dazzle-nu/autotrain-data-weather-classification'],,11.084609677556031,AutoTrain,Not Specified,Not Specified,Not Specified,0.959,0.137,0.963,,,347636625.0,True,16,0,"['transformers', 'pytorch']",2023-02-26 00:31:28+00:00,2023-02-26 00:22:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3739699409
- CO2 Emissions (in grams): 11.0846

## Validation Metrics

- Loss: 0.137
- Accuracy: 0.959
- Macro F1: 0.963
- Micro F1: 0.959
- Weighted F1: 0.959
- Macro Precision: 0.967
- Micro Precision: 0.959
- Weighted Precision: 0.961
- Macro Recall: 0.962
- Micro Recall: 0.959
- Weighted Recall: 0.959",,,1,[],[],Computer Vision,2023-02,31362098.90221845,0.9609958376690948,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
138851,autotrain-test3-3741499453,['chaphoto/autotrain-data-test3'],,0.016917225770271,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,1.0,,,110401009.0,True,4,0,"['transformers', 'pytorch']",2023-02-26 02:38:43+00:00,2023-02-26 02:35:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3741499453
- CO2 Emissions (in grams): 0.0169

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-02,6525952333.981972,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
138852,autotrain-test3-3741499455,['chaphoto/autotrain-data-test3'],,6.553923974267747,AutoTrain,Not Specified,Not Specified,Not Specified,0.982,0.355,0.983,,,94391373.0,True,4,0,"['transformers', 'pytorch']",2023-02-26 02:43:27+00:00,2023-02-26 02:35:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3741499455
- CO2 Emissions (in grams): 6.5539

## Validation Metrics

- Loss: 0.355
- Accuracy: 0.982
- Macro F1: 0.983
- Micro F1: 0.982
- Weighted F1: 0.982
- Macro Precision: 0.983
- Micro Precision: 0.982
- Weighted Precision: 0.983
- Macro Recall: 0.983
- Micro Recall: 0.982
- Weighted Recall: 0.982",,,1,[],[],Computer Vision,2023-02,14402268.529601932,0.982499745547074,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138853,autotrain-test3-3741499454,['chaphoto/autotrain-data-test3'],,0.0174878954778511,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'pytorch']",2023-02-26 02:38:24+00:00,,,,,1,[],[],Computer Vision,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138854,autotrain-test3-3741499457,['chaphoto/autotrain-data-test3'],,4.475400047856875,AutoTrain,Not Specified,Not Specified,Not Specified,0.998,0.008,0.998,,,346866553.0,True,1,0,"['transformers', 'pytorch']",2023-02-26 02:39:09+00:00,2023-02-26 02:35:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3741499457
- CO2 Emissions (in grams): 4.4754

## Validation Metrics

- Loss: 0.008
- Accuracy: 0.998
- Macro F1: 0.998
- Micro F1: 0.998
- Weighted F1: 0.998
- Macro Precision: 0.998
- Micro Precision: 0.998
- Weighted Precision: 0.998
- Macro Recall: 0.998
- Micro Recall: 0.998
- Weighted Recall: 0.998",,,1,[],[],Computer Vision,2023-02,77505150.2191638,0.998,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
138855,autotrain-test3-3741499456,['chaphoto/autotrain-data-test3'],,6.055784660601666,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,1.0,,,347607953.0,True,7,0,"['transformers', 'pytorch']",2023-02-26 02:39:42+00:00,2023-02-26 02:35:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3741499456
- CO2 Emissions (in grams): 6.0558

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-02,57400976.50127866,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
139224,autotrain-cz-sort-3752299804,['prjc/autotrain-data-cz-sort'],,4.044332290484004,AutoTrain,Not Specified,Not Specified,Not Specified,0.998,0.004,0.885,,,1334492789.0,True,3,0,"['transformers', 'pytorch']",2023-02-26 17:46:23+00:00,2023-02-26 17:43:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3752299804
- CO2 Emissions (in grams): 4.0443

## Validation Metrics

- Loss: 0.004
- Accuracy: 0.998
- Macro F1: 0.885
- Micro F1: 0.998
- Weighted F1: 0.998
- Macro Precision: 0.881
- Micro Precision: 0.998
- Weighted Precision: 0.997
- Macro Recall: 0.889
- Micro Recall: 0.998
- Weighted Recall: 0.998


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/prjc/autotrain-cz-sort-3752299804
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""prjc/autotrain-cz-sort-3752299804"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""prjc/autotrain-cz-sort-3752299804"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,329966158.35448456,0.9381093998937864,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
139448,autotrain-cardamage-3762299975,['opiljain/autotrain-data-cardamage'],,0.6713036785630181,AutoTrain,Not Specified,Not Specified,Not Specified,0.733,0.711,0.531,,,110397937.0,True,4,0,"['transformers', 'pytorch']",2023-02-26 23:58:26+00:00,2023-02-26 23:57:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 3762299975
- CO2 Emissions (in grams): 0.6713

## Validation Metrics

- Loss: 0.711
- Accuracy: 0.733
- Macro F1: 0.531
- Micro F1: 0.733
- Weighted F1: 0.649
- Macro Precision: 0.492
- Micro Precision: 0.733
- Weighted Precision: 0.588
- Macro Recall: 0.583
- Micro Recall: 0.733
- Weighted Recall: 0.733",,,1,[],[],Computer Vision,2023-02,164453049.3804474,0.6158591772151899,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
139944,autotrain-train-37756100191,['Yuvraj-Sharma-2000/autotrain-data-train'],,193.00140560471,AutoTrain,Not Specified,Not Specified,Not Specified,,1.998,,0.5004500000000001,0.3362,2283804653.0,True,13,0,"['transformers', 'pytorch']",2023-02-27 16:59:23+00:00,2023-02-27 15:27:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 37756100191
- CO2 Emissions (in grams): 193.0014

## Validation Metrics

- Loss: 1.998
- Rouge1: 50.045
- Rouge2: 25.522
- RougeL: 33.620
- RougeLsum: 44.884
- Gen Len: 125.793

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Yuvraj-Sharma-2000/autotrain-train-37756100191
```",,,1,[],[],NLP,2023-02,11833098.55098934,0.4022023307237196,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
140083,autotrain-test-token-classification-37792100226,['Sonnenblume/autotrain-data-test-token-classification'],,1.3281980015903283,AutoTrain,Not Specified,Not Specified,Not Specified,0.677,1.456,0.464,,,435774061.0,True,19,0,"['transformers', 'pytorch']",2023-02-27 17:43:04+00:00,2023-02-27 17:41:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 37792100226
- CO2 Emissions (in grams): 1.3282

## Validation Metrics

- Loss: 1.456
- Accuracy: 0.677
- Precision: 0.487
- Recall: 0.443
- F1: 0.464

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Sonnenblume/autotrain-test-token-classification-37792100226
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""Sonnenblume/autotrain-test-token-classification-37792100226"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Sonnenblume/autotrain-test-token-classification-37792100226"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-02,328094200.170624,0.5506187554776513,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
140245,autotrain-animals-vs-humans2-37846100283,['hg2001/autotrain-data-animals-vs-humans2'],,1.5958667599075285,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,,,,346860409.0,True,1,0,"['transformers', 'pytorch']",2023-02-27 21:37:10+00:00,2023-02-27 21:35:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 37846100283
- CO2 Emissions (in grams): 1.5959

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-02,217349228.46572644,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
140246,autotrain-animals-vs-humans2-37846100280,['hg2001/autotrain-data-animals-vs-humans2'],,0.0096302039940934,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.005,,,,343268717.0,True,6,0,"['transformers', 'pytorch']",2023-02-27 21:37:14+00:00,2023-02-27 21:35:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 37846100280
- CO2 Emissions (in grams): 0.0096

## Validation Metrics

- Loss: 0.005
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-02,35645009930.27155,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
140256,autotrain-male-vs-femalee-37851100302,['hg2001/autotrain-data-male-vs-femalee'],,0.0034341761338042,AutoTrain,Not Specified,Not Specified,Not Specified,0.979,0.06,,,,347599761.0,True,4,0,"['transformers', 'pytorch']",2023-02-27 22:04:39+00:00,2023-02-27 22:03:50+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 37851100302
- CO2 Emissions (in grams): 0.0034

## Validation Metrics

- Loss: 0.060
- Accuracy: 0.979
- Precision: 0.960
- Recall: 1.000
- AUC: 1.000
- F1: 0.980",,,1,[],[],Computer Vision,2023-02,101217802307.3462,0.979,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
140309,swords-attentive_t5_v1,['bsenker/autotrain-data-swords'],,0.0251054860314724,AutoTrain,Not Specified,Not Specified,Not Specified,,1.557,,0.6214,0.61331,2950848513.0,True,1,0,"['transformers', 'pytorch']",2023-02-28 00:58:34+00:00,2023-02-28 00:19:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 37880100395
- CO2 Emissions (in grams): 0.0251

## Validation Metrics

- Loss: 1.557
- Rouge1: 62.140
- Rouge2: 13.128
- RougeL: 61.331
- RougeLsum: 60.728
- Gen Len: 3.989

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/bsenker/autotrain-swords-37880100395
```",,,1,[],[],NLP,2023-02,117537995850.8191,0.6173284965700447,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
140674,autotrain-cpsl_28022023-38024100796,['ritheshwar/autotrain-data-cpsl_28022023'],,14.095333378950777,AutoTrain,Not Specified,Not Specified,Not Specified,,0.132,,,,891616913.0,True,1,0,"['transformers', 'pytorch']",2023-02-28 12:36:03+00:00,2023-02-28 12:29:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38024100796
- CO2 Emissions (in grams): 14.0953

## Validation Metrics

- Loss: 0.132
- SacreBLEU: 0.940
- Gen len: 18.789",,,1,[],[],NLP,2023-02,63256177.70286252,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
140675,autotrain-cpsl_28022023-38024100798,['ritheshwar/autotrain-data-cpsl_28022023'],,16.339082879848934,AutoTrain,Not Specified,Not Specified,Not Specified,,0.15,,,,891616913.0,True,1,0,"['transformers', 'pytorch']",2023-02-28 12:37:11+00:00,2023-02-28 12:29:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38024100798
- CO2 Emissions (in grams): 16.3391

## Validation Metrics

- Loss: 0.150
- SacreBLEU: 0.338
- Gen len: 18.859",,,1,[],[],NLP,2023-02,54569581.38694768,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
140676,autotrain-cpsl_28022023-38024100799,['ritheshwar/autotrain-data-cpsl_28022023'],,9.129177872175122,AutoTrain,Not Specified,Not Specified,Not Specified,,0.157,,,,891616913.0,True,1,0,"['transformers', 'pytorch']",2023-02-28 13:09:06+00:00,2023-02-28 12:29:45+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38024100799
- CO2 Emissions (in grams): 9.1292

## Validation Metrics

- Loss: 0.157
- SacreBLEU: 0.942
- Gen len: 18.844",,,1,[],[],NLP,2023-02,97666725.9072216,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
140819,autotrain-flower-classifier-6-38061100888,['omarques/autotrain-data-flower-classifier-6'],,0.0038337670741491,AutoTrain,Not Specified,Not Specified,Not Specified,0.99,0.017,0.99,,,346872697.0,True,1,1,"['transformers', 'pytorch']",2023-02-28 14:43:34+00:00,2023-02-28 14:42:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 38061100888
- CO2 Emissions (in grams): 0.0038

## Validation Metrics

- Loss: 0.017
- Accuracy: 0.990
- Macro F1: 0.990
- Micro F1: 0.990
- Weighted F1: 0.990
- Macro Precision: 0.990
- Micro Precision: 0.990
- Weighted Precision: 0.990
- Macro Recall: 0.990
- Micro Recall: 0.990
- Weighted Recall: 0.990",,,1,[],[],Computer Vision,2023-02,90478292053.51189,0.99,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
141472,autotrain-text-summa-38210101161,['KoddaDuck/autotrain-data-text-summa'],,0.0038803578366088,AutoTrain,Not Specified,Not Specified,Not Specified,,3.604,,0.13841,0.1246,242071641.0,True,1,0,"['transformers', 'pytorch']",2023-03-01 06:20:01+00:00,2023-03-01 06:19:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 38210101161
- CO2 Emissions (in grams): 0.0039

## Validation Metrics

- Loss: 3.604
- Rouge1: 13.841
- Rouge2: 1.190
- RougeL: 12.460
- RougeLsum: 12.795
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/KoddaDuck/autotrain-text-summa-38210101161
```",,,1,[],[],NLP,2023-03,62383844787.76733,0.1311424356488346,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
141473,Cylonix_text_sum,['KoddaDuck/autotrain-data-text-summa'],,0.0318800353974076,AutoTrain,Not Specified,Not Specified,Not Specified,,2.089,,0.2389,0.2076599999999999,2950848513.0,True,10,0,"['transformers', 'pytorch']",2023-03-01 06:25:08+00:00,2023-03-01 06:20:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 38210101163
- CO2 Emissions (in grams): 0.0319

## Validation Metrics

- Loss: 2.089
- Rouge1: 23.890
- Rouge2: 5.760
- RougeL: 20.766
- RougeLsum: 20.771
- Gen Len: 18.750

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/KoddaDuck/autotrain-text-summa-38210101163
```",,,1,[],[],NLP,2023-03,92561017458.59276,0.2221872715872447,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
141606,autotrain-fake-reviews-labelling-37433101195,['Sarwar242/autotrain-data-fake-reviews-labelling'],,0.0125100043456914,AutoTrain,Not Specified,Not Specified,Not Specified,0.941,0.204,0.939,,,737768761.0,True,29,0,"['transformers', 'pytorch']",2023-03-01 10:07:56+00:00,2023-03-01 10:06:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 37433101195
- CO2 Emissions (in grams): 0.0125

## Validation Metrics

- Loss: 0.204
- Accuracy: 0.941
- Precision: 0.975
- Recall: 0.905
- AUC: 0.992
- F1: 0.939

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Sarwar242/autotrain-fake-reviews-labelling-37433101195
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Sarwar242/autotrain-fake-reviews-labelling-37433101195"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Sarwar242/autotrain-fake-reviews-labelling-37433101195"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,58974300936.52179,0.9399989361702128,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
141611,autotrain-cpsl_large_01032023-38235101207,['ritheshwar/autotrain-data-cpsl_large_01032023'],,0.1765813138367071,AutoTrain,Not Specified,Not Specified,Not Specified,,0.087,,,,2950733825.0,True,1,0,"['transformers', 'pytorch']",2023-03-01 10:35:39+00:00,2023-03-01 10:15:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38235101207
- CO2 Emissions (in grams): 0.1766

## Validation Metrics

- Loss: 0.087
- SacreBLEU: 0.337
- Gen len: 18.826",,,1,[],[],NLP,2023-03,16710340187.68645,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
141778,RussiantoChukchiV1,['Tritkoman/autotrain-data-russiantochukchi'],,14.96406670067916,AutoTrain,Not Specified,Not Specified,Not Specified,,5.041,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-03-01 15:31:13+00:00,2023-03-01 15:17:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38297101278
- CO2 Emissions (in grams): 14.9641

## Validation Metrics

- Loss: 5.041
- SacreBLEU: 0.334
- Gen len: 13.289",,,1,[],[],NLP,2023-03,328682092.8682958,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
141797,autotrain-flower-classification-6-38312101280,['omarques/autotrain-data-flower-classification-6'],,0.2369622750892742,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.007,1.0,,,110407153.0,True,4,0,"['transformers', 'pytorch']",2023-03-01 15:41:09+00:00,2023-03-01 15:40:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 38312101280
- CO2 Emissions (in grams): 0.2370

## Validation Metrics

- Loss: 0.007
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-03,465927131.0524206,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
141803,RussiantoChukchiV2,['Tritkoman/autotrain-data-russiantochukchiv2'],,5.357461110677393,AutoTrain,Not Specified,Not Specified,Not Specified,,3.609,,,,4918420761.0,True,4,0,"['transformers', 'pytorch']",2023-03-01 16:04:53+00:00,2023-03-01 15:51:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38314101289
- CO2 Emissions (in grams): 5.3575

## Validation Metrics

- Loss: 3.609
- SacreBLEU: 2.052
- Gen len: 14.865",,,1,[],[],NLP,2023-03,918050669.7841656,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
141870,autotrain-auto_train-38325101316,['QianT/autotrain-data-auto_train'],,0.8412532264765644,AutoTrain,Not Specified,Not Specified,Not Specified,,1.005,,,,310022533.0,True,431,0,"['transformers', 'pytorch']",2023-03-01 17:27:02+00:00,2023-03-01 17:24:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38325101316
- CO2 Emissions (in grams): 0.8413

## Validation Metrics

- Loss: 1.005
- SacreBLEU: 42.915
- Gen len: 35.988",,,1,[],[],NLP,2023-03,368524628.7832652,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
141938,autotrain-mona-lisa-detection-38345101350,['drift-ai/autotrain-data-mona-lisa-detection'],,3.616228652448201,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,18,0,"['transformers', 'pytorch']",2023-03-01 19:27:52+00:00,,,,,1,[],[],Computer Vision,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
141970,GPT-NoSleep-355m,['chloeliu/reddit_nosleep_posts'],,60.0,MLCO2,fine-tuning,"Oregon, USA","1 T4, Google Colab",,,,,,1444569373.0,False,27,0,"['transformers', 'safetensors', 'pytorch']",2023-03-20 10:55:38+00:00,2023-03-01 20:16:46+00:00,"
# GPT-NoSleep-355m
A finetuned version of [GPT2-Medium](https://huggingface.co/gpt2-medium) on the 'reddit-nosleep-posts' dataset. (Linked above)

# Training Procedure
This was trained on the 'reddt-nosleep-posts' dataset, using the ""HappyTransformers"" library on Google Colab.
This model was trained for X epochs with learning rate 1e-2.

# Biases & Limitations
This likely contains the same biases and limitations as the original GPT2 that it is based on, and additionally heavy biases from the dataset.
It likely will generate offensive output. 

# Intended Use
This model is meant for fun, nothing else.",,,1,[],[],NLP,2023-03,24076156.216666665,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,1,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,0,1,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
142111,autotrain-img-classifier-march-2023-38397101427,['omarques/autotrain-data-img-classifier-march-2023'],,0.2885544186587613,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,1.0,,,347616145.0,True,4,0,"['transformers', 'pytorch']",2023-03-02 02:25:52+00:00,2023-03-02 02:25:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 38397101427
- CO2 Emissions (in grams): 0.2886

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-03,1204681413.7027092,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
142520,autotrain-godaddy2-38507101578,['wangdy/autotrain-data-godaddy2'],,0.0013755411915266,AutoTrain,Not Specified,Not Specified,Not Specified,,0.206,,,,,True,9,0,"['transformers', 'joblib']",2023-03-02 15:32:51+00:00,2023-03-02 15:32:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 38507101578
- CO2 Emissions (in grams): 0.0014

## Validation Metrics

- Loss: 0.206
- R2: 0.997
- MSE: 0.042
- MAE: 0.081
- RMSLE: 0.052

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-03,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
142971,mt5-sum-v1,['chenhg8680/autotrain-data-mt5-sum'],,3.857485636182076,AutoTrain,Not Specified,Not Specified,Not Specified,,2.309,,0.0,0.0,4918519065.0,True,1,0,"['transformers', 'pytorch']",2023-03-03 04:03:44+00:00,2023-03-03 03:54:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 38578101674
- CO2 Emissions (in grams): 3.8575

## Validation Metrics

- Loss: 2.309
- Rouge1: 0.000
- Rouge2: 0.000
- RougeL: 0.000
- RougeLsum: 0.000
- Gen Len: 9.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/chenhg8680/autotrain-mt5-sum-38578101674
```",,,1,[],[],NLP,2023-03,1275058296.74795,0.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
143025,autotrain-classify-reviews-38591101687,['mziyad/autotrain-data-classify-reviews'],,107.85305581130024,AutoTrain,Not Specified,Not Specified,Not Specified,0.684,0.74,0.34,,,438032501.0,True,9,0,"['transformers', 'pytorch']",2023-03-03 11:43:53+00:00,2023-03-03 07:00:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 38591101687
- CO2 Emissions (in grams): 107.8531

## Validation Metrics

- Loss: 0.740
- Accuracy: 0.684
- Macro F1: 0.340
- Micro F1: 0.684
- Weighted F1: 0.676
- Macro Precision: 0.341
- Micro Precision: 0.684
- Weighted Precision: 0.679
- Macro Recall: 0.343
- Micro Recall: 0.684
- Weighted Recall: 0.684


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/mziyad/autotrain-classify-reviews-38591101687
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""mziyad/autotrain-classify-reviews-38591101687"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""mziyad/autotrain-classify-reviews-38591101687"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,4061382.384625076,0.4542187500000001,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
143094,autotrain-test7-2644pc-linearregr-38619101723,['farouk97/autotrain-data-test7-2644pc-linearregr'],,3.801725033462415,AutoTrain,Not Specified,Not Specified,Not Specified,,0.145,,,,,True,4,0,"['transformers', 'joblib']",2023-03-03 09:52:41+00:00,2023-03-03 09:42:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 38619101723
- CO2 Emissions (in grams): 3.8017

## Validation Metrics

- Loss: 0.145
- R2: 0.000
- MSE: 0.021
- MAE: 0.099
- RMSLE: 0.101

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-03,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
143354,autotrain-reklambox2-38671101799,['fathyshalab/autotrain-data-reklambox2'],,0.4167271060397732,AutoTrain,Not Specified,Not Specified,Not Specified,0.593,1.484,0.136,,,439845045.0,True,26,0,"['transformers', 'pytorch']",2023-03-03 17:46:08+00:00,2023-03-03 17:45:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 38671101799
- CO2 Emissions (in grams): 0.4167

## Validation Metrics

- Loss: 1.484
- Accuracy: 0.593
- Macro F1: 0.136
- Micro F1: 0.593
- Weighted F1: 0.534
- Macro Precision: 0.125
- Micro Precision: 0.593
- Weighted Precision: 0.487
- Macro Recall: 0.150
- Micro Recall: 0.593
- Weighted Recall: 0.593


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/fathyshalab/autotrain-reklambox2-38671101799
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""fathyshalab/autotrain-reklambox2-38671101799"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""fathyshalab/autotrain-reklambox2-38671101799"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,1055475006.6054507,0.2212565157750343,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
143406,autotrain-translation_english-38691101815,['raghuram13/autotrain-data-translation_english'],,45.41212076277246,AutoTrain,Not Specified,Not Specified,Not Specified,,0.863,,,,4918420761.0,True,1,0,"['transformers', 'pytorch']",2023-03-03 21:26:56+00:00,2023-03-03 19:27:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38691101815
- CO2 Emissions (in grams): 45.4121

## Validation Metrics

- Loss: 0.863
- SacreBLEU: 9.523
- Gen len: 15.708",,,1,[],[],NLP,2023-03,108306343.73349898,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
143496,autotrain-test-3-38732101859,['Kluuking/autotrain-data-test-3'],,0.9466490190669988,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,0,0,"['transformers', 'joblib']",2023-03-04 04:08:04+00:00,,,,,1,[],[],Not Specified,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
143613,autotrain-fake_news_fine_tuned-38761101898,['systash/autotrain-data-fake_news_fine_tuned'],,1.824945175183436,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.005,0.999,,,267855533.0,True,3,0,"['transformers', 'pytorch']",2023-03-04 05:37:14+00:00,2023-03-04 05:32:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 38761101898
- CO2 Emissions (in grams): 1.8249

## Validation Metrics

- Loss: 0.005
- Accuracy: 0.999
- Precision: 0.998
- Recall: 1.000
- AUC: 1.000
- F1: 0.999

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/systash/autotrain-fake_news_fine_tuned-38761101898
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""systash/autotrain-fake_news_fine_tuned-38761101898"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""systash/autotrain-fake_news_fine_tuned-38761101898"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,146774564.3225015,0.999,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
143691,autotrain-t5-base-ft-38781101938,['qingyan/autotrain-data-t5-base-ft'],,1.2404479861099105,AutoTrain,Not Specified,Not Specified,Not Specified,,0.002,,,,891616913.0,True,287,0,"['transformers', 'pytorch']",2023-03-04 07:48:15+00:00,2023-03-04 07:45:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38781101938
- CO2 Emissions (in grams): 1.2404

## Validation Metrics

- Loss: 0.002
- SacreBLEU: 44.667
- Gen len: 17.552",,,1,[],[],NLP,2023-03,718786215.1287316,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
144052,EnglishtoOldTupiV1,['Tritkoman/autotrain-data-englishtotupi'],,4.570027035803065,AutoTrain,Not Specified,Not Specified,Not Specified,,7.512,,,,4918420761.0,True,3,0,"['transformers', 'pytorch']",2023-03-04 18:45:02+00:00,2023-03-04 18:33:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 38864102062
- CO2 Emissions (in grams): 4.5700

## Validation Metrics

- Loss: 7.512
- SacreBLEU: 0.811
- Gen len: 15.526",,,1,[],[],NLP,2023-03,1076234499.8109434,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
144151,creepy-wapo,['wendys-llc/autotrain-data-creepy-wapo'],,0.0028780538108779,AutoTrain,Not Specified,Not Specified,Not Specified,0.985,0.101,0.857,,,737768761.0,True,8,0,"['transformers', 'pytorch']",2023-03-04 21:50:56+00:00,2023-03-04 21:49:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 38907102177
- CO2 Emissions (in grams): 0.0029

## Validation Metrics

- Loss: 0.101
- Accuracy: 0.985
- Precision: 1.000
- Recall: 0.750
- AUC: 0.988
- F1: 0.857

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/wendys-llc/autotrain-creepy-wapo-38907102177
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""wendys-llc/autotrain-creepy-wapo-38907102177"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""wendys-llc/autotrain-creepy-wapo-38907102177"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,256342935010.98804,0.9165526601520088,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
144153,autotrain-creepy-wapo-38909102184,['reichaves/autotrain-data-creepy-wapo'],,0.3414432453230842,AutoTrain,Not Specified,Not Specified,Not Specified,0.985,0.091,0.857,,,433320053.0,True,8,0,"['transformers', 'pytorch']",2023-03-04 21:51:10+00:00,2023-03-04 21:50:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 38909102184
- CO2 Emissions (in grams): 0.3414

## Validation Metrics

- Loss: 0.091
- Accuracy: 0.985
- Precision: 1.000
- Recall: 0.750
- AUC: 0.984
- F1: 0.857

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/reichaves/autotrain-creepy-wapo-38909102184
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""reichaves/autotrain-creepy-wapo-38909102184"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""reichaves/autotrain-creepy-wapo-38909102184"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,1269083687.9493082,0.9165526601520088,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
144427,autotrain-fake_news_fine_tuned_v4-38998102353,['systash/autotrain-data-fake_news_fine_tuned_v4'],,0.00711258375656,AutoTrain,Not Specified,Not Specified,Not Specified,0.983,0.091,0.982,,,498662069.0,True,34,0,"['transformers', 'pytorch']",2023-03-05 23:15:24+00:00,2023-03-05 08:57:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 38998102353
- CO2 Emissions (in grams): 0.0071

## Validation Metrics

- Loss: 0.091
- Accuracy: 0.983
- Precision: 0.986
- Recall: 0.979
- AUC: 0.998
- F1: 0.982

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/systash/autotrain-fake_news_fine_tuned_v4-38998102353
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""systash/autotrain-fake_news_fine_tuned_v4-38998102353"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""systash/autotrain-fake_news_fine_tuned_v4-38998102353"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,70109834353.807,0.982499745547074,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
144428,autotrain-fake_news_fine_tuned_v4-38998102356,['systash/autotrain-data-fake_news_fine_tuned_v4'],,5.261861894487811,AutoTrain,Not Specified,Not Specified,Not Specified,0.993,0.03,0.993,,,498662069.0,True,13,0,"['transformers', 'pytorch']",2023-03-05 09:11:08+00:00,2023-03-05 08:57:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 38998102356
- CO2 Emissions (in grams): 5.2619

## Validation Metrics

- Loss: 0.030
- Accuracy: 0.993
- Precision: 0.991
- Recall: 0.995
- AUC: 1.000
- F1: 0.993

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/systash/autotrain-fake_news_fine_tuned_v4-38998102356
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""systash/autotrain-fake_news_fine_tuned_v4-38998102356"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""systash/autotrain-fake_news_fine_tuned_v4-38998102356"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,94769129.064825,0.993,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
145517,autotrain-long-t5-tglobal-base-16384-book-summary-39278102680,['zaib32/autotrain-data-long-t5-tglobal-base-16384-book-summary'],,15.082265058465753,AutoTrain,Not Specified,Not Specified,Not Specified,,1.091,,0.5282399999999999,0.3997699999999999,990452905.0,True,43,0,"['transformers', 'pytorch']",2023-03-06 18:38:47+00:00,2023-03-06 17:59:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 39278102680
- CO2 Emissions (in grams): 15.0823

## Validation Metrics

- Loss: 1.091
- Rouge1: 52.824
- Rouge2: 28.175
- RougeL: 39.977
- RougeLsum: 49.082
- Gen Len: 103.980

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zaib32/autotrain-long-t5-tglobal-base-16384-book-summary-39278102680
```",,,1,[],[],NLP,2023-03,65670037.03757704,0.4551125630111743,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
145641,autotrain-sentiment_analysis-39304102733,['raghuram13/autotrain-data-sentiment_analysis'],,1.3916852050499846,AutoTrain,Not Specified,Not Specified,Not Specified,0.937,0.2,0.938,,,556848625.0,True,9,0,"['transformers', 'pytorch']",2023-03-06 20:28:45+00:00,2023-03-06 20:25:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 39304102733
- CO2 Emissions (in grams): 1.3917

## Validation Metrics

- Loss: 0.200
- Accuracy: 0.937
- Precision: 0.936
- Recall: 0.940
- AUC: 0.982
- F1: 0.938

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/raghuram13/autotrain-sentiment_analysis-39304102733
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""raghuram13/autotrain-sentiment_analysis-39304102733"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""raghuram13/autotrain-sentiment_analysis-39304102733"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,400125418.4346954,0.9374997333333336,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
146051,bert-claimcoherence-mini,['lucafrost/autotrain-data-claimcoherence-lf'],,0.5905299701991715,AutoTrain,Not Specified,Not Specified,Not Specified,0.82,0.396,0.824,,,1334464117.0,True,19,0,"['transformers', 'pytorch']",2023-03-07 10:42:15+00:00,2023-03-07 10:41:09+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 39443102994
- CO2 Emissions (in grams): 0.5905

## Validation Metrics

- Loss: 0.396
- Accuracy: 0.820
- Precision: 0.913
- Recall: 0.750
- AUC: 0.907
- F1: 0.824

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lucafrost/autotrain-claimcoherence-lf-39443102994
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucafrost/autotrain-claimcoherence-lf-39443102994"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucafrost/autotrain-claimcoherence-lf-39443102994"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,2259773736.039032,0.8219951338199513,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
146143,autotrain-claimcoherence-n500-39483103025,['lucafrost/autotrain-data-claimcoherence-n500'],,0.0023152344981197,AutoTrain,Not Specified,Not Specified,Not Specified,0.78,0.517,0.804,,,1334464117.0,True,12,0,"['transformers', 'pytorch']",2023-03-07 13:15:05+00:00,2023-03-07 13:14:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 39483103025
- CO2 Emissions (in grams): 0.0023

## Validation Metrics

- Loss: 0.517
- Accuracy: 0.780
- Precision: 0.804
- Recall: 0.804
- AUC: 0.817
- F1: 0.804

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lucafrost/autotrain-claimcoherence-n500-39483103025
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucafrost/autotrain-claimcoherence-n500-39483103025"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucafrost/autotrain-claimcoherence-n500-39483103025"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,576383998287.7653,0.7918181818181819,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
146419,astrophotography-object-classifier-alpha,['ppicazo/autotrain-data-astrophotography-object-classifier-alpha3'],,1.911157752223008,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.011,1.0,,,344448365.0,True,28,1,"['transformers', 'pytorch']",2023-03-10 17:32:33+00:00,2023-03-07 19:56:22+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 39543103134
- CO2 Emissions (in grams): 1.9112

## Validation Metrics

- Loss: 0.011
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-03,180230211.0327349,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
146423,autotrain-spam-39547103148,['johnpaulbin/autotrain-data-spam'],,1.3372976003843626,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,1.0,,,438007925.0,True,16,0,"['transformers', 'pytorch']",2023-03-07 20:13:41+00:00,2023-03-07 20:10:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 39547103148
- CO2 Emissions (in grams): 1.3373

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/johnpaulbin/autotrain-spam-39547103148
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""johnpaulbin/autotrain-spam-39547103148"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""johnpaulbin/autotrain-spam-39547103148"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,327532125.1411121,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
146628,autotrain-iptc-es-39574103204,['milyiyo/autotrain-data-iptc-es'],,0.0015000099579637,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.014,1.0,,,439525557.0,True,62,0,"['transformers', 'pytorch']",2023-03-08 04:40:17+00:00,2023-03-08 04:39:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 39574103204
- CO2 Emissions (in grams): 0.0015

## Validation Metrics

- Loss: 0.014
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/milyiyo/autotrain-iptc-es-39574103204
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""milyiyo/autotrain-iptc-es-39574103204"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""milyiyo/autotrain-iptc-es-39574103204"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,293015092777.5617,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
146736,autotrain-aa-39629103274,['yzx123/autotrain-data-aa'],,0.0198628308885719,AutoTrain,Not Specified,Not Specified,Not Specified,0.346,1.545,0.103,,,1302250229.0,True,29,0,"['transformers', 'pytorch']",2023-03-08 09:37:48+00:00,2023-03-08 09:26:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 39629103274
- CO2 Emissions (in grams): 0.0199

## Validation Metrics

- Loss: 1.545
- Accuracy: 0.346
- Macro F1: 0.103
- Micro F1: 0.346
- Weighted F1: 0.178
- Macro Precision: 0.069
- Micro Precision: 0.346
- Weighted Precision: 0.119
- Macro Recall: 0.200
- Micro Recall: 0.346
- Weighted Recall: 0.346


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/yzx123/autotrain-aa-39629103274
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""yzx123/autotrain-aa-39629103274"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""yzx123/autotrain-aa-39629103274"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,65562166657.18333,0.1587438752783964,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
146837,autotrain-english_translation-39667103325,['QianT/autotrain-data-english_translation'],,0.0044486875510411,AutoTrain,Not Specified,Not Specified,Not Specified,,0.959,,,,310022533.0,True,446,0,"['transformers', 'pytorch']",2023-03-08 13:01:03+00:00,2023-03-08 12:58:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 39667103325
- CO2 Emissions (in grams): 0.0044

## Validation Metrics

- Loss: 0.959
- SacreBLEU: 21.605
- Gen len: 41.193",,,1,[],[],NLP,2023-03,69688538348.22527,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
146949,autotrain-map_no_map_twitter_demo-39701103400,['davanstrien/autotrain-data-map_no_map_twitter_demo'],,0.000979668302282,AutoTrain,Not Specified,Not Specified,Not Specified,0.947,0.137,,,,347599761.0,True,10,1,"['transformers', 'pytorch']",2023-03-08 15:34:19+00:00,2023-03-08 15:33:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 39701103400
- CO2 Emissions (in grams): 0.0010

## Validation Metrics

- Loss: 0.137
- Accuracy: 0.947
- Precision: 1.000
- Recall: 0.923
- AUC: 1.000
- F1: 0.960",,,1,[],[],Computer Vision,2023-03,354813726432.0129,0.9469999999999998,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
147069,autotrain-client-message-topics-39730103438,['unlabeledstudiosllc/autotrain-data-client-message-topics'],,0.5874712120343145,AutoTrain,Not Specified,Not Specified,Not Specified,0.996,0.029,0.997,,,1334468213.0,True,8,0,"['transformers', 'pytorch']",2023-03-08 18:23:55+00:00,2023-03-08 18:22:44+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 39730103438
- CO2 Emissions (in grams): 0.5875

## Validation Metrics

- Loss: 0.029
- Accuracy: 0.996
- Macro F1: 0.997
- Micro F1: 0.996
- Weighted F1: 0.996
- Macro Precision: 0.997
- Micro Precision: 0.996
- Weighted Precision: 0.996
- Macro Recall: 0.997
- Micro Recall: 0.996
- Weighted Recall: 0.996


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/unlabeledstudiosllc/autotrain-client-message-topics-39730103438
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""unlabeledstudiosllc/autotrain-client-message-topics-39730103438"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""unlabeledstudiosllc/autotrain-client-message-topics-39730103438"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,2271546563.752392,0.9964997491219268,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
147314,autotrain-iemocap_text_4-39809103601,['wofeishenling/autotrain-data-iemocap_text_4'],,0.438477125256298,AutoTrain,Not Specified,Not Specified,Not Specified,0.694,0.875,0.697,,,438014069.0,True,42,0,"['transformers', 'pytorch']",2023-03-09 04:13:05+00:00,2023-03-09 04:11:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 39809103601
- CO2 Emissions (in grams): 0.4385

## Validation Metrics

- Loss: 0.875
- Accuracy: 0.694
- Macro F1: 0.697
- Micro F1: 0.694
- Weighted F1: 0.695
- Macro Precision: 0.708
- Micro Precision: 0.694
- Weighted Precision: 0.700
- Macro Recall: 0.690
- Micro Recall: 0.694
- Weighted Recall: 0.694


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/wofeishenling/autotrain-iemocap_text_4-39809103601
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""wofeishenling/autotrain-iemocap_text_4-39809103601"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""wofeishenling/autotrain-iemocap_text_4-39809103601"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,998943944.3254256,0.6954967649173256,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
147624,iva_mt_wslot-m2m100_418M-en-pl,['cartesinus/iva_mt_wslot'],8890299.0,0.68,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,1944201353.0,False,60,0,"['tensorboard', 'transformers', 'safetensors', 'pytorch']",2023-03-20 22:14:46+00:00,2023-03-09 14:06:50+00:00,"
<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# iva_mt_wslot-m2m100_418M-0.1.0 en-pl

This model is a fine-tuned version of [facebook/m2m100_418M](https://huggingface.co/facebook/m2m100_418M) on the
[iva_mt_wslot](https://huggingface.co/datasets/cartesinus/iva_mt_wslot) dataset. It achieves the following results:

1) On the test set (iva_mt):
- BLEU (plain text): 39.1560
- BLEU (with slots): 63.8767
- F1 score: (in preparation)

For reference BLEU for baseline m2m100-418M (plain text) was 21.9468. Second result (63.8767) is when tags are treated as ""normal"" words in sentence. Therefore that result 
might be a bit misleading. Please refer to plain text results if you are not sure how to interpret them.

2) WMT20 (en2pl):
- BLEU (lowercased, tags removed): 15.0863
- BLEU for baseline m2m100-418M (plain text): 20.2750

For reference WMT20 submission systems in en-pl direction had between 25 and 30 BLEU
   
3) BLEU on the evaluation set (same as in below table 'Training results'): 61.6249

4) On the training set (to see how it adjusted to train):
- BLEU (plain text): 70.5597
- BLEU (with slots): 93.8200

BLEU was measured with [sacreBLEU](https://github.com/mjpost/sacrebleu) library.

## Model description, intended uses & limitations

Model is biased towards virtual assistant (IVA) sentences in prediction/translation. These sentences are short, imperatives with a lot of name entities (slots) and 
particular vocabulary (for example settings name). It can be observed in above results where WMT results are very low while in-domain test is very high.

This model will most probably force IVA translations on your text. As long as sentences that you are translating are more or less similar to massive and leyzer domains it
will be ok. If you will translate out-of-domain sentenences (such as for example News, Medical) that are not very similar then results will drop significantly up to the
point where baseline m2m100-418M will be better than this model.

This model will generate tags in output even if there is not tag in input sentence. Frequency of this depends on input text origin. When testing IVA utterances this occurs 
between 3 and 5%. When WMT20 was translated it happened in 40% cases (input text was from News domain).
This is not very severe problem and it can be fixed easily in post-processing (simple `sed 's/<[a-z]>//g'` should be enough in most cases).

Translations with slot annotation very often differ from same sentences when slots are removed. This is quite frequent and it happens between 30 and 50% of translated
utterances. For example there will be a difference between ""is it raining in barcelona"" and ""is it raining in \<a\>barcelona\<a\>"". In second case model will more likely
localize name of city to some Polish name (here Lublin, because such city was given in Massive train set). This might be useful if you want to generate more variants.

One last thing that needs to be mentioned is that BLEU is not particulary good metric to evaluate IVA sentences due to their length and it should be evalued with other
metrices (e.g. [GLEU](https://aclanthology.org/P15-2097.pdf)).

## How to use

First please make sure to install `pip install transformers`. First download model: 

```python
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
import torch

def translate(input_text, lang):
    input_ids = tokenizer(input_text, return_tensors=""pt"")
    generated_tokens = model.generate(**input_ids, forced_bos_token_id=tokenizer.get_lang_id(lang))
    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

model_name = ""cartesinus/iva_mt_wslot-m2m100_418M-0.1.0""
tokenizer = M2M100Tokenizer.from_pretrained(model_name, src_lang=""en"", tgt_lang=""pl"")
model = M2M100ForConditionalGeneration.from_pretrained(model_name)
```

Then you can translate either plan text like this:
```python
print(translate(""set the temperature on my thermostat"", ""pl""))
```
or you can translate with slot annotations that will be restored in tgt language:
```python
print(translate(""wake me up at <a>nine am<a> on <b>friday<b>"", ""pl"")) #translation: obudź mnie o <a>piątej rano<a> <b>w tym tygodniu<b>
```
Limitations of translation with slot transfer:
1) Annotated words must be placed between semi-xml tags like this ""this is \<a\>example\<a\>""
2) There is no closing tag for example ""\<\a\>"" in above example - this is done on purpose to ommit problems with backslash escape
3) If sentence consists of more than one slot then simply use next alphabet letter. For example ""this is \<a\>example\<a\> with more than \<b\>one\<b\> slot""
4) Please do not add space before first or last annotated word because this particular model was trained this way and it most probably will lower it's results 


## Training and evaluation data

## Dataset Composition (en-pl)
| Corpus                                                               | Train  | Dev   | Test  |
|----------------------------------------------------------------------|--------|-------|-------|
| [Massive 1.1](https://huggingface.co/datasets/AmazonScience/massive) | 11514  | 2033  | 2974  |
| [Leyzer 0.2.0](https://github.com/cartesinus/leyzer/tree/0.2.0)      | 3974   | 701   | 1380  |
| [OpenSubtitles from OPUS](https://opus.nlpl.eu/OpenSubtitles-v1.php) | 2329   | 411   | 500   |
| [KDE from OPUS](https://opus.nlpl.eu/KDE4.php)                       | 1154   | 241   | 241   |
| [CCMatrix from Opus](https://opus.nlpl.eu/CCMatrix.php)              | 1096   | 232   | 237   |
| [Ubuntu from OPUS](https://opus.nlpl.eu/Ubuntu.php)                  | 281    | 60    | 59    |
| [Gnome from OPUS](https://opus.nlpl.eu/GNOME.php)                    | 14     | 3     | 3     |
| *total*                                                              | 20362  | 3681  | 5394  |

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 10
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step  | Validation Loss | BLEU    | Gen Len |
|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|
| 0.0178        | 1.0   | 5091  | 0.0171          | 57.4439 | 21.1396 |
| 0.013         | 2.0   | 10182 | 0.0159          | 58.886  | 21.2285 |
| 0.0091        | 3.0   | 15273 | 0.0157          | 60.159  | 21.1222 |
| 0.0073        | 4.0   | 20364 | 0.0159          | 60.5893 | 21.1212 |
| 0.0054        | 5.0   | 25455 | 0.0161          | 60.6484 | 21.0679 |
| 0.004         | 6.0   | 30546 | 0.0166          | 61.5283 | 21.0875 |
| 0.0031        | 7.0   | 35637 | 0.0169          | 61.0439 | 21.1562 |
| 0.0024        | 8.0   | 40728 | 0.0172          | 61.9427 | 21.2203 |
| 0.0018        | 9.0   | 45819 | 0.0175          | 61.7325 | 21.1478 |
| 0.0014        | 10.0  | 50910 | 0.0176          | 61.6249 | 21.157  |


### Framework versions

- Transformers 4.26.1
- Pytorch 1.13.1+cu116
- Datasets 2.10.1
- Tokenizers 0.13.2",,,1,[],[],NLP,2023-03,2859119636.7647057,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
148150,autotrain-opus-100-40115104344,['kangketik/autotrain-data-opus-100'],,12.835507584437984,AutoTrain,Not Specified,Not Specified,Not Specified,,3.062,,,,4918420761.0,True,5,0,"['transformers', 'pytorch']",2023-03-10 05:00:23+00:00,2023-03-10 04:27:23+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 40115104344
- CO2 Emissions (in grams): 12.8355

## Validation Metrics

- Loss: 3.062
- SacreBLEU: 1.691
- Gen len: 8.960",,,1,[],[],NLP,2023-03,383188645.142728,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
148579,Spanish_to_Latino_V2,['Maghrebi/autotrain-data-a'],,0.0024297260542898,AutoTrain,Not Specified,Not Specified,Not Specified,,2.165,,,,310022533.0,True,11,0,"['transformers', 'pytorch']",2023-03-10 11:44:50+00:00,2023-03-10 11:43:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 40181104517
- CO2 Emissions (in grams): 0.0024

## Validation Metrics

- Loss: 2.165
- SacreBLEU: 30.664
- Gen len: 21.919",,,1,[],[],NLP,2023-03,127595673780.8528,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
148581,Spanish_to_Ladino,['Maghrebi/autotrain-data-a'],,0.011671825977948,AutoTrain,Not Specified,Not Specified,Not Specified,,8.607,,,,2329628725.0,True,24,0,"['transformers', 'pytorch']",2023-03-10 11:50:25+00:00,2023-03-10 11:43:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 40181104515
- CO2 Emissions (in grams): 0.0117

## Validation Metrics

- Loss: 8.607
- SacreBLEU: 1.115
- Gen len: 7.161",,,1,[],[],NLP,2023-03,199594196263.8452,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
148810,autotrain-uk-news-slant-classification-40226104645,['jwhandley/autotrain-data-uk-news-slant-classification'],,0.0164677703389178,AutoTrain,Not Specified,Not Specified,Not Specified,0.96,0.187,0.96,,,556851697.0,True,9,0,"['transformers', 'pytorch']",2023-03-10 17:16:44+00:00,2023-03-10 17:07:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40226104645
- CO2 Emissions (in grams): 0.0165

## Validation Metrics

- Loss: 0.187
- Accuracy: 0.960
- Macro F1: 0.960
- Micro F1: 0.960
- Weighted F1: 0.960
- Macro Precision: 0.960
- Micro Precision: 0.960
- Weighted Precision: 0.960
- Macro Recall: 0.960
- Micro Recall: 0.960
- Weighted Recall: 0.960


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/jwhandley/autotrain-uk-news-slant-classification-40226104645
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""jwhandley/autotrain-uk-news-slant-classification-40226104645"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""jwhandley/autotrain-uk-news-slant-classification-40226104645"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,33814638262.47374,0.96,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
148817,fitness_message_classification,['lucieackley/autotrain-data-fitness_message_classification'],,0.0027673309805279,AutoTrain,Not Specified,Not Specified,Not Specified,0.589,1.121,0.403,,,556860913.0,True,7,0,"['transformers', 'pytorch']",2023-03-10 17:24:03+00:00,2023-03-10 17:22:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40235104670
- CO2 Emissions (in grams): 0.0028

## Validation Metrics

- Loss: 1.121
- Accuracy: 0.589
- Macro F1: 0.403
- Micro F1: 0.589
- Weighted F1: 0.541
- Macro Precision: 0.433
- Micro Precision: 0.589
- Weighted Precision: 0.536
- Macro Recall: 0.415
- Micro Recall: 0.589
- Weighted Recall: 0.589


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/lucieackley/autotrain-fitness_message_classification-40235104670
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""lucieackley/autotrain-fitness_message_classification-40235104670"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""lucieackley/autotrain-fitness_message_classification-40235104670"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,201226711556.48047,0.4785624999999999,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
148919,GermantoHunsrikV1,['Tritkoman/autotrain-data-germantohunsrikv1'],,0.0077019578034322,AutoTrain,Not Specified,Not Specified,Not Specified,,9.256,,,,1200723333.0,True,17,0,"['transformers', 'pytorch']",2023-03-10 20:13:08+00:00,2023-03-10 20:08:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 40265104741
- CO2 Emissions (in grams): 0.0077

## Validation Metrics

- Loss: 9.256
- SacreBLEU: 0.112
- Gen len: 3.045",,,1,[],[],NLP,2023-03,155898456424.27765,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
149161,astrophotography-object-classifier-alpha2,['ppicazo/autotrain-data-astrophotography-object-classifier-beta4'],,0.011767756569323,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.013,1.0,,,344454509.0,True,6,0,"['transformers', 'pytorch']",2023-03-11 05:24:03+00:00,2023-03-11 05:17:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40327104863
- CO2 Emissions (in grams): 0.0118

## Validation Metrics

- Loss: 0.013
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-03,29271043037.88437,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
149192,astrophotography-object-classifier-alpha4,['ppicazo/autotrain-data-astrophotography-object-classifier-alpha4'],,0.0066292934865041,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.015,1.0,,,344454509.0,True,103,0,"['transformers', 'pytorch']",2023-03-11 06:41:09+00:00,2023-03-11 06:37:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40333104876
- CO2 Emissions (in grams): 0.0066

## Validation Metrics

- Loss: 0.015
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-03,51959459888.33345,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
149228,autotrain-nyx-40340104916,['dpogreb/autotrain-data-nyx'],,1.1040305125054486,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,3,0,"['transformers', 'pytorch']",2023-03-11 08:46:56+00:00,,,,,1,[],[],Computer Vision,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
149272,autotrain-aramaic-40367104972,['Maghrebi/autotrain-data-aramaic'],,0.372501788799609,AutoTrain,Not Specified,Not Specified,Not Specified,,7.425,,,,305510213.0,True,4,0,"['transformers', 'pytorch']",2023-03-11 10:05:12+00:00,2023-03-11 10:04:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 40367104972
- CO2 Emissions (in grams): 0.3725

## Validation Metrics

- Loss: 7.425
- SacreBLEU: 0.000
- Gen len: 23.500",,,1,[],[],NLP,2023-03,820157707.1200382,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
149295,autotrain-numeric_prediction-40376105012,['bibekbehera/autotrain-data-numeric_prediction'],,27.469239802379224,AutoTrain,Not Specified,Not Specified,Not Specified,,0.211,,,,,True,2,0,"['transformers', 'joblib']",2023-03-11 11:52:58+00:00,2023-03-11 10:40:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 40376105012
- CO2 Emissions (in grams): 27.4692

## Validation Metrics

- Loss: 0.211
- R2: 0.339
- MSE: 0.045
- MAE: 0.104
- RMSLE: 0.145

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-03,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
149296,autotrain-numeric_prediction-40376105019,['bibekbehera/autotrain-data-numeric_prediction'],,0.0987566567732708,AutoTrain,Not Specified,Not Specified,Not Specified,,0.152,,,,,True,3,0,"['transformers', 'joblib']",2023-03-11 11:40:14+00:00,2023-03-11 10:41:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 40376105019
- CO2 Emissions (in grams): 0.0988

## Validation Metrics

- Loss: 0.152
- R2: 0.659
- MSE: 0.023
- MAE: 0.062
- RMSLE: 0.105

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-03,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
149483,autotrain-diffusion-emotion-facial-expression-recognition-40429105176,['kdhht2334/autotrain-data-diffusion-emotion-facial-expression-recognition'],,0.8103871386449576,AutoTrain,Not Specified,Not Specified,Not Specified,0.847,0.49,0.779,,,110410225.0,True,98,3,"['transformers', 'pytorch']",2023-03-11 16:18:06+00:00,2023-03-11 16:15:59+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40429105176
- CO2 Emissions (in grams): 0.8104

## Validation Metrics

- Loss: 0.490
- Accuracy: 0.847
- Macro F1: 0.779
- Micro F1: 0.847
- Weighted F1: 0.843
- Macro Precision: 0.784
- Micro Precision: 0.847
- Weighted Precision: 0.850
- Macro Recall: 0.792
- Micro Recall: 0.847
- Weighted Recall: 0.847",,,1,[],[],Computer Vision,2023-03,136243802.17165852,0.8115781057810578,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
149484,autotrain-diffusion-emotion-facial-expression-recognition-40429105179,['kdhht2334/autotrain-data-diffusion-emotion-facial-expression-recognition'],,0.8786351270318874,AutoTrain,Not Specified,Not Specified,Not Specified,0.866,0.438,0.805,,,347620241.0,True,68,1,"['transformers', 'pytorch']",2023-03-11 16:18:21+00:00,2023-03-11 16:16:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40429105179
- CO2 Emissions (in grams): 0.8786

## Validation Metrics

- Loss: 0.438
- Accuracy: 0.866
- Macro F1: 0.805
- Micro F1: 0.866
- Weighted F1: 0.863
- Macro Precision: 0.820
- Micro Precision: 0.866
- Weighted Precision: 0.862
- Macro Recall: 0.796
- Micro Recall: 0.866
- Weighted Recall: 0.866",,,1,[],[],Computer Vision,2023-03,395636630.3886507,0.8343865948533813,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
149602,autotrain-cxr-cfdl-repro-40197105212,['katielink/autotrain-data-cxr-cfdl-repro'],,0.0082321369397393,AutoTrain,Not Specified,Not Specified,Not Specified,0.98,0.063,,,,343268717.0,True,12,0,"['transformers', 'pytorch']",2023-03-11 19:58:35+00:00,2023-03-11 19:53:48+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 40197105212
- CO2 Emissions (in grams): 0.0082

## Validation Metrics

- Loss: 0.063
- Accuracy: 0.980
- Precision: 0.984
- Recall: 0.990
- AUC: 0.997
- F1: 0.987",,,1,[],[],Computer Vision,2023-03,41698615986.68581,0.98,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
149644,autotrain-vision-tcg-40463105224,['micazevedo/autotrain-data-vision-tcg'],,1.6135086188105332,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.751,1.0,,,348009745.0,True,4,0,"['transformers', 'pytorch']",2023-03-11 21:25:08+00:00,2023-03-11 21:21:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40463105224
- CO2 Emissions (in grams): 1.6135

## Validation Metrics

- Loss: 0.751
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-03,215685085.86991635,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
150111,ainu-2-japanese,[''],,52.20776152155173,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,301229701.0,False,86,3,"['transformers', 'pytorch']",2023-03-18 03:40:00+00:00,2023-03-12 15:15:18+00:00,"
「.」がコーパスにないので，「.」を使わないようにしてください（.を使うといつも「そう言って」みたいなエラー翻訳が出ます）。英数字しか対応していません。（カタカナ表記でもいつか学習させてみます。）

```python
  from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

  tokenizer = AutoTokenizer.from_pretrained(""SoMiyagawa/ainu-2-japanese"")

  model = AutoModelForSeq2SeqLM.from_pretrained(""SoMiyagawa/ainu-2-japanese"")
```

# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 40582105303
- CO2 Emissions (in grams): 52.2078

## Validation Metrics

- Loss: 1.638
- SacreBLEU: 10.448
- Gen len: 14.110",,,1,[],[],NLP,2023-03,5769826.022432513,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
150619,autotrain-event_ar_test-40764105491,['Asma/autotrain-data-event_ar_test'],,0.4364148951699333,AutoTrain,Not Specified,Not Specified,Not Specified,0.973,0.117,0.789,,,438007925.0,True,6,0,"['transformers', 'pytorch']",2023-03-13 10:51:10+00:00,2023-03-13 10:50:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40764105491
- CO2 Emissions (in grams): 0.4364

## Validation Metrics

- Loss: 0.117
- Accuracy: 0.973
- Macro F1: 0.789
- Micro F1: 0.973
- Weighted F1: 0.970
- Macro Precision: 0.853
- Micro Precision: 0.973
- Weighted Precision: 0.969
- Macro Recall: 0.746
- Micro Recall: 0.973
- Weighted Recall: 0.973


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Asma/autotrain-event_ar_test-40764105491
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Asma/autotrain-event_ar_test-40764105491"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Asma/autotrain-event_ar_test-40764105491"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,1003650264.570934,0.8713927355278093,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
150803,article-summarizer-t5-large,['cnn_dailymail'],,0.0061140848393105,AutoTrain,Not Specified,Not Specified,Not Specified,,0.055,,0.44548,0.4453,3132793669.0,True,27,1,"['transformers', 'safetensors', 'pytorch']",2023-03-23 15:37:41+00:00,2023-03-13 15:53:30+00:00,"
# Model Trained Using AutoTrain

- Trained from FLAN-T5 large
- Problem type: Summarization
- Model ID: 40818105603
- CO2 Emissions (in grams): 0.0061

## Validation Metrics

- Loss: 0.055
- Rouge1: 44.548
- Rouge2: 42.697
- RougeL: 44.530
- RougeLsum: 44.567
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aszfcxcgszdx/autotrain-summary2.0-40818105603
```",,,1,[],[],NLP,2023-03,512389630064.9457,0.4453899818136914,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
150838,autotrain-mapreader-5000-40830105612,['davanstrien/autotrain-data-mapreader-5000'],,0.0080776577350643,AutoTrain,Not Specified,Not Specified,Not Specified,0.995,0.038,0.983,,,347607953.0,True,5,0,"['transformers', 'pytorch']",2023-03-13 17:05:03+00:00,2023-03-13 16:57:32+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40830105612
- CO2 Emissions (in grams): 0.0081

## Validation Metrics

- Loss: 0.038
- Accuracy: 0.995
- Macro F1: 0.983
- Micro F1: 0.995
- Weighted F1: 0.995
- Macro Precision: 0.991
- Micro Precision: 0.995
- Weighted Precision: 0.995
- Macro Recall: 0.975
- Micro Recall: 0.995
- Weighted Recall: 0.995",,,1,[],[],Computer Vision,2023-03,43033260927.98768,0.988963599595551,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
150854,summarizer_v3,['aszfcxcgszdx/autotrain-data-summary-v3'],,3.520254114566687,AutoTrain,Not Specified,Not Specified,Not Specified,,1.818,,0.44176,0.41172,3132793669.0,True,4,0,"['transformers', 'pytorch']",2023-03-13 17:46:14+00:00,2023-03-13 17:38:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 40835105619
- CO2 Emissions (in grams): 3.5203

## Validation Metrics

- Loss: 1.818
- Rouge1: 44.176
- Rouge2: 25.696
- RougeL: 41.172
- RougeLsum: 41.276
- Gen Len: 15.201

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aszfcxcgszdx/autotrain-summary-v3-40835105619
```",,,1,[],[],NLP,2023-03,889933955.6302514,0.4262113399259502,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
150868,autotrain-event_conflict-40840105627,['Asma/autotrain-data-event_conflict'],,0.0014174377692098,AutoTrain,Not Specified,Not Specified,Not Specified,0.975,0.118,0.975,,,540852341.0,True,11,0,"['transformers', 'pytorch']",2023-03-13 18:06:51+00:00,2023-03-13 18:06:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 40840105627
- CO2 Emissions (in grams): 0.0014

## Validation Metrics

- Loss: 0.118
- Accuracy: 0.975
- Precision: 0.961
- Recall: 0.990
- AUC: 0.990
- F1: 0.975

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Asma/autotrain-event_conflict-40840105627
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Asma/autotrain-event_conflict-40840105627"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Asma/autotrain-event_conflict-40840105627"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,381570431343.53406,0.975,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
150889,autotrain-glenn_ntsa_2-40841105633,['gjbooth2/autotrain-data-glenn_ntsa_2'],,0.0098203434537638,AutoTrain,Not Specified,Not Specified,Not Specified,0.912,0.333,0.764,,,1334533813.0,True,20,0,"['transformers', 'pytorch']",2023-03-13 19:01:29+00:00,2023-03-13 18:56:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40841105633
- CO2 Emissions (in grams): 0.0098

## Validation Metrics

- Loss: 0.333
- Accuracy: 0.912
- Macro F1: 0.764
- Micro F1: 0.912
- Weighted F1: 0.911
- Macro Precision: 0.778
- Micro Precision: 0.912
- Weighted Precision: 0.916
- Macro Recall: 0.767
- Micro Recall: 0.912
- Weighted Recall: 0.912


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/gjbooth2/autotrain-glenn_ntsa_2-40841105633
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""gjbooth2/autotrain-glenn_ntsa_2-40841105633"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""gjbooth2/autotrain-glenn_ntsa_2-40841105633"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,135894820714.08807,0.8314653937947496,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
150925,t5-large-en-de,['aszfcxcgszdx/autotrain-data-translator'],,4.2211417553362205,AutoTrain,Not Specified,Not Specified,Not Specified,,0.994,,,,2950733825.0,True,26,0,"['transformers', 'safetensors', 'pytorch']",2023-03-23 15:37:12+00:00,2023-03-13 19:48:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Finetuned from t5 large
- Model ID: 40847105640
- CO2 Emissions (in grams): 4.2211

## Validation Metrics

- Loss: 0.994
- SacreBLEU: 10.222
- Gen len: 16.562",,,1,[],[],NLP,2023-03,699036894.7618935,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
150936,reverse-summarizer,['aszfcxcgszdx/autotrain-data-reverse-sum'],,0.0159037893290565,AutoTrain,Not Specified,Not Specified,Not Specified,,2.577,,0.19482,0.15465,3132793669.0,True,6,0,"['transformers', 'pytorch']",2023-03-13 20:33:13+00:00,2023-03-13 20:08:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Reverse-Summarization
- Model ID: 40852105646
- CO2 Emissions (in grams): 0.0159

Given a headline, the model will attempt to generate an article that pairs well with the headline.

## Validation Metrics

- Loss: 2.577
- Rouge1: 19.482
- Rouge2: 6.359
- RougeL: 15.465
- RougeLsum: 17.852
- Gen Len: 18.956

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aszfcxcgszdx/autotrain-reverse-sum-40852105646
```",,,1,[],[],NLP,2023-03,196984102604.80072,0.1724263198557816,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
150960,dialog-summarizer-t5-large,['aszfcxcgszdx/autotrain-data-samsum'],,3.254351692657141,AutoTrain,Not Specified,Not Specified,Not Specified,,1.176,,0.50935,0.43216,3132793669.0,True,6,0,"['transformers', 'pytorch']",2023-03-13 21:00:11+00:00,2023-03-13 20:51:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 40867105678
- CO2 Emissions (in grams): 3.2544

## Validation Metrics

- Loss: 1.176
- Rouge1: 50.935
- Rouge2: 26.673
- RougeL: 43.216
- RougeLsum: 46.941
- Gen Len: 17.073

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aszfcxcgszdx/autotrain-samsum-40867105678
```",,,1,[],[],NLP,2023-03,962647545.460002,0.4675907765185711,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
151227,iva_mt_wslot-m2m100_1.2B-en-pl,['cartesinus/iva_mt_wslot'],8890299.0,0.68,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,4966633968.0,False,10,0,"['tensorboard', 'transformers', 'pytorch']",2023-03-14 19:33:51+00:00,2023-03-14 09:18:09+00:00,"
<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# iva_mt_wslot-m2m100_1.2B-en-pl (v0.1.0)

This model is a fine-tuned version of [facebook/m2m100_1.2B](https://huggingface.co/facebook/m2m100_1.2B) on the
[iva_mt_wslot](https://huggingface.co/datasets/cartesinus/iva_mt_wslot) dataset. There is also smaller version of this model here:
[cartesinus/iva_mt_wslot-m2m100_418M-en-pl](https://huggingface.co/cartesinus/iva_mt_wslot-m2m100_418M-0.1.0). This model (1.2B) achieves the following results:

1) On the test set (iva_mt):
- BLEU (plain text): **(result in preparation)**
- BLEU (with slots): **(result in preparation)**
- F1 score: (in preparation)

For reference BLEU for baseline m2m100-418M (plain text) was 21.9468 and for m2m100-1.2B was **(result in preparation)**. Second result (BLEU with slots) is when tags
are treated as ""normal"" words in sentence. Therefore that result might be a bit misleading. Please refer to plain text results if you are not sure how to interpret them.

2) WMT20 (en2pl):
- BLEU (lowercased, tags removed): **(result in preparation)**
- BLEU for baseline m2m100-1.2B (plain text): **(result in preparation)**

For reference WMT20 submission systems in en-pl direction had between 25 and 30 BLEU
   
3) BLEU on the evaluation set (same as in below table 'Training results'): **62.4604**

4) On the training set (to see how it adjusted to train):
- BLEU (plain text): **(result in preparation)**
- BLEU (with slots): **(result in preparation)**

BLEU was measured with [sacreBLEU](https://github.com/mjpost/sacrebleu) library.

## Model description, intended uses & limitations

Model is biased towards virtual assistant (IVA) sentences in prediction/translation. These sentences are short, imperatives with a lot of name entities (slots) and 
particular vocabulary (for example settings name). It can be observed in above results where WMT results are very low while in-domain test is very high.

This model will most probably force IVA translations on your text. As long as sentences that you are translating are more or less similar to massive and leyzer domains it
will be ok. If you will translate out-of-domain sentenences (such as for example News, Medical) that are not very similar then results will drop significantly.

One last thing that needs to be mentioned is that BLEU is not particulary good metric to evaluate IVA sentences due to their length and it should be evalued with other
metrices (e.g. [GLEU](https://aclanthology.org/P15-2097.pdf)).

## How to use

First please make sure to install `pip install transformers`. First download model: 

```python
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
import torch

def translate(input_text, lang):
    input_ids = tokenizer(input_text, return_tensors=""pt"")
    generated_tokens = model.generate(**input_ids, forced_bos_token_id=tokenizer.get_lang_id(lang))
    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

model_name = ""cartesinus/iva_mt_wslot-m2m100_1.2B-en-pl""
tokenizer = M2M100Tokenizer.from_pretrained(model_name, src_lang=""en"", tgt_lang=""pl"")
model = M2M100ForConditionalGeneration.from_pretrained(model_name)
```

Then you can translate either plan text like this:
```python
print(translate(""set the temperature on my thermostat"", ""pl""))
```
or you can translate with slot annotations that will be restored in tgt language:
```python
print(translate(""wake me up at <a>nine am<a> on <b>friday<b>"", ""pl"")) #translation: obudź mnie o <a>piątej rano<a> <b>w tym tygodniu<b>
```
Limitations of translation with slot transfer:
1) Annotated words must be placed between semi-xml tags like this ""this is \<a\>example\<a\>""
2) There is no closing tag for example ""\<\a\>"" in above example - this is done on purpose to ommit problems with backslash escape
3) If sentence consists of more than one slot then simply use next alphabet letter. For example ""this is \<a\>example\<a\> with more than \<b\>one\<b\> slot""
4) Please do not add space before first or last annotated word because this particular model was trained this way and it most probably will lower it's results 


## Training and evaluation data

## Dataset Composition (en-pl)
| Corpus                                                               | Train  | Dev   | Test  |
|----------------------------------------------------------------------|--------|-------|-------|
| [Massive 1.1](https://huggingface.co/datasets/AmazonScience/massive) | 11514  | 2033  | 2974  |
| [Leyzer 0.2.0](https://github.com/cartesinus/leyzer/tree/0.2.0)      | 3974   | 701   | 1380  |
| [OpenSubtitles from OPUS](https://opus.nlpl.eu/OpenSubtitles-v1.php) | 2329   | 411   | 500   |
| [KDE from OPUS](https://opus.nlpl.eu/KDE4.php)                       | 1154   | 241   | 241   |
| [CCMatrix from Opus](https://opus.nlpl.eu/CCMatrix.php)              | 1096   | 232   | 237   |
| [Ubuntu from OPUS](https://opus.nlpl.eu/Ubuntu.php)                  | 281    | 60    | 59    |
| [Gnome from OPUS](https://opus.nlpl.eu/GNOME.php)                    | 14     | 3     | 3     |
| *total*                                                              | 20362  | 3681  | 5394  |

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Bleu    | Gen Len |
|:-------------:|:-----:|:-----:|:---------------:|:-------:|:-------:|
| 0.2744        | 1.0   | 5091  | 0.2555          | 58.5119 | 21.0728 |
| 0.1829        | 2.0   | 10182 | 0.2475          | 59.7364 | 21.0769 |
| 0.1124        | 3.0   | 15273 | 0.2499          | 61.3552 | 21.06   |
| 0.0783        | 4.0   | 20364 | 0.2597          | 61.6618 | 21.2402 |
| 0.0496        | 5.0   | 25455 | 0.2698          | 62.1942 | 21.2901 |
| 0.0318        | 6.0   | 30546 | 0.2798          | 61.9068 | 21.3399 |
| 0.0204        | 7.0   | 35637 | 0.2893          | 61.7753 | 21.3102 |
| 0.0138        | 8.0   | 40728 | 0.2979          | 62.3925 | 21.3238 |
| 0.009         | 9.0   | 45819 | 0.3034          | 62.4942 | 21.2516 |
| 0.0058        | 10.0  | 50910 | 0.3082          | 62.4604 | 21.2847 |


### Framework versions

- Transformers 4.26.1
- Pytorch 1.13.1+cu116
- Datasets 2.10.1
- Tokenizers 0.13.2
",,,1,[],[],NLP,2023-03,7303873482.352941,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,1,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
151241,text_summarization_48_91_rouge_knowdocument,['nahorh/autotrain-data-text_summarization_knowdocument'],,27.26345745623384,AutoTrain,Not Specified,Not Specified,Not Specified,,0.753,,0.4891,0.3879599999999999,2283804653.0,True,16,0,"['transformers', 'pytorch']",2023-03-14 10:49:56+00:00,2023-03-14 09:38:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 40969105857
- CO2 Emissions (in grams): 27.2635

## Validation Metrics

- Loss: 0.753
- Rouge1: 48.910
- Rouge2: 28.780
- RougeL: 38.796
- RougeLsum: 46.262
- Gen Len: 68.490

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/nahorh/autotrain-text_summarization_knowdocument-40969105857
```",,,1,[],[],NLP,2023-03,83767976.1147757,0.4326984151597382,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
151244,autotrain-satellite-image-classification-40975105875,['victor/autotrain-data-satellite-image-classification-11a7e0c2'],,2.3259806262831075,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.002,1.0,,,344442221.0,True,34,0,"['transformers', 'pytorch']",2023-03-14 09:53:01+00:00,2023-03-14 09:46:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 40975105875
- CO2 Emissions (in grams): 2.3260

## Validation Metrics

- Loss: 0.002
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-03,148084733.42721474,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
151597,CodeExplainer,['sagard21/autotrain-data-code-explainer'],,5.393079045128973,AutoTrain,Not Specified,Not Specified,Not Specified,,2.156,,0.29375,0.25445,2950733825.0,True,86,0,"['transformers', 'pytorch']",2023-03-14 19:37:53+00:00,2023-03-14 19:27:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 2745581349
- CO2 Emissions (in grams): 5.3931

# Model Description

This model is an attempt to simplify code understanding by generating line by line explanation of a source code. This model was fine-tuned using the Salesforce/codet5-large model. Currently it is trained on a small subset of Python snippets.

# Model Usage

```py
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    AutoConfig,
    pipeline,
)

model_name = ""ashwinR/CodeExplainer""

tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

config = AutoConfig.from_pretrained(model_name)

model.eval()

pipe = pipeline(""summarization"", model=model_name, config=config, tokenizer=tokenizer)

raw_code = """"""
def preprocess(text: str) -> str:
    text = str(text)
    text = text.replace(""\n"", "" "")
    tokenized_text = text.split("" "")
    preprocessed_text = "" "".join([token for token in tokenized_text if token])

    return preprocessed_text
""""""

print(pipe(raw_code)[0][""summary_text""])

```

## Validation Metrics

- Loss: 2.156
- Rouge1: 29.375
- Rouge2: 18.128
- RougeL: 25.445
- RougeLsum: 28.084
- Gen Len: 19.000
",,,1,[],[],NLP,2023-03,547133427.9190849,0.2726913079168186,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
151681,autotrain-test-41086106044,['Younesao/autotrain-data-test'],,0.187128228983522,AutoTrain,Not Specified,Not Specified,Not Specified,0.1,1.11,0.074,,,94383181.0,True,3,0,"['transformers', 'pytorch']",2023-03-14 22:26:02+00:00,2023-03-14 22:25:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 41086106044
- CO2 Emissions (in grams): 0.1871

## Validation Metrics

- Loss: 1.110
- Accuracy: 0.100
- Macro F1: 0.074
- Micro F1: 0.100
- Weighted F1: 0.111
- Macro Precision: 0.083
- Micro Precision: 0.100
- Weighted Precision: 0.125
- Macro Recall: 0.067
- Micro Recall: 0.100
- Weighted Recall: 0.100",,,1,[],[],Computer Vision,2023-03,504377033.3994404,0.0850574712643678,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
151941,autotrain-bikes_1-41171106189,['mouss/autotrain-data-bikes_1'],,0.4166541049999939,AutoTrain,Not Specified,Not Specified,Not Specified,0.818,0.368,,,,110394865.0,True,15,0,"['transformers', 'pytorch']",2023-03-15 08:59:13+00:00,2023-03-15 08:58:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 41171106189
- CO2 Emissions (in grams): 0.4167

## Validation Metrics

- Loss: 0.368
- Accuracy: 0.818
- Precision: 0.882
- Recall: 0.789
- AUC: 0.921
- F1: 0.833",,,1,[],[],Computer Vision,2023-03,264955663.88335863,0.818,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
152205,multilingual-samsum,['aszfcxcgszdx/autotrain-data-multi-lingual-summarization'],,13.328572874208332,AutoTrain,Not Specified,Not Specified,Not Specified,,1.508,,0.4406799999999999,0.37071,4918519065.0,True,9,0,"['transformers', 'pytorch']",2023-03-15 14:29:30+00:00,2023-03-15 13:54:42+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 41234106312
- CO2 Emissions (in grams): 13.3286

## Validation Metrics

- Loss: 1.508
- Rouge1: 44.068
- Rouge2: 20.883
- RougeL: 37.071
- RougeLsum: 40.613
- Gen Len: 17.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aszfcxcgszdx/autotrain-multi-lingual-summarization-41234106312
```",,,1,[],[],NLP,2023-03,369020682.9658154,0.4026780778663774,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
152206,mt5-large-samsum,['aszfcxcgszdx/autotrain-data-multi-lingual-summarization'],,12.703463244389663,AutoTrain,Not Specified,Not Specified,Not Specified,,1.508,,0.44142,0.37127,4918519065.0,True,2,0,"['transformers', 'pytorch']",2023-03-15 14:27:58+00:00,2023-03-15 13:54:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 41234106313
- CO2 Emissions (in grams): 12.7035

## Validation Metrics

- Loss: 1.508
- Rouge1: 44.142
- Rouge2: 21.000
- RougeL: 37.127
- RougeLsum: 40.611
- Gen Len: 17.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aszfcxcgszdx/autotrain-multi-lingual-summarization-41234106313
```",,,1,[],[],NLP,2023-03,387179383.3207024,0.4033173864573208,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
152226,samsum,['aszfcxcgszdx/autotrain-data-samsum-auto'],,0.0077793677303344,AutoTrain,Not Specified,Not Specified,Not Specified,,1.565,,0.47592,0.3962299999999999,1625541389.0,True,7,0,"['transformers', 'pytorch']",2023-03-15 14:30:17+00:00,2023-03-15 14:25:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 41244106342
- CO2 Emissions (in grams): 0.0078

## Validation Metrics

- Loss: 1.565
- Rouge1: 47.592
- Rouge2: 23.270
- RougeL: 39.623
- RougeLsum: 43.180
- Gen Len: 18.305

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/aszfcxcgszdx/autotrain-samsum-auto-41244106342
```",,,1,[],[],NLP,2023-03,208955463393.44025,0.4324342867625981,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
152229,autotrain-bikes-ag-41243106351,['mouss/autotrain-data-bikes-ag'],,1.381064904462668,AutoTrain,Not Specified,Not Specified,Not Specified,0.936,0.161,0.936,,,347603857.0,True,4,0,"['transformers', 'pytorch']",2023-03-15 14:33:33+00:00,2023-03-15 14:30:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 41243106351
- CO2 Emissions (in grams): 1.3811

## Validation Metrics

- Loss: 0.161
- Accuracy: 0.936
- Macro F1: 0.936
- Micro F1: 0.936
- Weighted F1: 0.936
- Macro Precision: 0.936
- Micro Precision: 0.936
- Weighted Precision: 0.936
- Macro Recall: 0.936
- Micro Recall: 0.936
- Weighted Recall: 0.936",,,1,[],[],Computer Vision,2023-03,251692629.2723676,0.936,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
152239,autotrain-bbikes-41250106361,['mouss/autotrain-data-bbikes'],,1.333999157955412,AutoTrain,Not Specified,Not Specified,Not Specified,0.83,0.599,0.829,,,347616145.0,True,5,0,"['transformers', 'pytorch']",2023-03-15 14:47:01+00:00,2023-03-15 14:43:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 41250106361
- CO2 Emissions (in grams): 1.3340

## Validation Metrics

- Loss: 0.599
- Accuracy: 0.830
- Macro F1: 0.829
- Micro F1: 0.830
- Weighted F1: 0.830
- Macro Precision: 0.831
- Micro Precision: 0.830
- Weighted Precision: 0.833
- Macro Recall: 0.831
- Micro Recall: 0.830
- Weighted Recall: 0.830",,,1,[],[],Computer Vision,2023-03,260581982.32507345,0.8294996986136225,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
153221,s2g_summ_bart,['PavelDanek/autotrain-data-skill2go_summ_mbart'],,5.638732652622368,AutoTrain,Not Specified,Not Specified,Not Specified,,2.384,,0.17079,0.16808,3468694113.0,True,39,0,"['transformers', 'pytorch']",2023-03-16 17:31:32+00:00,2023-03-16 17:16:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 41524106867
- CO2 Emissions (in grams): 5.6387

## Validation Metrics

- Loss: 2.384
- Rouge1: 17.079
- Rouge2: 4.461
- RougeL: 16.808
- RougeLsum: 16.852
- Gen Len: 30.956

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/PavelDanek/autotrain-skill2go_summ_mbart-41524106867
```",,,1,[],[],NLP,2023-03,615154916.306741,0.1694241638386402,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,1,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
153262,autotrain-t5-hinglish-to-en,['RohanHBTU/autotrain-data-t5-autotrain'],,0.0035724951002547,AutoTrain,Not Specified,Not Specified,Not Specified,,1.785,,,,191650757.0,True,12,0,"['transformers', 'pytorch']",2023-03-16 18:35:03+00:00,2023-03-16 18:32:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 41534106887
- CO2 Emissions (in grams): 0.0036

## Validation Metrics

- Loss: 1.785
- SacreBLEU: 24.776
- Gen len: 9.347",,,1,[],[],NLP,2023-03,53646191701.23881,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
153425,autotrain-wx-en-zh-41586107006,['T1231415/autotrain-data-wx-en-zh'],,0.0037032259466193,AutoTrain,Not Specified,Not Specified,Not Specified,,1.799,,,,310022533.0,True,52,0,"['transformers', 'pytorch']",2023-03-16 23:43:46+00:00,2023-03-16 23:41:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 41586107006
- CO2 Emissions (in grams): 0.0037

## Validation Metrics

- Loss: 1.799
- SacreBLEU: 7.223
- Gen len: 62.436",,,1,[],[],NLP,2023-03,83716882920.15282,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
153580,autotrain-cv-sentiment-41629107126,['guriko/autotrain-data-cv-sentiment'],,0.707048910768399,AutoTrain,Not Specified,Not Specified,Not Specified,0.864,0.653,0.84,,,1334472309.0,True,4,0,"['transformers', 'pytorch']",2023-03-17 06:10:38+00:00,2023-03-17 06:08:43+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 41629107126
- CO2 Emissions (in grams): 0.7070

## Validation Metrics

- Loss: 0.653
- Accuracy: 0.864
- Macro F1: 0.840
- Micro F1: 0.864
- Weighted F1: 0.861
- Macro Precision: 0.874
- Micro Precision: 0.864
- Weighted Precision: 0.870
- Macro Recall: 0.826
- Micro Recall: 0.864
- Weighted Recall: 0.864


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/guriko/autotrain-cv-sentiment-41629107126
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""guriko/autotrain-cv-sentiment-41629107126"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""guriko/autotrain-cv-sentiment-41629107126"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,1887383303.581836,0.8518309859154929,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
153977,EnglishtoOldRussianV1,['Tritkoman/autotrain-data-engtoorv'],,1.2383909090027077,AutoTrain,Not Specified,Not Specified,Not Specified,,13.671,,,,2329628725.0,True,12,0,"['transformers', 'pytorch']",2023-03-17 17:18:50+00:00,2023-03-17 17:16:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 41771107469
- CO2 Emissions (in grams): 1.2384

## Validation Metrics

- Loss: 13.671
- SacreBLEU: 0.002
- Gen len: 12.000",,,1,[],[],NLP,2023-03,1881173955.7068293,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
154076,swissbert,[''],,0.6,Not Specified,Not Specified,Not Specified,RTX 2080 Ti.,,,,,,,False,103,6,"['transformers', 'pytorch']",2023-03-24 08:36:41+00:00,2023-03-17 20:32:32+00:00,"
SwissBERT is a masked language model for processing Switzerland-related text. It has been trained on more than 21 million Swiss news articles retrieved from [Swissdox@LiRI](https://t.uzh.ch/1hI).

<img src=""https://vamvas.ch/assets/swissbert/swissbert-diagram.png"" alt=""SwissBERT is a transformer encoder with language adapters in each layer. There is an adapter for each national language of Switzerland. The other parameters in the model are shared among the four languages."" width=""450"" style=""max-width: 100%;"">

SwissBERT is based on [X-MOD](https://huggingface.co/facebook/xmod-base), which has been pre-trained with language adapters in 81 languages.
For SwissBERT we trained adapters for the national languages of Switzerland – German, French, Italian, and Romansh Grischun.
In addition, we used a Switzerland-specific subword vocabulary.

The pre-training code and usage examples are available [here](https://github.com/ZurichNLP/swissbert). We also release a version that was fine-tuned on named entity recognition (NER): https://huggingface.co/ZurichNLP/swissbert-ner

## Languages

SwissBERT contains the following language adapters:

| lang_id (Adapter index) | Language code | Language              |
|-------------------------|---------------|-----------------------|
| 0                       | `de_CH`       | Swiss Standard German |
| 1                       | `fr_CH`       | French                |
| 2                       | `it_CH`       | Italian               |
| 3                       | `rm_CH`       | Romansh Grischun      |

## License
Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).

## Usage (masked language modeling)

```python
from transformers import pipeline

fill_mask = pipeline(model=""ZurichNLP/swissbert"")
```

### German example
```python
fill_mask.model.set_default_language(""de_CH"")
fill_mask(""Der schönste Kanton der Schweiz ist <mask>."")
```
Output:
```
[{'score': 0.1373230218887329,
  'token': 331,
  'token_str': 'Zürich',
  'sequence': 'Der schönste Kanton der Schweiz ist Zürich.'},
 {'score': 0.08464793860912323,
  'token': 5903,
  'token_str': 'Appenzell',
  'sequence': 'Der schönste Kanton der Schweiz ist Appenzell.'},
 {'score': 0.08250337839126587,
  'token': 10800,
  'token_str': 'Graubünden',
  'sequence': 'Der schönste Kanton der Schweiz ist Graubünden.'},
 ...]
```

### French example
```python
fill_mask.model.set_default_language(""fr_CH"")
fill_mask(""Je m'appelle <mask> Federer."")
```
Output:
```
[{'score': 0.9943694472312927,
  'token': 1371,
  'token_str': 'Roger',
  'sequence': ""Je m'appelle Roger Federer.""},
 ...]
```

## Bias, Risks, and Limitations
- SwissBERT is mainly intended for tagging tokens in written text (e.g., named entity recognition, part-of-speech tagging), text classification, and the encoding of words, sentences or documents into fixed-size embeddings.
SwissBERT is not designed for generating text.
- The model was adapted on written news articles and might perform worse on other domains or language varieties.
- While we have removed many author bylines, we did not anonymize the pre-training corpus. The model might have memorized information that has been described in the news but is no longer in the public interest.

## Training Details
- Training data: German, French, Italian and Romansh documents in the [Swissdox@LiRI](https://t.uzh.ch/1hI) database, until 2022.
- Training procedure: Masked language modeling

## Environmental Impact
- Hardware type: RTX 2080 Ti.
- Hours used: 10 epochs × 18 hours × 8 devices = 1440 hours
- Site: Zurich, Switzerland.
- Energy source: 100% hydropower ([source](https://t.uzh.ch/1rU))
- Carbon efficiency: 0.0016 kg CO2e/kWh ([source](https://t.uzh.ch/1rU))
- Carbon emitted: 0.6 kg CO2e ([source](https://mlco2.github.io/impact#compute))

## Citation
```bibtex
@article{vamvas-etal-2023-swissbert,
      title={Swiss{BERT}: The Multilingual Language Model for Switzerland}, 
      author={Jannis Vamvas and Johannes Gra\""en and Rico Sennrich},
      year={2023},
      eprint={2303.13310},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.13310}
}
```",10 epochs × 18 hours × 8 devices = 1440 hours,,1,[],[],NLP,2023-03,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,0,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,1.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
154242,autotrain-iptc-classification-v2-41840107634,['milyiyo/autotrain-data-iptc-classification-v2'],,0.7917416253329401,AutoTrain,Not Specified,Not Specified,Not Specified,0.732,1.345,0.514,,,439617781.0,True,13,1,"['transformers', 'pytorch']",2023-03-18 04:21:36+00:00,2023-03-18 04:19:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 41840107634
- CO2 Emissions (in grams): 0.7917

## Validation Metrics

- Loss: 1.345
- Accuracy: 0.732
- Macro F1: 0.514
- Micro F1: 0.732
- Weighted F1: 0.681
- Macro Precision: 0.513
- Micro Precision: 0.732
- Weighted Precision: 0.660
- Macro Recall: 0.542
- Micro Recall: 0.732
- Weighted Recall: 0.732


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/milyiyo/autotrain-iptc-classification-v2-41840107634
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""milyiyo/autotrain-iptc-classification-v2-41840107634"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""milyiyo/autotrain-iptc-classification-v2-41840107634"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,555254096.7075385,0.6039293739967897,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
154500,Marian-en-ar,['mobarmg/autotrain-data-trans-en-ar'],,39.58121152223037,AutoTrain,Not Specified,Not Specified,Not Specified,,0.524,,,,305510213.0,True,22,0,"['transformers', 'safetensors', 'pytorch']",2023-03-19 19:25:32+00:00,2023-03-18 13:13:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 41895107787
- CO2 Emissions (in grams): 39.5812

## Validation Metrics

- Loss: 0.524
- SacreBLEU: 64.117
- Gen len: 127.927",,,1,[],[],NLP,2023-03,7718566.492802107,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,1,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
154780,autotrain-iptc-classification-v3-41985107904,['milyiyo/autotrain-data-iptc-classification-v3'],,0.7705822753825974,AutoTrain,Not Specified,Not Specified,Not Specified,0.744,1.325,0.529,,,439620917.0,True,14,0,"['transformers', 'pytorch']",2023-03-18 22:43:13+00:00,2023-03-18 22:41:17+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 41985107904
- CO2 Emissions (in grams): 0.7706

## Validation Metrics

- Loss: 1.325
- Accuracy: 0.744
- Macro F1: 0.529
- Micro F1: 0.744
- Weighted F1: 0.690
- Macro Precision: 0.554
- Micro Precision: 0.744
- Weighted Precision: 0.680
- Macro Recall: 0.545
- Micro Recall: 0.744
- Weighted Recall: 0.744


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/milyiyo/autotrain-iptc-classification-v3-41985107904
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""milyiyo/autotrain-iptc-classification-v3-41985107904"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""milyiyo/autotrain-iptc-classification-v3-41985107904"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,570504839.0604707,0.618344069128044,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
154893,autotrain-iptc-classification-v4-42015107919,['milyiyo/autotrain-data-iptc-classification-v4'],,0.845545764970478,AutoTrain,Not Specified,Not Specified,Not Specified,0.758,1.231,0.531,,,439605493.0,True,41,0,"['transformers', 'pytorch']",2023-03-19 02:27:35+00:00,2023-03-19 02:25:40+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 42015107919
- CO2 Emissions (in grams): 0.8455

## Validation Metrics

- Loss: 1.231
- Accuracy: 0.758
- Macro F1: 0.531
- Micro F1: 0.758
- Weighted F1: 0.708
- Macro Precision: 0.532
- Micro Precision: 0.758
- Weighted Precision: 0.685
- Macro Recall: 0.553
- Micro Recall: 0.758
- Weighted Recall: 0.758


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/milyiyo/autotrain-iptc-classification-v4-42015107919
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""milyiyo/autotrain-iptc-classification-v4-42015107919"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""milyiyo/autotrain-iptc-classification-v4-42015107919"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,519907391.4294263,0.6245120248254462,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
155027,autotrain-opennohara-thread-title-classification-42043107973,['noharao/autotrain-data-opennohara-thread-title-classification'],,0.2737732363891665,AutoTrain,Not Specified,Not Specified,Not Specified,0.938,0.14,0.667,,,532369769.0,True,32,0,"['transformers', 'pytorch']",2023-03-19 07:00:39+00:00,2023-03-19 06:59:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 42043107973
- CO2 Emissions (in grams): 0.2738

## Validation Metrics

- Loss: 0.140
- Accuracy: 0.938
- Precision: 0.712
- Recall: 0.627
- AUC: 0.968
- F1: 0.667

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/noharao/autotrain-opennohara-thread-title-classification-42043107973
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""noharao/autotrain-opennohara-thread-title-classification-42043107973"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""noharao/autotrain-opennohara-thread-title-classification-42043107973"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,1944564691.6459007,0.7796211838006231,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
155520,autotrain-multic-42139108182,['vkheman/autotrain-data-multic'],,1.3867154772046042,AutoTrain,Not Specified,Not Specified,Not Specified,0.836,0.758,0.835,,,438134005.0,True,10,0,"['transformers', 'pytorch']",2023-03-19 23:38:57+00:00,2023-03-19 23:35:35+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 42139108182
- CO2 Emissions (in grams): 1.3867

## Validation Metrics

- Loss: 0.758
- Accuracy: 0.836
- Macro F1: 0.835
- Micro F1: 0.836
- Weighted F1: 0.836
- Macro Precision: 0.839
- Micro Precision: 0.836
- Weighted Precision: 0.840
- Macro Recall: 0.836
- Micro Recall: 0.836
- Weighted Recall: 0.836


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/vkheman/autotrain-multic-42139108182
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""vkheman/autotrain-multic-42139108182"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""vkheman/autotrain-multic-42139108182"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,315950901.39414024,0.8354997007779773,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
155762,autotrain-test2summbart-42231108366,['DmitriyVasiliev/autotrain-data-test2summbart'],,4.976184480111491,AutoTrain,Not Specified,Not Specified,Not Specified,,1.595,,0.05139,0.05056,3468694113.0,True,9,0,"['transformers', 'pytorch']",2023-03-20 07:48:54+00:00,2023-03-20 07:36:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 42231108366
- CO2 Emissions (in grams): 4.9762

## Validation Metrics

- Loss: 1.595
- Rouge1: 5.139
- Rouge2: 1.648
- RougeL: 5.056
- RougeLsum: 5.063
- Gen Len: 31.428

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/DmitriyVasiliev/autotrain-test2summbart-42231108366
```",,,1,[],[],NLP,2023-03,697058987.0338336,0.0509716213830309,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,1,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
155763,autotrain-test2summbart-42231108362,['DmitriyVasiliev/autotrain-data-test2summbart'],,4.4738873783495,AutoTrain,Not Specified,Not Specified,Not Specified,,1.635,,0.04765,0.04813,3468694113.0,True,3,0,"['transformers', 'pytorch']",2023-03-20 07:47:20+00:00,2023-03-20 07:36:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 42231108362
- CO2 Emissions (in grams): 4.4739

## Validation Metrics

- Loss: 1.635
- Rouge1: 4.765
- Rouge2: 1.074
- RougeL: 4.813
- RougeLsum: 4.841
- Gen Len: 32.287

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/DmitriyVasiliev/autotrain-test2summbart-42231108362
```",,,1,[],[],NLP,2023-03,775319944.3030382,0.0478887972436834,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,1,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
156012,t5_pegasus_ch_ans,['lambdarw/autotrain-data-t5-pegasus_ch_ansmrc'],,4.429613533710655,AutoTrain,Not Specified,Not Specified,Not Specified,,3.292,,0.06468,0.06485,1200772485.0,True,34,0,"['transformers', 'pytorch']",2023-03-20 13:46:31+00:00,2023-03-20 12:34:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 42285108445
- CO2 Emissions (in grams): 4.4296

## Validation Metrics

- Loss: 3.292
- Rouge1: 6.468
- Rouge2: 1.995
- RougeL: 6.485
- RougeLsum: 6.428
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/lambdarw/autotrain-t5-pegasus_ch_ansmrc-42285108445
```",,,1,[],[],NLP,2023-03,271078385.4757915,0.0647648884428317,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,1,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
156075,autotrain-pokemonclassification-42305108508,['amiune/autotrain-data-pokemonclassification'],,0.342784723518607,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.078,,,,347599761.0,True,11,0,"['transformers', 'pytorch']",2023-03-20 14:08:01+00:00,2023-03-20 14:07:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 42305108508
- CO2 Emissions (in grams): 0.3428

## Validation Metrics

- Loss: 0.078
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-03,1014046826.334522,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
156150,amber-mines,['wendys-llc/autotrain-data-amber-mines'],,0.4211017716440378,AutoTrain,Not Specified,Not Specified,Not Specified,0.95,0.195,,,,343268717.0,True,10,0,"['transformers', 'pytorch']",2023-03-20 15:37:53+00:00,2023-03-20 15:37:00+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 42327108535
- CO2 Emissions (in grams): 0.4211

## Validation Metrics

- Loss: 0.195
- Accuracy: 0.950
- Precision: 0.941
- Recall: 0.960
- AUC: 0.984
- F1: 0.950",,,1,[],[],Computer Vision,2023-03,815168066.5218598,0.95,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
156486,autotrain-summerswipe-42402108659,['antoineds/autotrain-data-summerswipe'],,8.112429648080305,AutoTrain,Not Specified,Not Specified,Not Specified,,1.084,,0.20604,0.1764099999999999,891623057.0,True,15,0,"['transformers', 'pytorch']",2023-03-20 23:12:44+00:00,2023-03-20 22:51:21+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 42402108659
- CO2 Emissions (in grams): 8.1124

## Validation Metrics

- Loss: 1.084
- Rouge1: 20.604
- Rouge2: 12.159
- RougeL: 17.641
- RougeLsum: 19.008
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/antoineds/autotrain-summerswipe-42402108659
```",,,1,[],[],NLP,2023-03,109908263.69890188,0.1900772200287619,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
156661,autotrain-t5baseparaphrase-42430108692,['krenerd/autotrain-data-t5baseparaphrase'],,2.6793230772092427,AutoTrain,Not Specified,Not Specified,Not Specified,,0.072,,0.63306,0.62478,1102414005.0,True,31,0,"['transformers', 'pytorch']",2023-03-21 03:37:38+00:00,2023-03-21 03:30:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 42430108692
- CO2 Emissions (in grams): 2.6793

## Validation Metrics

- Loss: 0.072
- Rouge1: 63.306
- Rouge2: 53.109
- RougeL: 62.478
- RougeLsum: 62.252
- Gen Len: 202.325

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/krenerd/autotrain-t5baseparaphrase-42430108692
```",,,1,[],[],NLP,2023-03,411452435.2726674,0.6288927475672581,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
156843,autotrain-bartswipe-42462108732,['antoineds/autotrain-data-bartswipe'],,5.357796203313839,AutoTrain,Not Specified,Not Specified,Not Specified,,1.436,,0.5234599999999999,0.37443,1625537293.0,True,24,0,"['transformers', 'pytorch']",2023-03-21 08:03:39+00:00,2023-03-21 07:50:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 42462108732
- CO2 Emissions (in grams): 5.3578

## Validation Metrics

- Loss: 1.436
- Rouge1: 52.346
- Rouge2: 30.300
- RougeL: 37.443
- RougeLsum: 46.145
- Gen Len: 124.900

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/antoineds/autotrain-bartswipe-42462108732
```",,,1,[],[],NLP,2023-03,303396626.39549303,0.4365771482030092,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
156968,autotrain-t5te-42492108820,['antoineds/autotrain-data-t5te'],,2.965259996157936,AutoTrain,Not Specified,Not Specified,Not Specified,,1.242,,,,891616913.0,True,25,0,"['transformers', 'pytorch']",2023-03-21 10:53:57+00:00,2023-03-21 10:46:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 42492108820
- CO2 Emissions (in grams): 2.9653

## Validation Metrics

- Loss: 1.242
- SacreBLEU: 3.463
- Gen len: 19.000",,,1,[],[],NLP,2023-03,300687600.46514,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
157558,s2g_class_cours,['PavelDanek/autotrain-data-s2g_text_class'],,0.4838643866239026,AutoTrain,Not Specified,Not Specified,Not Specified,0.845,0.392,0.793,,,217044653.0,True,15,0,"['transformers', 'pytorch']",2023-03-22 05:02:39+00:00,2023-03-22 05:01:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 42689109103
- CO2 Emissions (in grams): 0.4839

## Validation Metrics

- Loss: 0.392
- Accuracy: 0.845
- Precision: 0.800
- Recall: 0.787
- AUC: 0.898
- F1: 0.793

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/PavelDanek/autotrain-s2g_text_class-42689109103
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""PavelDanek/autotrain-s2g_text_class-42689109103"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""PavelDanek/autotrain-s2g_text_class-42689109103"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,448565050.4563878,0.8181746031746032,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
157767,autotrain-classify-42751109216,['vevlins/autotrain-data-classify'],,0.852147336270292,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.01,,,,346860409.0,True,18,0,"['transformers', 'pytorch']",2023-03-22 10:43:30+00:00,2023-03-22 10:41:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 42751109216
- CO2 Emissions (in grams): 0.8521

## Validation Metrics

- Loss: 0.010
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-03,407042766.2405784,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
157938,autotrain-names-10k-en-cn-in-kr-jp-vn-42832109361,['FEIMENG/autotrain-data-names-10k-en-cn-in-kr-jp-vn'],,2.3523995441070964,AutoTrain,Not Specified,Not Specified,Not Specified,0.951,0.207,0.898,,,1334480501.0,True,50,0,"['transformers', 'pytorch']",2023-03-22 15:05:49+00:00,2023-03-22 14:59:56+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 42832109361
- CO2 Emissions (in grams): 2.3524

## Validation Metrics

- Loss: 0.207
- Accuracy: 0.951
- Macro F1: 0.898
- Micro F1: 0.951
- Weighted F1: 0.950
- Macro Precision: 0.954
- Micro Precision: 0.951
- Weighted Precision: 0.952
- Macro Recall: 0.869
- Micro Recall: 0.951
- Weighted Recall: 0.951


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/FEIMENG/autotrain-names-10k-en-cn-in-kr-jp-vn-42832109361
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""FEIMENG/autotrain-names-10k-en-cn-in-kr-jp-vn-42832109361"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""FEIMENG/autotrain-names-10k-en-cn-in-kr-jp-vn-42832109361"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,567284798.3425922,0.923740400216333,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
157955,wip-gpt2-finetuned-imdbreviews-fast,['imdb'],133190346.0,66.0,Not Specified,fine-tuning,"Frankfurt an Main, Germany",Apple Mac Studio 2022 || M1 Max/32GB RAM/10C CPU(8+2)/24C GPU,,,,,,438005109.0,False,4,0,"['transformers', 'pytorch']",2023-03-26 00:22:51+00:00,2023-03-22 15:27:53+00:00,"543 gCO2eq/kWh

Apple Mac Studio 2022 || M1 Max/32GB RAM/10C CPU(8+2)/24C GPU

60-70 gCO2eq = 543 gCO2eq/kWh * 0,060 kW * 2 h
",,,1,[],[],NLP,2023-03,6636441.045454546,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
158019,autotrain-meme-classification-42897109437,['Hrishikesh332/autotrain-data-meme-classification'],,1.132924473643039,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.025,1.0,,,347599761.0,True,32,7,"['transformers', 'pytorch']",2023-03-24 20:23:08+00:00,2023-03-22 17:44:58+00:00,"
**Dataset**

The dataset consist of two label images:
* Meme
* Not Meme

Meme folder consist of 222 meme images and Not Meme folder consist of 108 non meme files. Meme file consist most of the images contaning the text on the picture and not meme consist of all type of images from sports to the text in various forms like document, image text to get the higher accuracy and understand about the meme in a most efficient way.

**UseCase**

* **Content Moderation** - The meme classification model can be used to filter out the content of meme from the vast amount of data generated for the specific domain from the social media for the better understanding.

**Future Scope**

* Further work on the sentiment of the meme image like positive, voilence, offensive, sarcasm, neutral, etc. This can be used for various task like:
* **Education** - To eliminate the offensive content from the curated memes for education
* **Brand Monitoring** - To understand the sentiments of the user by understanding the representation by meme culture for decision making process.
  
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 42897109437
- CO2 Emissions (in grams): 1.1329

## Validation Metrics

- Loss: 0.025
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

",,,1,[],[],Computer Vision,2023-03,306816358.09513056,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
158193,autotrain-swipetest-42970109574,['antoineds/autotrain-data-swipetest'],,2.526148177073709,AutoTrain,Not Specified,Not Specified,Not Specified,,1.018,,0.61114,0.4624,1625537293.0,True,18,0,"['transformers', 'pytorch']",2023-03-22 22:43:13+00:00,2023-03-22 22:36:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 42970109574
- CO2 Emissions (in grams): 2.5261

## Validation Metrics

- Loss: 1.018
- Rouge1: 61.114
- Rouge2: 38.781
- RougeL: 46.240
- RougeLsum: 56.531
- Gen Len: 125.044

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/antoineds/autotrain-swipetest-42970109574
```",,,1,[],[],NLP,2023-03,643484538.1409981,0.526465964938428,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
158296,autotrain-prop65-43011109672,['Cleighton071/autotrain-data-prop65'],,0.5936852940497532,AutoTrain,Not Specified,Not Specified,Not Specified,0.946,0.16,0.945,,,267855533.0,True,25,0,"['transformers', 'pytorch']",2023-03-23 02:48:40+00:00,2023-03-23 02:47:13+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 43011109672
- CO2 Emissions (in grams): 0.5937

## Validation Metrics

- Loss: 0.160
- Accuracy: 0.946
- Macro F1: 0.945
- Micro F1: 0.946
- Weighted F1: 0.946
- Macro Precision: 0.946
- Micro Precision: 0.946
- Weighted Precision: 0.946
- Macro Recall: 0.945
- Micro Recall: 0.946
- Weighted Recall: 0.946


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Cleighton071/autotrain-prop65-43011109672
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Cleighton071/autotrain-prop65-43011109672"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Cleighton071/autotrain-prop65-43011109672"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,451174276.4804826,0.9454997355896352,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,1,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
158485,TreeClassification,[''],,0.8942374660281194,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,347644817.0,False,31,0,"['transformers', 'pytorch']",2023-03-23 11:20:04+00:00,2023-03-23 09:00:16+00:00,"
## Validation Metrics

- Loss: 0.772
- Accuracy: 0.792
- Macro F1: 0.754
- Micro F1: 0.792
- Weighted F1: 0.747
- Macro Precision: 0.744
- Micro Precision: 0.792
- Weighted Precision: 0.743
- Macro Recall: 0.808
- Micro Recall: 0.792
- Weighted Recall: 0.792",,,1,[],[],Computer Vision,2023-03,388761185.039711,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
158524,it-emotion-analyzer,"['tradicio/autotrain-data-it-emotion-analysis', 'dair-ai/emotion']",,0.4489187526120041,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,439801973.0,False,35,1,"['transformers', 'pytorch']",2023-03-23 10:56:15+00:00,2023-03-23 09:57:00+00:00,"# IT-EMOTION-ANALYZER

This is a model for emotion analysis of italian sentences trained on a translated dataset by [Google Translator](https://pypi.org/project/deep-translator/). It maps sentences & paragraphs with 6 emotions which are:

- 0: sadness
- 1: joy
- 2: love
- 3: anger
- 4: fear
- 5: surprise

<!--- Describe your model here -->

## Model in action

Using this model becomes easy when you have [transformers](https://github.com/huggingface/transformers) installed:

```
pip install -U transformers
```

Then you can use the model like this:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import pipeline

sentences = [""Questa è una frase triste"", ""Questa è una frase felice"", ""Questa è una frase di stupore""]

tokenizer = AutoTokenizer.from_pretrained(""aiknowyou/it-emotion-analyzer"")
model = AutoModelForSequenceClassification.from_pretrained(""aiknowyou/it-emotion-analyzer"")

emotion_analysis = pipeline(""sentiment-analysis"", model=model, tokenizer=tokenizer)
emotion_analysis(sentences)
```
Obtaining the following result:
```python
[{'label': '0', 'score': 0.9481984972953796},
 {'label': '1', 'score': 0.9299975037574768},
 {'label': '5', 'score': 0.9543816447257996}]
```

# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 43095109829
- CO2 Emissions (in grams): 0.4489

## Validation Metrics

- Loss: 0.566
- Accuracy: 0.828
- Macro F1: 0.828
- Micro F1: 0.828
- Weighted F1: 0.828
- Macro Precision: 0.828
- Micro Precision: 0.828
- Weighted Precision: 0.828
- Macro Recall: 0.828
- Micro Recall: 0.828
- Weighted Recall: 0.828",,,1,[],[],NLP,2023-03,979691693.5214696,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
159165,GermantoSwabianV1,['Tritkoman/autotrain-data-germantoswabian'],,2.273273133617672,AutoTrain,Not Specified,Not Specified,Not Specified,,2.108,,,,295863749.0,True,2,0,"['transformers', 'pytorch']",2023-03-24 05:42:01+00:00,2023-03-24 05:36:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 43360110283
- CO2 Emissions (in grams): 2.2733

## Validation Metrics

- Loss: 2.108
- SacreBLEU: 17.460
- Gen len: 22.558",,,1,[],[],NLP,2023-03,130148790.58072723,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
159202,GermantoNorthFrisianV1,['Tritkoman/autotrain-data-germantonorthfrisian'],,3.429799463313944,AutoTrain,Not Specified,Not Specified,Not Specified,,1.137,,,,295863749.0,True,2,0,"['transformers', 'pytorch']",2023-03-24 06:31:11+00:00,2023-03-24 06:22:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 43368110298
- CO2 Emissions (in grams): 3.4298

## Validation Metrics

- Loss: 1.137
- SacreBLEU: 50.890
- Gen len: 13.543",,,1,[],[],NLP,2023-03,86262696.1618713,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
159212,autotrain-cybersecurity-summarization-pegasus-x-book-43369110299,['starcatmeow/autotrain-data-cybersecurity-summarization-pegasus-x-book'],,13.98857715454734,AutoTrain,Not Specified,Not Specified,Not Specified,,2.95,,0.3786,0.3434,2274845861.0,True,5,0,"['transformers', 'pytorch']",2023-03-24 07:06:52+00:00,2023-03-24 06:30:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 43369110299
- CO2 Emissions (in grams): 13.9886

## Validation Metrics

- Loss: 2.950
- Rouge1: 37.860
- Rouge2: 20.146
- RougeL: 34.340
- RougeLsum: 34.254
- Gen Len: 13.848

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/starcatmeow/autotrain-cybersecurity-summarization-pegasus-x-book-43369110299
```",,,1,[],[],NLP,2023-03,162621675.94797188,0.3601419390581717,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
159514,LeafCondition,['OttoYu/LeafCondition'],,0.4269107292670934,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,346869625.0,False,11,0,"['transformers', 'pytorch']",2023-03-24 14:35:23+00:00,2023-03-24 14:30:32+00:00,"
## Validation Metrics

- Loss: 0.021
- Accuracy: 1.000
- Macro F1: 1.000
- Micro F1: 1.000
- Weighted F1: 1.000
- Macro Precision: 1.000
- Micro Precision: 1.000
- Weighted Precision: 1.000
- Macro Recall: 1.000
- Micro Recall: 1.000
- Weighted Recall: 1.000",,,1,[],[],Computer Vision,2023-03,812510909.7058643,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
159588,suripe-transformer,['XDawned/autotrain-data-minecraft-modpack-quests-transformer-1'],,0.0046968912206003,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,13,0,"['transformers', 'pytorch']",2023-03-24 16:42:17+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
159640,autotrain-js-classfication-test-2-43390110454,['Rickyfwh/autotrain-data-js-classfication-test-2'],,3.495241141356263,AutoTrain,Not Specified,Not Specified,Not Specified,0.751,0.75,0.678,,,347640721.0,True,28,0,"['transformers', 'pytorch']",2023-03-24 18:36:46+00:00,2023-03-24 18:27:41+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 43390110454
- CO2 Emissions (in grams): 3.4952

## Validation Metrics

- Loss: 0.750
- Accuracy: 0.751
- Macro F1: 0.678
- Micro F1: 0.751
- Weighted F1: 0.746
- Macro Precision: 0.726
- Micro Precision: 0.751
- Weighted Precision: 0.753
- Macro Recall: 0.665
- Micro Recall: 0.751
- Weighted Recall: 0.751",,,1,[],[],Computer Vision,2023-03,99461155.02208368,0.712635409377187,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
160101,pegasus-reddit-summarizer2,['stevied67/autotrain-data-pegasus-reddit-summarizer'],,85.71551225539321,AutoTrain,Not Specified,Not Specified,Not Specified,,1.712,,0.47072,0.32773,2283804653.0,True,20,0,"['transformers', 'pytorch']",2023-03-28 16:36:14+00:00,2023-03-25 11:54:06+00:00,"
# Model Trained Using AutoTrain

This model was trained using chatgpt generated summaries of subreddit submissions from four subreddits:  talesfromtechsupport, talesfromretail, callcentres, and talesfromcustomer

- Problem type: Summarization
- Model ID: 43644110754
- CO2 Emissions (in grams): 85.7155

## Validation Metrics

- Loss: 1.712
- Rouge1: 47.072
- Rouge2: 19.245
- RougeL: 32.773
- RougeLsum: 42.178
- Gen Len: 108.663

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/stevied67/autotrain-pegasus-reddit-summarizer-43644110754
```",,,1,[],[],NLP,2023-03,26644006.352027643,0.3864213553760411,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
161486,Tree-Condition,['OttoYu/Treecondition'],,1.3038362907488008,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,347681745.0,False,11,0,"['transformers', 'pytorch']",2023-03-28 11:49:07+00:00,2023-03-26 22:26:39+00:00,"
# 🌳 Tree Condition Classification 樹況分類 (bilingual)
### Model Description
This online application covers 22 most typical tree disease over 290+ images. If you find any trees that has hidden injures, you can classifies with our model and report the tree condition via this form (https://rb.gy/c1sfja). 此在線程式涵蓋22種官方部門樹況分類的標準，超過290張圖像。如果您發現任何樹木有隱傷，您可以使用我們的模型進行分類並通過此表格報告樹木狀況。 

- **Developed by:** Yu Kai Him Otto 
- **Shared via:** Huggingface.co
- **Model type:** Opensource

## Uses
You can use the this model for tree condition image classification. 

## Training Details
### Training Data

- Loss: 0.355
- Accuracy: 0.852
- Macro F1: 0.787
- Micro F1: 0.852
- Weighted F1: 0.825
- Macro Precision: 0.808
- Micro Precision: 0.852
- Weighted Precision: 0.854
- Macro Recall: 0.811
- Micro Recall: 0.852
- Weighted Recall: 0.852",,,1,[],[],Computer Vision,2023-03,266660582.6720196,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
161842,autotrain-yasniysum-44181111477,['DmitriyVasiliev/autotrain-data-yasniysum'],,1.36421018234082,AutoTrain,Not Specified,Not Specified,Not Specified,,0.778,,0.2,0.2,3468694113.0,True,4,0,"['transformers', 'pytorch']",2023-03-27 10:03:26+00:00,2023-03-27 10:00:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44181111477
- CO2 Emissions (in grams): 1.3642

## Validation Metrics

- Loss: 0.778
- Rouge1: 20.000
- Rouge2: 0.000
- RougeL: 20.000
- RougeLsum: 20.000
- Gen Len: 33.800

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/DmitriyVasiliev/autotrain-yasniysum-44181111477
```",,,1,[],[],NLP,2023-03,2542639072.7036943,0.2,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,1,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
161933,autotrain-banking77-distilroberta-44209111546,['derek-thomas/autotrain-data-banking77-distilroberta'],,2.424235975981841,AutoTrain,Not Specified,Not Specified,Not Specified,0.927,0.305,0.928,,,328750581.0,True,62,0,"['transformers', 'pytorch']",2023-03-27 12:37:50+00:00,2023-03-27 12:31:25+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44209111546
- CO2 Emissions (in grams): 2.4242

## Validation Metrics

- Loss: 0.305
- Accuracy: 0.927
- Macro F1: 0.928
- Micro F1: 0.927
- Weighted F1: 0.927
- Macro Precision: 0.935
- Micro Precision: 0.927
- Weighted Precision: 0.932
- Macro Recall: 0.925
- Micro Recall: 0.927
- Weighted Recall: 0.927


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/derek-thomas/autotrain-banking77-distilroberta-44209111546
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""derek-thomas/autotrain-banking77-distilroberta-44209111546"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""derek-thomas/autotrain-banking77-distilroberta-44209111546"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,135609975.3724893,0.927499730458221,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162132,autotrain-tableros_factibilidad-44246111620,['SebasV/autotrain-data-tableros_factibilidad'],,0.5449792702985709,AutoTrain,Not Specified,Not Specified,Not Specified,0.6,1.067,0.542,,,110401009.0,True,6,0,"['transformers', 'pytorch']",2023-03-27 16:45:39+00:00,2023-03-27 16:44:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44246111620
- CO2 Emissions (in grams): 0.5450

## Validation Metrics

- Loss: 1.067
- Accuracy: 0.600
- Macro F1: 0.542
- Micro F1: 0.600
- Weighted F1: 0.567
- Macro Precision: 0.583
- Micro Precision: 0.600
- Weighted Precision: 0.667
- Macro Recall: 0.625
- Micro Recall: 0.600
- Weighted Recall: 0.600",,,1,[],[],Computer Vision,2023-03,202578364.01651755,0.5695271453590193,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
162133,autotrain-tableros_factibilidad-44246111621,['SebasV/autotrain-data-tableros_factibilidad'],,0.6678858266803156,AutoTrain,Not Specified,Not Specified,Not Specified,0.2,1.097,0.167,,,343274861.0,True,7,0,"['transformers', 'pytorch']",2023-03-27 16:46:06+00:00,2023-03-27 16:44:26+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44246111621
- CO2 Emissions (in grams): 0.6679

## Validation Metrics

- Loss: 1.097
- Accuracy: 0.200
- Macro F1: 0.167
- Micro F1: 0.200
- Weighted F1: 0.133
- Macro Precision: 0.125
- Micro Precision: 0.200
- Weighted Precision: 0.100
- Macro Recall: 0.250
- Micro Recall: 0.200
- Weighted Recall: 0.200",,,1,[],[],Computer Vision,2023-03,513972369.6581885,0.1820163487738419,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,1,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162134,autotrain-tableros_factibilidad-44246111622,['SebasV/autotrain-data-tableros_factibilidad'],,0.1825346039037265,AutoTrain,Not Specified,Not Specified,Not Specified,0.2,1.397,0.1,,,94391373.0,True,4,0,"['transformers', 'pytorch']",2023-03-27 16:44:55+00:00,2023-03-27 16:44:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44246111622
- CO2 Emissions (in grams): 0.1825

## Validation Metrics

- Loss: 1.397
- Accuracy: 0.200
- Macro F1: 0.100
- Micro F1: 0.200
- Weighted F1: 0.080
- Macro Precision: 0.062
- Micro Precision: 0.200
- Weighted Precision: 0.050
- Macro Recall: 0.250
- Micro Recall: 0.200
- Weighted Recall: 0.200",,,1,[],[],Computer Vision,2023-03,517114952.3505388,0.1333333333333333,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162135,autotrain-tableros_factibilidad-44246111623,['SebasV/autotrain-data-tableros_factibilidad'],,0.7280371574302341,AutoTrain,Not Specified,Not Specified,Not Specified,0.4,0.962,0.375,,,347607953.0,True,5,0,"['transformers', 'pytorch']",2023-03-27 16:46:40+00:00,2023-03-27 16:44:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44246111623
- CO2 Emissions (in grams): 0.7280

## Validation Metrics

- Loss: 0.962
- Accuracy: 0.400
- Macro F1: 0.375
- Micro F1: 0.400
- Weighted F1: 0.300
- Macro Precision: 0.333
- Micro Precision: 0.400
- Weighted Precision: 0.267
- Macro Recall: 0.500
- Micro Recall: 0.400
- Weighted Recall: 0.400",,,1,[],[],Computer Vision,2023-03,477459082.2080539,0.3870967741935484,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
162136,autotrain-tableros_factibilidad-44246111624,['SebasV/autotrain-data-tableros_factibilidad'],,0.3629630434568734,AutoTrain,Not Specified,Not Specified,Not Specified,0.4,1.413,0.375,,,346866553.0,True,2,0,"['transformers', 'pytorch']",2023-03-27 16:45:41+00:00,2023-03-27 16:44:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44246111624
- CO2 Emissions (in grams): 0.3630

## Validation Metrics

- Loss: 1.413
- Accuracy: 0.400
- Macro F1: 0.375
- Micro F1: 0.400
- Weighted F1: 0.400
- Macro Precision: 0.375
- Micro Precision: 0.400
- Weighted Precision: 0.400
- Macro Recall: 0.375
- Micro Recall: 0.400
- Weighted Recall: 0.400",,,1,[],[],Computer Vision,2023-03,955652536.1271776,0.3870967741935484,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162161,autotrain-detection-for-product-location-44269111681,['Cleighton071/autotrain-data-detection-for-product-location'],,2.30199726014708,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.005,0.999,,,556848625.0,True,7,0,"['transformers', 'pytorch']",2023-03-27 17:50:11+00:00,2023-03-27 17:44:20+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44269111681
- CO2 Emissions (in grams): 2.3020

## Validation Metrics

- Loss: 0.005
- Accuracy: 0.999
- Macro F1: 0.999
- Micro F1: 0.999
- Weighted F1: 0.999
- Macro Precision: 0.999
- Micro Precision: 0.999
- Weighted Precision: 0.999
- Macro Recall: 0.999
- Micro Recall: 0.999
- Weighted Recall: 0.999


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Cleighton071/autotrain-detection-for-product-location-44269111681
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Cleighton071/autotrain-detection-for-product-location-44269111681"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Cleighton071/autotrain-detection-for-product-location-44269111681"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,241898039.86318457,0.999,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162162,autotrain-detection-for-product-location-44269111684,['Cleighton071/autotrain-data-detection-for-product-location'],,1.9511985418671696,AutoTrain,Not Specified,Not Specified,Not Specified,0.988,0.038,0.988,,,433320053.0,True,7,0,"['transformers', 'pytorch']",2023-03-27 17:49:34+00:00,2023-03-27 17:44:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44269111684
- CO2 Emissions (in grams): 1.9512

## Validation Metrics

- Loss: 0.038
- Accuracy: 0.988
- Macro F1: 0.988
- Micro F1: 0.988
- Weighted F1: 0.988
- Macro Precision: 0.988
- Micro Precision: 0.988
- Weighted Precision: 0.988
- Macro Recall: 0.987
- Micro Recall: 0.988
- Weighted Recall: 0.988


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/Cleighton071/autotrain-detection-for-product-location-44269111684
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""Cleighton071/autotrain-detection-for-product-location-44269111684"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""Cleighton071/autotrain-detection-for-product-location-44269111684"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,222078913.9096737,0.988,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162267,autotrain-dialogsumgerman-44305111787,['fathyshalab/autotrain-data-dialogsumgerman'],,86.21246024573398,AutoTrain,Not Specified,Not Specified,Not Specified,,1.069,,0.33702,0.29431,4918519065.0,True,42,0,"['transformers', 'pytorch']",2023-03-27 23:35:58+00:00,2023-03-27 19:49:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44305111787
- CO2 Emissions (in grams): 86.2125

## Validation Metrics

- Loss: 1.069
- Rouge1: 33.702
- Rouge2: 13.478
- RougeL: 29.431
- RougeLsum: 30.710
- Gen Len: 18.952

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/fathyshalab/autotrain-dialogsumgerman-44305111787
```",,,1,[],[],NLP,2023-03,57051139.13905944,0.3142203164747438,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,1,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162332,headline-predictor,['u23429/autotrain-data-stock-distil'],,2.960971697133151,AutoTrain,Not Specified,Not Specified,Not Specified,0.94,1.634,0.882,,,1336423925.0,True,4,0,"['transformers', 'pytorch']",2023-03-27 22:05:18+00:00,2023-03-27 21:58:02+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44339111846
- CO2 Emissions (in grams): 2.9610

## Validation Metrics

- Loss: 1.634
- Accuracy: 0.940
- Macro F1: 0.882
- Micro F1: 0.940
- Weighted F1: 0.924
- Macro Precision: 0.876
- Micro Precision: 0.940
- Weighted Precision: 0.914
- Macro Recall: 0.900
- Micro Recall: 0.940
- Weighted Recall: 0.940


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/u23429/autotrain-stock-distil-44339111846
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""u23429/autotrain-stock-distil-44339111846"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""u23429/autotrain-stock-distil-44339111846"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,451346403.0385505,0.9100768386388584,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162493,autotrain-enfermedadespt2-44370111920,[''],,0.7148635752326786,AutoTrain,Not Specified,Not Specified,Not Specified,0.95,0.18,0.95,,,347607953.0,True,5,0,"['transformers', 'pytorch']",2023-03-28 01:57:12+00:00,2023-03-28 01:44:49+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44370111920
- CO2 Emissions (in grams): 0.7149

## Validation Metrics

- Loss: 0.180
- Accuracy: 0.950
- Macro F1: 0.950
- Micro F1: 0.950
- Weighted F1: 0.950
- Macro Precision: 0.950
- Micro Precision: 0.950
- Weighted Precision: 0.950
- Macro Recall: 0.950
- Micro Recall: 0.950
- Weighted Recall: 0.950",,,1,[],[],Computer Vision,2023-03,486257749.09129786,0.95,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
162561,autotrain-jira-again-44396111956,['hifructose/autotrain-data-jira-again'],,6.270223463049431,AutoTrain,Not Specified,Not Specified,Not Specified,,2.432,,0.20545,0.18502,2950848513.0,True,4,0,"['transformers', 'pytorch']",2023-03-28 04:22:04+00:00,2023-03-28 04:06:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44396111956
- CO2 Emissions (in grams): 6.2702

## Validation Metrics

- Loss: 2.432
- Rouge1: 20.545
- Rouge2: 9.628
- RougeL: 18.502
- RougeLsum: 18.666
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/hifructose/autotrain-jira-again-44396111956
```",,,1,[],[],NLP,2023-03,470612974.224829,0.1947005352523881,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162627,autotrain-zz-44428111999,['fathyshalab/autotrain-data-zz'],,17.63749609911408,AutoTrain,Not Specified,Not Specified,Not Specified,,1.117,,0.36577,0.3029,1625537293.0,True,12,0,"['transformers', 'pytorch']",2023-03-28 07:59:33+00:00,2023-03-28 07:13:04+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44428111999
- CO2 Emissions (in grams): 17.6375

## Validation Metrics

- Loss: 1.117
- Rouge1: 36.577
- Rouge2: 13.743
- RougeL: 30.290
- RougeLsum: 32.338
- Gen Len: 51.092

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/fathyshalab/autotrain-zz-44428111999
```",,,1,[],[],NLP,2023-03,92163722.32573588,0.3313794038913066,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162846,autotrain-marianmt-shi-en-fr-44506112181,['vuilleminethan/autotrain-data-marianmt-shi-en-fr'],,26.0022920107111,AutoTrain,Not Specified,Not Specified,Not Specified,,2.816,,,,4918420761.0,True,4,0,"['transformers', 'pytorch']",2023-03-28 14:13:25+00:00,2023-03-28 13:05:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 44506112181
- CO2 Emissions (in grams): 26.0023

## Validation Metrics

- Loss: 2.816
- SacreBLEU: 2.102
- Gen len: 4.674",,,1,[],[],NLP,2023-03,189153354.59558564,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162911,autotrain-test-news-44534112235,['datadmg/autotrain-data-test-news'],,2.552195145818587,AutoTrain,Not Specified,Not Specified,Not Specified,0.333,2.231,0.042,,,,True,5,0,"['transformers', 'joblib']",2023-03-28 14:46:11+00:00,2023-03-28 14:39:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44534112235
- CO2 Emissions (in grams): 2.5522

## Validation Metrics

- Loss: 2.231
- Accuracy: 0.333
- Macro F1: 0.042
- Micro F1: 0.333
- Weighted F1: 0.167
- Macro Precision: 0.028
- Micro Precision: 0.333
- Weighted Precision: 0.111
- Macro Recall: 0.083
- Micro Recall: 0.333
- Weighted Recall: 0.333

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-03,,0.074592,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
162955,potato_model,['bazudde/autotrain-data-sweet-potato-classification'],,0.2585547491917275,AutoTrain,Not Specified,Not Specified,Not Specified,0.923,0.098,0.911,,,346863481.0,True,4,0,"['transformers', 'pytorch']",2023-03-28 15:42:41+00:00,2023-03-28 15:42:05+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 44552112263
- CO2 Emissions (in grams): 0.2586

## Validation Metrics

- Loss: 0.098
- Accuracy: 0.923
- Macro F1: 0.911
- Micro F1: 0.923
- Weighted F1: 0.918
- Macro Precision: 0.958
- Micro Precision: 0.923
- Weighted Precision: 0.933
- Macro Recall: 0.889
- Micro Recall: 0.923
- Weighted Recall: 0.923",,,1,[],[],Computer Vision,2023-03,1341547513.957241,0.9169607415485278,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163371,autotrain-influencer-brand-classification-44707112576,['bdevore17/autotrain-data-influencer-brand-classification'],,0.5009176805435052,AutoTrain,Not Specified,Not Specified,Not Specified,0.869,0.352,0.869,,,737768761.0,True,9,0,"['transformers', 'pytorch']",2023-03-29 04:00:09+00:00,2023-03-29 03:58:55+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 44707112576
- CO2 Emissions (in grams): 0.5009

## Validation Metrics

- Loss: 0.352
- Accuracy: 0.869
- Precision: 0.809
- Recall: 0.939
- AUC: 0.932
- F1: 0.869

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/bdevore17/autotrain-influencer-brand-classification-44707112576
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""bdevore17/autotrain-influencer-brand-classification-44707112576"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""bdevore17/autotrain-influencer-brand-classification-44707112576"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,1472834339.1662817,0.869,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163465,radiology-report-en-zh-ft-base,['zhaozh/autotrain-data-radiology-report-en-zh-ft-base'],,1.5370817042101073,AutoTrain,Not Specified,Not Specified,Not Specified,,0.925,,,,310022533.0,True,60,1,"['transformers', 'pytorch']",2023-03-29 07:26:12+00:00,2023-03-29 07:22:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 44764112685
- CO2 Emissions (in grams): 1.5371

## Validation Metrics

- Loss: 0.925
- SacreBLEU: 23.060
- Gen len: 75.417",,,1,[],[],NLP,2023-03,201695545.6244389,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163468,autotrain-translator-44772112701,['Azzizz17/autotrain-data-translator'],,1.695441563883438,AutoTrain,Not Specified,Not Specified,Not Specified,,2.936,,,,891616913.0,True,7,0,"['transformers', 'pytorch']",2023-03-29 07:32:22+00:00,2023-03-29 07:28:03+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 44772112701
- CO2 Emissions (in grams): 1.6954

## Validation Metrics

- Loss: 2.936
- SacreBLEU: 1.565
- Gen len: 18.750",,,1,[],[],NLP,2023-03,525890677.6814745,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163469,autotrain-translator-44772112704,['Azzizz17/autotrain-data-translator'],,1.6332201411420315,AutoTrain,Not Specified,Not Specified,Not Specified,,2.93,,,,891616913.0,True,7,0,"['transformers', 'pytorch']",2023-03-29 07:32:33+00:00,2023-03-29 07:28:11+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 44772112704
- CO2 Emissions (in grams): 1.6332

## Validation Metrics

- Loss: 2.930
- SacreBLEU: 1.592
- Gen len: 18.672",,,1,[],[],NLP,2023-03,545925739.3045224,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163474,autotrain-t5-base-44767112714,['april49/autotrain-data-t5-base'],,15.98247816985612,AutoTrain,Not Specified,Not Specified,Not Specified,,0.856,,0.28704,0.28278,1102414005.0,True,33,0,"['transformers', 'pytorch']",2023-03-29 08:15:49+00:00,2023-03-29 07:33:33+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44767112714
- CO2 Emissions (in grams): 15.9825

## Validation Metrics

- Loss: 0.856
- Rouge1: 28.704
- Rouge2: 6.275
- RougeL: 28.278
- RougeLsum: 28.253
- Gen Len: 57.661

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/april49/autotrain-t5-base-44767112714
```",,,1,[],[],NLP,2023-03,68976412.3738468,0.2848940760240075,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163616,autotrain-telugu_summarization-44817112805,['Pavan27/autotrain-data-telugu_summarization'],,553.9241452628997,AutoTrain,Not Specified,Not Specified,Not Specified,,1.24,,0.2522,0.24642,2329702453.0,True,12,0,"['transformers', 'pytorch']",2023-03-30 10:16:53+00:00,2023-03-29 09:53:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44817112805
- CO2 Emissions (in grams): 553.9241

## Validation Metrics

- Loss: 1.240
- Rouge1: 25.220
- Rouge2: 6.815
- RougeL: 24.642
- RougeLsum: 25.120
- Gen Len: 82.823

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Pavan27/autotrain-telugu_summarization-44817112805
```",,,1,[],[],NLP,2023-03,4205814.953046129,0.2492764991376198,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163617,autotrain-telugu_summarization-44817112806,['Pavan27/autotrain-data-telugu_summarization'],,304.57370965004566,AutoTrain,Not Specified,Not Specified,Not Specified,,1.288,,0.25042,0.24483,2329702453.0,True,15,0,"['transformers', 'pytorch']",2023-03-29 23:18:20+00:00,2023-03-29 09:53:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44817112806
- CO2 Emissions (in grams): 304.5737

## Validation Metrics

- Loss: 1.288
- Rouge1: 25.042
- Rouge2: 6.486
- RougeL: 24.483
- RougeLsum: 24.899
- Gen Len: 82.861

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Pavan27/autotrain-telugu_summarization-44817112806
```",,,1,[],[],NLP,2023-03,7649059.5845479295,0.2475934521958606,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163618,autotrain-telugu_summarization-44817112802,['Pavan27/autotrain-data-telugu_summarization'],,306.7675447142532,AutoTrain,Not Specified,Not Specified,Not Specified,,1.265,,0.2587399999999999,0.25267,2329702453.0,True,12,0,"['transformers', 'pytorch']",2023-03-29 23:23:48+00:00,2023-03-29 09:53:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44817112802
- CO2 Emissions (in grams): 306.7675

## Validation Metrics

- Loss: 1.265
- Rouge1: 25.874
- Rouge2: 6.960
- RougeL: 25.267
- RougeLsum: 25.748
- Gen Len: 82.868

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Pavan27/autotrain-telugu_summarization-44817112802
```",,,1,[],[],NLP,2023-03,7594357.659869344,0.255668977141628,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163619,autotrain-telugu_summarization-44817112803,['Pavan27/autotrain-data-telugu_summarization'],,318.4985343917117,AutoTrain,Not Specified,Not Specified,Not Specified,,1.266,,0.25887,0.25306,2329702453.0,True,12,0,"['transformers', 'pytorch']",2023-03-29 23:54:55+00:00,2023-03-29 09:53:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44817112803
- CO2 Emissions (in grams): 318.4985

## Validation Metrics

- Loss: 1.266
- Rouge1: 25.887
- Rouge2: 6.968
- RougeL: 25.306
- RougeLsum: 25.776
- Gen Len: 82.840

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Pavan27/autotrain-telugu_summarization-44817112803
```",,,1,[],[],NLP,2023-03,7314641.046777219,0.2559320305510519,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163620,autotrain-telugu_summarization-44817112804,['Pavan27/autotrain-data-telugu_summarization'],,468.9115461709108,AutoTrain,Not Specified,Not Specified,Not Specified,,1.25,,0.25579,0.2501,2329702453.0,True,12,0,"['transformers', 'pytorch']",2023-03-30 06:32:15+00:00,2023-03-29 09:53:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44817112804
- CO2 Emissions (in grams): 468.9115

## Validation Metrics

- Loss: 1.250
- Rouge1: 25.579
- Rouge2: 6.849
- RougeL: 25.010
- RougeLsum: 25.453
- Gen Len: 82.852

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Pavan27/autotrain-telugu_summarization-44817112804
```",,,1,[],[],NLP,2023-03,4968319.658631013,0.2529130008499871,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163645,autotrain-mooyaho_v2_real-44822112832,['april49/autotrain-data-mooyaho_v2_real'],,0.0727966831523268,AutoTrain,Not Specified,Not Specified,Not Specified,,0.727,,0.12013,0.11896,1102414005.0,True,45,0,"['transformers', 'pytorch']",2023-03-29 11:04:48+00:00,2023-03-29 10:21:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44822112832
- CO2 Emissions (in grams): 0.0728

## Validation Metrics

- Loss: 0.727
- Rouge1: 12.013
- Rouge2: 3.396
- RougeL: 11.896
- RougeLsum: 11.877
- Gen Len: 58.593

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/april49/autotrain-mooyaho_v2_real-44822112832
```",,,1,[],[],NLP,2023-03,15143739484.575176,0.1195421372704839,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
163982,autotrain-mooyaho_v4-44949112969,['april49/autotrain-data-mooyaho_v4'],,0.0714759514027418,AutoTrain,Not Specified,Not Specified,Not Specified,,0.553,,0.20797,0.2055999999999999,1102414005.0,True,24,0,"['transformers', 'pytorch']",2023-03-29 18:10:49+00:00,2023-03-29 17:28:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44949112969
- CO2 Emissions (in grams): 0.0715

## Validation Metrics

- Loss: 0.553
- Rouge1: 20.797
- Rouge2: 6.694
- RougeL: 20.560
- RougeLsum: 20.573
- Gen Len: 62.816

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/april49/autotrain-mooyaho_v4-44949112969
```",,,1,[],[],NLP,2023-03,15423565316.231829,0.2067782092511546,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
164668,autotrain-mooyaho_v5-44979113066,['april49/autotrain-data-mooyaho_v5'],,0.9105215965400348,AutoTrain,Not Specified,Not Specified,Not Specified,,0.582,,0.23595,0.23442,1102414005.0,True,73,0,"['transformers', 'pytorch']",2023-03-29 20:36:00+00:00,2023-03-29 20:33:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44979113066
- CO2 Emissions (in grams): 0.9105

## Validation Metrics

- Loss: 0.582
- Rouge1: 23.595
- Rouge2: 5.595
- RougeL: 23.442
- RougeLsum: 23.336
- Gen Len: 61.500

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/april49/autotrain-mooyaho_v5-44979113066
```",,,1,[],[],NLP,2023-03,1210749980.2191982,0.2351825116397729,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
164674,autotrain-imdb-sentiment-analysis-44994113085,['desertdev/autotrain-data-imdb-sentiment-analysis'],,0.5768634462494043,AutoTrain,Not Specified,Not Specified,Not Specified,0.565,0.685,0.722,,,,True,2,0,"['transformers', 'joblib']",2023-03-29 20:42:12+00:00,2023-03-29 20:40:38+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 44994113085
- CO2 Emissions (in grams): 0.5769

## Validation Metrics

- Loss: 0.685
- Accuracy: 0.565
- Precision: 0.565
- Recall: 1.000
- AUC: 0.500
- F1: 0.722

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-03,,0.6339238539238539,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
164680,autotrain-train2-shi-fr-45006113090,['vuilleminethan/autotrain-data-train2-shi-fr'],,6.82281507415207,AutoTrain,Not Specified,Not Specified,Not Specified,,2.591,,,,4918420761.0,True,4,0,"['transformers', 'pytorch']",2023-03-29 21:03:42+00:00,2023-03-29 20:46:46+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 45006113090
- CO2 Emissions (in grams): 6.8228

## Validation Metrics

- Loss: 2.591
- SacreBLEU: 4.904
- Gen len: 4.249",,,1,[],[],NLP,2023-03,720878509.463523,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
164809,autotrain-dialogsum4-45048113162,['xiaoql/autotrain-data-dialogsum4'],,10.8554870545861,AutoTrain,Not Specified,Not Specified,Not Specified,,0.959,,0.82819,0.82525,1625537293.0,True,3,0,"['transformers', 'pytorch']",2023-03-30 02:09:24+00:00,2023-03-30 01:40:53+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 45048113162
- CO2 Emissions (in grams): 10.8555

## Validation Metrics

- Loss: 0.959
- Rouge1: 82.819
- Rouge2: 65.516
- RougeL: 82.525
- RougeLsum: 82.825
- Gen Len: 131.487

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/xiaoql/autotrain-dialogsum4-45048113162
```",,,1,[],[],NLP,2023-03,149743377.22721174,0.8267173861766983,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
164896,bert2,['madmancity/autotrain-data-bert1'],,0.3063901452083847,AutoTrain,Not Specified,Not Specified,Not Specified,0.825,0.527,0.815,,,438010997.0,True,36,1,"['transformers', 'pytorch']",2023-03-30 04:01:08+00:00,2023-03-30 03:48:00+00:00,"
- Problem type: Multi-class Classification
- Model ID: 45066113192
- CO2 Emissions (in grams): 0.3064

## Validation Metrics

- Loss: 0.527
- Accuracy: 0.825
- Macro F1: 0.815
- Micro F1: 0.825
- Weighted F1: 0.816
- Macro Precision: 0.844
- Micro Precision: 0.825
- Weighted Precision: 0.843
- Macro Recall: 0.823
- Micro Recall: 0.825
- Weighted Recall: 0.825


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/madmancity/autotrain-bert1-45066113192
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""madmancity/autotrain-bert1-45066113192"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""madmancity/autotrain-bert1-45066113192"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,1429585787.4348283,0.819969512195122,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
164931,autotrain-ps-pilot-45085113211,['kentao/autotrain-data-ps-pilot'],,0.0026832642071841,AutoTrain,Not Specified,Not Specified,Not Specified,0.829,0.486,0.829,,,737768761.0,True,4,0,"['transformers', 'pytorch']",2023-03-30 04:43:09+00:00,2023-03-30 04:41:37+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 45085113211
- CO2 Emissions (in grams): 0.0027

## Validation Metrics

- Loss: 0.486
- Accuracy: 0.829
- Precision: 0.850
- Recall: 0.810
- AUC: 0.861
- F1: 0.829

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/kentao/autotrain-ps-pilot-45085113211
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""kentao/autotrain-ps-pilot-45085113211"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""kentao/autotrain-ps-pilot-45085113211"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,274951962995.1898,0.829,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
164985,autotrain-further-train-chatdoctor-45099113239,['zhaozh/autotrain-data-further-train-chatdoctor'],,0.6371864100333517,AutoTrain,Not Specified,Not Specified,Not Specified,,1.468,,,,310022533.0,True,8,0,"['transformers', 'pytorch']",2023-03-30 06:15:10+00:00,2023-03-30 06:13:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 45099113239
- CO2 Emissions (in grams): 0.6372

## Validation Metrics

- Loss: 1.468
- SacreBLEU: 27.918
- Gen len: 74.588",,,1,[],[],NLP,2023-03,486549192.0704535,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
165066,autotrain-translator3-45113113262,['Azzizz17/autotrain-data-translator3'],,4.9833884202277225,AutoTrain,Not Specified,Not Specified,Not Specified,,2.009,,,,891616913.0,True,4,0,"['transformers', 'pytorch']",2023-03-30 08:02:10+00:00,2023-03-30 07:49:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 45113113262
- CO2 Emissions (in grams): 4.9834

## Validation Metrics

- Loss: 2.009
- SacreBLEU: 0.618
- Gen len: 16.308",,,1,[],[],NLP,2023-03,178917804.07501456,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,1,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
165090,autotrain-data-45127113281,['vuilleminethan/autotrain-data-data'],,11.772933096301358,AutoTrain,Not Specified,Not Specified,Not Specified,,2.231,,,,4918420761.0,True,3,0,"['transformers', 'pytorch']",2023-03-30 08:52:13+00:00,2023-03-30 08:22:06+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 45127113281
- CO2 Emissions (in grams): 11.7729

## Validation Metrics

- Loss: 2.231
- SacreBLEU: 9.111
- Gen len: 23.073",,,1,[],[],NLP,2023-03,417773610.0908613,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
165091,autotrain-data-45127113285,['vuilleminethan/autotrain-data-data'],,11.879054179738578,AutoTrain,Not Specified,Not Specified,Not Specified,,2.231,,,,4918420761.0,True,8,0,"['transformers', 'pytorch']",2023-03-30 08:52:51+00:00,2023-03-30 08:22:07+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 45127113285
- CO2 Emissions (in grams): 11.8791

## Validation Metrics

- Loss: 2.231
- SacreBLEU: 9.111
- Gen len: 23.073",,,1,[],[],NLP,2023-03,414041445.2683504,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
165132,summ_arp_org,['Hinataaa/autotrain-data-text_summary_arp'],,4.2992847624934365,AutoTrain,Not Specified,Not Specified,Not Specified,,1.285,,0.49529,0.46465,2950848513.0,True,0,1,"['transformers', 'pytorch']",2023-03-30 09:09:14+00:00,2023-03-30 08:59:01+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 45146113307
- CO2 Emissions (in grams): 4.2993

## Validation Metrics

- Loss: 1.285
- Rouge1: 49.529
- Rouge2: 25.404
- RougeL: 46.465
- RougeLsum: 46.645
- Gen Len: 18.803

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Hinataaa/autotrain-text_summary_arp-45146113307
```",,,1,[],[],NLP,2023-03,686358005.113532,0.4794810061045482,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
165197,autotrain-aaaa-45159113325,['Azzizz17/autotrain-data-aaaa'],,9.547886962542258,AutoTrain,Not Specified,Not Specified,Not Specified,,3.047,,,,2329628725.0,True,4,0,"['transformers', 'pytorch']",2023-03-30 10:26:41+00:00,2023-03-30 10:01:36+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 45159113325
- CO2 Emissions (in grams): 9.5479

## Validation Metrics

- Loss: 3.047
- SacreBLEU: 0.540
- Gen len: 13.876",,,1,[],[],NLP,2023-03,243994166.8915301,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
165233,medical_chat-en-zh,['zhaozh/autotrain-data-chatdoctor-reft-en-zh'],,2.240193635056679,AutoTrain,Not Specified,Not Specified,Not Specified,,1.636,,,,310022533.0,True,2,2,"['transformers', 'pytorch']",2023-03-30 10:51:21+00:00,2023-03-30 10:45:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 45173113346
- CO2 Emissions (in grams): 2.2402

## Validation Metrics

- Loss: 1.636
- SacreBLEU: 29.513
- Gen len: 176.613",,,1,[],[],NLP,2023-03,138390953.41959408,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
165284,NLP_summarization_model1,['Abishani/autotrain-data-nlp_text_summarization'],,8.961640862273923,AutoTrain,Not Specified,Not Specified,Not Specified,,2.499,,0.2381899999999999,0.18571,2279610349.0,True,10,0,"['transformers', 'pytorch']",2023-03-30 12:39:22+00:00,2023-03-30 12:16:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 44866113357
- CO2 Emissions (in grams): 8.9616

## Validation Metrics

- Loss: 2.499
- Rouge1: 23.819
- Rouge2: 7.753
- RougeL: 18.571
- RougeLsum: 18.606
- Gen Len: 27.896

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Abishani/autotrain-nlp_text_summarization-44866113357
```",,,1,[],[],NLP,2023-03,254374213.83360064,0.2087014149563576,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
165326,autotrain-nlp-45198113367,['bingcheng45/autotrain-data-nlp'],,1.866801699206036,AutoTrain,Not Specified,Not Specified,Not Specified,0.051,5.278,0.057,,,1335316917.0,True,10,0,"['transformers', 'pytorch']",2023-03-30 13:06:01+00:00,2023-03-30 13:01:51+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 45198113367
- CO2 Emissions (in grams): 1.8668

## Validation Metrics

- Loss: 5.278
- Accuracy: 0.051
- Macro F1: 0.057
- Micro F1: 0.051
- Weighted F1: 0.044
- Macro Precision: 0.063
- Micro Precision: 0.051
- Weighted Precision: 0.049
- Macro Recall: 0.069
- Micro Recall: 0.051
- Weighted Recall: 0.051


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/bingcheng45/autotrain-nlp-45198113367
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""bingcheng45/autotrain-nlp-45198113367"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""bingcheng45/autotrain-nlp-45198113367"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,715296604.6516455,0.0538333333333333,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
165699,autotrain-moodify-45307113562,['arigot3/autotrain-data-moodify'],,0.5586165468777098,AutoTrain,Not Specified,Not Specified,Not Specified,,,,,,,True,8,0,"['transformers', 'pytorch']",2023-03-30 23:06:22+00:00,,,,,1,[],[],NLP,,,,0.0,0.0,0.0,0.0,0,0.0,0,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
166239,autotrain-prknsn-2-45473113800,['samvelkoch/autotrain-data-prknsn-2'],,1.051663865934213,AutoTrain,Not Specified,Not Specified,Not Specified,,5.079,,,,,True,1,0,"['transformers', 'joblib']",2023-03-31 14:04:03+00:00,2023-03-31 14:01:14+00:00,"
# Model Trained Using AutoTrain

- Problem type: Single Column Regression
- Model ID: 45473113800
- CO2 Emissions (in grams): 1.0517

## Validation Metrics

- Loss: 5.079
- R2: 0.109
- MSE: 25.795
- MAE: 3.780
- RMSLE: 0.849

## Usage

```python
import json
import joblib
import pandas as pd

model = joblib.load('model.joblib')
config = json.load(open('config.json'))

features = config['features']

# data = pd.read_csv(""data.csv"")
data = data[features]
data.columns = [""feat_"" + str(col) for col in data.columns]

predictions = model.predict(data)  # or model.predict_proba(data)

```",,,1,[],[],Not Specified,2023-03,,,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
166386,autotrain-alpaca-gigo-detector-45529113937,['dvilasuero/autotrain-data-alpaca-gigo-detector'],,0.3078125269826994,AutoTrain,Not Specified,Not Specified,Not Specified,0.825,0.481,0.823,,,737768761.0,True,0,0,"['transformers', 'pytorch']",2023-03-31 17:58:02+00:00,2023-03-31 17:57:19+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 45529113937
- CO2 Emissions (in grams): 0.3078

## Validation Metrics

- Loss: 0.481
- Accuracy: 0.825
- Macro F1: 0.823
- Micro F1: 0.825
- Weighted F1: 0.825
- Macro Precision: 0.824
- Micro Precision: 0.825
- Weighted Precision: 0.825
- Macro Recall: 0.821
- Micro Recall: 0.825
- Weighted Recall: 0.825


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dvilasuero/autotrain-alpaca-gigo-detector-45529113937
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dvilasuero/autotrain-alpaca-gigo-detector-45529113937"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dvilasuero/autotrain-alpaca-gigo-detector-45529113937"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-03,2396812008.3737407,0.823998786407767,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
166457,pegasus-subreddit-comments-summarizer,['stevied67/autotrain-data-pegasus-subreddit-comments-summarizer'],,27.833269754820986,AutoTrain,Not Specified,Not Specified,Not Specified,,1.467,,0.51832,0.40226,2283804653.0,True,9,0,"['transformers', 'pytorch']",2023-03-31 21:26:05+00:00,2023-03-31 20:13:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 45559114001
- CO2 Emissions (in grams): 27.8333

## Validation Metrics

- Loss: 1.467
- Rouge1: 51.832
- Rouge2: 25.213
- RougeL: 40.226
- RougeLsum: 45.554
- Gen Len: 57.035

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/stevied67/autotrain-pegasus-subreddit-comments-summarizer-45559114001
```",,,1,[],[],NLP,2023-03,82053049.21475938,0.4529740016076821,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
166888,synopsize-v1.0,['altafalam3/autotrain-data-synopsize'],,0.0014579314567438,AutoTrain,Not Specified,Not Specified,Not Specified,,0.767,,0.84624,0.84631,557971229.0,True,0,1,"['transformers', 'pytorch']",2023-04-01 11:48:42+00:00,2023-04-01 11:47:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 45723114297
- CO2 Emissions (in grams): 0.0015

## Validation Metrics

- Loss: 0.767
- Rouge1: 84.624
- Rouge2: 57.742
- RougeL: 84.631
- RougeLsum: 84.946
- Gen Len: 6.419

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/altafalam3/autotrain-synopsize-45723114297
```",,,1,[],[],NLP,2023-04,382714308288.6724,0.84627499855248,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
167477,autotrain-song_lyrics-45948114663,['billster45/autotrain-data-song_lyrics'],,0.2944444228317013,AutoTrain,Not Specified,Not Specified,Not Specified,0.708,0.666,0.708,,,737768761.0,True,1,0,"['transformers', 'pytorch']",2023-04-02 10:38:15+00:00,2023-04-02 10:37:30+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 45948114663
- CO2 Emissions (in grams): 0.2944

## Validation Metrics

- Loss: 0.666
- Accuracy: 0.708
- Macro F1: 0.708
- Micro F1: 0.708
- Weighted F1: 0.708
- Macro Precision: 0.708
- Micro Precision: 0.708
- Weighted Precision: 0.708
- Macro Recall: 0.708
- Micro Recall: 0.708
- Weighted Recall: 0.708


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/billster45/autotrain-song_lyrics-45948114663
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""billster45/autotrain-song_lyrics-45948114663"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""billster45/autotrain-song_lyrics-45948114663"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-04,2505629938.2572927,0.708,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
167494,autotrain-imdb-sentiment-45954114684,['billster45/autotrain-data-imdb-sentiment'],,1.6951829788409294,AutoTrain,Not Specified,Not Specified,Not Specified,0.953,0.156,0.954,,,556848625.0,True,0,0,"['transformers', 'pytorch']",2023-04-02 11:20:00+00:00,2023-04-02 11:15:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 45954114684
- CO2 Emissions (in grams): 1.6952

## Validation Metrics

- Loss: 0.156
- Accuracy: 0.953
- Precision: 0.951
- Recall: 0.957
- AUC: 0.989
- F1: 0.954

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/billster45/autotrain-imdb-sentiment-45954114684
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""billster45/autotrain-imdb-sentiment-45954114684"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""billster45/autotrain-imdb-sentiment-45954114684"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-04,328488801.47483647,0.9534997378080754,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
167873,autotrain-cat_dog-46040114726,['billster45/autotrain-data-cat_dog'],,1.094614881827817,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.001,,,,347599761.0,True,17,0,"['transformers', 'pytorch']",2023-04-02 20:57:09+00:00,2023-04-02 20:54:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 46040114726
- CO2 Emissions (in grams): 1.0946

## Validation Metrics

- Loss: 0.001
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000",,,1,[],[],Computer Vision,2023-04,317554389.923485,1.0,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,1,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,0,0.0,1,0.0,0,1,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
168105,autotrain-summ_arp_2-46098114797,['Hinataaa/autotrain-data-summ_arp_2'],,2.584620959475704,AutoTrain,Not Specified,Not Specified,Not Specified,,0.914,,0.5536099999999999,0.47968,2950848513.0,True,0,0,"['transformers', 'pytorch']",2023-04-03 07:14:00+00:00,2023-04-03 07:08:28+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 46098114797
- CO2 Emissions (in grams): 2.5846

## Validation Metrics

- Loss: 0.914
- Rouge1: 55.361
- Rouge2: 27.454
- RougeL: 47.968
- RougeLsum: 47.978
- Gen Len: 13.540

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Hinataaa/autotrain-summ_arp_2-46098114797
```",,,1,[],[],NLP,2023-04,1141694878.7719288,0.5140002222028666,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
168134,alpaca-bad-instruction-detector,['dvilasuero/autotrain-data-alpaca-bs-detector'],,0.4102361717910936,AutoTrain,Not Specified,Not Specified,Not Specified,0.891,0.305,0.887,,,737768761.0,True,0,0,"['transformers', 'pytorch']",2023-04-03 09:23:02+00:00,2023-04-03 07:44:18+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 46079114807
- CO2 Emissions (in grams): 0.4102

## Validation Metrics

- Loss: 0.305
- Accuracy: 0.891
- Macro F1: 0.887
- Micro F1: 0.891
- Weighted F1: 0.891
- Macro Precision: 0.890
- Micro Precision: 0.891
- Weighted Precision: 0.891
- Macro Recall: 0.885
- Micro Recall: 0.891
- Weighted Recall: 0.891


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/dvilasuero/autotrain-alpaca-bs-detector-46079114807
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""dvilasuero/autotrain-alpaca-bs-detector-46079114807"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""dvilasuero/autotrain-alpaca-bs-detector-46079114807"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-04,1798400072.2776277,0.8889955005624298,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
168250,autotrain-evangil-jean-train-46246114854,['vuilleminethan/autotrain-data-evangil-jean-train'],,13.333381258423453,AutoTrain,Not Specified,Not Specified,Not Specified,,1.717,,,,4918420761.0,True,0,0,"['transformers', 'pytorch']",2023-04-03 10:26:21+00:00,2023-04-03 09:51:34+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 46246114854
- CO2 Emissions (in grams): 13.3334

## Validation Metrics

- Loss: 1.717
- SacreBLEU: 21.874
- Gen len: 32.097",,,1,[],[],NLP,2023-04,368880231.1786258,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
168456,autotrain-summ_arp_4-46233114888,['Hinataaa/autotrain-data-summ_arp_4'],,2.0839852645801367,AutoTrain,Not Specified,Not Specified,Not Specified,,0.914,,0.5520499999999999,0.47973,2950848513.0,True,0,0,"['transformers', 'pytorch']",2023-04-03 13:31:37+00:00,2023-04-03 13:26:08+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 46233114888
- CO2 Emissions (in grams): 2.0840

## Validation Metrics

- Loss: 0.914
- Rouge1: 55.205
- Rouge2: 27.752
- RougeL: 47.973
- RougeLsum: 48.231
- Gen Len: 13.540

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Hinataaa/autotrain-summ_arp_4-46233114888
```",,,1,[],[],NLP,2023-04,1415964192.8152075,0.513355456589583,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
168715,autotrain-chat-sum-dialogsum-samsum-46317114985,['Ybhav14/autotrain-data-chat-sum-dialogsum-samsum'],,3.0774487291128,AutoTrain,Not Specified,Not Specified,Not Specified,,1.27,,0.39115,0.30158,1625537293.0,True,0,0,"['transformers', 'pytorch']",2023-04-03 19:01:17+00:00,2023-04-03 18:53:31+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 46317114985
- CO2 Emissions (in grams): 3.0774

## Validation Metrics

- Loss: 1.270
- Rouge1: 39.115
- Rouge2: 17.283
- RougeL: 30.158
- RougeLsum: 34.226
- Gen Len: 61.380

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/Ybhav14/autotrain-chat-sum-dialogsum-samsum-46317114985
```",,,1,[],[],NLP,2023-04,528209382.5389668,0.3405742987888499,0.0,0.0,0.0,0.0,0,0.0,1,1,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,0,0,0,0,0.0,1,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
168751,autotrain-translation-test-for-luc-46356115004,['cityhand2/autotrain-data-translation-test-for-luc'],,13.29206069239237,AutoTrain,Not Specified,Not Specified,Not Specified,,1.939,,,,4918420761.0,True,0,0,"['transformers', 'pytorch']",2023-04-03 20:24:33+00:00,2023-04-03 19:49:54+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 46356115004
- CO2 Emissions (in grams): 13.2921

## Validation Metrics

- Loss: 1.939
- SacreBLEU: 13.975
- Gen len: 34.883",,,1,[],[],NLP,2023-04,370026956.3029477,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,1,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
168959,autotrain-imdb-textclassification-46471115127,['davis901/autotrain-data-imdb-textclassification'],,2.683579313085358,AutoTrain,Not Specified,Not Specified,Not Specified,1.0,0.0,1.0,,,1421587189.0,True,0,0,"['transformers', 'pytorch']",2023-04-04 03:22:52+00:00,2023-04-04 03:15:58+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 46471115127
- CO2 Emissions (in grams): 2.6836

## Validation Metrics

- Loss: 0.000
- Accuracy: 1.000
- Precision: 1.000
- Recall: 1.000
- AUC: 1.000
- F1: 1.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/davis901/autotrain-imdb-textclassification-46471115127
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""davis901/autotrain-imdb-textclassification-46471115127"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""davis901/autotrain-imdb-textclassification-46471115127"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-04,529735485.0174249,1.0,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
168960,roberta-frame-CP,['davis901/autotrain-data-imdb-textclassification'],,3.313265712444502,AutoTrain,Not Specified,Not Specified,Not Specified,0.999,0.006,0.999,,,1421587189.0,True,0,0,"['transformers', 'pytorch']",2023-04-04 04:40:41+00:00,2023-04-04 03:16:27+00:00,"
# Model Trained Using AutoTrain

- Problem type: Binary Classification
- Model ID: 46471115134
- CO2 Emissions (in grams): 3.3133

## Validation Metrics

- Loss: 0.006
- Accuracy: 0.999
- Precision: 0.999
- Recall: 1.000
- AUC: 1.000
- F1: 0.999

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/davis901/autotrain-imdb-textclassification-46471115134
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""davis901/autotrain-imdb-textclassification-46471115134"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""davis901/autotrain-imdb-textclassification-46471115134"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-04,429059216.0056984,0.999,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,1,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
169072,minecraft-modpack-quests-transformer,[''],,1.1513248050762204,Not Specified,Not Specified,Not Specified,Not Specified,,,,,,310022533.0,False,431,1,"['transformers', 'pytorch']",2023-04-04 06:29:40+00:00,2023-04-04 05:27:12+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 46533115159
- CO2 Emissions (in grams): 1.1513

## Validation Metrics

- Loss: 1.798
- SacreBLEU: 20.567
- Gen len: 21.393",,,1,[],[],NLP,2023-04,269274605.7698946,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
169267,postnashville_antitrans_telegram-46622115298,['sefaozalpadl/autotrain-data-postnashville_antitrans_telegram'],,0.4434488215878769,AutoTrain,Not Specified,Not Specified,Not Specified,0.818,0.569,0.707,,,737771833.0,True,0,0,"['transformers', 'pytorch']",2023-04-04 10:50:33+00:00,2023-04-04 10:49:24+00:00,"
# Model Trained Using AutoTrain

- Problem type: Multi-class Classification
- Model ID: 46622115298
- CO2 Emissions (in grams): 0.4434

## Validation Metrics

- Loss: 0.569
- Accuracy: 0.818
- Macro F1: 0.707
- Micro F1: 0.818
- Weighted F1: 0.807
- Macro Precision: 0.777
- Micro Precision: 0.818
- Weighted Precision: 0.814
- Macro Recall: 0.674
- Micro Recall: 0.818
- Weighted Recall: 0.818


## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/sefaozalpadl/autotrain-postnashville_antitrans_telegram-46622115298
```

Or Python API:

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(""sefaozalpadl/autotrain-postnashville_antitrans_telegram-46622115298"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""sefaozalpadl/autotrain-postnashville_antitrans_telegram-46622115298"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-04,1663713594.6335986,0.7584603278688524,0.0,0.0,0.0,0.0,0,0.0,0,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,1,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,1,0,0.0,0,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
169521,autotrain-activity_parameters-46735115465,['WilliamWen/autotrain-data-activity_parameters'],,0.8044039338743204,AutoTrain,Not Specified,Not Specified,Not Specified,0.99,0.034,0.782,,,1336519661.0,True,0,0,"['transformers', 'pytorch']",2023-04-04 16:16:58+00:00,2023-04-04 16:15:15+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 46735115465
- CO2 Emissions (in grams): 0.8044

## Validation Metrics

- Loss: 0.034
- Accuracy: 0.990
- Precision: 0.739
- Recall: 0.829
- F1: 0.782

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/WilliamWen/autotrain-activity_parameters-46735115465
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""WilliamWen/autotrain-activity_parameters-46735115465"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""WilliamWen/autotrain-activity_parameters-46735115465"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-04,1661503138.8059037,0.8737923250564333,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
169602,activity_params_02_the_best,['WilliamWen/autotrain-data-activity_parameters_02'],,0.401256563816351,AutoTrain,Not Specified,Not Specified,Not Specified,0.989,0.04,0.815,,,1336519661.0,True,0,0,"['transformers', 'pytorch']",2023-04-04 17:59:53+00:00,2023-04-04 17:58:47+00:00,"
# Model Trained Using AutoTrain

- Problem type: Entity Extraction
- Model ID: 46761115496
- CO2 Emissions (in grams): 0.4013

## Validation Metrics

- Loss: 0.040
- Accuracy: 0.989
- Precision: 0.894
- Recall: 0.748
- F1: 0.815

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/models/WilliamWen/autotrain-activity_parameters_02-46761115496
```

Or Python API:

```
from transformers import AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained(""WilliamWen/autotrain-activity_parameters_02-46761115496"", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained(""WilliamWen/autotrain-activity_parameters_02-46761115496"", use_auth_token=True)

inputs = tokenizer(""I love AutoTrain"", return_tensors=""pt"")

outputs = model(**inputs)
```",,,1,[],[],NLP,2023-04,3330835633.6613216,0.8936086474501108,0.0,0.0,0.0,0.0,0,0.0,1,0,1,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,0,0,1,0.0,1,0.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
169901,autotrain-pubmed-medft-tiny-46854115566,['zhaozh/autotrain-data-pubmed-medft-tiny'],,0.0157860826194656,AutoTrain,Not Specified,Not Specified,Not Specified,,1.217,,,,310022533.0,True,0,0,"['transformers', 'pytorch']",2023-04-05 02:58:49+00:00,2023-04-05 02:49:29+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 46854115566
- CO2 Emissions (in grams): 0.0158

## Validation Metrics

- Loss: 1.217
- SacreBLEU: 21.719
- Gen len: 141.275",,,1,[],[],NLP,2023-04,19638978236.292484,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
169931,autotrain-pubmed-tiny-46871115574,['zhaozh/autotrain-data-pubmed-tiny'],,3.673900197759129,AutoTrain,Not Specified,Not Specified,Not Specified,,1.241,,,,310022533.0,True,0,0,"['transformers', 'pytorch']",2023-04-05 03:47:31+00:00,2023-04-05 03:37:52+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 46871115574
- CO2 Emissions (in grams): 3.6739

## Validation Metrics

- Loss: 1.241
- SacreBLEU: 20.899
- Gen len: 141.172",,,1,[],[],NLP,2023-04,84385126.5173442,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
169965,autotrain-trans-pubmed-46884115623,['zhaozh/autotrain-data-trans-pubmed'],,5.741513475338592,AutoTrain,Not Specified,Not Specified,Not Specified,,1.724,,,,310022533.0,True,0,0,"['transformers', 'pytorch']",2023-04-05 05:07:49+00:00,2023-04-05 04:52:39+00:00,"
# Model Trained Using AutoTrain

- Problem type: Translation
- Model ID: 46884115623
- CO2 Emissions (in grams): 5.7415

## Validation Metrics

- Loss: 1.724
- SacreBLEU: 21.904
- Gen len: 126.360",,,1,[],[],NLP,2023-04,53996656.862625785,,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,1.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,0,0.0,0,0,0,0,0.0,1,0,0,0.0,1,1.0,1,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
170353,autotrain-t5-billsum-47010115876,['zap8600/autotrain-data-t5-billsum'],,0.0111316645461598,AutoTrain,Not Specified,Not Specified,Not Specified,,2.472,,0.2000199999999999,0.17035,242071641.0,True,0,0,"['transformers', 'pytorch']",2023-04-05 13:46:49+00:00,2023-04-05 13:40:16+00:00,"
# Model Trained Using AutoTrain

- Problem type: Summarization
- Model ID: 47010115876
- CO2 Emissions (in grams): 0.0111

## Validation Metrics

- Loss: 2.472
- Rouge1: 20.002
- Rouge2: 10.000
- RougeL: 17.035
- RougeLsum: 18.427
- Gen Len: 19.000

## Usage

You can use cURL to access this model:

```
$ curl -X POST -H ""Authorization: Bearer YOUR_HUGGINGFACE_API_KEY"" -H ""Content-Type: application/json"" -d '{""inputs"": ""I love AutoTrain""}' https://api-inference.huggingface.co/zap8600/autotrain-t5-billsum-47010115876
```",,,1,[],[],NLP,2023-04,21746221330.70924,0.1839965817965818,0.0,0.0,0.0,0.0,0,0.0,1,0,0,0.0,0.0,0,0.0,1,0,0.0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,0,0.0,0,0.0,0.0,0.0,0,0,0,0.0,0.0,0,0.0,0.0,0.0,0,0,0.0,0.0,0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,1,0,0,0,0.0,0.0,0,0,0,0.0,0.0,0,1,0.0,1,0,0,0,0.0,1,0,0,0.0,1,0.0,0,0,0,0,0,0.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
