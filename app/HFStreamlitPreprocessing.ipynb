{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f288ea97-0a0a-4872-9731-273822e8d7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import Levenshtein\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import ModelSearchArguments, DatasetSearchArguments, ModelCard\n",
    "from huggingface_hub import hf_hub_url, get_hf_file_metadata\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "23aef743-df39-4c3c-ad9e-f300c075a473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39114/175888158.py:2: DtypeWarning: Columns (6,7,8,9,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('datasets/HFTotalProcessed.csv')\n"
     ]
    }
   ],
   "source": [
    "def read_df_processed():\n",
    "    df = pd.read_csv('datasets/HFTotalProcessed.csv')\n",
    "    df = df.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1)\n",
    "    df['library_name'] = df['library_name'].apply(lambda libraries:  ast.literal_eval(libraries) if not isinstance(libraries, list) else libraries)\n",
    "    df['datasets'] = df['datasets'].apply(lambda datasets: [''] if pd.isnull(datasets) else [datasets] if '[' not in datasets else ast.literal_eval(datasets))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = read_df_processed()\n",
    "\n",
    "def read_df_clean():\n",
    "    df = pd.read_csv('datasets/HFClean.csv')\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)\n",
    "    df['library_name'] = df['library_name'].apply(lambda libraries:  ast.literal_eval(libraries) if not isinstance(libraries, list) else libraries)\n",
    "    df['datasets'] = df['datasets'].apply(lambda datasets: [''] if pd.isnull(datasets) else [datasets] if '[' not in datasets else ast.literal_eval(datasets))\n",
    "\n",
    "    return df\n",
    "\n",
    "df_clean = read_df_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "12932bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wanted_columns = ['modelId', 'datasets_size', 'co2_eq_emissions', 'source', 'training_type', 'geographical_location',\n",
    "#                   'downloads', 'likes', 'library_name', 'created_at', 'co2_reported', 'domain',\n",
    "#                   'year_month', 'size', 'size_efficency', 'performance_score']\n",
    "\n",
    "wanted_columns = [col for col in df.columns if not col.startswith('is_')]\n",
    "\n",
    "df = df[wanted_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f1309f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df1 = df\n",
    "df2 = pd.read_csv('datasets/HFClean.csv')\n",
    "\n",
    "# Merge the dataframes\n",
    "merged = df1.merge(df2, on='modelId', how='left', suffixes=('', '_y'))\n",
    "\n",
    "# Replace _x columns (from HFStreamlit.csv) with _y columns (from HFClean.csv) when _y is not null\n",
    "for column in merged.columns:\n",
    "    if '_y' in column:\n",
    "        merged[column.replace('_y', '')] = merged[column].where(merged[column].notnull(), merged[column.replace('_y', '')])\n",
    "\n",
    "# Drop _y columns\n",
    "df = merged[df1.columns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9759120e-32ed-476e-b38c-27ae3cf28a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['domain'] = df['domain'].fillna('Not Specified')\n",
    "df['training_type'] = df['training_type'].fillna('Not Specified')\n",
    "df['source'] = df['source'].fillna('Not Specified')\n",
    "df['geographical_location'] = df['geographical_location'].fillna('Not Specified')\n",
    "df['hardware_used'] = df['hardware_used'].fillna('Not Specified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "860e75d3-897b-4976-a96c-5630f2972901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_sources(source, auto):\n",
    "\n",
    "    if auto:\n",
    "        return 'AutoTrain'\n",
    "    if source == 'code carbon':\n",
    "        return 'Code Carbon'\n",
    "    if 'mlco2' in source or 'ML CO2' in source:\n",
    "        return 'MLCO2'\n",
    "    if 'BLOOM' in source:\n",
    "        return 'Article'\n",
    "    if 'Google Cloud' in source:\n",
    "        return 'Google Cloud Footprint'\n",
    "    \n",
    "    return 'Not Specified'\n",
    "\n",
    "def combine_location(location):\n",
    "\n",
    "    if 'East US' in location:\n",
    "        return 'East US'\n",
    "    if location == 'Frankfurt an Main, Germany (500-600 gCO2eq/kWh)':\n",
    "        return 'Frankfurt an Main, Germany'\n",
    "    return location\n",
    "\n",
    "\n",
    "\n",
    "def combine_training_type(training_type):\n",
    "\n",
    "    if 'fine' in training_type:\n",
    "        return 'fine-tuning'\n",
    "    if 'pre' in training_type:\n",
    "        return 'pretraining'\n",
    "    \n",
    "    return 'Not Specified'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d79d63ec-e072-4efc-abc9-43c0cd253e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['source'] = df.apply(lambda row: combine_sources(row['source'], row['auto']), axis=1)\n",
    "df['geographical_location'] = df['geographical_location'].apply(lambda location: combine_location(location))\n",
    "df['training_type'] = df['training_type'].apply(lambda training_type: combine_training_type(training_type))\n",
    "df['size_efficency'] = df['size'] / df['co2_eq_emissions']\n",
    "df['datasets_size_efficency'] = df['datasets_size'] / df['co2_eq_emissions']\n",
    "df['downloads'] = df['downloads'].astype(int)\n",
    "df['likes'] = df['likes'].astype(int)\n",
    "df['co2_reported'] = df['co2_reported'].astype(int)\n",
    "\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "df = df[df['co2_reported'] == True]\n",
    "\n",
    "\n",
    "# # format the date part\n",
    "# df['created_at'] = df['created_at'].apply(lambda x: f\"{x.month}/{x.day}/{x.year}\")\n",
    "\n",
    "# df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# # get the date part\n",
    "df['created_at'] = df['created_at'].dt.date\n",
    "\n",
    "def create_performance_metrics(row):\n",
    "    return {'accuracy': row['accuracy'], 'f1': row['f1'], 'rouge1': row['rouge1'], 'rouge2': row['rougeL']}\n",
    "\n",
    "df['performance_metrics'] = df.apply(create_performance_metrics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8c2f9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'hardware_used': 'environment'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34180609-5f58-4f3b-9039-fdec22904282",
   "metadata": {},
   "source": [
    "Let's delete unnecessary attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "92691fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_columns = ['modelId', 'datasets', 'datasets_size', 'co2_eq_emissions', 'co2_reported', 'source', 'training_type', 'geographical_location', 'environment', 'performance_metrics', 'performance_score',\n",
    "                  'downloads', 'likes', 'library_name', 'domain', 'size', 'created_at', 'size_efficency', 'datasets_size_efficency']\n",
    "\n",
    "\n",
    "df = df[wanted_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0568bda2-8090-41f6-8c31-44893afbaae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('HFStreamlit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bc5928c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-11 03:38:41+0000\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# get current datetime\n",
    "now = datetime.now(pytz.utc)\n",
    "\n",
    "# format datetime\n",
    "formatted_now = now.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "print(formatted_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5671ea4-5039-4e7e-bf5d-7fdc2ccb5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_df_processed():\n",
    "    df = pd.read_csv('datasets/HFStreamlit.csv')\n",
    "    df['library_name'] = df['library_name'].apply(lambda libraries:  ast.literal_eval(libraries) if not isinstance(libraries, list) else libraries)\n",
    "    # df['performance_metrics'] = df['performance_metrics'].apply(lambda metrics_dict:  ast.literal_eval(metrics_dict) if isinstance(metrics_dict, str) else metrics_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = read_df_processed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
